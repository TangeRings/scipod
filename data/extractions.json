[
  {
    "id": "extraction_1754346864141_eh2xnwfoa",
    "timestamp": "2025-08-04T22:34:24.141Z",
    "files": [
      {
        "name": "story.txt",
        "size": 0,
        "type": "text/plain",
        "extractedText": "[This text file appears to be empty]",
        "extractionStatus": "empty"
      },
      {
        "name": "[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf",
        "size": 2138028,
        "type": "application/pdf",
        "extractedText": "[PDF PLACEHOLDER - [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf]\n\nThis is a placeholder for PDF content extraction. \nThe actual file \"[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\" (2088 KB) was uploaded successfully.\n\nIn a production environment, this would contain the actual extracted text from the PDF using a library like:\n- pdf-parse (for server-side extraction)\n- PDF.js (for client-side extraction)\n- Apache Tika (for enterprise-level extraction)\n\nFile details:\n- Name: [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\n- Size: 2138028 bytes\n- Type: application/pdf\n\nThis placeholder confirms that your file upload and processing pipeline is working correctly!",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== story.txt ===\n[This text file appears to be empty]\n\n==================================================\n\n=== [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf ===\n[PDF PLACEHOLDER - [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf]\n\nThis is a placeholder for PDF content extraction. \nThe actual file \"[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\" (2088 KB) was uploaded successfully.\n\nIn a production environment, this would contain the actual extracted text from the PDF using a library like:\n- pdf-parse (for server-side extraction)\n- PDF.js (for client-side extraction)\n- Apache Tika (for enterprise-level extraction)\n\nFile details:\n- Name: [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\n- Size: 2138028 bytes\n- Type: application/pdf\n\nThis placeholder confirms that your file upload and processing pipeline is working correctly!\n",
    "totalFiles": 2
  },
  {
    "id": "extraction_1754346865798_o0m2l1qu1",
    "timestamp": "2025-08-04T22:34:25.798Z",
    "files": [
      {
        "name": "story.txt",
        "size": 0,
        "type": "text/plain",
        "extractedText": "[This text file appears to be empty]",
        "extractionStatus": "empty"
      },
      {
        "name": "[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf",
        "size": 2138028,
        "type": "application/pdf",
        "extractedText": "[PDF PLACEHOLDER - [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf]\n\nThis is a placeholder for PDF content extraction. \nThe actual file \"[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\" (2088 KB) was uploaded successfully.\n\nIn a production environment, this would contain the actual extracted text from the PDF using a library like:\n- pdf-parse (for server-side extraction)\n- PDF.js (for client-side extraction)\n- Apache Tika (for enterprise-level extraction)\n\nFile details:\n- Name: [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\n- Size: 2138028 bytes\n- Type: application/pdf\n\nThis placeholder confirms that your file upload and processing pipeline is working correctly!",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== story.txt ===\n[This text file appears to be empty]\n\n==================================================\n\n=== [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf ===\n[PDF PLACEHOLDER - [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf]\n\nThis is a placeholder for PDF content extraction. \nThe actual file \"[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\" (2088 KB) was uploaded successfully.\n\nIn a production environment, this would contain the actual extracted text from the PDF using a library like:\n- pdf-parse (for server-side extraction)\n- PDF.js (for client-side extraction)\n- Apache Tika (for enterprise-level extraction)\n\nFile details:\n- Name: [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\n- Size: 2138028 bytes\n- Type: application/pdf\n\nThis placeholder confirms that your file upload and processing pipeline is working correctly!\n",
    "totalFiles": 2
  },
  {
    "id": "extraction_1754346927948_16w5t5dmx",
    "timestamp": "2025-08-04T22:35:27.948Z",
    "files": [
      {
        "name": "story.txt",
        "size": 0,
        "type": "text/plain",
        "extractedText": "[This text file appears to be empty]",
        "extractionStatus": "empty"
      },
      {
        "name": "[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf",
        "size": 2138028,
        "type": "application/pdf",
        "extractedText": "[PDF PLACEHOLDER - [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf]\n\nThis is a placeholder for PDF content extraction. \nThe actual file \"[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\" (2088 KB) was uploaded successfully.\n\nIn a production environment, this would contain the actual extracted text from the PDF using a library like:\n- pdf-parse (for server-side extraction)\n- PDF.js (for client-side extraction)\n- Apache Tika (for enterprise-level extraction)\n\nFile details:\n- Name: [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\n- Size: 2138028 bytes\n- Type: application/pdf\n\nThis placeholder confirms that your file upload and processing pipeline is working correctly!",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== story.txt ===\n[This text file appears to be empty]\n\n==================================================\n\n=== [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf ===\n[PDF PLACEHOLDER - [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf]\n\nThis is a placeholder for PDF content extraction. \nThe actual file \"[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\" (2088 KB) was uploaded successfully.\n\nIn a production environment, this would contain the actual extracted text from the PDF using a library like:\n- pdf-parse (for server-side extraction)\n- PDF.js (for client-side extraction)\n- Apache Tika (for enterprise-level extraction)\n\nFile details:\n- Name: [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\n- Size: 2138028 bytes\n- Type: application/pdf\n\nThis placeholder confirms that your file upload and processing pipeline is working correctly!\n",
    "totalFiles": 2
  },
  {
    "id": "extraction_1754346956571_gcbog9tmi",
    "timestamp": "2025-08-04T22:35:56.571Z",
    "files": [
      {
        "name": "story.txt",
        "size": 0,
        "type": "text/plain",
        "extractedText": "[This text file appears to be empty]",
        "extractionStatus": "empty"
      },
      {
        "name": "[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf",
        "size": 2138028,
        "type": "application/pdf",
        "extractedText": "[PDF PLACEHOLDER - [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf]\n\nThis is a placeholder for PDF content extraction. \nThe actual file \"[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\" (2088 KB) was uploaded successfully.\n\nIn a production environment, this would contain the actual extracted text from the PDF using a library like:\n- pdf-parse (for server-side extraction)\n- PDF.js (for client-side extraction)\n- Apache Tika (for enterprise-level extraction)\n\nFile details:\n- Name: [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\n- Size: 2138028 bytes\n- Type: application/pdf\n\nThis placeholder confirms that your file upload and processing pipeline is working correctly!",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== story.txt ===\n[This text file appears to be empty]\n\n==================================================\n\n=== [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf ===\n[PDF PLACEHOLDER - [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf]\n\nThis is a placeholder for PDF content extraction. \nThe actual file \"[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\" (2088 KB) was uploaded successfully.\n\nIn a production environment, this would contain the actual extracted text from the PDF using a library like:\n- pdf-parse (for server-side extraction)\n- PDF.js (for client-side extraction)\n- Apache Tika (for enterprise-level extraction)\n\nFile details:\n- Name: [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\n- Size: 2138028 bytes\n- Type: application/pdf\n\nThis placeholder confirms that your file upload and processing pipeline is working correctly!\n",
    "totalFiles": 2
  },
  {
    "id": "extraction_1754347070580_eecanrwxj",
    "timestamp": "2025-08-04T22:37:50.580Z",
    "files": [
      {
        "name": "story.txt",
        "size": 0,
        "type": "text/plain",
        "extractedText": "[This text file appears to be empty]",
        "extractionStatus": "empty"
      },
      {
        "name": "[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf",
        "size": 2138028,
        "type": "application/pdf",
        "extractedText": "[PDF PLACEHOLDER - [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf]\n\nThis is a placeholder for PDF content extraction. \nThe actual file \"[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\" (2088 KB) was uploaded successfully.\n\nIn a production environment, this would contain the actual extracted text from the PDF using a library like:\n- pdf-parse (for server-side extraction)\n- PDF.js (for client-side extraction)\n- Apache Tika (for enterprise-level extraction)\n\nFile details:\n- Name: [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\n- Size: 2138028 bytes\n- Type: application/pdf\n\nThis placeholder confirms that your file upload and processing pipeline is working correctly!",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== story.txt ===\n[This text file appears to be empty]\n\n==================================================\n\n=== [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf ===\n[PDF PLACEHOLDER - [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf]\n\nThis is a placeholder for PDF content extraction. \nThe actual file \"[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\" (2088 KB) was uploaded successfully.\n\nIn a production environment, this would contain the actual extracted text from the PDF using a library like:\n- pdf-parse (for server-side extraction)\n- PDF.js (for client-side extraction)\n- Apache Tika (for enterprise-level extraction)\n\nFile details:\n- Name: [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\n- Size: 2138028 bytes\n- Type: application/pdf\n\nThis placeholder confirms that your file upload and processing pipeline is working correctly!\n",
    "totalFiles": 2
  },
  {
    "id": "extraction_1754347080040_brs29tkr0",
    "timestamp": "2025-08-04T22:38:00.041Z",
    "files": [
      {
        "name": "story.txt",
        "size": 0,
        "type": "text/plain",
        "extractedText": "[This text file appears to be empty]",
        "extractionStatus": "empty"
      },
      {
        "name": "[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf",
        "size": 2138028,
        "type": "application/pdf",
        "extractedText": "[PDF PLACEHOLDER - [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf]\n\nThis is a placeholder for PDF content extraction. \nThe actual file \"[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\" (2088 KB) was uploaded successfully.\n\nIn a production environment, this would contain the actual extracted text from the PDF using a library like:\n- pdf-parse (for server-side extraction)\n- PDF.js (for client-side extraction)\n- Apache Tika (for enterprise-level extraction)\n\nFile details:\n- Name: [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\n- Size: 2138028 bytes\n- Type: application/pdf\n\nThis placeholder confirms that your file upload and processing pipeline is working correctly!",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== story.txt ===\n[This text file appears to be empty]\n\n==================================================\n\n=== [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf ===\n[PDF PLACEHOLDER - [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf]\n\nThis is a placeholder for PDF content extraction. \nThe actual file \"[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\" (2088 KB) was uploaded successfully.\n\nIn a production environment, this would contain the actual extracted text from the PDF using a library like:\n- pdf-parse (for server-side extraction)\n- PDF.js (for client-side extraction)\n- Apache Tika (for enterprise-level extraction)\n\nFile details:\n- Name: [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\n- Size: 2138028 bytes\n- Type: application/pdf\n\nThis placeholder confirms that your file upload and processing pipeline is working correctly!\n",
    "totalFiles": 2
  },
  {
    "id": "extraction_1754347290225_hyiyyxe5t",
    "timestamp": "2025-08-04T22:41:30.226Z",
    "files": [
      {
        "name": "story.txt",
        "size": 0,
        "type": "text/plain",
        "extractedText": "[EMPTY TXT FILE - story.txt]\n\nThis .txt file appears to be completely empty (0 bytes).\nPlease check that:\n1. The file contains text content\n2. The file was properly uploaded\n3. The file encoding is supported (UTF-8)\n\nFile details:\n- Name: story.txt\n- Reported size: 0 bytes\n- Actual content length: 0 characters",
        "extractionStatus": "empty"
      },
      {
        "name": "[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf",
        "size": 2138028,
        "type": "application/pdf",
        "extractedText": "[PDF EXTRACTION ERROR - [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf]\n\nFailed to extract text from this PDF file.\nError: ENOENT: no such file or directory, open 'D:\\VS Projects\\scipod\\test\\data\\05-versions-space.pdf'\n\nThis could happen if:\n- The PDF file is corrupted\n- The PDF uses unsupported features\n- The PDF is password protected\n- The file is not actually a valid PDF\n\nFile details:\n- Name: [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\n- Size: 2088 KB\n- Error details: ENOENT: no such file or directory, open 'D:\\VS Projects\\scipod\\test\\data\\05-versions-space.pdf'",
        "extractionStatus": "error"
      }
    ],
    "combinedText": "=== story.txt ===\n[EMPTY TXT FILE - story.txt]\n\nThis .txt file appears to be completely empty (0 bytes).\nPlease check that:\n1. The file contains text content\n2. The file was properly uploaded\n3. The file encoding is supported (UTF-8)\n\nFile details:\n- Name: story.txt\n- Reported size: 0 bytes\n- Actual content length: 0 characters\n\n==================================================\n\n=== [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf ===\n[PDF EXTRACTION ERROR - [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf]\n\nFailed to extract text from this PDF file.\nError: ENOENT: no such file or directory, open 'D:\\VS Projects\\scipod\\test\\data\\05-versions-space.pdf'\n\nThis could happen if:\n- The PDF file is corrupted\n- The PDF uses unsupported features\n- The PDF is password protected\n- The file is not actually a valid PDF\n\nFile details:\n- Name: [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\n- Size: 2088 KB\n- Error details: ENOENT: no such file or directory, open 'D:\\VS Projects\\scipod\\test\\data\\05-versions-space.pdf'\n",
    "totalFiles": 2
  },
  {
    "id": "extraction_1754347587611_jb4a4mcb1",
    "timestamp": "2025-08-04T22:46:27.611Z",
    "files": [
      {
        "name": "story.txt",
        "size": 0,
        "type": "text/plain",
        "extractedText": "[EMPTY TXT FILE - story.txt]\n\nThis .txt file appears to be completely empty (0 bytes).\nPlease check that:\n1. The file contains text content\n2. The file was properly uploaded\n3. The file encoding is supported (UTF-8)\n\nFile details:\n- Name: story.txt\n- Reported size: 0 bytes\n- Actual content length: 0 characters",
        "extractionStatus": "empty"
      },
      {
        "name": "[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf",
        "size": 2138028,
        "type": "application/pdf",
        "extractedText": "[PDF FILE PROCESSED - [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf]\n\n✅ PDF SUCCESSFULLY UPLOADED AND VERIFIED ✅\n\nFile Analysis:\n- File Name: [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\n- File Size: 2088 KB (2,138,028 bytes)\n- PDF Version: 1.4\n- File Type: Valid PDF document\n- Upload Status: SUCCESS\n\nBuffer Information:\n- Buffer Size: 2,138,028 bytes\n- PDF Header: %PDF- ✓\n- File Structure: Valid PDF format detected\n\n📋 CONTENT SIMULATION:\nThis is where the actual extracted text from your research paper would appear. \nYour PDF \"[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\" has been successfully received and processed.\n\nThe file contains research content about \"Constructive Activity and Learning in Collaborative Small Groups\" \nwhich would typically include:\n- Abstract and introduction\n- Methodology and research approach  \n- Results and findings\n- Discussion and conclusions\n- References and citations\n\n🔧 TECHNICAL NOTE:\nPDF text extraction temporarily disabled due to library compatibility issues with Next.js.\nIn production, this would use libraries like:\n- pdf-parse (Node.js server-side)\n- PDF.js (client-side processing)\n- Adobe PDF Extract API (cloud-based)\n\nYour file upload and processing pipeline is working perfectly! ✅",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== story.txt ===\n[EMPTY TXT FILE - story.txt]\n\nThis .txt file appears to be completely empty (0 bytes).\nPlease check that:\n1. The file contains text content\n2. The file was properly uploaded\n3. The file encoding is supported (UTF-8)\n\nFile details:\n- Name: story.txt\n- Reported size: 0 bytes\n- Actual content length: 0 characters\n\n==================================================\n\n=== [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf ===\n[PDF FILE PROCESSED - [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf]\n\n✅ PDF SUCCESSFULLY UPLOADED AND VERIFIED ✅\n\nFile Analysis:\n- File Name: [1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\n- File Size: 2088 KB (2,138,028 bytes)\n- PDF Version: 1.4\n- File Type: Valid PDF document\n- Upload Status: SUCCESS\n\nBuffer Information:\n- Buffer Size: 2,138,028 bytes\n- PDF Header: %PDF- ✓\n- File Structure: Valid PDF format detected\n\n📋 CONTENT SIMULATION:\nThis is where the actual extracted text from your research paper would appear. \nYour PDF \"[1995.00] Constructive Activity and Learning in Collaborative Small Groups.pdf\" has been successfully received and processed.\n\nThe file contains research content about \"Constructive Activity and Learning in Collaborative Small Groups\" \nwhich would typically include:\n- Abstract and introduction\n- Methodology and research approach  \n- Results and findings\n- Discussion and conclusions\n- References and citations\n\n🔧 TECHNICAL NOTE:\nPDF text extraction temporarily disabled due to library compatibility issues with Next.js.\nIn production, this would use libraries like:\n- pdf-parse (Node.js server-side)\n- PDF.js (client-side processing)\n- Adobe PDF Extract API (cloud-based)\n\nYour file upload and processing pipeline is working perfectly! ✅\n",
    "totalFiles": 2
  },
  {
    "id": "extraction_1754347857302_0hkya56dj",
    "timestamp": "2025-08-04T22:50:57.302Z",
    "files": [
      {
        "name": "story.txt",
        "size": 730,
        "type": "text/plain",
        "extractedText": "I remember the moment that sparked this project. It was during a walk across Middlebury’s campus late one fall afternoon—just after office hours. A student had come in, clearly bright but struggling, and told me they’d been using ChatGPT late at night to help with assignments because they were too anxious to speak up in class. That confession stuck with me.\r\n\r\nLater, over coffee with Germán, we started wondering—how many students were turning to generative AI as a quiet lifeline? Not to cheat, but to learn? That simple question became the seed for this entire study. What we found wasn’t just statistics. It was a story of adaptation, of students reshaping how they learn, and of institutions racing to catch up.\"",
        "extractionStatus": "success"
      },
      {
        "name": "Generative AI in Higher Education E.txt",
        "size": 117470,
        "type": "text/plain",
        "extractedText": "Generative AI in Higher Education: Evidence from an Elite College†\r\nZara Contractor\r\nGermán Reyes\r\nAbstract\r\nGenerative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT’s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI’s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI’s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\r\n\r\n1Introduction\r\nThe rapid advancement of generative artificial intelligence (AI) is transforming higher education at an unprecedented pace. The launch of ChatGPT and similar tools has introduced technologies capable of performing tasks central to academic assessment and learning—writing essays, solving complex problems, and explaining intricate concepts—instantly and at near-zero marginal cost. Yet systematic evidence on the nature and implications of this new technology remains scarce. How widespread is generative AI adoption among students, and what factors drive it? Do students primarily use AI to augment their learning or to automate coursework, potentially harming human capital development? Could disparities in access to premium AI resources amplify existing educational inequalities?\r\n\r\nThese questions are at the heart of ongoing debates about the role of AI in education, which have relied heavily on anecdotal evidence and speculation rather than rigorous evidence. As a result, universities have implemented policies that vary dramatically, from outright bans to permissive adoption, often without clear evidence on their effectiveness or unintended consequences (Nolan, , 2023; Xiao et al., , 2023; McDonald et al., , 2025).\r\n\r\nThis paper addresses this gap by systematically examining generative AI adoption at Middlebury College, a highly selective liberal arts college in Vermont. With approximately 2,800 undergraduate students, Middlebury offers 49 majors across the arts, humanities, languages, social sciences, and natural sciences. Over the past decade, Middlebury has consistently ranked among the top 10 liberal arts colleges in U.S. News & World Report, with an average ranking of 8.5 between 2015 and 2025. The college’s selectivity is comparable to that of many elite research universities—Middlebury’s acceptance rate of approximately 10 percent is similar to that of Boston University (11 percent), Georgetown University (13 percent), and the University of Virginia (16 percent).\r\n\r\nOur analysis draws on survey data collected from the student population between December 2024 and February 2025. The survey collected detailed information about AI usage, including the frequency and purpose of use, perceived impacts, and responses to institutional policies. To minimize selection bias, we framed the survey broadly as examining technology use and provided incentives for participation, achieving a 22.9 percent response rate with 634 responses representing a broad cross-section of the student population.\r\n\r\nWe document five main findings. First, generative AI is approaching near-universal adoption at an unprecedented speed. Over 80 percent of students use AI for academic purposes, up from less than 10 percent before Spring 2023. This represents one of the fastest technology adoption episodes ever documented, dramatically exceeding the 40 percent adoption rate among U.S. workers (Bick et al., , 2025) and the 23 percent among all U.S. adults (McClain, , 2024). These levels are consistent with international findings of 50–70 percent adoption in university contexts (Nam, , 2023; Stöhr et al., , 2024; Ravšelj et al., , 2025).\r\n\r\nSecond, AI adoption is markedly unequal across academic disciplines and demographic groups. Field of study is the strongest predictor of adoption, likely reflecting how well AI capabilities align with the academic tasks required across fields. Adoption ranges from 91.1 percent in Natural Sciences majors (including mathematics and computer science) to significantly lower levels in Literature (48.6 percent) and Languages (57.4 percent). Additionally, demographic disparities exist. For example, males adopt AI at higher rates than females (88.7 versus 78.4 percent), a pattern that is consistent with documented AI gender gaps in other contexts (Otis et al., , 2024). Most notably, lower-achieving students have a higher adoption rate than their higher-achieving peers (87.1 versus 80.3 percent). As a result, AI could serve as an equalizing force if it enhances student learning—helping struggling students catch up to their peers—but could also widen achievement gaps if AI undermines skill development.\r\n\r\nThird, generative AI transforms the students’ learning production function by both augmenting student capabilities and automating academic tasks. We classify academic tasks as augmentation when AI enhances human capabilities while maintaining student engagement (e.g., explaining concepts, proofreading) versus automation when AI directly produces outputs with minimal cognitive involvement (e.g., writing essays, creating images), and average usage rates across tasks within each category. We find that 61.2 percent of AI users employ these tool for augmentation purposes, while 41.9 percent use it for automation. Qualitative evidence reinforces these patterns: students describe AI as an “on-demand tutor” for augmentation purposes, particularly valuable when traditional resources like office hours are unavailable, while automation use centers on time savings during periods of overwhelming workload. These self-reported patterns align closely with actual usage data from Claude conversation logs (Handa et al., 2025a,).\r\n\r\nFourth, institutional policies can significantly influence AI adoption, though their effectiveness can be undermined by informational frictions. We find that explicit prohibitions dramatically reduce self-reported intended use 39 percentage points (pp). Importantly, the effects of this policy vary substantially across student groups, with females showing larger reductions in usage under prohibition than males, suggesting that one-size-fits-all policies may inadvertently create disparate impacts across groups. Moreover, we also document substantial informational gaps about institutional AI policies. Many students (19.2 percent) do not understand AI policy rules, few know about institution-provided premium AI resources (10.1 percent) or proper AI citation practices (32.6 percent)—a skill that is necessary for academic integrity.\r\n\r\nFifth, students believe that AI has a positive learning impact and these beliefs strongly predict their own AI usage. Most students believe that AI improves their understanding of course materials (70.2 percent) and learning ability (60.1 percent), though fewer believe it improves grades (41.1 percent). Perhaps unsurprisingly, students who believe that AI improves their academic outcomes are significantly more likely to use these tools. For example, a ten-percentage-point increase in the belief that AI improves learning corresponds to a 4.9 percentage-point increase in adoption, highlighting how perceptions can affect technology diffusion in educational settings.\r\n\r\nTaken together, our findings reveal that generative AI has already reshaped students’ college experience. The substantial information gaps that we document—regarding both permitted uses and available resources—suggest straightforward opportunities for policy intervention. Yet the observed heterogeneity across disciplines and student groups indicates that uniform policies risk unintended consequences: blanket prohibitions may disproportionately disadvantage students who benefit most from AI augmentation, while unrestricted use may encourage automation practices detrimental to skill development. Thus, effective policy requires distinguishing clearly between AI uses that enhance learning and those that undermine it.\r\n\r\nWhile our findings are specific to the population we study—students at an elite liberal arts college—this setting offers valuable into AI’s broader societal impact. The rapid uptake of generative AI in higher education may reflect a generalizable principle regarding the diffusion of generative AI: adoption may be fastest in contexts where AI consolidates fragmented tools into a unified platform. Students have long had access to various tools for the academic tasks for which they now use generative AI.1 What distinguishes AI is its ability to provide all these services through a single, instantly accessible interface at virtually zero marginal cost. This consolidation likely drives students’ perception of enhanced learning efficiency and contributes to the fast adoption. This principle may extend well beyond education: industries with fragmented, specialized tools could experience similar trajectories of rapid AI adoption, even if AI itself does not introduce fundamentally new capabilities.\r\n\r\nOur findings contribute to a rapidly growing literature examining the adoption and impacts of generative AI. Recent work has focused on AI’s effects on worker productivity (Noy and Zhang, , 2023; Dell’Acqua et al., , 2023; Peng et al., , 2023; Cui et al., , 2024; Brynjolfsson et al., , 2025) and its potential to transform occupations (Felten et al., , 2021, 2023; Eloundou et al., , 2024). While several papers document AI adoption in workplace settings (Bick et al., , 2025; Hartley et al., , 2025; Humlum and Vestergaard, , 2025) and at the firm level (McElheran et al., , 2023; Bonney et al., , 2024; Kharazian, , 2025), we examine adoption in higher education—a critical setting where future high-skilled workers develop human capital.\r\n\r\nWe add to an emerging literature examining generative AI in education. Some studies focus on the learning impacts of generative AI, finding mixed impacts depending on the setting (e.g., Lehmann et al., , 2024; Bastani et al., , 2025). A complementary strand of the literature documents adoption patterns across diverse educational contexts, including universities in Australia (Kelly et al., , 2023), Ghana (Bonsu and Baffour-Koduah, , 2023), Norway (Carvajal et al., , 2024), Sweden (Stöhr et al., , 2024), and multi-country studies (Ravšelj et al., , 2025). To the best of our knowledge, this paper provides the first systematic evidence on generative AI adoption in U.S. higher education. Importantly, we focus specifically on how students use AI for academic purposes—unlike studies that measure more general usage including personal lives (e.g., Ravšelj et al., , 2025)—and go beyond documenting adoption rates to examine how students integrate AI into their academic workflows, distinguishing between augmentation and automation, the role of student beliefs about AI’s educational impacts, and how institutional policies shape usage patterns.\r\n\r\nOur findings also contribute to the literature on technology diffusion. Unlike the S-shaped adoption curves documented for most technologies (Griliches, , 1957; Rogers, , 1962), we observe extremely rapid adoption of generative AI, with 80 percent uptake within two years of ChatGPT’s release. This pattern contrasts sharply with the adoption trajectories of historical General Purpose Technologies (GPTs) like electricity, which required decades to achieve widespread use (David, , 1990; Bresnahan and Trajtenberg, , 1995). The rapid diffusion we document aligns with evidence that newer technologies diffuse faster than older ones (Comin and Hobijn, 2010a,; Comin and Hobijn, 2010b,). Two factors likely explain AI’s exceptional adoption speed compared to other GPTs. First, unlike electricity or steam power, generative AI requires minimal physical infrastructure—students access it through existing devices—and is available at no cost to consumers, eliminating the financial barriers that typically slow technology adoption. Second, as noted above, AI consolidates multiple specialized tools into a single platform, making its benefits immediately apparent without requiring specialized training or organizational restructuring, unlike previous GPTs (Brynjolfsson and Hitt, , 2000).\r\n\r\n2Data: Novel Student Survey\r\n2.1Recruitment and Structure\r\nWe conducted the survey from December 2024 to February 2025. All Middlebury College students were contacted via email and received a reminder a few weeks and two months after the initial invitation. To minimize selection bias, the recruitment materials described the survey broadly as a study on students’ use of technology in their academic and personal lives. To incentivize participation, students who completed the survey qualified for entry into a lottery for Amazon gift cards ranging in value from $50 to $500. The full survey instrument is provided in Appendix LABEL:app:survey.\r\n\r\nThe survey contains three main sections (see Appendix Figure A1 for the survey flow). First, we gather demographic and academic information, including gender, race/ethnicity, type of high school attended (private or public), current academic year, and declared or intended major.2 We also collect data on academic inputs and performance through students’ self-reported typical weekly hours spent studying and their first-year GPA.3\r\n\r\nSecond, we measure students’ experience with generative AI tools. We begin by asking whether students have ever used generative AI tools like ChatGPT or Claude. For those who have, we collect information about their usage patterns, including frequency of use during the academic semester, which specific AI models they use, and whether they pay for AI tools. We also gather data on how students use AI for different academic tasks, including writing assistance, learning support, and coding.\r\n\r\nThe final part of the survey elicits students’ beliefs about generative AI’s adoption and impacts. We ask students about their perceptions of AI’s effects on their academic experience across multiple dimensions, including learning, grades, time management, and understanding of course material. We then collect information on the role of institutional policies, asking students how different policy environments influence their likelihood of using AI. We also elicit students’ beliefs about AI usage among their peers, including their estimates of the fraction of Middlebury students who use AI for schoolwork and leisure. The survey concludes with two open-ended questions that allow students to share their experiences with AI in academic settings and provide feedback on the college’s AI policies and support services.\r\n\r\n2.2Sample and Summary Statistics\r\nOut of Middlebury’s 2,760 enrolled students, 739 began the survey. We exclude 105 respondents who left the survey before reaching the generative AI usage module, leaving us with an analysis sample of 634 students (22.9 percent of the student body). This response rate is comparable to that of similar surveys (Wu et al., , 2022). To make our sample more representative of Middlebury’s student body, we construct poststratification weights based on the distribution of declared majors from administrative records.4 Specifically, we weight observations by the ratio of each major’s share in the student population to its share in our survey responses.5\r\n\r\nTable Figures and Tables presents summary statistics for our sample. Column 1 reports unweighted survey averages, column 2 shows averages after applying poststratification weights, and column 3 provides population benchmarks from administrative records where available. Our unweighted sample consists of 44.6 percent male and 50.8 percent females. The racial/ethnic composition includes 61.8 percent white, 15.5 percent Asian, 9.9 percent Hispanic, and 3.6 percent Black students. The majority of students (54.3 percent) attended a public school, while 42.0 percent attended a private high school. Our sample represents 43 different majors across seven fields of study, with 31.1 percent of respondents not yet having declared a major. In our analysis, we group these undeclared students by their intended field of study as reported in the survey.\r\n\r\nComparing our unweighted sample to administrative records reveals notable differences. Our sample overrepresents white students (61.8 versus 53.8 percent) and Asian students (15.5 versus 7.3 percent), while underrepresenting Black students (3.6 versus 5.2 percent) and Hispanic students (9.9 versus 12.4 percent). First-year students are overrepresented (31.1 versus 25.5 percent), while senior students are underrepresented (21.3 versus 28.7 percent). Our weighting procedure partially addresses these discrepancies—particularly in the distribution of academic fields, where the weighted figures closely approximate administrative records—but some demographic differences persist. Despite these differences, our weighted sample provides reasonably close approximations to the college population on most dimensions, particularly for academic characteristics and field distributions.\r\n\r\n3Generative AI Usage Patterns Among Students\r\n3.1Adoption of Generative AI\r\nGenerative AI has achieved a remarkably high adoption rate among Middlebury College students. Figure 1 presents the distribution of AI usage frequency during the academic semester, categorized into four levels: “Rarely” (a few times per semester), “Occasionally” (a few times per month), “Frequently” (a few times per week), and “Very Frequently” (daily or almost daily). Overall, 82.5 percent of students report using generative AI for academic purposes, with substantial variation in usage intensity: 23.5 percent use it rarely, 22.2 percent occasionally, 26.2 percent frequently, and 10.6 percent very frequently.\r\n\r\nThe adoption rate at Middlebury aligns closely with patterns observed across other higher education institutions. A BestColleges survey of U.S. undergraduate and graduate students in fall 2023 found that 56 percent had used AI on assignments or exams (Nam, , 2023). Ravšelj et al., (2025) surveyed higher education students globally between late 2023 and early 2024, finding that 71.4 percent had ever used ChatGPT—though this figure encompasses all usage rather than academic use specifically. Similarly, Stöhr et al., (2024) document that 63 percent of Swedish university students had used ChatGPT by spring 2023, with 35.4 percent reporting regular use and 27.6 percent rare use. Among Norwegian university students, Carvajal et al., (2024) find that 68.9 percent use AI tools occasionally or more frequently.\r\n\r\nThese adoption rates in higher education far exceed those in the general population and workforce. Pew Research finds that only 23 percent of U.S. adults have ever used ChatGPT (McClain, , 2024), while Gallup reports that just one-third of U.S. workers have used AI for work (Gallup, , 2024). Bick et al., (2025) and Hartley et al., (2025) estimate that about 40 percent of the U.S. working-age population used generative AI for work as of late 2024 and early 2025. Both studies document substantial heterogeneity across industries, with information services showing the highest adoption rates at 56–62 percent—still considerably below the 80 percent adoption rate at Middlebury. Even among workers in AI-exposed occupations, Humlum and Vestergaard, (2025) find adoption rates of only 41 percent.6\r\n\r\n3.2Adoption by Student Characteristics and Field of Study\r\nAI adoption varies considerably across demographic groups and academic disciplines (Figure 1 and Appendix Table LABEL:tab:ai_freq_het). Males report higher usage rates than females (88.7 versus 78.4 percent). Usage patterns differ markedly by race/ethnicity: Black students (92.3 percent) and Asian students (91.3 percent) exhibit the highest adoption rates, while white students (80.2 percent) and Hispanic students (77.9 percent) report lower usage. Students from private high schools use AI more frequently than those from public schools (84.1 versus 80.4 percent). Notably, students with below-median GPAs report higher usage rates than their higher-achieving peers (87.1 versus 80.3 percent). Adoption varies widely by field of study: Natural Sciences leads with 91.1 percent usage, followed by Social Sciences at 84.6 percent, while Languages (57.4 percent) and Literature (48.6 percent) show substantially lower adoption rates.\r\n\r\nTo examine how student characteristics jointly relate to AI adoption, we estimate multivariate OLS regressions that include all observed characteristics simultaneously.7 Table LABEL:tab:ai_usage_correlates presents regression estimates using four usage thresholds as outcomes. Each column represents the probability of meeting progressively higher frequency thresholds: any AI use (column 1), at least monthly use (column 2), at least weekly use (column 3), and daily use (column 4).\r\n\r\nThe regression results confirm the patterns observed in the descriptive statistics. Holding other characteristics constant, males are 10.3 pp more likely than females to use AI (column 1, \r\np\r\n<\r\n0.05\r\n), with this gender gap typically widening at higher usage frequencies (columns 2–4, all \r\np\r\n<\r\n0.01\r\n). Black and Asian students show substantially higher adoption rates than white students, at 11.8 and 10.7 pp respectively (column 1, both \r\np\r\n<\r\n0.01\r\n). Students from public high schools are 3.0 pp less likely to use AI than those from private schools, but this difference is not statistically significant. Field of study emerges as the strongest predictor of adoption. Compared to Natural Sciences majors, students in Literature, Languages, Arts, and Humanities all show lower usage rates, with the differences being statistically significant at high usage frequencies (columns 3 and 4). Social Sciences majors exhibit adoption rates similar in magnitude to Natural Sciences majors across all usage frequency thresholds.\r\n\r\nOur findings on heterogeneity in AI adoption align with patterns documented in other settings. The gender gap in AI adoption at Middlebury—10.3 pp higher for males—is consistent with evidence across multiple studies. A meta-analysis of 18 studies by Otis et al., (2024) finds that males are 10–20 percentage points more likely to use generative AI than females. This gender gap in AI adoption appears in all educational studies (Nam, , 2023; Carvajal et al., , 2024; Stöhr et al., , 2024; Ravšelj et al., , 2025). Our finding that students with below-median GPAs are more likely to use AI aligns with Carvajal et al., (2024), who document higher adoption rates among students with lower admission grades. This pattern of greater adoption among lower-achieving students suggests that AI could narrow achievement gaps if it enhances learning and skill development, but could widen these gaps if it undermines the acquisition of fundamental skills.\r\n\r\nThe differences in adoption rates across academic fields at Middlebury mirror patterns documented in other educational settings. Stöhr et al., (2024) find that technology and engineering students exhibit significantly higher ChatGPT usage compared to students in humanities. Similarly, Nam, (2023) report that 62 percent of business majors and 59 percent of STEM majors have used AI tools for coursework, compared to 52 percent of humanities majors. Ravšelj et al., (2025) document comparable disciplinary differences, with applied sciences students showing substantially higher usage rates than arts and humanities students.\r\n\r\nThese academic differences persist into the workforce. Bick et al., (2025) find stark variation by college major: STEM graduates have the highest AI adoption rates (46.0 percent), followed by Business/Economics graduates (40.0 percent), while Liberal Arts graduates show substantially lower rates (22.4 percent). Humlum and Vestergaard, (2025) document similar patterns by occupation, finding that roles requiring strong writing and technical skills—such as marketing specialists and journalists—have the highest adoption rates. These consistent patterns across educational and professional contexts suggest that field-specific factors, particularly the applicability of AI tools to different types of tasks, shape adoption in systematic ways.\r\n\r\n3.3Timing of Generative AI Adoption\r\nThe speed of technology diffusion is a critical determinant of its economic and social impact (David, , 1990; Hall and Khan, , 2003; Stokey, , 2021). In educational contexts, rapid technology adoption can create or exacerbate inequalities between early and late adopters, particularly if the technology confers significant learning advantages (World Bank, , 2016). To track the timing of AI adoption among Middlebury students, we asked them when they first began using generative AI for academic purposes, with options ranging from “This semester (Fall 2024)” to “Before Spring 2023” (as a reference, ChatGPT’s public launch was in November 2022).\r\n\r\nStudents adopted generative AI at an extraordinary pace. Figure 2 shows that the cumulative adoption rate grew dramatically from less than 10 percent before Spring 2023 to slightly above 80 percent by Fall 2024. The pace of adoption has accelerated over time, likely reflecting improvements in AI capabilities. Among current users, 25.7 percent adopted AI in Fall 2024 alone, compared to 19.9 percent in Spring 2024 and 16.3 percent in Fall 2023.\r\n\r\nThe adoption rate among Middlebury students far exceeds that observed in other populations and represents one of the fastest technology adoption episodes ever documented. For comparison, Bick et al., (2025) show that it took over 20 years for computers to reach an 80 percent adoption rate in the U.S. working-age population, and about 15 years for internet adoption to reach similar levels. Even generative AI adoption in the broader population has been markedly slower: Pew Research found that just 23 percent of U.S. adults had ever used ChatGPT as of February 2024, up from 18 percent in July 2023 (McClain, , 2024). The dramatically faster adoption rate we document—reaching over 80 percent in less than two years—suggests academic settings may uniquely accelerate AI diffusion.\r\n\r\nTo identify early versus late adopters of generative AI, in Table LABEL:tab:ai_adopt_correlates and Appendix Figure A2, we analyze how adoption timing varies across student characteristics. The rate of adoption varies substantially across student characteristics. Male students led adoption, with a 8.9 pp higher probability of using AI before Spring 2023 compared to females (column 1, \r\np\r\n<\r\n0.01\r\n)—a gender gap that persists across all periods (columns 2–5). Black and Asian students also adopted earlier than white students, though these differences reach statistical significance only in later periods (columns 4–5). Field of study is a strong predictor of adoption timing. For example, students in Languages consistently lagged behind Natural and Social Sciences majors, with significantly lower adoption rates across nearly all time periods.\r\n\r\n3.4Choice of Generative AI Models\r\nMajor AI companies operate on a freemium model, where free versions coexist with premium subscriptions that offer higher usage limits and access to more advanced models. This tiered structure creates potential for a new form of educational disparities: if paid versions confer substantial academic advantages, students who cannot afford subscriptions may be systematically disadvantaged. To investigate these potential disparities, we presented respondents with a list of options including both free and paid versions of popular models. We also collected information on monthly subscription expenditures, with response options ranging from no active subscription to spending more than $40 monthly (standard subscriptions to major models like ChatGPT cost $20 per month at the time).\r\n\r\nOpenAI’s ChatGPT dominates AI usage among Middlebury AI users, with the free version capturing the largest market share. Figure 3 shows that 89.3 percent of AI users rely on the free version of ChatGPT, making it dramatically more popular than any alternative. Google Gemini (13.5 percent) and Microsoft Copilot (7.7 percent) are distant competitors, while other platforms each capture less than 5 percent of users. This dominance mirrors patterns in other academic settings, where Stöhr et al., (2024) find substantially higher familiarity with ChatGPT compared to alternative platforms, and aligns with broader workforce trends documented by Bick et al., (2025).8\r\n\r\nDespite near-universal AI adoption, only 11.3 percent of AI users pay for any AI service.9 This figure is remarkably similar to the 8 percent found in Ravšelj et al., (2025)’s multi-country survey. This low payment rate suggests that for most students, the premium features of paid versions—primarily higher usage limits and access to more advanced models—do not justify the subscription cost. However, payment patterns reveal significant disparities: males and Asian students are substantially more likely to purchase AI subscriptions (Appendix Table LABEL:tab:ai_model_correlates and Appendix Figure A3), potentially reflecting differences in usage intensity across demographic groups.\r\n\r\n4Generative AI and the Production of Learning\r\n4.1The Use of Generative AI across Academic Tasks\r\nHow is generative AI transforming the traditional inputs to student learning? The educational production function includes inputs like time spent studying (e.g., Stinebrickner and Stinebrickner, , 2008), faculty instruction (e.g., Fairlie et al., , 2014), peer interactions (e.g., Sacerdote, , 2001), and academic support services (e.g., Angrist et al., , 2009). AI tools have the potential to complement or substitute for these traditional inputs. For example, using AI to explain concepts might substitute for faculty office hours, while using it for proofreading might reduce time needed for academic support services.\r\n\r\nTo understand the role of AI in students’ learning production function, we collected information about students’ AI usage across ten common academic tasks: proofreading, generating ideas, writing essays, editing essay drafts, coding assistance, creating images, explaining concepts, composing emails, summarizing materials, and finding information. For each task, students indicated their frequency of AI use on a five-point scale ranging from never to daily. We supplemented this quantitative data with open-ended responses about how AI influences their academic work process.\r\n\r\nStudents use generative AI for a wide range of academic tasks, with the highest adoption rates for learning support and text-processing activities. Figure 4, Panel A shows that explaining concepts is the most common use case, with 80.3 percent of AI users using it for this task. Summarizing texts follows as the second most common task (74.0 percent), followed by finding information and generating ideas (63.1 and 61.9 percent). Writing assistance tasks like proofreading and editing essays are also common, used by 54.1 and 47.3 percent of AI users, respectively. Technical applications like coding help are significant (34.4 percent), considering that many academic degrees involve no programming. Notably, while 23.5 percent of AI users report using it for writing essays, this represents a relatively low adoption rate compared to other academic uses, suggesting students may be more hesitant to use AI for primary content creation. The lowest adoption rate is for creating images at 20.4 percent, likely reflecting fewer academic use cases for this capability.\r\n\r\nAn important limitation of our survey is that it relies on self-reported usage, which may introduce measurement error. For example, students might underreport uses they perceive as academically inappropriate—such as essay writing—while overreporting those viewed as legitimate learning tools (Ling and Imas, , 2025). To assess the validity of our self-reported measures, we compare our findings with Handa et al., 2025a, who analyze actual Claude usage patterns among users with university email addresses. Several caveats apply: their data captures conversation-level interactions rather than student-level usage, and most students in our sample use ChatGPT rather than Claude (Section 3.4). Despite these limitations, the comparison provides a useful benchmark for evaluating our survey responses.\r\n\r\nReassuringly, our results are consistent with Anthropic’s data. Both studies identify explaining concepts and technical problem-solving as primary use cases. Handa et al., 2025a report that the second largest use case (33.5 percent of conversations) involves “technical explanations or solutions for academic assignments,” while we find that 80.3 percent of AI users use it for explaining concepts—a difference likely attributable to our student-level versus their conversation-level measurement. Similarly, the most common usage category in Claude involves “designing practice questions, editing essays, or summarizing academic material” (39.3 percent of conversations), aligning with our findings that 74.0 percent of AI users use it for summarizing texts and 47.3 percent for editing essays. The disciplinary patterns also converge: Handa et al., 2025a finds that computer science, natural sciences, and mathematics conversations are overrepresented, which mirrors our finding that Natural Science majors show significantly higher AI adoption rates (Figure 1). Overall, these convergent findings from self-reported survey data and actual usage logs suggest that our results capture genuine patterns of student AI engagement rather than merely reflecting social desirability in responses.\r\n\r\n4.2Automation versus Augmentation\r\nAre students using generative AI primarily to augment their learning or to automate their coursework? This distinction is crucial for understanding AI’s impacts (Autor and Thompson, , 2025)—augmentation may enhance students’ learning processes while maintaining their active engagement and critical thinking, whereas automation produces fully-formed outputs that could be submitted with minimal student input, potentially harming learning.\r\n\r\nTo examine this empirically, Figure 4, Panel B categorizes the ten measured tasks based on whether they augment or automate academic tasks. We classify tasks as augmentation when they enhance human capabilities (explaining concepts, finding information, proofreading, and editing drafts) and as automation when they directly produce outputs (writing essays, creating images, composing emails, summarizing texts, generating ideas, and coding assistance). We then calculate the percentage of AI users who employ each category at various frequencies.10\r\n\r\nStudents use generative AI for both augmentation and automation, but with markedly different frequency. While 61.2 percent of AI users report using AI for augmentation tasks, 41.9 percent report using it for automation—thus, there is a substantial 19.3 pp difference. This gap is mainly driven by occasional use (19.7 percent for augmentation versus 12.7 percent for automation) and frequent use (16.3 versus 9.0 percent). Most strikingly, students are more than twice as likely to use AI daily for augmentation (5.0 percent) compared to automation (2.5 percent). The higher frequency of augmentation use also suggests that students find augmentation uses more valuable for their day-to-day academic activities. Our findings align closely with patterns observed in actual AI usage data: Handa et al., 2025b analyze real conversation logs from Claude and find that 57 percent of workplace AI interactions involve augmentation while 43 percent involve automation.\r\n\r\nQualitative evidence from open-ended responses provides additional insights into students’ motivations for using AI (see Appendix C for additional results and validation of the open-ended measure). Students’ descriptions align closely with the augmentation-automation framework. For augmentation, many characterize AI as an “on-demand tutor,” particularly valuable when traditional resources like office hours are unavailable. Non-native English speakers frequently mention using AI for proofreading to overcome language barriers, while students from technical majors describe using it to debug code and understand error messages. For automation, time savings emerged as the dominant motivation, with 21.7 percent of open-ended responses explicitly mentioning efficiency benefits (Appendix Figure C3). Students describe turning to AI during periods of overwhelming workload or looming deadlines, using it to generate initial drafts or complete routine assignments. That students automate tasks mostly when time-pressed helps explain why automation is less common than augmentation.11\r\n\r\nOur finding that students favor augmentation over automation extends beyond Middlebury. In Appendix Figure A4, we re-analyze data from Ravšelj et al., (2025)—a multi-country survey of higher education students—to examine how this balance varies across institutional quality.12 We classify universities into quintiles based on their Times Higher Education World University Rankings. Overall, students who use AI worldwide show similar adoption rates for augmentation (64.6 percent) and automation (63.1 percent) tasks—a much smaller gap than at Middlebury (Panel A). However, this aggregate pattern masks heterogeneity by institutional quality. Students at top-quintile universities show a modest preference for augmentation over automation, with this gap narrowing monotonically down the institution quality distribution. By the bottom quintile, augmentation and automation usage are virtually identical (Panel B).\r\n\r\n4.3Heterogeneity in Augmentation versus Automation Usage\r\nUnderstanding whether augmentation and automation patterns vary across student populations may help to design targeted support policies. To assess this, we construct four measures of augmentation and automation usage. First, we create binary indicators for whether students use AI for any augmentation or automation task. Second, we calculate the proportion of tasks in each category for which students employ AI. Third, using the Likert-scale responses, we create intensity measures that capture how frequently students use AI for augmentation and automation purposes. Finally, we compute the difference between augmentation and automation variables, as a measure that directly compare students’ relative preference for augmentation versus automation. Table LABEL:tab:ai_autom_regs presents regression estimates using these measures as outcomes.\r\n\r\nThe balance between augmentation and automation varies substantially by student characteristics. Males show higher adoption of both augmentation and automation tasks, with a similar magnitude in each case—they are 6.0 pp more likely to use augmentation prompts and 5.5 pp more likely to use automation prompts compared to females (columns 1 and 4, \r\np\r\n<\r\n0.05\r\n). Black students have a substantially higher augmentation intensity than white students (0.871 points higher, equivalent to 38.1 percent of the outcome mean, \r\np\r\n<\r\n0.01\r\n), but show no statistically significant differences in automation usage. This results in Black students having the strongest preference for augmentation over automation, with a 19.5 pp higher share of augmentation tasks relative to automation (column 7, \r\np\r\n<\r\n0.01\r\n). Asian students show high usage of both kinds relative to white students. Differences across field of study effects are sizable: humanities, languages, and literature majors tend to show lower usage of both augmentation and automation tools compared to natural science majors.\r\n\r\n5Institutional Policies and AI Adoption\r\n5.1The Role of Institutional Policies in Shaping Student Behavior\r\nInstitutional policies are central to shaping the adoption and diffusion of new technologies (Acemoglu, , 2025). In the context of generative AI in higher education, understanding how policies affect student behavior is essential for guiding evidence-based decision-making. To examine this, we asked students to report their likelihood of using AI under various policy scenarios, ranging from complete prohibition to unrestricted use. This analysis is particularly relevant in light of the ongoing debate about how universities should regulate generative AI use in academic settings (Nolan, , 2023), and the widespread variation in institutional policies across colleges (Xiao et al., , 2023).\r\n\r\nInstitutional policies substantially influence students’ reported likelihood of using generative AI. Figure 5 shows that when generative AI use is unrestricted, 52.4 percent of students report being likely or extremely likely to use it. This likelihood decreases modestly when policies require citation (40.9 percent) or when no explicit policy exists (42.3 percent). However, explicit prohibition creates a dramatic shift: only 13.4 percent of students report they would be likely or extremely likely to use AI when it is banned, while 72.9 percent say they would be unlikely or extremely unlikely to do so (note that these figures include both current AI users and students who do not use generative AI for academic purposes). These results suggest that institutional policies can significantly influence generative AI usage patterns, though a small fraction of students report they would likely use AI even when explicitly prohibited.\r\n\r\nThe magnitude of policy effects in our study aligns closely with findings from other contexts. Carvajal et al., (2024) estimate that banning AI reduces usage by 37.2 pp among females and 20.6 pp among males, for an overall drop of about 28.9 pp. In our survey, a ban leads to a 37.8 pp decline in usage, with a larger decrease among females (49.6 pp) than males (40.1 pp). These parallel results underscore how institutional policies can unintentionally produce disparate effects across gender. Notably, these differential policy effects extend beyond gender. Other demographic and academic characteristics—such as race and field of study—also moderate how students respond to policy restrictions (Appendix Table LABEL:tab:ai_policy_het), underscoring that institutional policies can produce non-neutral impacts along multiple dimensions.\r\n\r\n5.2Understanding of Institutional Policies and Resources\r\nInformation gaps and inattention can significantly affect technology adoption decisions (Duflo et al., , 2011; Hanna et al., , 2014). For example, imperfect information about rules, available resources, or proper usage guidelines could lead to underadoption of beneficial technologies or inadvertent policy violations. To test for the existence of information gaps, we examine three dimensions of policy understanding. First, we asked students whether they find AI policies in their current classes clear. Second, we measured awareness of free access to premium AI tools through the college—a resource that could reduce inequality but only if students know it exists. Third, we assessed whether students know how to properly cite AI when required, a mechanical skill necessary for academic integrity. We supplemented these measures with open-ended feedback about the college’s AI policies and support services.\r\n\r\nStudent understanding of institutional policies is high, though significant gaps remain. Figure 6, Panel A shows that most students (79.1 percent) understand when and where they are allowed to use AI in their classes, but a nontrivial minority (19.2 percent) find AI policies unclear. Moreover, critical knowledge gaps persist elsewhere. Only 10.1 percent know they have free access to Microsoft Copilot through the college (Panel B), and just 32.6 percent understand how to properly cite AI use (Panel C). These gaps vary systematically: females show better policy understanding than males (81.5 versus 75.8 percent), and non-white students demonstrate higher awareness across all three dimensions compared to white students.13\r\n\r\nThe qualitative evidence from open-ended responses reinforces these information frictions and reveals implementation challenges (Appendix D). Students express frustration with vague guidelines, requesting specific examples of acceptable versus unacceptable use cases. Many advocate for formal training, noting that simply knowing policies exist differs from understanding how to effectively integrate AI into their workflow. A particularly striking theme is students’ perception of blanket prohibitions as both ineffective and unfair, as they create a prisoner’s dilemma situation where compliant students are disadvantaged relative to those who secretly violate restrictions. Many responses call for a balanced approach—permitting AI use that supports learning while restricting uses that replace it—though the boundary between the two remains contested.\r\n\r\n6Beliefs About AI’s Educational Impact and Peer Usage\r\n6.1Student Beliefs of AI’s Impact on Educational Outcomes\r\nTechnology adoption decisions are shaped by beliefs about potential returns (Foster and Rosenzweig, , 2010). Students’ perceptions of how AI affects their learning may influence whether they adopt these tools and how they integrate them into their academic workflows. To elicit these beliefs, we asked students to evaluate AI’s impact across four dimensions of their academic performance: understanding of course materials, overall learning ability, time management, and course grades. For each dimension, students rated AI’s effect on a five-point scale ranging from “significantly reduces” to “significantly improves.” These subjective assessments provide insight into the perceived value of AI tools from the student perspective, which may differ from their actual effects on learning outcomes.\r\n\r\nStudents tend to believe that generative AI is beneficial for their academic performance, though the perceived benefits vary across dimensions. Figure 7 shows that the majority of students (70.2 percent) believe that generative AI improves their understanding of course materials, and 60.1 percent report improvement in their ability to grasp concepts, retain information, or learn new skills. Similarly, 59.4 percent report that AI improves their ability to complete assignments on time. Notably, while students believe AI helps their learning and assignment completion, they are less confident about its impact on course grades—41.1 percent believe it improves their grades, while 55.4 percent report no effect and 3.5 percent report negative effects. This pattern suggests that while students perceive generative AI as improving their learning process and workflow—through better understanding, skill development, and timely completion of work—these benefits do not necessarily translate into better course grades.14\r\n\r\nBeliefs about AI’s benefits strongly predict adoption. In Figure 8, we plot the relationship between the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis) for different subgroups of students (e.g., males, white students, public-school students, etc.). Across all four measured academic dimensions, there is a strong and statistically significant relationship between perceiving positive AI effects and AI adoption. The relationship is strongest for beliefs about the ability to improve course grades (Panel D): a 10-pp increase in the belief that AI improves grades is associated with a 5.6 pp increase in AI adoption (\r\np\r\n<\r\n0.01\r\n). Similar positive relationships exist for beliefs about learning ability (4.3 pp, \r\np\r\n<\r\n0.05\r\n), understanding of course materials (4.5 pp, \r\np\r\n<\r\n0.01\r\n), and timely assignment completion (5.3 pp, \r\np\r\n<\r\n0.05\r\n). These findings suggest that student beliefs about AI’s academic benefits—irrespective of the actual benefits—may play a crucial role in shaping adoption decisions.\r\n\r\nThese positive beliefs align with findings from other contexts. Ravšelj et al., (2025) report that the majority of students in their sample believe that ChatGPT improves their general knowledge (68.8 percent) and specific knowledge (62.7 percent)—remarkably similar to our finding that most students believe AI improves understanding of course materials (68.5 percent) and learning ability (57.6 percent). Similarly, 57.4 percent of students in Ravšelj et al., (2025)’s sample believe that ChatGPT helps meet assignment deadlines, while 59.4 percent in our data report AI improves timely assignment completion. Stöhr et al., (2024) provide complementary evidence from Swedish universities, where 47.7 percent of students believe that AI makes them more effective learners, yet only 17.3 percent believe these tools improve their grades—mirroring our finding that perceived learning benefits from AI use exceed perceived effects on course performance.\r\n\r\n6.2Student Beliefs About Peer Use of Generative AI\r\nStudents’ beliefs about their peers’ AI usage may influence their own adoption through multiple channels, including social norms (e.g., Giaccherini et al., , 2019), social learning (e.g., Foster and Rosenzweig, , 1995; Beaman et al., , 2021), peer effects (e.g., Bailey et al., , 2022), and competitive pressure to avoid falling behind (e.g., Goehring et al., , 2024). To measure these beliefs, we asked students to estimate what fraction of their peers use generative AI for different purposes and would use under different policy environments. Figure 9 presents the distribution of these beliefs. Panels A-C show students’ estimates of peer AI usage for schoolwork, leisure, and any purpose respectively. Panels D-F display students’ beliefs about AI usage under three policy environments: when classes have no explicit AI policy, when classes allow AI use, and when classes prohibit AI use.\r\n\r\nStudents systematically underestimate their peers’ AI usage. On average, they believe 65.2 percent of peers regularly use generative AI for schoolwork (Panel A), while our survey reveals an actual usage rate of 82.5 percent—a 17.3 pp gap. This underestimation appears consistent across educational contexts: Stöhr et al., (2024) find that only 38.7 percent of Swedish students believe AI chatbot use is common among peers, despite 63 percent actual usage. When we examine beliefs about policy-contingent behavior, students estimate that 62.7 percent of peers use AI when no explicit policy exists (Panel D), rising to 72.2 percent under permissive policies (Panel E) and falling to 43.5 percent under prohibition (Panel F). These beliefs align directionally with self-reported intentions, though with notable magnitude differences (Appendix Figure A5). For instance, while students believe 43.5 percent of their peers use AI in classes that prohibit it, only 28.4 percent report they themselves would be likely to do so.15\r\n\r\nNotably, students’ perceptions of peer AI adoption are closely linked to their own adoption behavior. Appendix Figure A6 shows a strong positive association between beliefs about peer usage and actual usage rates across student groups. This behavior is consistent with several psychological mechanisms, including the “false consensus effect” (Ross et al., , 1977), selection neglect (Jehiel, , 2018), or interpersonal projection bias (Bushong and Gagnon-Bartsch, , 2024). Still, virtually all groups systematically underestimate peer usage—actual usage rates exceed believed usage for every demographic and academic group we examine. This figure also shows that underestimation varies substantially across student characteristics: males use AI at 88.7 percent but believe only 66.6 percent of peers do so (22.1 pp gap), while females show a smaller gap (77.6 percent actual versus 65.1 percent believed, 13.3 pp gap).\r\n\r\nTaken together, students’ systematic underestimation of peer AI usage coupled with the strong relationship between beliefs and adoption suggests that misperceptions about social norms may shape technology diffusion. If AI enhances learning, then correcting these misperceptions through information provision could accelerate beneficial adoption; conversely, if AI undermines skill development, then students’ underestimation of peer usage may serve as an unintentional safeguard against harmful overadoption.\r\n\r\n7Discussion\r\nThis paper presents the first systematic evidence on generative AI adoption at a highly selective U.S. college. Using novel survey data, we document exceptionally rapid and widespread adoption, substantial shifts in the educational production function through augmentation and automation, and the significant roles of students’ beliefs and institutional policies in shaping AI use.\r\n\r\nOur results offer three implications for institutional policy and ongoing debates about AI in education. First, we identify low-cost opportunities to improve institutional policy effectiveness through targeted information provision. The significant gaps we document in students’ understanding of institutional AI policies, citation practices, and available AI resources suggest that simple interventions—such as clear guidelines, illustrative examples of acceptable uses, and AI literacy programs—can reduce unintentional academic integrity violations and support beneficial AI integration. Qualitative feedback strongly indicates student demand for more explicit guidance on responsible AI use.\r\n\r\nSecond, our evidence challenges alarmist narratives that conflate widespread AI adoption with universal academic dishonesty based on anecdotal accounts.16 Although AI use is indeed near-universal, we find clear evidence that students primarily employ AI as a tool for strategic task management: to enhance learning (augmentation) and selectively automate tasks when facing high time opportunity cost—not solely to circumvent academic effort. This distinction matters: by normalizing academic dishonesty as inevitable and universal, these narratives may shift social norms and encourage students who would otherwise use AI responsibly to engage in prohibited behaviors, believing “everyone else is doing it.”\r\n\r\nThird, our findings caution against policy extremes of either blanket prohibition or unrestricted AI use (Merchant, , 2024; McDonald et al., , 2025). Blanket prohibitions disproportionately harm students who benefit most from AI’s augmentation functions—particularly lower-achieving students—while also creating uneven compliance, placing conscientious students at a disadvantage relative to rule-breakers. Conversely, unrestricted AI use based solely on revealed preference arguments ignores important market failures in educational settings. Students often hold overly optimistic beliefs about AI’s learning benefits despite mixed empirical evidence, potentially leading to unintended negative learning outcomes. Most concerningly, permissive policies risk creating competitive dynamics where students feel compelled to adopt AI not for its learning benefits but simply to avoid falling behind in an educational “arms race” (Goehring et al., , 2024).\r\n\r\nFigures and Tables\r\nFigure 1:The Adoption of Generative AI among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the fraction of students who report using AI during the academic semester, categorized by demographic characteristics, high school type, academic cohort, GPA, and field of study. Usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily).\r\n\r\nThe category “All students” provides the baseline usage rate for the full sample. Gender categories are based on self-identification, with non-binary responses excluded due to a small sample size. “Private HS” refers to students who attended private high schools, while “Public HS” includes public institutions. “Cohort” denotes the student’s academic year, ranging from first-year (“Freshman”) to fourth-year and beyond (“Senior”). GPA categories (“GPA \r\n>\r\n p50” and “GPA \r\n<\r\n p50”) split students into groups above or below the median first-year GPA, as self-reported on a 4.0 scale. See Appendix B.1 for the classification of majors into fields of study.\r\n\r\nFigure 2:The Evolution of Generative AI Adoption among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the cumulative percent of students who reported using generative AI tools for academic purposes over time. The data is based on retrospective self-reports collected in our December 2024 survey, where students were asked “When did you first start using any form of Generative AI for academic purposes?” Response options ranged from “Before Spring 2023” to “This semester (Fall 2024).” The \r\nx\r\n-axis represents academic semesters, while the \r\ny\r\naxis represents the cumulative adoption rate. Vertical lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure 3:Adoption of Generative AI Models Among College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the adoption rates of various AI models as of Fall 2024. The horizontal axis shows the percent of students who reported using each tool, and the vertical axis lists the tools in descending order of adoption rates.\r\n\r\nFigure 4:Academic Uses of Generative AI\r\nPanel A. Across Common Academic Tasks\r\nRefer to caption\r\nPanel B. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of AI students who use generative AI for different academic tasks. For each task, usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). The number at the end of each bar represents the total percent of students who use AI for that purpose at any frequency. Tasks are ordered by total usage, from highest to lowest. Results are based on responses to the question: “For academic purposes, which of the following tasks do you typically use generative AI for?” Sample includes all students who reported using AI during the academic semester.\r\n\r\nFigure 5:Student Reported Likelihood of Using AI under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of students who report different likelihoods of using AI under various policy scenarios. For each policy, responses are categorized on a five-point scale from “Extremely unlikely to use AI” to “Extremely likely to use AI.” The sample includes all survey respondents. The question asked was: “How likely are you to use generative AI in a class with each of the following AI policies?”\r\n\r\nFigure 6:Understanding of Generative AI Policies and Resources\r\nPanel A. Understanding of AI Policies\r\nRefer to caption\r\nPanel B. Awareness of Copilot Access\r\nRefer to caption\r\nPanel C. Knowledge of Citation Requirements\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows students’ understanding of institutional AI policies and resources. Panel A displays the percent of students who report understanding AI policies in their classes, those who report having no explicit policy, and those who find policies unclear, broken down by demographic characteristics. Panel B shows the percent of students who are aware of their free access to Microsoft Copilot through Middlebury College. Panel C presents the percent of students who report knowing how to properly cite AI use in their academic work when required. For Panels B and C, horizontal lines represent 95 percent confidence intervals. Sample includes all survey respondents.\r\n\r\nFigure 7:Student Beliefs about the Impact of AI on their Academic Performance\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of Middlebury students who believe that AI improves, reduces, or has no effect on different aspects of their academic experience. For each outcome, responses are categorized into three groups: “Improves” combines “significantly improves” and “somewhat improves” responses, “Reduces” combines “significantly reduces” and “somewhat reduces” responses, and “No effect” represents neutral responses. Sample includes all students who report using AI during the academic semester. “Don’t know” responses are excluded.\r\n\r\nFigure 8:Relationship Between AI Adoption and Beliefs About AI’s Academic Benefits\r\nPanel A. Learning ability (e.g., ability to grasp concepts, learn new skills, etc.)\r\nRefer to caption\r\nPanel B. Understanding of\r\ncourse materials\r\nRefer to caption\r\nPanel C. Ability to complete\r\nassignments on time\r\nRefer to caption\r\nPanel D. Course grades\r\n \r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the relationship between AI adoption rates and beliefs that AI improves various academic outcomes across different student groups. Each panel plots the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis). Points represent different student groups categorized by demographics (circles), academic characteristics (triangles), and field of study (squares). The dashed line shows the linear fit across all groups. Groups with fewer than 10 students are excluded.\r\n\r\nFigure 9:Student Beliefs about Generative AI Usage at Middlebury College\r\nPanel A. For Schoolwork\r\nRefer to caption\r\nPanel B. For Leisure\r\nRefer to caption\r\nPanel C. For Any Purpose\r\nRefer to caption\r\nPanel D. Classes with No Policy\r\nRefer to caption\r\nPanel E. Classes that Allow AI\r\nRefer to caption\r\nPanel F. Classes that Disallow AI\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the distribution of students’ beliefs about generative AI usage among their peers at Middlebury College. Panels A-C display students’ estimates of the percent of their peers who regularly use AI for schoolwork, leisure activities, and any purpose, respectively. Panels D-F show students’ beliefs about AI usage in classes with different AI policies: those without an explicit policy (Panel D), those that allow AI use (Panel E), and those that prohibit AI use (Panel F). Each panel shows a histogram with bins of width ten percentage points (e.g., responses between 1-10 fall in the 10 bin, 11-20 in the 20 bin, etc.). The red dashed line indicates the mean response. Sample excludes respondents with missing values or who selected the default response for all six categories (which equals zero).\r\n\r\nTable 1:Summary Statistics of Survey Participants\r\nSurvey Sample\tAdmin records\r\nUnweighted\tWeighted\t\r\n(1)\t(2)\t(3)\r\nPanel A. Demographics\r\nMale\t0.446\t0.433\t0.463\r\nFemale\t0.508\t0.516\t0.533\r\nWhite\t0.618\t0.603\t0.538\r\nBlack\t0.036\t0.033\t0.052\r\nHispanic\t0.099\t0.106\t0.124\r\nAsian\t0.155\t0.162\t0.073\r\nPrivate high school\t0.420\t0.399\t–\r\nPublic high school\t0.543\t0.556\t–\r\nPanel B. Academic Characteristics    GPA 3.740 3.736 3.670 Hours spent on academics per week 17.899 17.889 – Freshman 0.311 0.355 0.255 Sophomore 0.273 0.272 0.257 Junior 0.202 0.179 0.201 Senior 0.213 0.194 0.287   Panel C. Field of Study    Arts 0.011 0.021 0.023 Humanities 0.052 0.068 0.073 Languages 0.021 0.020 0.025 Literature 0.035 0.025 0.025 Natural Sciences 0.218 0.249 0.244 Social Sciences 0.353 0.247 0.243 Has not declared major 0.311 0.371 0.364\r\n\r\nN (# degrees) 43 43 49 N (# students) 634 2,760 2,760\r\n\r\n\\justify\r\nNotes: This table presents summary statistics from our survey of college students. Panel A reports demographic characteristics, including the proportion of participants identifying as male, female, white, Black, Hispanic, Asian, or who attended a private or public high school. Panel B provides academic characteristics, such as GPA (only available for non-freshmen), average weekly hours spent on academics, and academic year distribution (Freshman, Sophomore, Junior, and Senior). Note that in column 1–2, GPA refers to self-reported first-year GPA while in column 3 it is the overall GPA during Spring 2024. Panel C summarizes the distribution of participants across different fields of study. Major groups are mutually exclusive.\r\n\r\n0.8250.5890.3680.106 R\r\n−\r\nsquared0.0770.0890.1160.058 N (Students)616616616616 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI usage frequency threshold and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column uses a different threshold for AI usage frequency, categorized as: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). Column 1 defines usage as any nonzero frequency; column 2 includes at least occasional use; column 3 includes frequent or higher use; and column 4 captures only very frequent use.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.0890.1990.3570.5510.801 R\r\n−\r\nsquared0.0450.0610.0880.0800.069 N (Students)633633633633633 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI adoption date and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column presents results for a different threshold of AI adoption. Column 1 shows the probability of adopting AI before Spring 2023; column 2 by Spring 2023; column 3 by Fall 2023; column 4 by Spring 2024; and column 5 by Fall 2024. The dependent variable in each regression is a binary indicator equal to one if the student had adopted AI by the specified time period.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.9120.6122.2850.9130.4191.8010.1930.484 R\r\n−\r\nsquared0.0360.0730.1260.0530.1170.1290.0420.063 N (Students)515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and their use of generative AI for academic tasks. In columns 1 and 4, the outcome is a dummy that equals one if a student reports using AI with any frequency for at least one augmentation or automation task, respectively. In columns 2 and 5, the outcome is the share of tasks within each category for which the student reports any use. In columns 3 and 6, the outcome is a continuous measure capturing average usage frequency for each task category, based on raw Likert-style responses. In columns 7 and 8, the outcome is the difference in average task share and usage intensity between augmentation and automation, respectively. Regressions are weighted and report robust standard errors clustered at the student level. ∗∗∗ \r\np\r\n<\r\n0.01\r\n, ∗∗ \r\np\r\n<\r\n0.05\r\n, ∗ \r\np\r\n<\r\n0.10\r\n.\r\n\r\nReferences\r\nAcemoglu, (2025)\r\nAcemoglu, D. (2025).Nobel lecture: Institutions, technology, and prosperity.American Economic Review, 115(6):1709–1748.\r\nAngrist et al., (2009)\r\nAngrist, J., Lang, D., and Oreopoulos, P. (2009).Incentives and services for college achievement: Evidence from a randomized trial.American Economic Journal: Applied Economics, 1(1):136–163.\r\nAutor and Thompson, (2025)\r\nAutor, D. and Thompson, N. (2025).Expertise.\r\nBailey et al., (2022)\r\nBailey, M., Johnston, D., Kuchler, T., Stroebel, J., and Wong, A. (2022).Peer effects in product adoption.American Economic Journal: Applied Economics, 14(3):488–526.\r\nBastani et al., (2025)\r\nBastani, H., Bastani, O., Sungu, A., Ge, H., Kabakcı, Ö., and Mariman, R. (2025).Generative AI without guardrails can harm learning: Evidence from high school mathematics.Proceedings of the National Academy of Sciences, 122(26):e2422633122.\r\nBeaman et al., (2021)\r\nBeaman, L., BenYishay, A., Magruder, J., and Mobarak, A. M. (2021).Can network theory-based targeting increase technology adoption?American Economic Review, 111(6):1918–1943.\r\nBick et al., (2025)\r\nBick, A., Blandin, A., and Deming, D. J. (2025).The rapid adoption of generative AI.Technical report, National Bureau of Economic Research.\r\nBonney et al., (2024)\r\nBonney, K., Breaux, C., Buffington, C., Dinlersoz, E., Foster, L. S., Goldschlag, N., Haltiwanger, J. C., Kroff, Z., and Savage, K. (2024).Tracking Firm Use of AI in Real Time: A Snapshot from the Business Trends and Outlook Survey.\r\nBonsu and Baffour-Koduah, (2023)\r\nBonsu, E. M. and Baffour-Koduah, D. (2023).From the consumers’ side: Determining students’ perception and intention to use chatgpt in ghanaian higher education.Journal of Education, Society & Multiculturalism, 4(1):1–29.\r\nBresnahan and Trajtenberg, (1995)\r\nBresnahan, T. F. and Trajtenberg, M. (1995).General purpose technologies ‘engines of growth’?Journal of econometrics, 65(1):83–108.\r\nBrynjolfsson and Hitt, (2000)\r\nBrynjolfsson, E. and Hitt, L. M. (2000).Beyond computation: Information technology, organizational transformation and business performance.Journal of Economic Perspectives, 14(4):23–48.\r\nBrynjolfsson et al., (2025)\r\nBrynjolfsson, E., Li, D., and Raymond, L. (2025).Generative AI at Work.The Quarterly Journal of Economics, 140(2):889–942.\r\nBushong and Gagnon-Bartsch, (2024)\r\nBushong, B. and Gagnon-Bartsch, T. (2024).Failures in forecasting: An experiment on interpersonal projection bias.Management Science, 70(12):8735–8752.\r\nCarvajal et al., (2024)\r\nCarvajal, D., Franco, C., and Isaksson, S. (2024).Will artificial intelligence get in the way of achieving gender equality?Discussion Paper 03, NHH Norwegian School of Economics.\r\n(15)\r\nComin, D. and Hobijn, B. (2010a).An exploration of technology diffusion.American Economic Review, 100(5):2031–2059.\r\n(16)\r\nComin, D. and Hobijn, B. (2010b).Technology Diffusion and Postwar Growth, volume 25, pages 209–246.University of Chicago Press.\r\nCui et al., (2024)\r\nCui, Z. K., Demirer, M., Jaffe, S., Musolff, L., Peng, S., and Salz, T. (2024).The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers.\r\nDavid, (1990)\r\nDavid, P. A. (1990).The dynamo and the computer: an historical perspective on the modern productivity paradox.The American economic review, 80(2):355–361.\r\nDell’Acqua et al., (2023)\r\nDell’Acqua, F., McFowland, E., Mollick, E. R., Lifshitz-Assaf, H., Kellogg, K., Rajendran, S., Krayer, L., Candelon, F., and Lakhani, K. R. (2023).Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality.\r\nDuflo et al., (2011)\r\nDuflo, E., Kremer, M., and Robinson, J. (2011).Nudging Farmers to Use Fertilizer: Theory and Experimental Evidence from Kenya.American Economic Review, 101(6):2350–2390.\r\nEloundou et al., (2024)\r\nEloundou, T., Manning, S., Mishkin, P., and Rock, D. (2024).GPTs are GPTs: Labor market impact potential of LLMs.Science, 384(6702):1306–1308.\r\nFairlie et al., (2014)\r\nFairlie, R. W., Hoffmann, F., and Oreopoulos, P. (2014).A community college instructor like me: Race and ethnicity interactions in the classroom.American Economic Review, 104(8):2567–2591.\r\nFelten et al., (2021)\r\nFelten, E. W., Raj, M., and Seamans, R. (2021).Occupational, Industry, and Geographic Exposure to Artificial Intelligence: A Novel Dataset and Its Potential Uses.\r\nFelten et al., (2023)\r\nFelten, E. W., Raj, M., and Seamans, R. (2023).How will Language Modelers like ChatGPT Affect Occupations and Industries?\r\nFoster and Rosenzweig, (1995)\r\nFoster, A. D. and Rosenzweig, M. R. (1995).Learning by doing and learning from others: Human capital and technical change in agriculture.Journal of political Economy, 103(6):1176–1209.\r\nFoster and Rosenzweig, (2010)\r\nFoster, A. D. and Rosenzweig, M. R. (2010).Microeconomics of technology adoption.Annu. Rev. Econ., 2(1):395–424.\r\nGallup, (2024)\r\nGallup (2024).AI in the Workplace: Answering 3 Big Questions.\r\nGiaccherini et al., (2019)\r\nGiaccherini, M., Herberich, D. H., Jimenez-Gomez, D., List, J. A., Ponti, G., and Price, M. K. (2019).The behavioralist goes door-to-door: Understanding household technological diffusion using a theory-driven natural field experiment.Technical report, National Bureau of Economic Research.\r\nGoehring et al., (2024)\r\nGoehring, G., Mezzanotti, F., and Ravid, S. A. A. (2024).Technology adoption and career concerns: Evidence from the adoption of digital technology in motion pictures.Technical report, National Bureau of Economic Research.\r\nGriliches, (1957)\r\nGriliches, Z. (1957).Hybrid corn: An exploration in the economics of technological change.Econometrica, 25(4):501–522.\r\nHall and Khan, (2003)\r\nHall, B. H. and Khan, B. (2003).Adoption of new technology.\r\n(32)\r\nHanda, K., Bent, D., Tamkin, A., McCain, M., Durmus, E., Stern, M., Schiraldi, M., Huang, S., Ritchie, S., Syverud, S., Jagadish, K., Vo, M., Bell, M., and Ganguli, D. (2025a).Anthropic education report: How university students use claude.\r\n(33)\r\nHanda, K., Tamkin, A., McCain, M., Huang, S., Durmus, E., Heck, S., Mueller, J., Hong, J., Ritchie, S., Belonax, T., et al. (2025b).Which economic tasks are performed with ai? evidence from millions of claude conversations.arXiv preprint arXiv:2503.04761.\r\nHanna et al., (2014)\r\nHanna, R., Mullainathan, S., and Schwartzstein, J. (2014).Learning Through Noticing: Theory and Evidence from a Field Experiment.The Quarterly Journal of Economics, 129(3):1311–1353.\r\nHartley et al., (2025)\r\nHartley, J., Jolevski, F., Melo, V., and Moore, B. (2025).The Labor Market Effects of Generative Artificial Intelligence.\r\nHumlum and Vestergaard, (2025)\r\nHumlum, A. and Vestergaard, E. (2025).Large Language Models, Small Labor Market Effects.\r\nHutto and Gilbert, (2014)\r\nHutto, C. and Gilbert, E. (2014).Vader: A parsimonious rule-based model for sentiment analysis of social media text.In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216–225.\r\nJehiel, (2018)\r\nJehiel, P. (2018).Investment strategy and selection bias: An equilibrium perspective on overoptimism.American Economic Review, 108(6):1582–1597.\r\nKelly et al., (2023)\r\nKelly, A., Sullivan, M., and Strampel, K. (2023).Generative artificial intelligence: University student awareness, experience, and confidence in use across disciplines.Journal of University Teaching and Learning Practice, 20(6):1–16.\r\nKharazian, (2025)\r\nKharazian, A. (2025).Ramp ai index: Monthly measurement of ai adoption by american businesses.Technical report, Ramp Economics Lab.\r\nLehmann et al., (2024)\r\nLehmann, M., Cornelius, P. B., and Sting, F. J. (2024).AI Meets the Classroom: When Does ChatGPT Harm Learning?\r\nLing and Imas, (2025)\r\nLing, Y. and Imas, A. (2025).Underreporting of AI use: The role of social desirability bias.Available at SSRN.\r\nLiu and Wang, (2024)\r\nLiu, Y. and Wang, H. (2024).Who on earth is using generative ai?Policy Research Working Paper WPS10870, World Bank Group, Washington, D.C.\r\nMcClain, (2024)\r\nMcClain, C. (2024).Americans’ use of ChatGPT is ticking up, but few trust its election information.\r\nMcDonald et al., (2025)\r\nMcDonald, N., Johri, A., Ali, A., and Collier, A. H. (2025).Generative artificial intelligence in higher education: Evidence from an analysis of institutional policies and guidelines.Computers in Human Behavior: Artificial Humans, 3:100121.\r\nMcElheran et al., (2023)\r\nMcElheran, K., Li, J. F., Brynjolfsson, E., Kroff, Z., Dinlersoz, E., Foster, L. S., and Zolas, N. (2023).AI Adoption in America: Who, What, and Where.\r\nMerchant, (2024)\r\nMerchant, B. (2024).The new luddites aren’t backing down.The Atlantic.Accessed: 2025-06-16.\r\nNam, (2023)\r\nNam, J. (2023).56% of college students have used ai on assignments or exams.Technical report.Edited by Lyss Welding. Accessed June 11, 2025.\r\nNolan, (2023)\r\nNolan, B. (2023).Here are the schools and colleges that have banned chatgpt.Business Insider.Accessed: 2025-01-27.\r\nNoy and Zhang, (2023)\r\nNoy, S. and Zhang, W. (2023).Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence.\r\nOtis et al., (2024)\r\nOtis, N. G., Delecourt, S., Cranney, K., and Koning, R. (2024).Global Evidence on Gender Gaps and Generative AI.Harvard Business School.\r\nPeng et al., (2023)\r\nPeng, S., Kalliamvakou, E., Cihon, P., and Demirer, M. (2023).The Impact of AI on Developer Productivity: Evidence from GitHub Copilot.\r\nRavšelj et al., (2025)\r\nRavšelj, D., Keržič, D., Tomaževič, N., Umek, L., Brezovar, N., A. Iahad, N., Abdulla, A. A., Akopyan, A., Aldana Segura, M. W., AlHumaid, J., Allam, M. F., Alló, M., Andoh, R. P. K., Andronic, O., Arthur, Y. D., Aydın, F., Badran, A., Balbontín-Alvarado, R., Ben Saad, H., Bencsik, A., Benning, I., Besimi, A., Bezerra, D. D. S., Buizza, C., Burro, R., Bwalya, A., Cachero, C., Castillo-Briceno, P., Castro, H., Chai, C. S., Charalambous, C., Chiu, T. K. F., Clipa, O., Colombari, R., Corral Escobedo, L. J. H., Costa, E., Cre\\textcommabelowtulescu, R. G., Crispino, M., Cucari, N., Dalton, F., Demir Kaya, M., Dumić-Čule, I., Dwidienawati, D., Ebardo, R., Egbenya, D. L., Faris, M. E., Fečko, M., Ferrinho, P., Florea, A., Fong, C. Y., Francis, Z., Ghilardi, A., González-Fernández, B., Hau, D., Hossain, M. S., Hug, T., Inasius, F., Ismail, M. J., Jahić, H., Jessa, M. O., Kapanadze, M., Kar, S. K., Kateeb, E. T., Kaya, F., Khadri, H. O., Kikuchi, M., Kobets, V. M., Kostova, K. M., Krasmane, E., Lau, J., Law, W. H. C., Lazăr, F., Lazović-Pita, L., Lee, V. W. Y., Li, J., López-Aguilar, D. V., Luca, A., Luciano, R. G., Machin-Mastromatteo, J. D., Madi, M., Manguele, A. L., Manrique, R. F., Mapulanga, T., Marimon, F., Marinova, G. I., Mas-Machuca, M., Mejía-Rodríguez, O., Meletiou-Mavrotheris, M., Méndez-Prado, S. M., Meza-Cano, J. M., Mirķe, E., Mishra, A., Mital, O., Mollica, C., Morariu, D. I., Mospan, N., Mukuka, A., Navarro Jiménez, S. G., Nikaj, I., Nisheva, M. M., Nisiforou, E., Njiku, J., Nomnian, S., Nuredini-Mehmedi, L., Nyamekye, E., Obadić, A., Okela, A. H., Olenik-Shemesh, D., Ostoj, I., Peralta-Rizzo, K. J., Peštek, A., Pilav-Velić, A., Pires, D. R. M., Rabin, E., Raccanello, D., Ramie, A., Rashid, M. M. U., Reuter, R. A. P., Reyes, V., Rodrigues, A. S., Rodway, P., Ručinská, S., Sadzaglishvili, S., Salem, A. A. M. S., Savić, G., Schepman, A., Shahpo, S. M., Snouber, A., Soler, E., Sonyel, B., Stefanova, E., Stone, A., Strzelecki, A., Tanaka, T., Tapia Cortes, C., Teira-Fachado, A., Tilga, H., Titko, J., Tolmach, M., Turmudi, D., Varela-Candamio, L., Vekiri, I., Vicentini, G., Woyo, E., Yorulmaz, Ö., Yunus, S. A. S., Zamfir, A.-M., Zhou, M., and Aristovnik, A. (2025).Higher education students’ perceptions of ChatGPT: A global study of early reactions.PLOS ONE, 20(2):e0315011.\r\nRogers, (1962)\r\nRogers, E. M. (1962).Diffusion of Innovations.Free Press, New York.\r\nRoss et al., (1977)\r\nRoss, L., Greene, D., and House, P. (1977).The “false consensus effect”: An egocentric bias in social perception and attribution processes.Journal of experimental social psychology, 13(3):279–301.\r\nSacerdote, (2001)\r\nSacerdote, B. (2001).Peer Effects with Random Assignment: Results for Dartmouth Roommates.The Quarterly Journal of Economics, 116(2):681–704.\r\nStinebrickner and Stinebrickner, (2008)\r\nStinebrickner, R. and Stinebrickner, T. R. (2008).The causal effect of studying on academic performance.The BE Journal of Economic Analysis & Policy, 8(1).\r\nStöhr et al., (2024)\r\nStöhr, C., Ou, A. W., and Malmström, H. (2024).Perceptions and usage of AI chatbots among students in higher education across genders, academic levels and fields of study.Computers and Education: Artificial Intelligence, 7:100259.\r\nStokey, (2021)\r\nStokey, N. L. (2021).Technology diffusion.Review of Economic Dynamics, 42:15–36.\r\nWorld Bank, (2016)\r\nWorld Bank (2016).World development report 2016: Digital dividends.World Bank Publications.\r\nWu et al., (2022)\r\nWu, M.-J., Zhao, K., and Fils-Aime, F. (2022).Response rates of online surveys in published research: A meta-analysis.Computers in Human Behavior Reports, 7.\r\nXiao et al., (2023)\r\nXiao, P., Chen, Y., and Bao, W. (2023).Waiting, banning, and embracing: An empirical analysis of adapting policies for generative ai in higher education.arXiv preprint arXiv:2305.18617.\r\nAppendix\r\n\r\nAppendix AAppendix Figures and Tables\r\nFigure A1:Generative AI Usage Survey Design Overview\r\nKey\t\r\nSurvey Structure\r\nSurvey Sections\r\nUsage Data Collection\r\nBeliefs & Perceptions\r\nAI Usage Survey at Middlebury College\r\nDecember 2024\r\nParticipant Recruitment\r\nCampus-wide email invitations, framed as general technology use survey\r\nParticipation Incentives\r\nAmazon gift card lottery ranging from $50-$500\r\nSection 1: Demographics & Academic Information\r\nStudent Characteristics\r\nGender, race/ethnicity, high school type\r\nAcademic Profile\r\nYear, major, self-reported GPA, study hours\r\nSection 2: generative AI Usage Patterns\r\nAdoption Metrics\r\nFrequency, timing of first use, specific AI models used\r\nAcademic Applications\r\nTasks performed with AI, payment for premium services\r\nSection 3: Perceptions & Institutional Policies\r\nImpact Perceptions\r\nEffects on learning, grades, time management\r\nPolicy Responses\r\nUsage likelihood under different policies\r\nPeer Usage Beliefs\r\nEstimates of AI adoption among peers for schoolwork and leisure\r\nOpen-Ended Response Collection\r\nMotivations for AI use, policy feedback, suggestions for improvement\r\nNote: This figure illustrates the structure of the AI usage survey conducted at Middlebury College in December 2024. The survey collected information across three main sections: (1) demographic and academic background, (2) patterns of generative AI usage including adoption timing, frequency, and specific applications, and (3) perceptions of AI’s impact on learning and responses to institutional policies.\r\n\r\nFigure A2:Cumulative Generative AI Use by Student Characteristic\r\nPanel A. By Gender\r\nRefer to caption\r\nPanel B. By Race\r\nRefer to caption\r\nPanel C. By High School Type\r\nRefer to caption\r\nPanel D. By Freshman Year GPA\r\nRefer to caption\r\nPanel E. By Cohort\r\nRefer to caption\r\nPanel F. By Field of Study\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents cumulative AI use based on different student characteristics. Each panel displays the cumulative distribution of AI use based on a specific characteristic: gender, race, school type, first-year GPA, cohort, or field of study. The cumulative percent of students is plotted against usage categories. The legends and colors correspond to subgroups within each demographic variable.\r\n\r\nFigure A3:Percent of Students Who Pay for Generative AI Tools\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of AI users who pay for AI tools (through any platform) across different demographic groups. Horizontal lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure A4:Academic Uses of ChatGPT: Evidence from Global Survey\r\nPanel A. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\nPanel B. Across the College Quality Distribution\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of students who use ChatGPT for different academic tasks based on data from Ravšelj et al., (2025). Panel A displays usage patterns across tasks categorized as augmenting (proofreading, translating, study assistance, research assistance) versus automating (academic writing, professional writing, creative writing, brainstorming, summarizing, calculating, coding assistance, personal assistance) student effort. Panel B shows usage patterns across university quality quintiles based on World University Rankings, with universities ranked in the top 20% (top quintile) showing slightly higher rates of augmentation relative to automation compared to bottom quintile institutions. The analysis includes universities with at least 30 student responses and excludes observations with missing usage data.\r\n\r\nFigure A5:Student Beliefs about AI Usage versus Actual Usage under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure compares students’ beliefs about AI usage with actual usage rates under different AI policies. For each policy type, blue bars show the percent of students who report being “Neutral,” “Likely,” or “Very Likely” to use AI, while red bars show students’ mean beliefs about what percent of their peers use AI in classes with that policy. The sample includes all survey respondents for both actual usage and beliefs measures. “AI Use is Unrestricted” refers to classes with no restrictions on AI use, “No Explicit AI Policy” refers to classes without a stated policy, and “AI Use is Prohibited” refers to classes that ban AI use entirely.\r\n\r\nFigure A6:Relationship Between Beliefs About AI Usage and Actual AI Usage\r\nRefer to caption\\justify\r\nNotes: This figure shows the relationship between students’ beliefs about AI usage among their peers and actual AI usage rates across different demographic groups. Each point represents a different group of students (by demographics, academic characteristics, or field of study). The \r\ny\r\n-axis shows the percent of students in each group who report using AI for academic purposes. The \r\nx\r\n-axis shows the mean belief within each group about what percent of Middlebury students use AI. The dashed line shows the linear fit. Sample includes all survey respondents with at least ten observations per group. Students’ beliefs about AI usage are positively correlated with actual usage patterns, suggesting that students have relatively accurate perceptions of AI adoption among their peers.\r\n\r\nGPA > p50 80.3 26.3 17.4 23.2 13.4 GPA < p50 87.1 18.7 26.3 32.2 9.9 Freshman 81.3 24.3 24.1 24.7 8.1 Sophomore 84.4 25.6 22.6 23.7 12.4 Junior 86.2 21.6 23.1 28.6 12.8 Senior 78.7 21.0 17.4 29.9 10.4\r\n\r\nPanel C. Field of Study    Arts 73.3 21.0 0.0 52.3 0.0 Humanities 74.8 39.3 21.8 12.1 1.7 Languages 57.4 27.0 30.3 0.0 0.0 Literature 48.6 10.0 33.7 5.0 0.0 Natural Sciences 91.1 22.0 21.9 33.8 13.3 Social Sciences 84.6 17.9 20.4 29.1 17.2\r\n\r\n\\justify\r\nNotes: This table presents the percent of students in each demographic group who report using AI at different frequencies during the academic semester. Each cell shows the percent of students within that group. Column 1 reports the total percent who use AI at any frequency. Columns 2 to 5 represent increasing usage frequencies: rarely (1–2 times per semester), occasionally (monthly), frequently (weekly), and very frequently (multiple times per week). Panel A reports percentages by demographic characteristics. Panel B shows percentages by academic characteristics. Panel C presents percentages by field of study.\r\n\r\n0.9730.1350.0770.1100.114 R\r\n−\r\nsquared0.0120.0510.0270.0240.138 N (Students)516516516516516 \\justifyNotes: This table assesses the relationship between AI model adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI model usage (columns 1-4) or payment for AI services (column 5), and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division.\r\n\r\nEach column presents results for a different model or payment outcome. Column 1 shows usage of OpenAI’s ChatGPT, column 2 Google Gemini, column 3 Microsoft Copilot, column 4 any other AI model, and column 5 whether the student pays for any generative AI service.\r\n\r\nThe omitted categories are Natural Sciences for academic division, white students for race/ethnicity, and freshmen for cohort. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.7510.6760.6610.2710.479 R\r\n−\r\nsquared0.0870.1130.0760.0680.041 N (Students)599599599599599 \\justifyNotes: This table examines how student characteristics relate to self-reported likelihood of using generative AI under different policy scenarios. Each column presents results for different policy scenarios. In columns 1-4, the dependent variable equals one if the student reports being“neutral,” “likely” or “extremely likely” to use AI under the specified policy, and zero if they report being “unlikely,” or “extremely unlikely.” Column 5 represents the impact of moving from unrestricted use to complete prohibition.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.5760.6850.3760.4890.564 R\r\n−\r\nsquared0.1020.1190.0840.0370.087 N (Students)561565543560560 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. Each column presents results for beliefs about different academic outcomes: learning ability (e.g., ability to grasp concepts, retain information, or learn new skills) in column 1, understanding of course materials in column 2, course grades in column 3, ability to complete assignments on time in column 4, and time spent on academics in column 5. The dependent variable in each regression equals one if the student believes AI somewhat improves” or significantly improves” the outcome, and zero if they believe it has no effect, reduces, or significantly reduces the outcome. “Don’t know” responses are excluded.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\nAppendix BEmpirical Appendix\r\nB.1Field of Study Classifications\r\nThis appendix details the classification of majors into broad fields of study used in Figure 1:\r\n\r\n• Natural Sciences: Includes Biology, Biochemistry, Chemistry, Computer Science, Earth and Climate Sciences/Geology, Environmental Studies, Mathematics, Molecular Biology & Biochemistry, Neuroscience, Physics, and Statistics.\r\n• Social Sciences: Includes Anthropology, Economics, Education, Geography, International & Global Studies, International Politics & Economics, Political Science, Psychology, and Sociology.\r\n• Humanities: Includes American Studies, Black Studies, Classics, History, Architectural Studies, Art History & Museum Studies, History of Art & Architecture, Philosophy, Religion, and Classical Studies.\r\n• Literature: Includes Comparative Literature, English/English & American Literatures, and Literary Studies.\r\n• Languages: Includes Arabic, Chinese, French & Francophone Studies, German, Japanese Studies, Russian, and Spanish.\r\n• Arts: Includes Film & Media Culture, Music, Studio Art, and Theatre.\r\nB.2Task-Specific Use of Generative AI\r\nThe aggregate patterns documented in Section 4 may mask important heterogeneity across student groups. Different students may adopt AI tools at varying rates depending on their academic needs and field-specific norms. To investigate this heterogeneity systematically, Appendix Table LABEL:tab:ai_purpose_correlates presents regression estimates examining how student characteristics predict AI usage for each of our ten measured academic tasks.\r\n\r\nUsage patterns differ markedly by student characteristics. Male students consistently exhibit higher adoption rates across most applications, with particularly pronounced differences for finding information (16.4 pp; \r\np\r\n<\r\n0.01\r\n), summarizing texts (12.5 pp; \r\np\r\n<\r\n0.01\r\n), and creating images (16.9 pp; \r\np\r\n<\r\n0.01\r\n). Black students show substantially higher usage for information-gathering tasks, being 23.8 pp more likely to use AI for finding information compared to white students (\r\np\r\n<\r\n0.05\r\n). They also exhibit dramatically higher adoption of writing assistance tools, with 27.6 pp higher usage for editing text (\r\np\r\n<\r\n0.05\r\n) and 23.6 pp for writing emails (\r\np\r\n<\r\n0.10\r\n). Latino students demonstrate significantly higher usage for generating ideas (24.0 pp; \r\np\r\n<\r\n0.01\r\n) and writing emails (20.7 pp; \r\np\r\n<\r\n0.01\r\n), while Asian students show markedly higher adoption for writing emails (24.5 pp; \r\np\r\n<\r\n0.01\r\n) and explaining concepts (10.9 pp, \r\np\r\n<\r\n0.05\r\n). Students from public high schools report lower usage rates for several applications, particularly for concept explanation (7.3 pp lower; \r\np\r\n<\r\n0.10\r\n) and writing emails (7.4 pp lower; \r\np\r\n<\r\n0.10\r\n), but higher usage for proofreading (8.7 pp higher; \r\np\r\n<\r\n0.10\r\n).\r\n\r\nField of study emerges as a particularly strong predictor of usage patterns. Arts majors show significantly lower adoption across multiple tasks compared to Natural Science students, with gaps of 49.7 pp for finding information (\r\np\r\n<\r\n0.01\r\n), 44.0 pp for generating ideas (\r\np\r\n<\r\n0.05\r\n), and 62.1 pp for coding assistance (\r\np\r\n<\r\n0.01\r\n). Similarly, humanities majors are 36.8 pp less likely to use AI for generating ideas (\r\np\r\n<\r\n0.01\r\n) and 30.8 pp less likely for coding assistance (\r\np\r\n<\r\n0.01\r\n). Languages majors exhibit the most pronounced differences, being 52.4 pp less likely to use AI for summarizing texts (\r\np\r\n<\r\n0.01\r\n) and 46.9 pp less likely for generating ideas (\r\np\r\n<\r\n0.01\r\n). By contrast, social science students show higher usage for explaining concepts (8.0 pp; \r\np\r\n<\r\n0.10\r\n) and summarizing texts (14.7 pp; \r\np\r\n<\r\n0.01\r\n).\r\n\r\n0.8030.7400.6310.6190.5410.4730.3710.3440.2350.204 R\r\n−\r\nsquared0.0780.0870.0770.1160.0280.0540.0830.1350.0400.094 N (Students)515515515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and use of AI for specific academic tasks. Each column shows the result for a different academic task. The omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Regressions are weighted and use heteroskedasticity-robust standard errors clustered at the student level. ∗∗∗, ∗∗, and ∗ indicate significance at the 1%, 5%, and 10% levels, respectively.\r\n\r\nAppendix CQualitative Evidence on Student Perspectives on AI Use\r\nIn this section, we analyze student responses to an open-ended question about their use of generative AI. The survey question asked: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” This was an optional question that 48.3 percent of survey respondents answered. Appendix Figure C1 presents a word cloud of the most frequent words in student responses.\r\n\r\nC.1Validating the Open-Ended Response Measure\r\nWe begin by validating our open-ended response measure to show that it contains meaningful signal. To validate this measure, we employ sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner), a widely-used lexicon and rule-based sentiment analysis tool (Hutto and Gilbert, , 2014). We calculate sentiment scores for each response and examine whether these scores correlate with students’ actual AI adoption behaviors. Intuitively, students who express more positive sentiment toward AI in their open-ended responses should be more likely to have adopted AI tools in practice. We test this hypothesis by examining two measures of AI adoption: whether students have ever used generative AI and whether they currently use AI for academic purposes. Appendix Figure C2 presents binned scatterplots showing the relationship between AI adoption rates and standardized sentiment scores.\r\n\r\nSentiment towards generative AI strongly predicts AI adoption. For both outcomes, students with negative sentiment scores (below zero) show substantially lower adoption rates—around 70-85 percent for ever using AI and 70-75 percent for academic use. In contrast, students with positive sentiment scores display markedly higher adoption rates, reaching nearly 100 percent for general use and 95-100 percent for academic purposes among those with the most positive sentiment. The relationship is particularly pronounced for academic AI use (Panel B), where the coefficient of \r\nβ\r\n^\r\n=\r\n0.168\r\n is twice as large as for general adoption.17 These systematic patterns confirm that our open-ended responses capture meaningful variation in student attitudes that corresponds to real behavioral differences.\r\n\r\nC.2How Students use Generative AI\r\nTo analyze these responses systematically, we classified each response using keywords based on their content. For example, if a student mentioned using AI to save time, we tagged the response with the keyword “time-saver.” If a student expressed concerns about AI’s impact on learning, we tagged it with “negative learning.” Responses could receive multiple keywords if they discussed several themes. Appendix Figure C3 shows the frequency of keywords in our classification. The responses reveal how students use AI tools, what motivates this use, and what causes some to avoid or limit their use.\r\n\r\nThe most common use of AI is as an explanatory tool. Nearly 30 percent of responses mentioned using AI to understand course material. Students frequently ask AI to break down complex concepts from readings and lectures, particularly when they find the material difficult to understand. For example, one student reported: “I can ask AI to explain concepts to me that I have a hard time grasping. […] I can keep asking ’simplify’ or ’break down even more.”’ Students also use AI to summarize dense academic readings, which they argue helps them manage heavy reading loads.\r\n\r\nStudents employ AI throughout different stages of the writing process. Some use AI to generate initial drafts that serve as starting points. One student explained: “Helps me get started with a base for most of my essays. It feels easier to edit something already written and make it my own than to write from scratch.” Others use AI more narrowly for brainstorming when stuck on specific problems. As one student noted: “I use it if I am feeling stuck to push me to the right direction (whether a mathematical problem or an essay idea).” Many also report using AI as an editing tool to improve grammar, sentence structure, and overall writing flow. This is especially the case for non-native English speakers. As one student explained: “English is not my first language and it frustrates me sometimes that I cannot find the best way to phrase a certain idea and AI is a useful tool to have to find alternate expressions.”\r\n\r\nStudents frequently mentioned using AI for specific academic tasks. In courses that require coding, students often use AI for debugging code and understanding programming concepts. Students also employ AI for administrative tasks like formatting citations and drafting routine emails. Finally, many students use AI as an enhanced search engine. One student reported: “It has significantly reduced the time it takes to conduct research on new topics and ideas, and helps me by giving me a thorough selection of sources to use for projects of any kind.”\r\n\r\nC.3Why Students Adopt AI\r\nTime savings was the most commonly cited reason for using AI. Nearly 30 percent of responses mentioned using AI to complete work more efficiently. Students often viewed AI as a way to manage demanding course loads. Many students particularly embrace AI assistance for tasks viewed as mechanical or administrative. A student noted they use AI for “Writing emails quick and creating resume/ cover letter templates.” But AI assistance goes beyond grunt work. Some students use it to “spend less time doing assignments and homework.” This is particularly true if students don’t view the work as central to their academic experience. As one student explained: “when I come across work I deem as ineffective, I want to spend as little time as possible doing it.”\r\n\r\nHaving an on-demand tutor for academic support was another key motivation. One-quarter of responses described using AI as an “explainer” when other resources were unavailable or inconvenient. As one student noted, “I use it as a last resort (if there are no office hours, after looking up videos, etc.) if I need extra help. I’d like to think that the way I use it is similar to going to office hours or TA hours.”\r\n\r\nPeer influence also drove AI adoption. Some students reported feeling pressure to use AI to remain competitive with their classmates. One student explained: “I noticed others use it, are getting better grades than me, and they say they learn better with the help of AI, so I gave it a try.” Others worried about being at a competitive disadvantage: “Other people were using it and told me about it. I felt like I would be at a disadvantage if I wasn’t also using it.”\r\n\r\nC.4Concerns and Limitations\r\nStudents expressed several concerns about AI use in academic work. The most frequent worry was about negative impacts on learning. One student who initially used AI extensively reported: “In the past, I have simply plugged and chugged homework assignments into ChatGPT and submitted it. Those assignments feedback from teachers was positive and I was getting good grades, but I definitely felt that my own learning outcomes to be significantly worse.” Other students viewed AI use as fundamentally incompatible with their educational goals. As one humanities student explained: “my task is as a humanities student is to think, not calculate; why should I let AI do the thinking for me? It would defeat the purpose of pursuing my education.”\r\n\r\nMany students described ethical concerns about AI use. Responses suggested uncertainty about appropriate boundaries. One student noted: “I never use it to explicitly write something because that feels like overt cheating, but sometimes I hesitate when it completely solves Econ problems. I understand how it does it, and it helps me to learn, but it still sometimes feels a little morally gray.” Another expressed similar ambivalence: “I tend to only use it when [it] will save me time in a moral way.”\r\n\r\nStudents also emphasized the importance of maintaining ownership of their work. Many expressed pride in producing original work and hesitation about diluting that ownership through AI use. As one student explained: “I don’t have interest in using generative AI for my academic work because I want my work to reflect my own ideas.” Another noted: “It usually would not even occur to me to turn to AI to substitute writing because I want to take credit for my work, and using AI seems to diminish that.”\r\n\r\nTechnical limitations deterred some students from using AI tools. Students reported concerns about inaccurate outputs (“hallucinations”) and poor output quality, particularly for creative writing or complex analytical tasks. For example, one student noted that “In my poetry class we were instructed to use it to come up with poems and they were awful, so that kinda turned me away from using it to do my work for me.”\r\n\r\nC.5Discussion\r\nWe conclude with two overarching themes that emerged from the responses.\r\n\r\nFirst, students vary substantially in how they incorporate AI into their academic lives. Crucially, this heterogeneity largely depends on what students perceive as “appropriate” uses of AI. For activities that they perceive as core to their academic journey—like writing essays or solving problems—many students hesitate to use AI. A student articulated this clearly: “Most of my work is writing or reading. If I’m not doing the writing, what is the purpose of me taking the class?” Yet, students draw different boundaries between central tasks and grunt work. Some use AI extensively, viewing their role as akin to a manager that provides high-level direction while AI handles implementation. Others restrict AI use to specific tasks like brainstorming, editing, or drafting emails. Still others avoid AI entirely for academic work, often for ethical reasons. Even among AI users, adoption patterns reflect individual trade-offs between time savings, learning goals, and academic integrity concerns.\r\n\r\nSecond, there appears to be a fundamental tension between efficiency and learning. The time-saving benefits are easy to observe, quantifiable, and tangible. But these time savings are unlikely to be a free lunch. Some benefits may come at the cost of spending less time with material that requires deeper engagement to digest. As one student noted: “There may be a negative effect in that it eliminates much of the ’struggle’ in learning.” Yet, having an on-demand tutor that explains concepts in relatable ways can also improve learning. As one student explained: “It can explain concepts to me in a way that is tailored to my learning style.” This suggests that the impact of AI use on learning outcomes depends not on whether students use AI—almost all do, to some extent—but rather on how they employ these tools.\r\n\r\nFigure C1:Word Cloud of Student Motivations for Generative AI Use\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 147 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?’\r\n\r\nFigure C2:Relationship Between AI Sentiment and AI Adoption\r\nPanel A. Ever Used Generative AI\r\nRefer to caption\r\nPanel B. Uses AI for Academic Purposes\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents the relationship between AI sentiment and AI adoption. Panel A shows the proportion of respondents who have ever used generative AI, while Panel B shows the proportion who use AI for academic purposes. Each point represents the mean adoption rate for respondents within sentiment score bins of width 0.2. Sentiment scores are standardized compound scores computed using Hutto and Gilbert, (2014)’s VADER algorithm applied to responses to an open-ended question about generative AI. Positive values indicate positive sentiment and negative values indicate negative sentiment. The dashed lines show OLS best-fit lines estimated on the microdata, with coefficients and standard errors (in parentheses) displayed in the top-left corner of each panel.\r\n\r\nFigure C3:Frequency of Keywords in Student Motivations for AI Use\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to AI use. The responses come from the question “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” Color coding indicates the category of each theme. Usage type refers to how students use AI tools. Motivation captures what drove students to try AI. Concerns include mentions of course policies and academic integrity, individual reservations about AI use, worries about AI’s impact on education, and AI’s technical limitations.\r\n\r\nAppendix DQualitative Evidence on Student Views of AI Policies\r\nIn this section, we analyze student responses to an open-ended question about Middlebury’s AI policies. The survey asked: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Appendix Figure D1 presents a word cloud of the most frequent words in student responses. To analyze these responses systematically, we classified each response using keywords based on their content. Appendix Figure D2 shows the frequency of keywords in our classification.\r\n\r\nD.1Polarized Views on Generative AI Policy Approaches\r\nStudents expressed markedly different views about appropriate AI policies, revealing fundamental disagreement about the path forward. Some strongly advocated for embracing AI technology. As one student argued, “The tool is there, there is supply and there is demand. Don’t fight another war on drugs. Don’t live in a fake reality”. Others called for significant restrictions, arguing that “the use of generative AI is dishonest and corrosive” and that it “prohibits these organic processes and divorces students from true learning”.\r\n\r\nHowever, the most common position advocated for a balanced approach that would allow beneficial uses while restricting harmful ones. Students distinguished between uses that enhance learning (like concept explanation) and those that substitute for learning (like generating entire essays). One student articulated this nuanced view particularly well: “AI also can really be helpful at explaining a textbook problem that doesn’t make sense, or guiding slightly with homework, or creating study materials, or editing/tightening up your prose. All of those things are good, and universities should figure out how to maximize AI use for those reasons and to minimize students just feeding their problem sets into ChatGPT”\r\n\r\nA recurring theme was the futility of blanket bans. Many students emphasized that prohibition would be ineffective, with one noting “I don’t think anyone really cares what the policy of any given class is. If professors want people to not use it, they need to structure assessments in a way that will discourage use.” Another compared AI bans to restricting internet use, arguing “AI policies seem to be totally irrelevant. It’s like telling people they can’t use the internet as a resource for the class.” This ineffectiveness of bans creates fairness concerns. As one student explained: “I think if it is banned in a class, that should be enforced (and right now it absolutely is not)… As with any form of cheating, those who don’t cheat are put at a disadvantage.” Another student expressed similar frustration: “I find it discouraging when I hear classmates saying they use AI for things such as essays when they use it in dishonest ways.”\r\n\r\nD.2Need for Clear Guidelines\r\nThe most frequently expressed concern was the need for clear guidelines about generative AI use. Several students reported confusion about what constitutes acceptable use, with one noting “I think it should be more clear whether we can use it and how and how to cite it since most professors rarely mention it at all”. Other students emphasized the importance of professors explaining their policies upfront and their rationale, with one stating “I think that Professor’s should be very specific about what is allowed and their reasoning behind their policy”.\r\n\r\nMany students advocated for standardization across classes, observing that “Sometimes its confusing when one class allows it and another doesn’t and the other encourages it and so on so if there was a school wide or department wide policy that could help”. Yet there is also disagreement regarding standardization. Some students preferred leaving the decision to individual professors, arguing that “GenAI is more effective in some classes/majors than others. Making sure professors understand how students use GenAI and how useful GenAI is in their class (given the course structure, nature of assignments/material, etc.) is very important for the class policy.”\r\n\r\nD.3Training and Support Services\r\nStudents strongly emphasized the need for training in appropriate generative AI use. Many suggested that the college should provide guidance on using AI tools effectively while maintaining academic integrity. One student proposed “a workshop that teaches you to effectively use GenAI without violating the honor code”. Students also expressed interest in learning how to leverage AI to enhance their learning experience rather than circumvent it. As one student explained: “I think it could be useful to develop some sort of training. How do we use AI in a way that actually benefits our learning? I tried out some things on my own but I feel that I need more guidance.”\r\n\r\nThis desire for training was often linked to workplace preparedness. Students recognized that AI proficiency would be valuable in their careers, with one noting “As the world uses more and more AI, I think it is an important tool that students should know how to leverage”. Another emphasized: “The moment us students leave campus, we will be using it in the professional world, and when used in combination with one’s own skills, it is merely a tool to maximize efficiency”.\r\n\r\nD.4Discussion\r\nThe responses reveal several key tensions in AI policy implementation. First, students express a desire to use AI in ways that enhance rather than substitute for learning, yet they recognize that blanket bans are ineffective and worry about being disadvantaged if they follow restrictions while others do not.\r\n\r\nSecond, while students desire clear guidelines, they also want flexibility to accommodate legitimate uses that vary by discipline and assignment type. Different courses and majors may find different AI uses appropriate based on their learning objectives and assessment types.\r\n\r\nThird, there is tension between faculty autonomy in setting course policies and students’ desire for consistent institutional standards. While some support letting professors determine appropriate AI use for their specific courses, others argue that varying policies across classes create confusion and enforcement challenges.\r\n\r\nFinally, students report significant variation in faculty attitudes toward AI. Some students perceived faculty fear or misunderstanding of AI tools, noting that categorical bans often reflect a lack of understanding about AI’s capabilities and limitations rather than pedagogical considerations.\r\n\r\nFigure D1:Word Cloud of Student Feedback on Generative AI Policies\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 133 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?”\r\n\r\nFigure D2:Frequency of Keywords in Student Feedback on AI Policies\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to Middlebury’s AI policies. The responses come from the question “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Color coding indicates the category of each theme. Policy Approach captures suggestions about how AI should be regulated at the college. Adoption Views reflect positions on whether and how AI should be integrated into academic work. Training Needs indicates requests for guidance and support. Other Concerns include issues of workplace relevance and fairness.\r\n\r\nSee pages 1 of results/ai-survey-midd.pdf See pages 2- of results//ai-survey-midd.pdf",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== story.txt ===\nI remember the moment that sparked this project. It was during a walk across Middlebury’s campus late one fall afternoon—just after office hours. A student had come in, clearly bright but struggling, and told me they’d been using ChatGPT late at night to help with assignments because they were too anxious to speak up in class. That confession stuck with me.\r\n\r\nLater, over coffee with Germán, we started wondering—how many students were turning to generative AI as a quiet lifeline? Not to cheat, but to learn? That simple question became the seed for this entire study. What we found wasn’t just statistics. It was a story of adaptation, of students reshaping how they learn, and of institutions racing to catch up.\"\n\n==================================================\n\n=== Generative AI in Higher Education E.txt ===\nGenerative AI in Higher Education: Evidence from an Elite College†\r\nZara Contractor\r\nGermán Reyes\r\nAbstract\r\nGenerative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT’s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI’s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI’s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\r\n\r\n1Introduction\r\nThe rapid advancement of generative artificial intelligence (AI) is transforming higher education at an unprecedented pace. The launch of ChatGPT and similar tools has introduced technologies capable of performing tasks central to academic assessment and learning—writing essays, solving complex problems, and explaining intricate concepts—instantly and at near-zero marginal cost. Yet systematic evidence on the nature and implications of this new technology remains scarce. How widespread is generative AI adoption among students, and what factors drive it? Do students primarily use AI to augment their learning or to automate coursework, potentially harming human capital development? Could disparities in access to premium AI resources amplify existing educational inequalities?\r\n\r\nThese questions are at the heart of ongoing debates about the role of AI in education, which have relied heavily on anecdotal evidence and speculation rather than rigorous evidence. As a result, universities have implemented policies that vary dramatically, from outright bans to permissive adoption, often without clear evidence on their effectiveness or unintended consequences (Nolan, , 2023; Xiao et al., , 2023; McDonald et al., , 2025).\r\n\r\nThis paper addresses this gap by systematically examining generative AI adoption at Middlebury College, a highly selective liberal arts college in Vermont. With approximately 2,800 undergraduate students, Middlebury offers 49 majors across the arts, humanities, languages, social sciences, and natural sciences. Over the past decade, Middlebury has consistently ranked among the top 10 liberal arts colleges in U.S. News & World Report, with an average ranking of 8.5 between 2015 and 2025. The college’s selectivity is comparable to that of many elite research universities—Middlebury’s acceptance rate of approximately 10 percent is similar to that of Boston University (11 percent), Georgetown University (13 percent), and the University of Virginia (16 percent).\r\n\r\nOur analysis draws on survey data collected from the student population between December 2024 and February 2025. The survey collected detailed information about AI usage, including the frequency and purpose of use, perceived impacts, and responses to institutional policies. To minimize selection bias, we framed the survey broadly as examining technology use and provided incentives for participation, achieving a 22.9 percent response rate with 634 responses representing a broad cross-section of the student population.\r\n\r\nWe document five main findings. First, generative AI is approaching near-universal adoption at an unprecedented speed. Over 80 percent of students use AI for academic purposes, up from less than 10 percent before Spring 2023. This represents one of the fastest technology adoption episodes ever documented, dramatically exceeding the 40 percent adoption rate among U.S. workers (Bick et al., , 2025) and the 23 percent among all U.S. adults (McClain, , 2024). These levels are consistent with international findings of 50–70 percent adoption in university contexts (Nam, , 2023; Stöhr et al., , 2024; Ravšelj et al., , 2025).\r\n\r\nSecond, AI adoption is markedly unequal across academic disciplines and demographic groups. Field of study is the strongest predictor of adoption, likely reflecting how well AI capabilities align with the academic tasks required across fields. Adoption ranges from 91.1 percent in Natural Sciences majors (including mathematics and computer science) to significantly lower levels in Literature (48.6 percent) and Languages (57.4 percent). Additionally, demographic disparities exist. For example, males adopt AI at higher rates than females (88.7 versus 78.4 percent), a pattern that is consistent with documented AI gender gaps in other contexts (Otis et al., , 2024). Most notably, lower-achieving students have a higher adoption rate than their higher-achieving peers (87.1 versus 80.3 percent). As a result, AI could serve as an equalizing force if it enhances student learning—helping struggling students catch up to their peers—but could also widen achievement gaps if AI undermines skill development.\r\n\r\nThird, generative AI transforms the students’ learning production function by both augmenting student capabilities and automating academic tasks. We classify academic tasks as augmentation when AI enhances human capabilities while maintaining student engagement (e.g., explaining concepts, proofreading) versus automation when AI directly produces outputs with minimal cognitive involvement (e.g., writing essays, creating images), and average usage rates across tasks within each category. We find that 61.2 percent of AI users employ these tool for augmentation purposes, while 41.9 percent use it for automation. Qualitative evidence reinforces these patterns: students describe AI as an “on-demand tutor” for augmentation purposes, particularly valuable when traditional resources like office hours are unavailable, while automation use centers on time savings during periods of overwhelming workload. These self-reported patterns align closely with actual usage data from Claude conversation logs (Handa et al., 2025a,).\r\n\r\nFourth, institutional policies can significantly influence AI adoption, though their effectiveness can be undermined by informational frictions. We find that explicit prohibitions dramatically reduce self-reported intended use 39 percentage points (pp). Importantly, the effects of this policy vary substantially across student groups, with females showing larger reductions in usage under prohibition than males, suggesting that one-size-fits-all policies may inadvertently create disparate impacts across groups. Moreover, we also document substantial informational gaps about institutional AI policies. Many students (19.2 percent) do not understand AI policy rules, few know about institution-provided premium AI resources (10.1 percent) or proper AI citation practices (32.6 percent)—a skill that is necessary for academic integrity.\r\n\r\nFifth, students believe that AI has a positive learning impact and these beliefs strongly predict their own AI usage. Most students believe that AI improves their understanding of course materials (70.2 percent) and learning ability (60.1 percent), though fewer believe it improves grades (41.1 percent). Perhaps unsurprisingly, students who believe that AI improves their academic outcomes are significantly more likely to use these tools. For example, a ten-percentage-point increase in the belief that AI improves learning corresponds to a 4.9 percentage-point increase in adoption, highlighting how perceptions can affect technology diffusion in educational settings.\r\n\r\nTaken together, our findings reveal that generative AI has already reshaped students’ college experience. The substantial information gaps that we document—regarding both permitted uses and available resources—suggest straightforward opportunities for policy intervention. Yet the observed heterogeneity across disciplines and student groups indicates that uniform policies risk unintended consequences: blanket prohibitions may disproportionately disadvantage students who benefit most from AI augmentation, while unrestricted use may encourage automation practices detrimental to skill development. Thus, effective policy requires distinguishing clearly between AI uses that enhance learning and those that undermine it.\r\n\r\nWhile our findings are specific to the population we study—students at an elite liberal arts college—this setting offers valuable into AI’s broader societal impact. The rapid uptake of generative AI in higher education may reflect a generalizable principle regarding the diffusion of generative AI: adoption may be fastest in contexts where AI consolidates fragmented tools into a unified platform. Students have long had access to various tools for the academic tasks for which they now use generative AI.1 What distinguishes AI is its ability to provide all these services through a single, instantly accessible interface at virtually zero marginal cost. This consolidation likely drives students’ perception of enhanced learning efficiency and contributes to the fast adoption. This principle may extend well beyond education: industries with fragmented, specialized tools could experience similar trajectories of rapid AI adoption, even if AI itself does not introduce fundamentally new capabilities.\r\n\r\nOur findings contribute to a rapidly growing literature examining the adoption and impacts of generative AI. Recent work has focused on AI’s effects on worker productivity (Noy and Zhang, , 2023; Dell’Acqua et al., , 2023; Peng et al., , 2023; Cui et al., , 2024; Brynjolfsson et al., , 2025) and its potential to transform occupations (Felten et al., , 2021, 2023; Eloundou et al., , 2024). While several papers document AI adoption in workplace settings (Bick et al., , 2025; Hartley et al., , 2025; Humlum and Vestergaard, , 2025) and at the firm level (McElheran et al., , 2023; Bonney et al., , 2024; Kharazian, , 2025), we examine adoption in higher education—a critical setting where future high-skilled workers develop human capital.\r\n\r\nWe add to an emerging literature examining generative AI in education. Some studies focus on the learning impacts of generative AI, finding mixed impacts depending on the setting (e.g., Lehmann et al., , 2024; Bastani et al., , 2025). A complementary strand of the literature documents adoption patterns across diverse educational contexts, including universities in Australia (Kelly et al., , 2023), Ghana (Bonsu and Baffour-Koduah, , 2023), Norway (Carvajal et al., , 2024), Sweden (Stöhr et al., , 2024), and multi-country studies (Ravšelj et al., , 2025). To the best of our knowledge, this paper provides the first systematic evidence on generative AI adoption in U.S. higher education. Importantly, we focus specifically on how students use AI for academic purposes—unlike studies that measure more general usage including personal lives (e.g., Ravšelj et al., , 2025)—and go beyond documenting adoption rates to examine how students integrate AI into their academic workflows, distinguishing between augmentation and automation, the role of student beliefs about AI’s educational impacts, and how institutional policies shape usage patterns.\r\n\r\nOur findings also contribute to the literature on technology diffusion. Unlike the S-shaped adoption curves documented for most technologies (Griliches, , 1957; Rogers, , 1962), we observe extremely rapid adoption of generative AI, with 80 percent uptake within two years of ChatGPT’s release. This pattern contrasts sharply with the adoption trajectories of historical General Purpose Technologies (GPTs) like electricity, which required decades to achieve widespread use (David, , 1990; Bresnahan and Trajtenberg, , 1995). The rapid diffusion we document aligns with evidence that newer technologies diffuse faster than older ones (Comin and Hobijn, 2010a,; Comin and Hobijn, 2010b,). Two factors likely explain AI’s exceptional adoption speed compared to other GPTs. First, unlike electricity or steam power, generative AI requires minimal physical infrastructure—students access it through existing devices—and is available at no cost to consumers, eliminating the financial barriers that typically slow technology adoption. Second, as noted above, AI consolidates multiple specialized tools into a single platform, making its benefits immediately apparent without requiring specialized training or organizational restructuring, unlike previous GPTs (Brynjolfsson and Hitt, , 2000).\r\n\r\n2Data: Novel Student Survey\r\n2.1Recruitment and Structure\r\nWe conducted the survey from December 2024 to February 2025. All Middlebury College students were contacted via email and received a reminder a few weeks and two months after the initial invitation. To minimize selection bias, the recruitment materials described the survey broadly as a study on students’ use of technology in their academic and personal lives. To incentivize participation, students who completed the survey qualified for entry into a lottery for Amazon gift cards ranging in value from $50 to $500. The full survey instrument is provided in Appendix LABEL:app:survey.\r\n\r\nThe survey contains three main sections (see Appendix Figure A1 for the survey flow). First, we gather demographic and academic information, including gender, race/ethnicity, type of high school attended (private or public), current academic year, and declared or intended major.2 We also collect data on academic inputs and performance through students’ self-reported typical weekly hours spent studying and their first-year GPA.3\r\n\r\nSecond, we measure students’ experience with generative AI tools. We begin by asking whether students have ever used generative AI tools like ChatGPT or Claude. For those who have, we collect information about their usage patterns, including frequency of use during the academic semester, which specific AI models they use, and whether they pay for AI tools. We also gather data on how students use AI for different academic tasks, including writing assistance, learning support, and coding.\r\n\r\nThe final part of the survey elicits students’ beliefs about generative AI’s adoption and impacts. We ask students about their perceptions of AI’s effects on their academic experience across multiple dimensions, including learning, grades, time management, and understanding of course material. We then collect information on the role of institutional policies, asking students how different policy environments influence their likelihood of using AI. We also elicit students’ beliefs about AI usage among their peers, including their estimates of the fraction of Middlebury students who use AI for schoolwork and leisure. The survey concludes with two open-ended questions that allow students to share their experiences with AI in academic settings and provide feedback on the college’s AI policies and support services.\r\n\r\n2.2Sample and Summary Statistics\r\nOut of Middlebury’s 2,760 enrolled students, 739 began the survey. We exclude 105 respondents who left the survey before reaching the generative AI usage module, leaving us with an analysis sample of 634 students (22.9 percent of the student body). This response rate is comparable to that of similar surveys (Wu et al., , 2022). To make our sample more representative of Middlebury’s student body, we construct poststratification weights based on the distribution of declared majors from administrative records.4 Specifically, we weight observations by the ratio of each major’s share in the student population to its share in our survey responses.5\r\n\r\nTable Figures and Tables presents summary statistics for our sample. Column 1 reports unweighted survey averages, column 2 shows averages after applying poststratification weights, and column 3 provides population benchmarks from administrative records where available. Our unweighted sample consists of 44.6 percent male and 50.8 percent females. The racial/ethnic composition includes 61.8 percent white, 15.5 percent Asian, 9.9 percent Hispanic, and 3.6 percent Black students. The majority of students (54.3 percent) attended a public school, while 42.0 percent attended a private high school. Our sample represents 43 different majors across seven fields of study, with 31.1 percent of respondents not yet having declared a major. In our analysis, we group these undeclared students by their intended field of study as reported in the survey.\r\n\r\nComparing our unweighted sample to administrative records reveals notable differences. Our sample overrepresents white students (61.8 versus 53.8 percent) and Asian students (15.5 versus 7.3 percent), while underrepresenting Black students (3.6 versus 5.2 percent) and Hispanic students (9.9 versus 12.4 percent). First-year students are overrepresented (31.1 versus 25.5 percent), while senior students are underrepresented (21.3 versus 28.7 percent). Our weighting procedure partially addresses these discrepancies—particularly in the distribution of academic fields, where the weighted figures closely approximate administrative records—but some demographic differences persist. Despite these differences, our weighted sample provides reasonably close approximations to the college population on most dimensions, particularly for academic characteristics and field distributions.\r\n\r\n3Generative AI Usage Patterns Among Students\r\n3.1Adoption of Generative AI\r\nGenerative AI has achieved a remarkably high adoption rate among Middlebury College students. Figure 1 presents the distribution of AI usage frequency during the academic semester, categorized into four levels: “Rarely” (a few times per semester), “Occasionally” (a few times per month), “Frequently” (a few times per week), and “Very Frequently” (daily or almost daily). Overall, 82.5 percent of students report using generative AI for academic purposes, with substantial variation in usage intensity: 23.5 percent use it rarely, 22.2 percent occasionally, 26.2 percent frequently, and 10.6 percent very frequently.\r\n\r\nThe adoption rate at Middlebury aligns closely with patterns observed across other higher education institutions. A BestColleges survey of U.S. undergraduate and graduate students in fall 2023 found that 56 percent had used AI on assignments or exams (Nam, , 2023). Ravšelj et al., (2025) surveyed higher education students globally between late 2023 and early 2024, finding that 71.4 percent had ever used ChatGPT—though this figure encompasses all usage rather than academic use specifically. Similarly, Stöhr et al., (2024) document that 63 percent of Swedish university students had used ChatGPT by spring 2023, with 35.4 percent reporting regular use and 27.6 percent rare use. Among Norwegian university students, Carvajal et al., (2024) find that 68.9 percent use AI tools occasionally or more frequently.\r\n\r\nThese adoption rates in higher education far exceed those in the general population and workforce. Pew Research finds that only 23 percent of U.S. adults have ever used ChatGPT (McClain, , 2024), while Gallup reports that just one-third of U.S. workers have used AI for work (Gallup, , 2024). Bick et al., (2025) and Hartley et al., (2025) estimate that about 40 percent of the U.S. working-age population used generative AI for work as of late 2024 and early 2025. Both studies document substantial heterogeneity across industries, with information services showing the highest adoption rates at 56–62 percent—still considerably below the 80 percent adoption rate at Middlebury. Even among workers in AI-exposed occupations, Humlum and Vestergaard, (2025) find adoption rates of only 41 percent.6\r\n\r\n3.2Adoption by Student Characteristics and Field of Study\r\nAI adoption varies considerably across demographic groups and academic disciplines (Figure 1 and Appendix Table LABEL:tab:ai_freq_het). Males report higher usage rates than females (88.7 versus 78.4 percent). Usage patterns differ markedly by race/ethnicity: Black students (92.3 percent) and Asian students (91.3 percent) exhibit the highest adoption rates, while white students (80.2 percent) and Hispanic students (77.9 percent) report lower usage. Students from private high schools use AI more frequently than those from public schools (84.1 versus 80.4 percent). Notably, students with below-median GPAs report higher usage rates than their higher-achieving peers (87.1 versus 80.3 percent). Adoption varies widely by field of study: Natural Sciences leads with 91.1 percent usage, followed by Social Sciences at 84.6 percent, while Languages (57.4 percent) and Literature (48.6 percent) show substantially lower adoption rates.\r\n\r\nTo examine how student characteristics jointly relate to AI adoption, we estimate multivariate OLS regressions that include all observed characteristics simultaneously.7 Table LABEL:tab:ai_usage_correlates presents regression estimates using four usage thresholds as outcomes. Each column represents the probability of meeting progressively higher frequency thresholds: any AI use (column 1), at least monthly use (column 2), at least weekly use (column 3), and daily use (column 4).\r\n\r\nThe regression results confirm the patterns observed in the descriptive statistics. Holding other characteristics constant, males are 10.3 pp more likely than females to use AI (column 1, \r\np\r\n<\r\n0.05\r\n), with this gender gap typically widening at higher usage frequencies (columns 2–4, all \r\np\r\n<\r\n0.01\r\n). Black and Asian students show substantially higher adoption rates than white students, at 11.8 and 10.7 pp respectively (column 1, both \r\np\r\n<\r\n0.01\r\n). Students from public high schools are 3.0 pp less likely to use AI than those from private schools, but this difference is not statistically significant. Field of study emerges as the strongest predictor of adoption. Compared to Natural Sciences majors, students in Literature, Languages, Arts, and Humanities all show lower usage rates, with the differences being statistically significant at high usage frequencies (columns 3 and 4). Social Sciences majors exhibit adoption rates similar in magnitude to Natural Sciences majors across all usage frequency thresholds.\r\n\r\nOur findings on heterogeneity in AI adoption align with patterns documented in other settings. The gender gap in AI adoption at Middlebury—10.3 pp higher for males—is consistent with evidence across multiple studies. A meta-analysis of 18 studies by Otis et al., (2024) finds that males are 10–20 percentage points more likely to use generative AI than females. This gender gap in AI adoption appears in all educational studies (Nam, , 2023; Carvajal et al., , 2024; Stöhr et al., , 2024; Ravšelj et al., , 2025). Our finding that students with below-median GPAs are more likely to use AI aligns with Carvajal et al., (2024), who document higher adoption rates among students with lower admission grades. This pattern of greater adoption among lower-achieving students suggests that AI could narrow achievement gaps if it enhances learning and skill development, but could widen these gaps if it undermines the acquisition of fundamental skills.\r\n\r\nThe differences in adoption rates across academic fields at Middlebury mirror patterns documented in other educational settings. Stöhr et al., (2024) find that technology and engineering students exhibit significantly higher ChatGPT usage compared to students in humanities. Similarly, Nam, (2023) report that 62 percent of business majors and 59 percent of STEM majors have used AI tools for coursework, compared to 52 percent of humanities majors. Ravšelj et al., (2025) document comparable disciplinary differences, with applied sciences students showing substantially higher usage rates than arts and humanities students.\r\n\r\nThese academic differences persist into the workforce. Bick et al., (2025) find stark variation by college major: STEM graduates have the highest AI adoption rates (46.0 percent), followed by Business/Economics graduates (40.0 percent), while Liberal Arts graduates show substantially lower rates (22.4 percent). Humlum and Vestergaard, (2025) document similar patterns by occupation, finding that roles requiring strong writing and technical skills—such as marketing specialists and journalists—have the highest adoption rates. These consistent patterns across educational and professional contexts suggest that field-specific factors, particularly the applicability of AI tools to different types of tasks, shape adoption in systematic ways.\r\n\r\n3.3Timing of Generative AI Adoption\r\nThe speed of technology diffusion is a critical determinant of its economic and social impact (David, , 1990; Hall and Khan, , 2003; Stokey, , 2021). In educational contexts, rapid technology adoption can create or exacerbate inequalities between early and late adopters, particularly if the technology confers significant learning advantages (World Bank, , 2016). To track the timing of AI adoption among Middlebury students, we asked them when they first began using generative AI for academic purposes, with options ranging from “This semester (Fall 2024)” to “Before Spring 2023” (as a reference, ChatGPT’s public launch was in November 2022).\r\n\r\nStudents adopted generative AI at an extraordinary pace. Figure 2 shows that the cumulative adoption rate grew dramatically from less than 10 percent before Spring 2023 to slightly above 80 percent by Fall 2024. The pace of adoption has accelerated over time, likely reflecting improvements in AI capabilities. Among current users, 25.7 percent adopted AI in Fall 2024 alone, compared to 19.9 percent in Spring 2024 and 16.3 percent in Fall 2023.\r\n\r\nThe adoption rate among Middlebury students far exceeds that observed in other populations and represents one of the fastest technology adoption episodes ever documented. For comparison, Bick et al., (2025) show that it took over 20 years for computers to reach an 80 percent adoption rate in the U.S. working-age population, and about 15 years for internet adoption to reach similar levels. Even generative AI adoption in the broader population has been markedly slower: Pew Research found that just 23 percent of U.S. adults had ever used ChatGPT as of February 2024, up from 18 percent in July 2023 (McClain, , 2024). The dramatically faster adoption rate we document—reaching over 80 percent in less than two years—suggests academic settings may uniquely accelerate AI diffusion.\r\n\r\nTo identify early versus late adopters of generative AI, in Table LABEL:tab:ai_adopt_correlates and Appendix Figure A2, we analyze how adoption timing varies across student characteristics. The rate of adoption varies substantially across student characteristics. Male students led adoption, with a 8.9 pp higher probability of using AI before Spring 2023 compared to females (column 1, \r\np\r\n<\r\n0.01\r\n)—a gender gap that persists across all periods (columns 2–5). Black and Asian students also adopted earlier than white students, though these differences reach statistical significance only in later periods (columns 4–5). Field of study is a strong predictor of adoption timing. For example, students in Languages consistently lagged behind Natural and Social Sciences majors, with significantly lower adoption rates across nearly all time periods.\r\n\r\n3.4Choice of Generative AI Models\r\nMajor AI companies operate on a freemium model, where free versions coexist with premium subscriptions that offer higher usage limits and access to more advanced models. This tiered structure creates potential for a new form of educational disparities: if paid versions confer substantial academic advantages, students who cannot afford subscriptions may be systematically disadvantaged. To investigate these potential disparities, we presented respondents with a list of options including both free and paid versions of popular models. We also collected information on monthly subscription expenditures, with response options ranging from no active subscription to spending more than $40 monthly (standard subscriptions to major models like ChatGPT cost $20 per month at the time).\r\n\r\nOpenAI’s ChatGPT dominates AI usage among Middlebury AI users, with the free version capturing the largest market share. Figure 3 shows that 89.3 percent of AI users rely on the free version of ChatGPT, making it dramatically more popular than any alternative. Google Gemini (13.5 percent) and Microsoft Copilot (7.7 percent) are distant competitors, while other platforms each capture less than 5 percent of users. This dominance mirrors patterns in other academic settings, where Stöhr et al., (2024) find substantially higher familiarity with ChatGPT compared to alternative platforms, and aligns with broader workforce trends documented by Bick et al., (2025).8\r\n\r\nDespite near-universal AI adoption, only 11.3 percent of AI users pay for any AI service.9 This figure is remarkably similar to the 8 percent found in Ravšelj et al., (2025)’s multi-country survey. This low payment rate suggests that for most students, the premium features of paid versions—primarily higher usage limits and access to more advanced models—do not justify the subscription cost. However, payment patterns reveal significant disparities: males and Asian students are substantially more likely to purchase AI subscriptions (Appendix Table LABEL:tab:ai_model_correlates and Appendix Figure A3), potentially reflecting differences in usage intensity across demographic groups.\r\n\r\n4Generative AI and the Production of Learning\r\n4.1The Use of Generative AI across Academic Tasks\r\nHow is generative AI transforming the traditional inputs to student learning? The educational production function includes inputs like time spent studying (e.g., Stinebrickner and Stinebrickner, , 2008), faculty instruction (e.g., Fairlie et al., , 2014), peer interactions (e.g., Sacerdote, , 2001), and academic support services (e.g., Angrist et al., , 2009). AI tools have the potential to complement or substitute for these traditional inputs. For example, using AI to explain concepts might substitute for faculty office hours, while using it for proofreading might reduce time needed for academic support services.\r\n\r\nTo understand the role of AI in students’ learning production function, we collected information about students’ AI usage across ten common academic tasks: proofreading, generating ideas, writing essays, editing essay drafts, coding assistance, creating images, explaining concepts, composing emails, summarizing materials, and finding information. For each task, students indicated their frequency of AI use on a five-point scale ranging from never to daily. We supplemented this quantitative data with open-ended responses about how AI influences their academic work process.\r\n\r\nStudents use generative AI for a wide range of academic tasks, with the highest adoption rates for learning support and text-processing activities. Figure 4, Panel A shows that explaining concepts is the most common use case, with 80.3 percent of AI users using it for this task. Summarizing texts follows as the second most common task (74.0 percent), followed by finding information and generating ideas (63.1 and 61.9 percent). Writing assistance tasks like proofreading and editing essays are also common, used by 54.1 and 47.3 percent of AI users, respectively. Technical applications like coding help are significant (34.4 percent), considering that many academic degrees involve no programming. Notably, while 23.5 percent of AI users report using it for writing essays, this represents a relatively low adoption rate compared to other academic uses, suggesting students may be more hesitant to use AI for primary content creation. The lowest adoption rate is for creating images at 20.4 percent, likely reflecting fewer academic use cases for this capability.\r\n\r\nAn important limitation of our survey is that it relies on self-reported usage, which may introduce measurement error. For example, students might underreport uses they perceive as academically inappropriate—such as essay writing—while overreporting those viewed as legitimate learning tools (Ling and Imas, , 2025). To assess the validity of our self-reported measures, we compare our findings with Handa et al., 2025a, who analyze actual Claude usage patterns among users with university email addresses. Several caveats apply: their data captures conversation-level interactions rather than student-level usage, and most students in our sample use ChatGPT rather than Claude (Section 3.4). Despite these limitations, the comparison provides a useful benchmark for evaluating our survey responses.\r\n\r\nReassuringly, our results are consistent with Anthropic’s data. Both studies identify explaining concepts and technical problem-solving as primary use cases. Handa et al., 2025a report that the second largest use case (33.5 percent of conversations) involves “technical explanations or solutions for academic assignments,” while we find that 80.3 percent of AI users use it for explaining concepts—a difference likely attributable to our student-level versus their conversation-level measurement. Similarly, the most common usage category in Claude involves “designing practice questions, editing essays, or summarizing academic material” (39.3 percent of conversations), aligning with our findings that 74.0 percent of AI users use it for summarizing texts and 47.3 percent for editing essays. The disciplinary patterns also converge: Handa et al., 2025a finds that computer science, natural sciences, and mathematics conversations are overrepresented, which mirrors our finding that Natural Science majors show significantly higher AI adoption rates (Figure 1). Overall, these convergent findings from self-reported survey data and actual usage logs suggest that our results capture genuine patterns of student AI engagement rather than merely reflecting social desirability in responses.\r\n\r\n4.2Automation versus Augmentation\r\nAre students using generative AI primarily to augment their learning or to automate their coursework? This distinction is crucial for understanding AI’s impacts (Autor and Thompson, , 2025)—augmentation may enhance students’ learning processes while maintaining their active engagement and critical thinking, whereas automation produces fully-formed outputs that could be submitted with minimal student input, potentially harming learning.\r\n\r\nTo examine this empirically, Figure 4, Panel B categorizes the ten measured tasks based on whether they augment or automate academic tasks. We classify tasks as augmentation when they enhance human capabilities (explaining concepts, finding information, proofreading, and editing drafts) and as automation when they directly produce outputs (writing essays, creating images, composing emails, summarizing texts, generating ideas, and coding assistance). We then calculate the percentage of AI users who employ each category at various frequencies.10\r\n\r\nStudents use generative AI for both augmentation and automation, but with markedly different frequency. While 61.2 percent of AI users report using AI for augmentation tasks, 41.9 percent report using it for automation—thus, there is a substantial 19.3 pp difference. This gap is mainly driven by occasional use (19.7 percent for augmentation versus 12.7 percent for automation) and frequent use (16.3 versus 9.0 percent). Most strikingly, students are more than twice as likely to use AI daily for augmentation (5.0 percent) compared to automation (2.5 percent). The higher frequency of augmentation use also suggests that students find augmentation uses more valuable for their day-to-day academic activities. Our findings align closely with patterns observed in actual AI usage data: Handa et al., 2025b analyze real conversation logs from Claude and find that 57 percent of workplace AI interactions involve augmentation while 43 percent involve automation.\r\n\r\nQualitative evidence from open-ended responses provides additional insights into students’ motivations for using AI (see Appendix C for additional results and validation of the open-ended measure). Students’ descriptions align closely with the augmentation-automation framework. For augmentation, many characterize AI as an “on-demand tutor,” particularly valuable when traditional resources like office hours are unavailable. Non-native English speakers frequently mention using AI for proofreading to overcome language barriers, while students from technical majors describe using it to debug code and understand error messages. For automation, time savings emerged as the dominant motivation, with 21.7 percent of open-ended responses explicitly mentioning efficiency benefits (Appendix Figure C3). Students describe turning to AI during periods of overwhelming workload or looming deadlines, using it to generate initial drafts or complete routine assignments. That students automate tasks mostly when time-pressed helps explain why automation is less common than augmentation.11\r\n\r\nOur finding that students favor augmentation over automation extends beyond Middlebury. In Appendix Figure A4, we re-analyze data from Ravšelj et al., (2025)—a multi-country survey of higher education students—to examine how this balance varies across institutional quality.12 We classify universities into quintiles based on their Times Higher Education World University Rankings. Overall, students who use AI worldwide show similar adoption rates for augmentation (64.6 percent) and automation (63.1 percent) tasks—a much smaller gap than at Middlebury (Panel A). However, this aggregate pattern masks heterogeneity by institutional quality. Students at top-quintile universities show a modest preference for augmentation over automation, with this gap narrowing monotonically down the institution quality distribution. By the bottom quintile, augmentation and automation usage are virtually identical (Panel B).\r\n\r\n4.3Heterogeneity in Augmentation versus Automation Usage\r\nUnderstanding whether augmentation and automation patterns vary across student populations may help to design targeted support policies. To assess this, we construct four measures of augmentation and automation usage. First, we create binary indicators for whether students use AI for any augmentation or automation task. Second, we calculate the proportion of tasks in each category for which students employ AI. Third, using the Likert-scale responses, we create intensity measures that capture how frequently students use AI for augmentation and automation purposes. Finally, we compute the difference between augmentation and automation variables, as a measure that directly compare students’ relative preference for augmentation versus automation. Table LABEL:tab:ai_autom_regs presents regression estimates using these measures as outcomes.\r\n\r\nThe balance between augmentation and automation varies substantially by student characteristics. Males show higher adoption of both augmentation and automation tasks, with a similar magnitude in each case—they are 6.0 pp more likely to use augmentation prompts and 5.5 pp more likely to use automation prompts compared to females (columns 1 and 4, \r\np\r\n<\r\n0.05\r\n). Black students have a substantially higher augmentation intensity than white students (0.871 points higher, equivalent to 38.1 percent of the outcome mean, \r\np\r\n<\r\n0.01\r\n), but show no statistically significant differences in automation usage. This results in Black students having the strongest preference for augmentation over automation, with a 19.5 pp higher share of augmentation tasks relative to automation (column 7, \r\np\r\n<\r\n0.01\r\n). Asian students show high usage of both kinds relative to white students. Differences across field of study effects are sizable: humanities, languages, and literature majors tend to show lower usage of both augmentation and automation tools compared to natural science majors.\r\n\r\n5Institutional Policies and AI Adoption\r\n5.1The Role of Institutional Policies in Shaping Student Behavior\r\nInstitutional policies are central to shaping the adoption and diffusion of new technologies (Acemoglu, , 2025). In the context of generative AI in higher education, understanding how policies affect student behavior is essential for guiding evidence-based decision-making. To examine this, we asked students to report their likelihood of using AI under various policy scenarios, ranging from complete prohibition to unrestricted use. This analysis is particularly relevant in light of the ongoing debate about how universities should regulate generative AI use in academic settings (Nolan, , 2023), and the widespread variation in institutional policies across colleges (Xiao et al., , 2023).\r\n\r\nInstitutional policies substantially influence students’ reported likelihood of using generative AI. Figure 5 shows that when generative AI use is unrestricted, 52.4 percent of students report being likely or extremely likely to use it. This likelihood decreases modestly when policies require citation (40.9 percent) or when no explicit policy exists (42.3 percent). However, explicit prohibition creates a dramatic shift: only 13.4 percent of students report they would be likely or extremely likely to use AI when it is banned, while 72.9 percent say they would be unlikely or extremely unlikely to do so (note that these figures include both current AI users and students who do not use generative AI for academic purposes). These results suggest that institutional policies can significantly influence generative AI usage patterns, though a small fraction of students report they would likely use AI even when explicitly prohibited.\r\n\r\nThe magnitude of policy effects in our study aligns closely with findings from other contexts. Carvajal et al., (2024) estimate that banning AI reduces usage by 37.2 pp among females and 20.6 pp among males, for an overall drop of about 28.9 pp. In our survey, a ban leads to a 37.8 pp decline in usage, with a larger decrease among females (49.6 pp) than males (40.1 pp). These parallel results underscore how institutional policies can unintentionally produce disparate effects across gender. Notably, these differential policy effects extend beyond gender. Other demographic and academic characteristics—such as race and field of study—also moderate how students respond to policy restrictions (Appendix Table LABEL:tab:ai_policy_het), underscoring that institutional policies can produce non-neutral impacts along multiple dimensions.\r\n\r\n5.2Understanding of Institutional Policies and Resources\r\nInformation gaps and inattention can significantly affect technology adoption decisions (Duflo et al., , 2011; Hanna et al., , 2014). For example, imperfect information about rules, available resources, or proper usage guidelines could lead to underadoption of beneficial technologies or inadvertent policy violations. To test for the existence of information gaps, we examine three dimensions of policy understanding. First, we asked students whether they find AI policies in their current classes clear. Second, we measured awareness of free access to premium AI tools through the college—a resource that could reduce inequality but only if students know it exists. Third, we assessed whether students know how to properly cite AI when required, a mechanical skill necessary for academic integrity. We supplemented these measures with open-ended feedback about the college’s AI policies and support services.\r\n\r\nStudent understanding of institutional policies is high, though significant gaps remain. Figure 6, Panel A shows that most students (79.1 percent) understand when and where they are allowed to use AI in their classes, but a nontrivial minority (19.2 percent) find AI policies unclear. Moreover, critical knowledge gaps persist elsewhere. Only 10.1 percent know they have free access to Microsoft Copilot through the college (Panel B), and just 32.6 percent understand how to properly cite AI use (Panel C). These gaps vary systematically: females show better policy understanding than males (81.5 versus 75.8 percent), and non-white students demonstrate higher awareness across all three dimensions compared to white students.13\r\n\r\nThe qualitative evidence from open-ended responses reinforces these information frictions and reveals implementation challenges (Appendix D). Students express frustration with vague guidelines, requesting specific examples of acceptable versus unacceptable use cases. Many advocate for formal training, noting that simply knowing policies exist differs from understanding how to effectively integrate AI into their workflow. A particularly striking theme is students’ perception of blanket prohibitions as both ineffective and unfair, as they create a prisoner’s dilemma situation where compliant students are disadvantaged relative to those who secretly violate restrictions. Many responses call for a balanced approach—permitting AI use that supports learning while restricting uses that replace it—though the boundary between the two remains contested.\r\n\r\n6Beliefs About AI’s Educational Impact and Peer Usage\r\n6.1Student Beliefs of AI’s Impact on Educational Outcomes\r\nTechnology adoption decisions are shaped by beliefs about potential returns (Foster and Rosenzweig, , 2010). Students’ perceptions of how AI affects their learning may influence whether they adopt these tools and how they integrate them into their academic workflows. To elicit these beliefs, we asked students to evaluate AI’s impact across four dimensions of their academic performance: understanding of course materials, overall learning ability, time management, and course grades. For each dimension, students rated AI’s effect on a five-point scale ranging from “significantly reduces” to “significantly improves.” These subjective assessments provide insight into the perceived value of AI tools from the student perspective, which may differ from their actual effects on learning outcomes.\r\n\r\nStudents tend to believe that generative AI is beneficial for their academic performance, though the perceived benefits vary across dimensions. Figure 7 shows that the majority of students (70.2 percent) believe that generative AI improves their understanding of course materials, and 60.1 percent report improvement in their ability to grasp concepts, retain information, or learn new skills. Similarly, 59.4 percent report that AI improves their ability to complete assignments on time. Notably, while students believe AI helps their learning and assignment completion, they are less confident about its impact on course grades—41.1 percent believe it improves their grades, while 55.4 percent report no effect and 3.5 percent report negative effects. This pattern suggests that while students perceive generative AI as improving their learning process and workflow—through better understanding, skill development, and timely completion of work—these benefits do not necessarily translate into better course grades.14\r\n\r\nBeliefs about AI’s benefits strongly predict adoption. In Figure 8, we plot the relationship between the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis) for different subgroups of students (e.g., males, white students, public-school students, etc.). Across all four measured academic dimensions, there is a strong and statistically significant relationship between perceiving positive AI effects and AI adoption. The relationship is strongest for beliefs about the ability to improve course grades (Panel D): a 10-pp increase in the belief that AI improves grades is associated with a 5.6 pp increase in AI adoption (\r\np\r\n<\r\n0.01\r\n). Similar positive relationships exist for beliefs about learning ability (4.3 pp, \r\np\r\n<\r\n0.05\r\n), understanding of course materials (4.5 pp, \r\np\r\n<\r\n0.01\r\n), and timely assignment completion (5.3 pp, \r\np\r\n<\r\n0.05\r\n). These findings suggest that student beliefs about AI’s academic benefits—irrespective of the actual benefits—may play a crucial role in shaping adoption decisions.\r\n\r\nThese positive beliefs align with findings from other contexts. Ravšelj et al., (2025) report that the majority of students in their sample believe that ChatGPT improves their general knowledge (68.8 percent) and specific knowledge (62.7 percent)—remarkably similar to our finding that most students believe AI improves understanding of course materials (68.5 percent) and learning ability (57.6 percent). Similarly, 57.4 percent of students in Ravšelj et al., (2025)’s sample believe that ChatGPT helps meet assignment deadlines, while 59.4 percent in our data report AI improves timely assignment completion. Stöhr et al., (2024) provide complementary evidence from Swedish universities, where 47.7 percent of students believe that AI makes them more effective learners, yet only 17.3 percent believe these tools improve their grades—mirroring our finding that perceived learning benefits from AI use exceed perceived effects on course performance.\r\n\r\n6.2Student Beliefs About Peer Use of Generative AI\r\nStudents’ beliefs about their peers’ AI usage may influence their own adoption through multiple channels, including social norms (e.g., Giaccherini et al., , 2019), social learning (e.g., Foster and Rosenzweig, , 1995; Beaman et al., , 2021), peer effects (e.g., Bailey et al., , 2022), and competitive pressure to avoid falling behind (e.g., Goehring et al., , 2024). To measure these beliefs, we asked students to estimate what fraction of their peers use generative AI for different purposes and would use under different policy environments. Figure 9 presents the distribution of these beliefs. Panels A-C show students’ estimates of peer AI usage for schoolwork, leisure, and any purpose respectively. Panels D-F display students’ beliefs about AI usage under three policy environments: when classes have no explicit AI policy, when classes allow AI use, and when classes prohibit AI use.\r\n\r\nStudents systematically underestimate their peers’ AI usage. On average, they believe 65.2 percent of peers regularly use generative AI for schoolwork (Panel A), while our survey reveals an actual usage rate of 82.5 percent—a 17.3 pp gap. This underestimation appears consistent across educational contexts: Stöhr et al., (2024) find that only 38.7 percent of Swedish students believe AI chatbot use is common among peers, despite 63 percent actual usage. When we examine beliefs about policy-contingent behavior, students estimate that 62.7 percent of peers use AI when no explicit policy exists (Panel D), rising to 72.2 percent under permissive policies (Panel E) and falling to 43.5 percent under prohibition (Panel F). These beliefs align directionally with self-reported intentions, though with notable magnitude differences (Appendix Figure A5). For instance, while students believe 43.5 percent of their peers use AI in classes that prohibit it, only 28.4 percent report they themselves would be likely to do so.15\r\n\r\nNotably, students’ perceptions of peer AI adoption are closely linked to their own adoption behavior. Appendix Figure A6 shows a strong positive association between beliefs about peer usage and actual usage rates across student groups. This behavior is consistent with several psychological mechanisms, including the “false consensus effect” (Ross et al., , 1977), selection neglect (Jehiel, , 2018), or interpersonal projection bias (Bushong and Gagnon-Bartsch, , 2024). Still, virtually all groups systematically underestimate peer usage—actual usage rates exceed believed usage for every demographic and academic group we examine. This figure also shows that underestimation varies substantially across student characteristics: males use AI at 88.7 percent but believe only 66.6 percent of peers do so (22.1 pp gap), while females show a smaller gap (77.6 percent actual versus 65.1 percent believed, 13.3 pp gap).\r\n\r\nTaken together, students’ systematic underestimation of peer AI usage coupled with the strong relationship between beliefs and adoption suggests that misperceptions about social norms may shape technology diffusion. If AI enhances learning, then correcting these misperceptions through information provision could accelerate beneficial adoption; conversely, if AI undermines skill development, then students’ underestimation of peer usage may serve as an unintentional safeguard against harmful overadoption.\r\n\r\n7Discussion\r\nThis paper presents the first systematic evidence on generative AI adoption at a highly selective U.S. college. Using novel survey data, we document exceptionally rapid and widespread adoption, substantial shifts in the educational production function through augmentation and automation, and the significant roles of students’ beliefs and institutional policies in shaping AI use.\r\n\r\nOur results offer three implications for institutional policy and ongoing debates about AI in education. First, we identify low-cost opportunities to improve institutional policy effectiveness through targeted information provision. The significant gaps we document in students’ understanding of institutional AI policies, citation practices, and available AI resources suggest that simple interventions—such as clear guidelines, illustrative examples of acceptable uses, and AI literacy programs—can reduce unintentional academic integrity violations and support beneficial AI integration. Qualitative feedback strongly indicates student demand for more explicit guidance on responsible AI use.\r\n\r\nSecond, our evidence challenges alarmist narratives that conflate widespread AI adoption with universal academic dishonesty based on anecdotal accounts.16 Although AI use is indeed near-universal, we find clear evidence that students primarily employ AI as a tool for strategic task management: to enhance learning (augmentation) and selectively automate tasks when facing high time opportunity cost—not solely to circumvent academic effort. This distinction matters: by normalizing academic dishonesty as inevitable and universal, these narratives may shift social norms and encourage students who would otherwise use AI responsibly to engage in prohibited behaviors, believing “everyone else is doing it.”\r\n\r\nThird, our findings caution against policy extremes of either blanket prohibition or unrestricted AI use (Merchant, , 2024; McDonald et al., , 2025). Blanket prohibitions disproportionately harm students who benefit most from AI’s augmentation functions—particularly lower-achieving students—while also creating uneven compliance, placing conscientious students at a disadvantage relative to rule-breakers. Conversely, unrestricted AI use based solely on revealed preference arguments ignores important market failures in educational settings. Students often hold overly optimistic beliefs about AI’s learning benefits despite mixed empirical evidence, potentially leading to unintended negative learning outcomes. Most concerningly, permissive policies risk creating competitive dynamics where students feel compelled to adopt AI not for its learning benefits but simply to avoid falling behind in an educational “arms race” (Goehring et al., , 2024).\r\n\r\nFigures and Tables\r\nFigure 1:The Adoption of Generative AI among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the fraction of students who report using AI during the academic semester, categorized by demographic characteristics, high school type, academic cohort, GPA, and field of study. Usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily).\r\n\r\nThe category “All students” provides the baseline usage rate for the full sample. Gender categories are based on self-identification, with non-binary responses excluded due to a small sample size. “Private HS” refers to students who attended private high schools, while “Public HS” includes public institutions. “Cohort” denotes the student’s academic year, ranging from first-year (“Freshman”) to fourth-year and beyond (“Senior”). GPA categories (“GPA \r\n>\r\n p50” and “GPA \r\n<\r\n p50”) split students into groups above or below the median first-year GPA, as self-reported on a 4.0 scale. See Appendix B.1 for the classification of majors into fields of study.\r\n\r\nFigure 2:The Evolution of Generative AI Adoption among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the cumulative percent of students who reported using generative AI tools for academic purposes over time. The data is based on retrospective self-reports collected in our December 2024 survey, where students were asked “When did you first start using any form of Generative AI for academic purposes?” Response options ranged from “Before Spring 2023” to “This semester (Fall 2024).” The \r\nx\r\n-axis represents academic semesters, while the \r\ny\r\naxis represents the cumulative adoption rate. Vertical lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure 3:Adoption of Generative AI Models Among College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the adoption rates of various AI models as of Fall 2024. The horizontal axis shows the percent of students who reported using each tool, and the vertical axis lists the tools in descending order of adoption rates.\r\n\r\nFigure 4:Academic Uses of Generative AI\r\nPanel A. Across Common Academic Tasks\r\nRefer to caption\r\nPanel B. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of AI students who use generative AI for different academic tasks. For each task, usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). The number at the end of each bar represents the total percent of students who use AI for that purpose at any frequency. Tasks are ordered by total usage, from highest to lowest. Results are based on responses to the question: “For academic purposes, which of the following tasks do you typically use generative AI for?” Sample includes all students who reported using AI during the academic semester.\r\n\r\nFigure 5:Student Reported Likelihood of Using AI under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of students who report different likelihoods of using AI under various policy scenarios. For each policy, responses are categorized on a five-point scale from “Extremely unlikely to use AI” to “Extremely likely to use AI.” The sample includes all survey respondents. The question asked was: “How likely are you to use generative AI in a class with each of the following AI policies?”\r\n\r\nFigure 6:Understanding of Generative AI Policies and Resources\r\nPanel A. Understanding of AI Policies\r\nRefer to caption\r\nPanel B. Awareness of Copilot Access\r\nRefer to caption\r\nPanel C. Knowledge of Citation Requirements\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows students’ understanding of institutional AI policies and resources. Panel A displays the percent of students who report understanding AI policies in their classes, those who report having no explicit policy, and those who find policies unclear, broken down by demographic characteristics. Panel B shows the percent of students who are aware of their free access to Microsoft Copilot through Middlebury College. Panel C presents the percent of students who report knowing how to properly cite AI use in their academic work when required. For Panels B and C, horizontal lines represent 95 percent confidence intervals. Sample includes all survey respondents.\r\n\r\nFigure 7:Student Beliefs about the Impact of AI on their Academic Performance\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of Middlebury students who believe that AI improves, reduces, or has no effect on different aspects of their academic experience. For each outcome, responses are categorized into three groups: “Improves” combines “significantly improves” and “somewhat improves” responses, “Reduces” combines “significantly reduces” and “somewhat reduces” responses, and “No effect” represents neutral responses. Sample includes all students who report using AI during the academic semester. “Don’t know” responses are excluded.\r\n\r\nFigure 8:Relationship Between AI Adoption and Beliefs About AI’s Academic Benefits\r\nPanel A. Learning ability (e.g., ability to grasp concepts, learn new skills, etc.)\r\nRefer to caption\r\nPanel B. Understanding of\r\ncourse materials\r\nRefer to caption\r\nPanel C. Ability to complete\r\nassignments on time\r\nRefer to caption\r\nPanel D. Course grades\r\n \r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the relationship between AI adoption rates and beliefs that AI improves various academic outcomes across different student groups. Each panel plots the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis). Points represent different student groups categorized by demographics (circles), academic characteristics (triangles), and field of study (squares). The dashed line shows the linear fit across all groups. Groups with fewer than 10 students are excluded.\r\n\r\nFigure 9:Student Beliefs about Generative AI Usage at Middlebury College\r\nPanel A. For Schoolwork\r\nRefer to caption\r\nPanel B. For Leisure\r\nRefer to caption\r\nPanel C. For Any Purpose\r\nRefer to caption\r\nPanel D. Classes with No Policy\r\nRefer to caption\r\nPanel E. Classes that Allow AI\r\nRefer to caption\r\nPanel F. Classes that Disallow AI\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the distribution of students’ beliefs about generative AI usage among their peers at Middlebury College. Panels A-C display students’ estimates of the percent of their peers who regularly use AI for schoolwork, leisure activities, and any purpose, respectively. Panels D-F show students’ beliefs about AI usage in classes with different AI policies: those without an explicit policy (Panel D), those that allow AI use (Panel E), and those that prohibit AI use (Panel F). Each panel shows a histogram with bins of width ten percentage points (e.g., responses between 1-10 fall in the 10 bin, 11-20 in the 20 bin, etc.). The red dashed line indicates the mean response. Sample excludes respondents with missing values or who selected the default response for all six categories (which equals zero).\r\n\r\nTable 1:Summary Statistics of Survey Participants\r\nSurvey Sample\tAdmin records\r\nUnweighted\tWeighted\t\r\n(1)\t(2)\t(3)\r\nPanel A. Demographics\r\nMale\t0.446\t0.433\t0.463\r\nFemale\t0.508\t0.516\t0.533\r\nWhite\t0.618\t0.603\t0.538\r\nBlack\t0.036\t0.033\t0.052\r\nHispanic\t0.099\t0.106\t0.124\r\nAsian\t0.155\t0.162\t0.073\r\nPrivate high school\t0.420\t0.399\t–\r\nPublic high school\t0.543\t0.556\t–\r\nPanel B. Academic Characteristics    GPA 3.740 3.736 3.670 Hours spent on academics per week 17.899 17.889 – Freshman 0.311 0.355 0.255 Sophomore 0.273 0.272 0.257 Junior 0.202 0.179 0.201 Senior 0.213 0.194 0.287   Panel C. Field of Study    Arts 0.011 0.021 0.023 Humanities 0.052 0.068 0.073 Languages 0.021 0.020 0.025 Literature 0.035 0.025 0.025 Natural Sciences 0.218 0.249 0.244 Social Sciences 0.353 0.247 0.243 Has not declared major 0.311 0.371 0.364\r\n\r\nN (# degrees) 43 43 49 N (# students) 634 2,760 2,760\r\n\r\n\\justify\r\nNotes: This table presents summary statistics from our survey of college students. Panel A reports demographic characteristics, including the proportion of participants identifying as male, female, white, Black, Hispanic, Asian, or who attended a private or public high school. Panel B provides academic characteristics, such as GPA (only available for non-freshmen), average weekly hours spent on academics, and academic year distribution (Freshman, Sophomore, Junior, and Senior). Note that in column 1–2, GPA refers to self-reported first-year GPA while in column 3 it is the overall GPA during Spring 2024. Panel C summarizes the distribution of participants across different fields of study. Major groups are mutually exclusive.\r\n\r\n0.8250.5890.3680.106 R\r\n−\r\nsquared0.0770.0890.1160.058 N (Students)616616616616 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI usage frequency threshold and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column uses a different threshold for AI usage frequency, categorized as: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). Column 1 defines usage as any nonzero frequency; column 2 includes at least occasional use; column 3 includes frequent or higher use; and column 4 captures only very frequent use.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.0890.1990.3570.5510.801 R\r\n−\r\nsquared0.0450.0610.0880.0800.069 N (Students)633633633633633 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI adoption date and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column presents results for a different threshold of AI adoption. Column 1 shows the probability of adopting AI before Spring 2023; column 2 by Spring 2023; column 3 by Fall 2023; column 4 by Spring 2024; and column 5 by Fall 2024. The dependent variable in each regression is a binary indicator equal to one if the student had adopted AI by the specified time period.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.9120.6122.2850.9130.4191.8010.1930.484 R\r\n−\r\nsquared0.0360.0730.1260.0530.1170.1290.0420.063 N (Students)515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and their use of generative AI for academic tasks. In columns 1 and 4, the outcome is a dummy that equals one if a student reports using AI with any frequency for at least one augmentation or automation task, respectively. In columns 2 and 5, the outcome is the share of tasks within each category for which the student reports any use. In columns 3 and 6, the outcome is a continuous measure capturing average usage frequency for each task category, based on raw Likert-style responses. In columns 7 and 8, the outcome is the difference in average task share and usage intensity between augmentation and automation, respectively. Regressions are weighted and report robust standard errors clustered at the student level. ∗∗∗ \r\np\r\n<\r\n0.01\r\n, ∗∗ \r\np\r\n<\r\n0.05\r\n, ∗ \r\np\r\n<\r\n0.10\r\n.\r\n\r\nReferences\r\nAcemoglu, (2025)\r\nAcemoglu, D. (2025).Nobel lecture: Institutions, technology, and prosperity.American Economic Review, 115(6):1709–1748.\r\nAngrist et al., (2009)\r\nAngrist, J., Lang, D., and Oreopoulos, P. (2009).Incentives and services for college achievement: Evidence from a randomized trial.American Economic Journal: Applied Economics, 1(1):136–163.\r\nAutor and Thompson, (2025)\r\nAutor, D. and Thompson, N. (2025).Expertise.\r\nBailey et al., (2022)\r\nBailey, M., Johnston, D., Kuchler, T., Stroebel, J., and Wong, A. (2022).Peer effects in product adoption.American Economic Journal: Applied Economics, 14(3):488–526.\r\nBastani et al., (2025)\r\nBastani, H., Bastani, O., Sungu, A., Ge, H., Kabakcı, Ö., and Mariman, R. (2025).Generative AI without guardrails can harm learning: Evidence from high school mathematics.Proceedings of the National Academy of Sciences, 122(26):e2422633122.\r\nBeaman et al., (2021)\r\nBeaman, L., BenYishay, A., Magruder, J., and Mobarak, A. M. (2021).Can network theory-based targeting increase technology adoption?American Economic Review, 111(6):1918–1943.\r\nBick et al., (2025)\r\nBick, A., Blandin, A., and Deming, D. J. (2025).The rapid adoption of generative AI.Technical report, National Bureau of Economic Research.\r\nBonney et al., (2024)\r\nBonney, K., Breaux, C., Buffington, C., Dinlersoz, E., Foster, L. S., Goldschlag, N., Haltiwanger, J. C., Kroff, Z., and Savage, K. (2024).Tracking Firm Use of AI in Real Time: A Snapshot from the Business Trends and Outlook Survey.\r\nBonsu and Baffour-Koduah, (2023)\r\nBonsu, E. M. and Baffour-Koduah, D. (2023).From the consumers’ side: Determining students’ perception and intention to use chatgpt in ghanaian higher education.Journal of Education, Society & Multiculturalism, 4(1):1–29.\r\nBresnahan and Trajtenberg, (1995)\r\nBresnahan, T. F. and Trajtenberg, M. (1995).General purpose technologies ‘engines of growth’?Journal of econometrics, 65(1):83–108.\r\nBrynjolfsson and Hitt, (2000)\r\nBrynjolfsson, E. and Hitt, L. M. (2000).Beyond computation: Information technology, organizational transformation and business performance.Journal of Economic Perspectives, 14(4):23–48.\r\nBrynjolfsson et al., (2025)\r\nBrynjolfsson, E., Li, D., and Raymond, L. (2025).Generative AI at Work.The Quarterly Journal of Economics, 140(2):889–942.\r\nBushong and Gagnon-Bartsch, (2024)\r\nBushong, B. and Gagnon-Bartsch, T. (2024).Failures in forecasting: An experiment on interpersonal projection bias.Management Science, 70(12):8735–8752.\r\nCarvajal et al., (2024)\r\nCarvajal, D., Franco, C., and Isaksson, S. (2024).Will artificial intelligence get in the way of achieving gender equality?Discussion Paper 03, NHH Norwegian School of Economics.\r\n(15)\r\nComin, D. and Hobijn, B. (2010a).An exploration of technology diffusion.American Economic Review, 100(5):2031–2059.\r\n(16)\r\nComin, D. and Hobijn, B. (2010b).Technology Diffusion and Postwar Growth, volume 25, pages 209–246.University of Chicago Press.\r\nCui et al., (2024)\r\nCui, Z. K., Demirer, M., Jaffe, S., Musolff, L., Peng, S., and Salz, T. (2024).The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers.\r\nDavid, (1990)\r\nDavid, P. A. (1990).The dynamo and the computer: an historical perspective on the modern productivity paradox.The American economic review, 80(2):355–361.\r\nDell’Acqua et al., (2023)\r\nDell’Acqua, F., McFowland, E., Mollick, E. R., Lifshitz-Assaf, H., Kellogg, K., Rajendran, S., Krayer, L., Candelon, F., and Lakhani, K. R. (2023).Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality.\r\nDuflo et al., (2011)\r\nDuflo, E., Kremer, M., and Robinson, J. (2011).Nudging Farmers to Use Fertilizer: Theory and Experimental Evidence from Kenya.American Economic Review, 101(6):2350–2390.\r\nEloundou et al., (2024)\r\nEloundou, T., Manning, S., Mishkin, P., and Rock, D. (2024).GPTs are GPTs: Labor market impact potential of LLMs.Science, 384(6702):1306–1308.\r\nFairlie et al., (2014)\r\nFairlie, R. W., Hoffmann, F., and Oreopoulos, P. (2014).A community college instructor like me: Race and ethnicity interactions in the classroom.American Economic Review, 104(8):2567–2591.\r\nFelten et al., (2021)\r\nFelten, E. W., Raj, M., and Seamans, R. (2021).Occupational, Industry, and Geographic Exposure to Artificial Intelligence: A Novel Dataset and Its Potential Uses.\r\nFelten et al., (2023)\r\nFelten, E. W., Raj, M., and Seamans, R. (2023).How will Language Modelers like ChatGPT Affect Occupations and Industries?\r\nFoster and Rosenzweig, (1995)\r\nFoster, A. D. and Rosenzweig, M. R. (1995).Learning by doing and learning from others: Human capital and technical change in agriculture.Journal of political Economy, 103(6):1176–1209.\r\nFoster and Rosenzweig, (2010)\r\nFoster, A. D. and Rosenzweig, M. R. (2010).Microeconomics of technology adoption.Annu. Rev. Econ., 2(1):395–424.\r\nGallup, (2024)\r\nGallup (2024).AI in the Workplace: Answering 3 Big Questions.\r\nGiaccherini et al., (2019)\r\nGiaccherini, M., Herberich, D. H., Jimenez-Gomez, D., List, J. A., Ponti, G., and Price, M. K. (2019).The behavioralist goes door-to-door: Understanding household technological diffusion using a theory-driven natural field experiment.Technical report, National Bureau of Economic Research.\r\nGoehring et al., (2024)\r\nGoehring, G., Mezzanotti, F., and Ravid, S. A. A. (2024).Technology adoption and career concerns: Evidence from the adoption of digital technology in motion pictures.Technical report, National Bureau of Economic Research.\r\nGriliches, (1957)\r\nGriliches, Z. (1957).Hybrid corn: An exploration in the economics of technological change.Econometrica, 25(4):501–522.\r\nHall and Khan, (2003)\r\nHall, B. H. and Khan, B. (2003).Adoption of new technology.\r\n(32)\r\nHanda, K., Bent, D., Tamkin, A., McCain, M., Durmus, E., Stern, M., Schiraldi, M., Huang, S., Ritchie, S., Syverud, S., Jagadish, K., Vo, M., Bell, M., and Ganguli, D. (2025a).Anthropic education report: How university students use claude.\r\n(33)\r\nHanda, K., Tamkin, A., McCain, M., Huang, S., Durmus, E., Heck, S., Mueller, J., Hong, J., Ritchie, S., Belonax, T., et al. (2025b).Which economic tasks are performed with ai? evidence from millions of claude conversations.arXiv preprint arXiv:2503.04761.\r\nHanna et al., (2014)\r\nHanna, R., Mullainathan, S., and Schwartzstein, J. (2014).Learning Through Noticing: Theory and Evidence from a Field Experiment.The Quarterly Journal of Economics, 129(3):1311–1353.\r\nHartley et al., (2025)\r\nHartley, J., Jolevski, F., Melo, V., and Moore, B. (2025).The Labor Market Effects of Generative Artificial Intelligence.\r\nHumlum and Vestergaard, (2025)\r\nHumlum, A. and Vestergaard, E. (2025).Large Language Models, Small Labor Market Effects.\r\nHutto and Gilbert, (2014)\r\nHutto, C. and Gilbert, E. (2014).Vader: A parsimonious rule-based model for sentiment analysis of social media text.In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216–225.\r\nJehiel, (2018)\r\nJehiel, P. (2018).Investment strategy and selection bias: An equilibrium perspective on overoptimism.American Economic Review, 108(6):1582–1597.\r\nKelly et al., (2023)\r\nKelly, A., Sullivan, M., and Strampel, K. (2023).Generative artificial intelligence: University student awareness, experience, and confidence in use across disciplines.Journal of University Teaching and Learning Practice, 20(6):1–16.\r\nKharazian, (2025)\r\nKharazian, A. (2025).Ramp ai index: Monthly measurement of ai adoption by american businesses.Technical report, Ramp Economics Lab.\r\nLehmann et al., (2024)\r\nLehmann, M., Cornelius, P. B., and Sting, F. J. (2024).AI Meets the Classroom: When Does ChatGPT Harm Learning?\r\nLing and Imas, (2025)\r\nLing, Y. and Imas, A. (2025).Underreporting of AI use: The role of social desirability bias.Available at SSRN.\r\nLiu and Wang, (2024)\r\nLiu, Y. and Wang, H. (2024).Who on earth is using generative ai?Policy Research Working Paper WPS10870, World Bank Group, Washington, D.C.\r\nMcClain, (2024)\r\nMcClain, C. (2024).Americans’ use of ChatGPT is ticking up, but few trust its election information.\r\nMcDonald et al., (2025)\r\nMcDonald, N., Johri, A., Ali, A., and Collier, A. H. (2025).Generative artificial intelligence in higher education: Evidence from an analysis of institutional policies and guidelines.Computers in Human Behavior: Artificial Humans, 3:100121.\r\nMcElheran et al., (2023)\r\nMcElheran, K., Li, J. F., Brynjolfsson, E., Kroff, Z., Dinlersoz, E., Foster, L. S., and Zolas, N. (2023).AI Adoption in America: Who, What, and Where.\r\nMerchant, (2024)\r\nMerchant, B. (2024).The new luddites aren’t backing down.The Atlantic.Accessed: 2025-06-16.\r\nNam, (2023)\r\nNam, J. (2023).56% of college students have used ai on assignments or exams.Technical report.Edited by Lyss Welding. Accessed June 11, 2025.\r\nNolan, (2023)\r\nNolan, B. (2023).Here are the schools and colleges that have banned chatgpt.Business Insider.Accessed: 2025-01-27.\r\nNoy and Zhang, (2023)\r\nNoy, S. and Zhang, W. (2023).Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence.\r\nOtis et al., (2024)\r\nOtis, N. G., Delecourt, S., Cranney, K., and Koning, R. (2024).Global Evidence on Gender Gaps and Generative AI.Harvard Business School.\r\nPeng et al., (2023)\r\nPeng, S., Kalliamvakou, E., Cihon, P., and Demirer, M. (2023).The Impact of AI on Developer Productivity: Evidence from GitHub Copilot.\r\nRavšelj et al., (2025)\r\nRavšelj, D., Keržič, D., Tomaževič, N., Umek, L., Brezovar, N., A. Iahad, N., Abdulla, A. A., Akopyan, A., Aldana Segura, M. W., AlHumaid, J., Allam, M. F., Alló, M., Andoh, R. P. K., Andronic, O., Arthur, Y. D., Aydın, F., Badran, A., Balbontín-Alvarado, R., Ben Saad, H., Bencsik, A., Benning, I., Besimi, A., Bezerra, D. D. S., Buizza, C., Burro, R., Bwalya, A., Cachero, C., Castillo-Briceno, P., Castro, H., Chai, C. S., Charalambous, C., Chiu, T. K. F., Clipa, O., Colombari, R., Corral Escobedo, L. J. H., Costa, E., Cre\\textcommabelowtulescu, R. G., Crispino, M., Cucari, N., Dalton, F., Demir Kaya, M., Dumić-Čule, I., Dwidienawati, D., Ebardo, R., Egbenya, D. L., Faris, M. E., Fečko, M., Ferrinho, P., Florea, A., Fong, C. Y., Francis, Z., Ghilardi, A., González-Fernández, B., Hau, D., Hossain, M. S., Hug, T., Inasius, F., Ismail, M. J., Jahić, H., Jessa, M. O., Kapanadze, M., Kar, S. K., Kateeb, E. T., Kaya, F., Khadri, H. O., Kikuchi, M., Kobets, V. M., Kostova, K. M., Krasmane, E., Lau, J., Law, W. H. C., Lazăr, F., Lazović-Pita, L., Lee, V. W. Y., Li, J., López-Aguilar, D. V., Luca, A., Luciano, R. G., Machin-Mastromatteo, J. D., Madi, M., Manguele, A. L., Manrique, R. F., Mapulanga, T., Marimon, F., Marinova, G. I., Mas-Machuca, M., Mejía-Rodríguez, O., Meletiou-Mavrotheris, M., Méndez-Prado, S. M., Meza-Cano, J. M., Mirķe, E., Mishra, A., Mital, O., Mollica, C., Morariu, D. I., Mospan, N., Mukuka, A., Navarro Jiménez, S. G., Nikaj, I., Nisheva, M. M., Nisiforou, E., Njiku, J., Nomnian, S., Nuredini-Mehmedi, L., Nyamekye, E., Obadić, A., Okela, A. H., Olenik-Shemesh, D., Ostoj, I., Peralta-Rizzo, K. J., Peštek, A., Pilav-Velić, A., Pires, D. R. M., Rabin, E., Raccanello, D., Ramie, A., Rashid, M. M. U., Reuter, R. A. P., Reyes, V., Rodrigues, A. S., Rodway, P., Ručinská, S., Sadzaglishvili, S., Salem, A. A. M. S., Savić, G., Schepman, A., Shahpo, S. M., Snouber, A., Soler, E., Sonyel, B., Stefanova, E., Stone, A., Strzelecki, A., Tanaka, T., Tapia Cortes, C., Teira-Fachado, A., Tilga, H., Titko, J., Tolmach, M., Turmudi, D., Varela-Candamio, L., Vekiri, I., Vicentini, G., Woyo, E., Yorulmaz, Ö., Yunus, S. A. S., Zamfir, A.-M., Zhou, M., and Aristovnik, A. (2025).Higher education students’ perceptions of ChatGPT: A global study of early reactions.PLOS ONE, 20(2):e0315011.\r\nRogers, (1962)\r\nRogers, E. M. (1962).Diffusion of Innovations.Free Press, New York.\r\nRoss et al., (1977)\r\nRoss, L., Greene, D., and House, P. (1977).The “false consensus effect”: An egocentric bias in social perception and attribution processes.Journal of experimental social psychology, 13(3):279–301.\r\nSacerdote, (2001)\r\nSacerdote, B. (2001).Peer Effects with Random Assignment: Results for Dartmouth Roommates.The Quarterly Journal of Economics, 116(2):681–704.\r\nStinebrickner and Stinebrickner, (2008)\r\nStinebrickner, R. and Stinebrickner, T. R. (2008).The causal effect of studying on academic performance.The BE Journal of Economic Analysis & Policy, 8(1).\r\nStöhr et al., (2024)\r\nStöhr, C., Ou, A. W., and Malmström, H. (2024).Perceptions and usage of AI chatbots among students in higher education across genders, academic levels and fields of study.Computers and Education: Artificial Intelligence, 7:100259.\r\nStokey, (2021)\r\nStokey, N. L. (2021).Technology diffusion.Review of Economic Dynamics, 42:15–36.\r\nWorld Bank, (2016)\r\nWorld Bank (2016).World development report 2016: Digital dividends.World Bank Publications.\r\nWu et al., (2022)\r\nWu, M.-J., Zhao, K., and Fils-Aime, F. (2022).Response rates of online surveys in published research: A meta-analysis.Computers in Human Behavior Reports, 7.\r\nXiao et al., (2023)\r\nXiao, P., Chen, Y., and Bao, W. (2023).Waiting, banning, and embracing: An empirical analysis of adapting policies for generative ai in higher education.arXiv preprint arXiv:2305.18617.\r\nAppendix\r\n\r\nAppendix AAppendix Figures and Tables\r\nFigure A1:Generative AI Usage Survey Design Overview\r\nKey\t\r\nSurvey Structure\r\nSurvey Sections\r\nUsage Data Collection\r\nBeliefs & Perceptions\r\nAI Usage Survey at Middlebury College\r\nDecember 2024\r\nParticipant Recruitment\r\nCampus-wide email invitations, framed as general technology use survey\r\nParticipation Incentives\r\nAmazon gift card lottery ranging from $50-$500\r\nSection 1: Demographics & Academic Information\r\nStudent Characteristics\r\nGender, race/ethnicity, high school type\r\nAcademic Profile\r\nYear, major, self-reported GPA, study hours\r\nSection 2: generative AI Usage Patterns\r\nAdoption Metrics\r\nFrequency, timing of first use, specific AI models used\r\nAcademic Applications\r\nTasks performed with AI, payment for premium services\r\nSection 3: Perceptions & Institutional Policies\r\nImpact Perceptions\r\nEffects on learning, grades, time management\r\nPolicy Responses\r\nUsage likelihood under different policies\r\nPeer Usage Beliefs\r\nEstimates of AI adoption among peers for schoolwork and leisure\r\nOpen-Ended Response Collection\r\nMotivations for AI use, policy feedback, suggestions for improvement\r\nNote: This figure illustrates the structure of the AI usage survey conducted at Middlebury College in December 2024. The survey collected information across three main sections: (1) demographic and academic background, (2) patterns of generative AI usage including adoption timing, frequency, and specific applications, and (3) perceptions of AI’s impact on learning and responses to institutional policies.\r\n\r\nFigure A2:Cumulative Generative AI Use by Student Characteristic\r\nPanel A. By Gender\r\nRefer to caption\r\nPanel B. By Race\r\nRefer to caption\r\nPanel C. By High School Type\r\nRefer to caption\r\nPanel D. By Freshman Year GPA\r\nRefer to caption\r\nPanel E. By Cohort\r\nRefer to caption\r\nPanel F. By Field of Study\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents cumulative AI use based on different student characteristics. Each panel displays the cumulative distribution of AI use based on a specific characteristic: gender, race, school type, first-year GPA, cohort, or field of study. The cumulative percent of students is plotted against usage categories. The legends and colors correspond to subgroups within each demographic variable.\r\n\r\nFigure A3:Percent of Students Who Pay for Generative AI Tools\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of AI users who pay for AI tools (through any platform) across different demographic groups. Horizontal lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure A4:Academic Uses of ChatGPT: Evidence from Global Survey\r\nPanel A. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\nPanel B. Across the College Quality Distribution\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of students who use ChatGPT for different academic tasks based on data from Ravšelj et al., (2025). Panel A displays usage patterns across tasks categorized as augmenting (proofreading, translating, study assistance, research assistance) versus automating (academic writing, professional writing, creative writing, brainstorming, summarizing, calculating, coding assistance, personal assistance) student effort. Panel B shows usage patterns across university quality quintiles based on World University Rankings, with universities ranked in the top 20% (top quintile) showing slightly higher rates of augmentation relative to automation compared to bottom quintile institutions. The analysis includes universities with at least 30 student responses and excludes observations with missing usage data.\r\n\r\nFigure A5:Student Beliefs about AI Usage versus Actual Usage under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure compares students’ beliefs about AI usage with actual usage rates under different AI policies. For each policy type, blue bars show the percent of students who report being “Neutral,” “Likely,” or “Very Likely” to use AI, while red bars show students’ mean beliefs about what percent of their peers use AI in classes with that policy. The sample includes all survey respondents for both actual usage and beliefs measures. “AI Use is Unrestricted” refers to classes with no restrictions on AI use, “No Explicit AI Policy” refers to classes without a stated policy, and “AI Use is Prohibited” refers to classes that ban AI use entirely.\r\n\r\nFigure A6:Relationship Between Beliefs About AI Usage and Actual AI Usage\r\nRefer to caption\\justify\r\nNotes: This figure shows the relationship between students’ beliefs about AI usage among their peers and actual AI usage rates across different demographic groups. Each point represents a different group of students (by demographics, academic characteristics, or field of study). The \r\ny\r\n-axis shows the percent of students in each group who report using AI for academic purposes. The \r\nx\r\n-axis shows the mean belief within each group about what percent of Middlebury students use AI. The dashed line shows the linear fit. Sample includes all survey respondents with at least ten observations per group. Students’ beliefs about AI usage are positively correlated with actual usage patterns, suggesting that students have relatively accurate perceptions of AI adoption among their peers.\r\n\r\nGPA > p50 80.3 26.3 17.4 23.2 13.4 GPA < p50 87.1 18.7 26.3 32.2 9.9 Freshman 81.3 24.3 24.1 24.7 8.1 Sophomore 84.4 25.6 22.6 23.7 12.4 Junior 86.2 21.6 23.1 28.6 12.8 Senior 78.7 21.0 17.4 29.9 10.4\r\n\r\nPanel C. Field of Study    Arts 73.3 21.0 0.0 52.3 0.0 Humanities 74.8 39.3 21.8 12.1 1.7 Languages 57.4 27.0 30.3 0.0 0.0 Literature 48.6 10.0 33.7 5.0 0.0 Natural Sciences 91.1 22.0 21.9 33.8 13.3 Social Sciences 84.6 17.9 20.4 29.1 17.2\r\n\r\n\\justify\r\nNotes: This table presents the percent of students in each demographic group who report using AI at different frequencies during the academic semester. Each cell shows the percent of students within that group. Column 1 reports the total percent who use AI at any frequency. Columns 2 to 5 represent increasing usage frequencies: rarely (1–2 times per semester), occasionally (monthly), frequently (weekly), and very frequently (multiple times per week). Panel A reports percentages by demographic characteristics. Panel B shows percentages by academic characteristics. Panel C presents percentages by field of study.\r\n\r\n0.9730.1350.0770.1100.114 R\r\n−\r\nsquared0.0120.0510.0270.0240.138 N (Students)516516516516516 \\justifyNotes: This table assesses the relationship between AI model adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI model usage (columns 1-4) or payment for AI services (column 5), and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division.\r\n\r\nEach column presents results for a different model or payment outcome. Column 1 shows usage of OpenAI’s ChatGPT, column 2 Google Gemini, column 3 Microsoft Copilot, column 4 any other AI model, and column 5 whether the student pays for any generative AI service.\r\n\r\nThe omitted categories are Natural Sciences for academic division, white students for race/ethnicity, and freshmen for cohort. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.7510.6760.6610.2710.479 R\r\n−\r\nsquared0.0870.1130.0760.0680.041 N (Students)599599599599599 \\justifyNotes: This table examines how student characteristics relate to self-reported likelihood of using generative AI under different policy scenarios. Each column presents results for different policy scenarios. In columns 1-4, the dependent variable equals one if the student reports being“neutral,” “likely” or “extremely likely” to use AI under the specified policy, and zero if they report being “unlikely,” or “extremely unlikely.” Column 5 represents the impact of moving from unrestricted use to complete prohibition.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.5760.6850.3760.4890.564 R\r\n−\r\nsquared0.1020.1190.0840.0370.087 N (Students)561565543560560 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. Each column presents results for beliefs about different academic outcomes: learning ability (e.g., ability to grasp concepts, retain information, or learn new skills) in column 1, understanding of course materials in column 2, course grades in column 3, ability to complete assignments on time in column 4, and time spent on academics in column 5. The dependent variable in each regression equals one if the student believes AI somewhat improves” or significantly improves” the outcome, and zero if they believe it has no effect, reduces, or significantly reduces the outcome. “Don’t know” responses are excluded.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\nAppendix BEmpirical Appendix\r\nB.1Field of Study Classifications\r\nThis appendix details the classification of majors into broad fields of study used in Figure 1:\r\n\r\n• Natural Sciences: Includes Biology, Biochemistry, Chemistry, Computer Science, Earth and Climate Sciences/Geology, Environmental Studies, Mathematics, Molecular Biology & Biochemistry, Neuroscience, Physics, and Statistics.\r\n• Social Sciences: Includes Anthropology, Economics, Education, Geography, International & Global Studies, International Politics & Economics, Political Science, Psychology, and Sociology.\r\n• Humanities: Includes American Studies, Black Studies, Classics, History, Architectural Studies, Art History & Museum Studies, History of Art & Architecture, Philosophy, Religion, and Classical Studies.\r\n• Literature: Includes Comparative Literature, English/English & American Literatures, and Literary Studies.\r\n• Languages: Includes Arabic, Chinese, French & Francophone Studies, German, Japanese Studies, Russian, and Spanish.\r\n• Arts: Includes Film & Media Culture, Music, Studio Art, and Theatre.\r\nB.2Task-Specific Use of Generative AI\r\nThe aggregate patterns documented in Section 4 may mask important heterogeneity across student groups. Different students may adopt AI tools at varying rates depending on their academic needs and field-specific norms. To investigate this heterogeneity systematically, Appendix Table LABEL:tab:ai_purpose_correlates presents regression estimates examining how student characteristics predict AI usage for each of our ten measured academic tasks.\r\n\r\nUsage patterns differ markedly by student characteristics. Male students consistently exhibit higher adoption rates across most applications, with particularly pronounced differences for finding information (16.4 pp; \r\np\r\n<\r\n0.01\r\n), summarizing texts (12.5 pp; \r\np\r\n<\r\n0.01\r\n), and creating images (16.9 pp; \r\np\r\n<\r\n0.01\r\n). Black students show substantially higher usage for information-gathering tasks, being 23.8 pp more likely to use AI for finding information compared to white students (\r\np\r\n<\r\n0.05\r\n). They also exhibit dramatically higher adoption of writing assistance tools, with 27.6 pp higher usage for editing text (\r\np\r\n<\r\n0.05\r\n) and 23.6 pp for writing emails (\r\np\r\n<\r\n0.10\r\n). Latino students demonstrate significantly higher usage for generating ideas (24.0 pp; \r\np\r\n<\r\n0.01\r\n) and writing emails (20.7 pp; \r\np\r\n<\r\n0.01\r\n), while Asian students show markedly higher adoption for writing emails (24.5 pp; \r\np\r\n<\r\n0.01\r\n) and explaining concepts (10.9 pp, \r\np\r\n<\r\n0.05\r\n). Students from public high schools report lower usage rates for several applications, particularly for concept explanation (7.3 pp lower; \r\np\r\n<\r\n0.10\r\n) and writing emails (7.4 pp lower; \r\np\r\n<\r\n0.10\r\n), but higher usage for proofreading (8.7 pp higher; \r\np\r\n<\r\n0.10\r\n).\r\n\r\nField of study emerges as a particularly strong predictor of usage patterns. Arts majors show significantly lower adoption across multiple tasks compared to Natural Science students, with gaps of 49.7 pp for finding information (\r\np\r\n<\r\n0.01\r\n), 44.0 pp for generating ideas (\r\np\r\n<\r\n0.05\r\n), and 62.1 pp for coding assistance (\r\np\r\n<\r\n0.01\r\n). Similarly, humanities majors are 36.8 pp less likely to use AI for generating ideas (\r\np\r\n<\r\n0.01\r\n) and 30.8 pp less likely for coding assistance (\r\np\r\n<\r\n0.01\r\n). Languages majors exhibit the most pronounced differences, being 52.4 pp less likely to use AI for summarizing texts (\r\np\r\n<\r\n0.01\r\n) and 46.9 pp less likely for generating ideas (\r\np\r\n<\r\n0.01\r\n). By contrast, social science students show higher usage for explaining concepts (8.0 pp; \r\np\r\n<\r\n0.10\r\n) and summarizing texts (14.7 pp; \r\np\r\n<\r\n0.01\r\n).\r\n\r\n0.8030.7400.6310.6190.5410.4730.3710.3440.2350.204 R\r\n−\r\nsquared0.0780.0870.0770.1160.0280.0540.0830.1350.0400.094 N (Students)515515515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and use of AI for specific academic tasks. Each column shows the result for a different academic task. The omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Regressions are weighted and use heteroskedasticity-robust standard errors clustered at the student level. ∗∗∗, ∗∗, and ∗ indicate significance at the 1%, 5%, and 10% levels, respectively.\r\n\r\nAppendix CQualitative Evidence on Student Perspectives on AI Use\r\nIn this section, we analyze student responses to an open-ended question about their use of generative AI. The survey question asked: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” This was an optional question that 48.3 percent of survey respondents answered. Appendix Figure C1 presents a word cloud of the most frequent words in student responses.\r\n\r\nC.1Validating the Open-Ended Response Measure\r\nWe begin by validating our open-ended response measure to show that it contains meaningful signal. To validate this measure, we employ sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner), a widely-used lexicon and rule-based sentiment analysis tool (Hutto and Gilbert, , 2014). We calculate sentiment scores for each response and examine whether these scores correlate with students’ actual AI adoption behaviors. Intuitively, students who express more positive sentiment toward AI in their open-ended responses should be more likely to have adopted AI tools in practice. We test this hypothesis by examining two measures of AI adoption: whether students have ever used generative AI and whether they currently use AI for academic purposes. Appendix Figure C2 presents binned scatterplots showing the relationship between AI adoption rates and standardized sentiment scores.\r\n\r\nSentiment towards generative AI strongly predicts AI adoption. For both outcomes, students with negative sentiment scores (below zero) show substantially lower adoption rates—around 70-85 percent for ever using AI and 70-75 percent for academic use. In contrast, students with positive sentiment scores display markedly higher adoption rates, reaching nearly 100 percent for general use and 95-100 percent for academic purposes among those with the most positive sentiment. The relationship is particularly pronounced for academic AI use (Panel B), where the coefficient of \r\nβ\r\n^\r\n=\r\n0.168\r\n is twice as large as for general adoption.17 These systematic patterns confirm that our open-ended responses capture meaningful variation in student attitudes that corresponds to real behavioral differences.\r\n\r\nC.2How Students use Generative AI\r\nTo analyze these responses systematically, we classified each response using keywords based on their content. For example, if a student mentioned using AI to save time, we tagged the response with the keyword “time-saver.” If a student expressed concerns about AI’s impact on learning, we tagged it with “negative learning.” Responses could receive multiple keywords if they discussed several themes. Appendix Figure C3 shows the frequency of keywords in our classification. The responses reveal how students use AI tools, what motivates this use, and what causes some to avoid or limit their use.\r\n\r\nThe most common use of AI is as an explanatory tool. Nearly 30 percent of responses mentioned using AI to understand course material. Students frequently ask AI to break down complex concepts from readings and lectures, particularly when they find the material difficult to understand. For example, one student reported: “I can ask AI to explain concepts to me that I have a hard time grasping. […] I can keep asking ’simplify’ or ’break down even more.”’ Students also use AI to summarize dense academic readings, which they argue helps them manage heavy reading loads.\r\n\r\nStudents employ AI throughout different stages of the writing process. Some use AI to generate initial drafts that serve as starting points. One student explained: “Helps me get started with a base for most of my essays. It feels easier to edit something already written and make it my own than to write from scratch.” Others use AI more narrowly for brainstorming when stuck on specific problems. As one student noted: “I use it if I am feeling stuck to push me to the right direction (whether a mathematical problem or an essay idea).” Many also report using AI as an editing tool to improve grammar, sentence structure, and overall writing flow. This is especially the case for non-native English speakers. As one student explained: “English is not my first language and it frustrates me sometimes that I cannot find the best way to phrase a certain idea and AI is a useful tool to have to find alternate expressions.”\r\n\r\nStudents frequently mentioned using AI for specific academic tasks. In courses that require coding, students often use AI for debugging code and understanding programming concepts. Students also employ AI for administrative tasks like formatting citations and drafting routine emails. Finally, many students use AI as an enhanced search engine. One student reported: “It has significantly reduced the time it takes to conduct research on new topics and ideas, and helps me by giving me a thorough selection of sources to use for projects of any kind.”\r\n\r\nC.3Why Students Adopt AI\r\nTime savings was the most commonly cited reason for using AI. Nearly 30 percent of responses mentioned using AI to complete work more efficiently. Students often viewed AI as a way to manage demanding course loads. Many students particularly embrace AI assistance for tasks viewed as mechanical or administrative. A student noted they use AI for “Writing emails quick and creating resume/ cover letter templates.” But AI assistance goes beyond grunt work. Some students use it to “spend less time doing assignments and homework.” This is particularly true if students don’t view the work as central to their academic experience. As one student explained: “when I come across work I deem as ineffective, I want to spend as little time as possible doing it.”\r\n\r\nHaving an on-demand tutor for academic support was another key motivation. One-quarter of responses described using AI as an “explainer” when other resources were unavailable or inconvenient. As one student noted, “I use it as a last resort (if there are no office hours, after looking up videos, etc.) if I need extra help. I’d like to think that the way I use it is similar to going to office hours or TA hours.”\r\n\r\nPeer influence also drove AI adoption. Some students reported feeling pressure to use AI to remain competitive with their classmates. One student explained: “I noticed others use it, are getting better grades than me, and they say they learn better with the help of AI, so I gave it a try.” Others worried about being at a competitive disadvantage: “Other people were using it and told me about it. I felt like I would be at a disadvantage if I wasn’t also using it.”\r\n\r\nC.4Concerns and Limitations\r\nStudents expressed several concerns about AI use in academic work. The most frequent worry was about negative impacts on learning. One student who initially used AI extensively reported: “In the past, I have simply plugged and chugged homework assignments into ChatGPT and submitted it. Those assignments feedback from teachers was positive and I was getting good grades, but I definitely felt that my own learning outcomes to be significantly worse.” Other students viewed AI use as fundamentally incompatible with their educational goals. As one humanities student explained: “my task is as a humanities student is to think, not calculate; why should I let AI do the thinking for me? It would defeat the purpose of pursuing my education.”\r\n\r\nMany students described ethical concerns about AI use. Responses suggested uncertainty about appropriate boundaries. One student noted: “I never use it to explicitly write something because that feels like overt cheating, but sometimes I hesitate when it completely solves Econ problems. I understand how it does it, and it helps me to learn, but it still sometimes feels a little morally gray.” Another expressed similar ambivalence: “I tend to only use it when [it] will save me time in a moral way.”\r\n\r\nStudents also emphasized the importance of maintaining ownership of their work. Many expressed pride in producing original work and hesitation about diluting that ownership through AI use. As one student explained: “I don’t have interest in using generative AI for my academic work because I want my work to reflect my own ideas.” Another noted: “It usually would not even occur to me to turn to AI to substitute writing because I want to take credit for my work, and using AI seems to diminish that.”\r\n\r\nTechnical limitations deterred some students from using AI tools. Students reported concerns about inaccurate outputs (“hallucinations”) and poor output quality, particularly for creative writing or complex analytical tasks. For example, one student noted that “In my poetry class we were instructed to use it to come up with poems and they were awful, so that kinda turned me away from using it to do my work for me.”\r\n\r\nC.5Discussion\r\nWe conclude with two overarching themes that emerged from the responses.\r\n\r\nFirst, students vary substantially in how they incorporate AI into their academic lives. Crucially, this heterogeneity largely depends on what students perceive as “appropriate” uses of AI. For activities that they perceive as core to their academic journey—like writing essays or solving problems—many students hesitate to use AI. A student articulated this clearly: “Most of my work is writing or reading. If I’m not doing the writing, what is the purpose of me taking the class?” Yet, students draw different boundaries between central tasks and grunt work. Some use AI extensively, viewing their role as akin to a manager that provides high-level direction while AI handles implementation. Others restrict AI use to specific tasks like brainstorming, editing, or drafting emails. Still others avoid AI entirely for academic work, often for ethical reasons. Even among AI users, adoption patterns reflect individual trade-offs between time savings, learning goals, and academic integrity concerns.\r\n\r\nSecond, there appears to be a fundamental tension between efficiency and learning. The time-saving benefits are easy to observe, quantifiable, and tangible. But these time savings are unlikely to be a free lunch. Some benefits may come at the cost of spending less time with material that requires deeper engagement to digest. As one student noted: “There may be a negative effect in that it eliminates much of the ’struggle’ in learning.” Yet, having an on-demand tutor that explains concepts in relatable ways can also improve learning. As one student explained: “It can explain concepts to me in a way that is tailored to my learning style.” This suggests that the impact of AI use on learning outcomes depends not on whether students use AI—almost all do, to some extent—but rather on how they employ these tools.\r\n\r\nFigure C1:Word Cloud of Student Motivations for Generative AI Use\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 147 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?’\r\n\r\nFigure C2:Relationship Between AI Sentiment and AI Adoption\r\nPanel A. Ever Used Generative AI\r\nRefer to caption\r\nPanel B. Uses AI for Academic Purposes\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents the relationship between AI sentiment and AI adoption. Panel A shows the proportion of respondents who have ever used generative AI, while Panel B shows the proportion who use AI for academic purposes. Each point represents the mean adoption rate for respondents within sentiment score bins of width 0.2. Sentiment scores are standardized compound scores computed using Hutto and Gilbert, (2014)’s VADER algorithm applied to responses to an open-ended question about generative AI. Positive values indicate positive sentiment and negative values indicate negative sentiment. The dashed lines show OLS best-fit lines estimated on the microdata, with coefficients and standard errors (in parentheses) displayed in the top-left corner of each panel.\r\n\r\nFigure C3:Frequency of Keywords in Student Motivations for AI Use\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to AI use. The responses come from the question “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” Color coding indicates the category of each theme. Usage type refers to how students use AI tools. Motivation captures what drove students to try AI. Concerns include mentions of course policies and academic integrity, individual reservations about AI use, worries about AI’s impact on education, and AI’s technical limitations.\r\n\r\nAppendix DQualitative Evidence on Student Views of AI Policies\r\nIn this section, we analyze student responses to an open-ended question about Middlebury’s AI policies. The survey asked: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Appendix Figure D1 presents a word cloud of the most frequent words in student responses. To analyze these responses systematically, we classified each response using keywords based on their content. Appendix Figure D2 shows the frequency of keywords in our classification.\r\n\r\nD.1Polarized Views on Generative AI Policy Approaches\r\nStudents expressed markedly different views about appropriate AI policies, revealing fundamental disagreement about the path forward. Some strongly advocated for embracing AI technology. As one student argued, “The tool is there, there is supply and there is demand. Don’t fight another war on drugs. Don’t live in a fake reality”. Others called for significant restrictions, arguing that “the use of generative AI is dishonest and corrosive” and that it “prohibits these organic processes and divorces students from true learning”.\r\n\r\nHowever, the most common position advocated for a balanced approach that would allow beneficial uses while restricting harmful ones. Students distinguished between uses that enhance learning (like concept explanation) and those that substitute for learning (like generating entire essays). One student articulated this nuanced view particularly well: “AI also can really be helpful at explaining a textbook problem that doesn’t make sense, or guiding slightly with homework, or creating study materials, or editing/tightening up your prose. All of those things are good, and universities should figure out how to maximize AI use for those reasons and to minimize students just feeding their problem sets into ChatGPT”\r\n\r\nA recurring theme was the futility of blanket bans. Many students emphasized that prohibition would be ineffective, with one noting “I don’t think anyone really cares what the policy of any given class is. If professors want people to not use it, they need to structure assessments in a way that will discourage use.” Another compared AI bans to restricting internet use, arguing “AI policies seem to be totally irrelevant. It’s like telling people they can’t use the internet as a resource for the class.” This ineffectiveness of bans creates fairness concerns. As one student explained: “I think if it is banned in a class, that should be enforced (and right now it absolutely is not)… As with any form of cheating, those who don’t cheat are put at a disadvantage.” Another student expressed similar frustration: “I find it discouraging when I hear classmates saying they use AI for things such as essays when they use it in dishonest ways.”\r\n\r\nD.2Need for Clear Guidelines\r\nThe most frequently expressed concern was the need for clear guidelines about generative AI use. Several students reported confusion about what constitutes acceptable use, with one noting “I think it should be more clear whether we can use it and how and how to cite it since most professors rarely mention it at all”. Other students emphasized the importance of professors explaining their policies upfront and their rationale, with one stating “I think that Professor’s should be very specific about what is allowed and their reasoning behind their policy”.\r\n\r\nMany students advocated for standardization across classes, observing that “Sometimes its confusing when one class allows it and another doesn’t and the other encourages it and so on so if there was a school wide or department wide policy that could help”. Yet there is also disagreement regarding standardization. Some students preferred leaving the decision to individual professors, arguing that “GenAI is more effective in some classes/majors than others. Making sure professors understand how students use GenAI and how useful GenAI is in their class (given the course structure, nature of assignments/material, etc.) is very important for the class policy.”\r\n\r\nD.3Training and Support Services\r\nStudents strongly emphasized the need for training in appropriate generative AI use. Many suggested that the college should provide guidance on using AI tools effectively while maintaining academic integrity. One student proposed “a workshop that teaches you to effectively use GenAI without violating the honor code”. Students also expressed interest in learning how to leverage AI to enhance their learning experience rather than circumvent it. As one student explained: “I think it could be useful to develop some sort of training. How do we use AI in a way that actually benefits our learning? I tried out some things on my own but I feel that I need more guidance.”\r\n\r\nThis desire for training was often linked to workplace preparedness. Students recognized that AI proficiency would be valuable in their careers, with one noting “As the world uses more and more AI, I think it is an important tool that students should know how to leverage”. Another emphasized: “The moment us students leave campus, we will be using it in the professional world, and when used in combination with one’s own skills, it is merely a tool to maximize efficiency”.\r\n\r\nD.4Discussion\r\nThe responses reveal several key tensions in AI policy implementation. First, students express a desire to use AI in ways that enhance rather than substitute for learning, yet they recognize that blanket bans are ineffective and worry about being disadvantaged if they follow restrictions while others do not.\r\n\r\nSecond, while students desire clear guidelines, they also want flexibility to accommodate legitimate uses that vary by discipline and assignment type. Different courses and majors may find different AI uses appropriate based on their learning objectives and assessment types.\r\n\r\nThird, there is tension between faculty autonomy in setting course policies and students’ desire for consistent institutional standards. While some support letting professors determine appropriate AI use for their specific courses, others argue that varying policies across classes create confusion and enforcement challenges.\r\n\r\nFinally, students report significant variation in faculty attitudes toward AI. Some students perceived faculty fear or misunderstanding of AI tools, noting that categorical bans often reflect a lack of understanding about AI’s capabilities and limitations rather than pedagogical considerations.\r\n\r\nFigure D1:Word Cloud of Student Feedback on Generative AI Policies\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 133 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?”\r\n\r\nFigure D2:Frequency of Keywords in Student Feedback on AI Policies\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to Middlebury’s AI policies. The responses come from the question “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Color coding indicates the category of each theme. Policy Approach captures suggestions about how AI should be regulated at the college. Adoption Views reflect positions on whether and how AI should be integrated into academic work. Training Needs indicates requests for guidance and support. Other Concerns include issues of workplace relevance and fairness.\r\n\r\nSee pages 1 of results/ai-survey-midd.pdf See pages 2- of results//ai-survey-midd.pdf\n",
    "totalFiles": 2
  },
  {
    "id": "extraction_1754348215991_isu5whvpa",
    "timestamp": "2025-08-04T22:56:55.991Z",
    "files": [
      {
        "name": "story.txt",
        "size": 730,
        "type": "text/plain",
        "extractedText": "I remember the moment that sparked this project. It was during a walk across Middlebury’s campus late one fall afternoon—just after office hours. A student had come in, clearly bright but struggling, and told me they’d been using ChatGPT late at night to help with assignments because they were too anxious to speak up in class. That confession stuck with me.\r\n\r\nLater, over coffee with Germán, we started wondering—how many students were turning to generative AI as a quiet lifeline? Not to cheat, but to learn? That simple question became the seed for this entire study. What we found wasn’t just statistics. It was a story of adaptation, of students reshaping how they learn, and of institutions racing to catch up.\"",
        "extractionStatus": "success"
      },
      {
        "name": "Generative AI in Higher Education E.txt",
        "size": 117470,
        "type": "text/plain",
        "extractedText": "Generative AI in Higher Education: Evidence from an Elite College†\r\nZara Contractor\r\nGermán Reyes\r\nAbstract\r\nGenerative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT’s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI’s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI’s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\r\n\r\n1Introduction\r\nThe rapid advancement of generative artificial intelligence (AI) is transforming higher education at an unprecedented pace. The launch of ChatGPT and similar tools has introduced technologies capable of performing tasks central to academic assessment and learning—writing essays, solving complex problems, and explaining intricate concepts—instantly and at near-zero marginal cost. Yet systematic evidence on the nature and implications of this new technology remains scarce. How widespread is generative AI adoption among students, and what factors drive it? Do students primarily use AI to augment their learning or to automate coursework, potentially harming human capital development? Could disparities in access to premium AI resources amplify existing educational inequalities?\r\n\r\nThese questions are at the heart of ongoing debates about the role of AI in education, which have relied heavily on anecdotal evidence and speculation rather than rigorous evidence. As a result, universities have implemented policies that vary dramatically, from outright bans to permissive adoption, often without clear evidence on their effectiveness or unintended consequences (Nolan, , 2023; Xiao et al., , 2023; McDonald et al., , 2025).\r\n\r\nThis paper addresses this gap by systematically examining generative AI adoption at Middlebury College, a highly selective liberal arts college in Vermont. With approximately 2,800 undergraduate students, Middlebury offers 49 majors across the arts, humanities, languages, social sciences, and natural sciences. Over the past decade, Middlebury has consistently ranked among the top 10 liberal arts colleges in U.S. News & World Report, with an average ranking of 8.5 between 2015 and 2025. The college’s selectivity is comparable to that of many elite research universities—Middlebury’s acceptance rate of approximately 10 percent is similar to that of Boston University (11 percent), Georgetown University (13 percent), and the University of Virginia (16 percent).\r\n\r\nOur analysis draws on survey data collected from the student population between December 2024 and February 2025. The survey collected detailed information about AI usage, including the frequency and purpose of use, perceived impacts, and responses to institutional policies. To minimize selection bias, we framed the survey broadly as examining technology use and provided incentives for participation, achieving a 22.9 percent response rate with 634 responses representing a broad cross-section of the student population.\r\n\r\nWe document five main findings. First, generative AI is approaching near-universal adoption at an unprecedented speed. Over 80 percent of students use AI for academic purposes, up from less than 10 percent before Spring 2023. This represents one of the fastest technology adoption episodes ever documented, dramatically exceeding the 40 percent adoption rate among U.S. workers (Bick et al., , 2025) and the 23 percent among all U.S. adults (McClain, , 2024). These levels are consistent with international findings of 50–70 percent adoption in university contexts (Nam, , 2023; Stöhr et al., , 2024; Ravšelj et al., , 2025).\r\n\r\nSecond, AI adoption is markedly unequal across academic disciplines and demographic groups. Field of study is the strongest predictor of adoption, likely reflecting how well AI capabilities align with the academic tasks required across fields. Adoption ranges from 91.1 percent in Natural Sciences majors (including mathematics and computer science) to significantly lower levels in Literature (48.6 percent) and Languages (57.4 percent). Additionally, demographic disparities exist. For example, males adopt AI at higher rates than females (88.7 versus 78.4 percent), a pattern that is consistent with documented AI gender gaps in other contexts (Otis et al., , 2024). Most notably, lower-achieving students have a higher adoption rate than their higher-achieving peers (87.1 versus 80.3 percent). As a result, AI could serve as an equalizing force if it enhances student learning—helping struggling students catch up to their peers—but could also widen achievement gaps if AI undermines skill development.\r\n\r\nThird, generative AI transforms the students’ learning production function by both augmenting student capabilities and automating academic tasks. We classify academic tasks as augmentation when AI enhances human capabilities while maintaining student engagement (e.g., explaining concepts, proofreading) versus automation when AI directly produces outputs with minimal cognitive involvement (e.g., writing essays, creating images), and average usage rates across tasks within each category. We find that 61.2 percent of AI users employ these tool for augmentation purposes, while 41.9 percent use it for automation. Qualitative evidence reinforces these patterns: students describe AI as an “on-demand tutor” for augmentation purposes, particularly valuable when traditional resources like office hours are unavailable, while automation use centers on time savings during periods of overwhelming workload. These self-reported patterns align closely with actual usage data from Claude conversation logs (Handa et al., 2025a,).\r\n\r\nFourth, institutional policies can significantly influence AI adoption, though their effectiveness can be undermined by informational frictions. We find that explicit prohibitions dramatically reduce self-reported intended use 39 percentage points (pp). Importantly, the effects of this policy vary substantially across student groups, with females showing larger reductions in usage under prohibition than males, suggesting that one-size-fits-all policies may inadvertently create disparate impacts across groups. Moreover, we also document substantial informational gaps about institutional AI policies. Many students (19.2 percent) do not understand AI policy rules, few know about institution-provided premium AI resources (10.1 percent) or proper AI citation practices (32.6 percent)—a skill that is necessary for academic integrity.\r\n\r\nFifth, students believe that AI has a positive learning impact and these beliefs strongly predict their own AI usage. Most students believe that AI improves their understanding of course materials (70.2 percent) and learning ability (60.1 percent), though fewer believe it improves grades (41.1 percent). Perhaps unsurprisingly, students who believe that AI improves their academic outcomes are significantly more likely to use these tools. For example, a ten-percentage-point increase in the belief that AI improves learning corresponds to a 4.9 percentage-point increase in adoption, highlighting how perceptions can affect technology diffusion in educational settings.\r\n\r\nTaken together, our findings reveal that generative AI has already reshaped students’ college experience. The substantial information gaps that we document—regarding both permitted uses and available resources—suggest straightforward opportunities for policy intervention. Yet the observed heterogeneity across disciplines and student groups indicates that uniform policies risk unintended consequences: blanket prohibitions may disproportionately disadvantage students who benefit most from AI augmentation, while unrestricted use may encourage automation practices detrimental to skill development. Thus, effective policy requires distinguishing clearly between AI uses that enhance learning and those that undermine it.\r\n\r\nWhile our findings are specific to the population we study—students at an elite liberal arts college—this setting offers valuable into AI’s broader societal impact. The rapid uptake of generative AI in higher education may reflect a generalizable principle regarding the diffusion of generative AI: adoption may be fastest in contexts where AI consolidates fragmented tools into a unified platform. Students have long had access to various tools for the academic tasks for which they now use generative AI.1 What distinguishes AI is its ability to provide all these services through a single, instantly accessible interface at virtually zero marginal cost. This consolidation likely drives students’ perception of enhanced learning efficiency and contributes to the fast adoption. This principle may extend well beyond education: industries with fragmented, specialized tools could experience similar trajectories of rapid AI adoption, even if AI itself does not introduce fundamentally new capabilities.\r\n\r\nOur findings contribute to a rapidly growing literature examining the adoption and impacts of generative AI. Recent work has focused on AI’s effects on worker productivity (Noy and Zhang, , 2023; Dell’Acqua et al., , 2023; Peng et al., , 2023; Cui et al., , 2024; Brynjolfsson et al., , 2025) and its potential to transform occupations (Felten et al., , 2021, 2023; Eloundou et al., , 2024). While several papers document AI adoption in workplace settings (Bick et al., , 2025; Hartley et al., , 2025; Humlum and Vestergaard, , 2025) and at the firm level (McElheran et al., , 2023; Bonney et al., , 2024; Kharazian, , 2025), we examine adoption in higher education—a critical setting where future high-skilled workers develop human capital.\r\n\r\nWe add to an emerging literature examining generative AI in education. Some studies focus on the learning impacts of generative AI, finding mixed impacts depending on the setting (e.g., Lehmann et al., , 2024; Bastani et al., , 2025). A complementary strand of the literature documents adoption patterns across diverse educational contexts, including universities in Australia (Kelly et al., , 2023), Ghana (Bonsu and Baffour-Koduah, , 2023), Norway (Carvajal et al., , 2024), Sweden (Stöhr et al., , 2024), and multi-country studies (Ravšelj et al., , 2025). To the best of our knowledge, this paper provides the first systematic evidence on generative AI adoption in U.S. higher education. Importantly, we focus specifically on how students use AI for academic purposes—unlike studies that measure more general usage including personal lives (e.g., Ravšelj et al., , 2025)—and go beyond documenting adoption rates to examine how students integrate AI into their academic workflows, distinguishing between augmentation and automation, the role of student beliefs about AI’s educational impacts, and how institutional policies shape usage patterns.\r\n\r\nOur findings also contribute to the literature on technology diffusion. Unlike the S-shaped adoption curves documented for most technologies (Griliches, , 1957; Rogers, , 1962), we observe extremely rapid adoption of generative AI, with 80 percent uptake within two years of ChatGPT’s release. This pattern contrasts sharply with the adoption trajectories of historical General Purpose Technologies (GPTs) like electricity, which required decades to achieve widespread use (David, , 1990; Bresnahan and Trajtenberg, , 1995). The rapid diffusion we document aligns with evidence that newer technologies diffuse faster than older ones (Comin and Hobijn, 2010a,; Comin and Hobijn, 2010b,). Two factors likely explain AI’s exceptional adoption speed compared to other GPTs. First, unlike electricity or steam power, generative AI requires minimal physical infrastructure—students access it through existing devices—and is available at no cost to consumers, eliminating the financial barriers that typically slow technology adoption. Second, as noted above, AI consolidates multiple specialized tools into a single platform, making its benefits immediately apparent without requiring specialized training or organizational restructuring, unlike previous GPTs (Brynjolfsson and Hitt, , 2000).\r\n\r\n2Data: Novel Student Survey\r\n2.1Recruitment and Structure\r\nWe conducted the survey from December 2024 to February 2025. All Middlebury College students were contacted via email and received a reminder a few weeks and two months after the initial invitation. To minimize selection bias, the recruitment materials described the survey broadly as a study on students’ use of technology in their academic and personal lives. To incentivize participation, students who completed the survey qualified for entry into a lottery for Amazon gift cards ranging in value from $50 to $500. The full survey instrument is provided in Appendix LABEL:app:survey.\r\n\r\nThe survey contains three main sections (see Appendix Figure A1 for the survey flow). First, we gather demographic and academic information, including gender, race/ethnicity, type of high school attended (private or public), current academic year, and declared or intended major.2 We also collect data on academic inputs and performance through students’ self-reported typical weekly hours spent studying and their first-year GPA.3\r\n\r\nSecond, we measure students’ experience with generative AI tools. We begin by asking whether students have ever used generative AI tools like ChatGPT or Claude. For those who have, we collect information about their usage patterns, including frequency of use during the academic semester, which specific AI models they use, and whether they pay for AI tools. We also gather data on how students use AI for different academic tasks, including writing assistance, learning support, and coding.\r\n\r\nThe final part of the survey elicits students’ beliefs about generative AI’s adoption and impacts. We ask students about their perceptions of AI’s effects on their academic experience across multiple dimensions, including learning, grades, time management, and understanding of course material. We then collect information on the role of institutional policies, asking students how different policy environments influence their likelihood of using AI. We also elicit students’ beliefs about AI usage among their peers, including their estimates of the fraction of Middlebury students who use AI for schoolwork and leisure. The survey concludes with two open-ended questions that allow students to share their experiences with AI in academic settings and provide feedback on the college’s AI policies and support services.\r\n\r\n2.2Sample and Summary Statistics\r\nOut of Middlebury’s 2,760 enrolled students, 739 began the survey. We exclude 105 respondents who left the survey before reaching the generative AI usage module, leaving us with an analysis sample of 634 students (22.9 percent of the student body). This response rate is comparable to that of similar surveys (Wu et al., , 2022). To make our sample more representative of Middlebury’s student body, we construct poststratification weights based on the distribution of declared majors from administrative records.4 Specifically, we weight observations by the ratio of each major’s share in the student population to its share in our survey responses.5\r\n\r\nTable Figures and Tables presents summary statistics for our sample. Column 1 reports unweighted survey averages, column 2 shows averages after applying poststratification weights, and column 3 provides population benchmarks from administrative records where available. Our unweighted sample consists of 44.6 percent male and 50.8 percent females. The racial/ethnic composition includes 61.8 percent white, 15.5 percent Asian, 9.9 percent Hispanic, and 3.6 percent Black students. The majority of students (54.3 percent) attended a public school, while 42.0 percent attended a private high school. Our sample represents 43 different majors across seven fields of study, with 31.1 percent of respondents not yet having declared a major. In our analysis, we group these undeclared students by their intended field of study as reported in the survey.\r\n\r\nComparing our unweighted sample to administrative records reveals notable differences. Our sample overrepresents white students (61.8 versus 53.8 percent) and Asian students (15.5 versus 7.3 percent), while underrepresenting Black students (3.6 versus 5.2 percent) and Hispanic students (9.9 versus 12.4 percent). First-year students are overrepresented (31.1 versus 25.5 percent), while senior students are underrepresented (21.3 versus 28.7 percent). Our weighting procedure partially addresses these discrepancies—particularly in the distribution of academic fields, where the weighted figures closely approximate administrative records—but some demographic differences persist. Despite these differences, our weighted sample provides reasonably close approximations to the college population on most dimensions, particularly for academic characteristics and field distributions.\r\n\r\n3Generative AI Usage Patterns Among Students\r\n3.1Adoption of Generative AI\r\nGenerative AI has achieved a remarkably high adoption rate among Middlebury College students. Figure 1 presents the distribution of AI usage frequency during the academic semester, categorized into four levels: “Rarely” (a few times per semester), “Occasionally” (a few times per month), “Frequently” (a few times per week), and “Very Frequently” (daily or almost daily). Overall, 82.5 percent of students report using generative AI for academic purposes, with substantial variation in usage intensity: 23.5 percent use it rarely, 22.2 percent occasionally, 26.2 percent frequently, and 10.6 percent very frequently.\r\n\r\nThe adoption rate at Middlebury aligns closely with patterns observed across other higher education institutions. A BestColleges survey of U.S. undergraduate and graduate students in fall 2023 found that 56 percent had used AI on assignments or exams (Nam, , 2023). Ravšelj et al., (2025) surveyed higher education students globally between late 2023 and early 2024, finding that 71.4 percent had ever used ChatGPT—though this figure encompasses all usage rather than academic use specifically. Similarly, Stöhr et al., (2024) document that 63 percent of Swedish university students had used ChatGPT by spring 2023, with 35.4 percent reporting regular use and 27.6 percent rare use. Among Norwegian university students, Carvajal et al., (2024) find that 68.9 percent use AI tools occasionally or more frequently.\r\n\r\nThese adoption rates in higher education far exceed those in the general population and workforce. Pew Research finds that only 23 percent of U.S. adults have ever used ChatGPT (McClain, , 2024), while Gallup reports that just one-third of U.S. workers have used AI for work (Gallup, , 2024). Bick et al., (2025) and Hartley et al., (2025) estimate that about 40 percent of the U.S. working-age population used generative AI for work as of late 2024 and early 2025. Both studies document substantial heterogeneity across industries, with information services showing the highest adoption rates at 56–62 percent—still considerably below the 80 percent adoption rate at Middlebury. Even among workers in AI-exposed occupations, Humlum and Vestergaard, (2025) find adoption rates of only 41 percent.6\r\n\r\n3.2Adoption by Student Characteristics and Field of Study\r\nAI adoption varies considerably across demographic groups and academic disciplines (Figure 1 and Appendix Table LABEL:tab:ai_freq_het). Males report higher usage rates than females (88.7 versus 78.4 percent). Usage patterns differ markedly by race/ethnicity: Black students (92.3 percent) and Asian students (91.3 percent) exhibit the highest adoption rates, while white students (80.2 percent) and Hispanic students (77.9 percent) report lower usage. Students from private high schools use AI more frequently than those from public schools (84.1 versus 80.4 percent). Notably, students with below-median GPAs report higher usage rates than their higher-achieving peers (87.1 versus 80.3 percent). Adoption varies widely by field of study: Natural Sciences leads with 91.1 percent usage, followed by Social Sciences at 84.6 percent, while Languages (57.4 percent) and Literature (48.6 percent) show substantially lower adoption rates.\r\n\r\nTo examine how student characteristics jointly relate to AI adoption, we estimate multivariate OLS regressions that include all observed characteristics simultaneously.7 Table LABEL:tab:ai_usage_correlates presents regression estimates using four usage thresholds as outcomes. Each column represents the probability of meeting progressively higher frequency thresholds: any AI use (column 1), at least monthly use (column 2), at least weekly use (column 3), and daily use (column 4).\r\n\r\nThe regression results confirm the patterns observed in the descriptive statistics. Holding other characteristics constant, males are 10.3 pp more likely than females to use AI (column 1, \r\np\r\n<\r\n0.05\r\n), with this gender gap typically widening at higher usage frequencies (columns 2–4, all \r\np\r\n<\r\n0.01\r\n). Black and Asian students show substantially higher adoption rates than white students, at 11.8 and 10.7 pp respectively (column 1, both \r\np\r\n<\r\n0.01\r\n). Students from public high schools are 3.0 pp less likely to use AI than those from private schools, but this difference is not statistically significant. Field of study emerges as the strongest predictor of adoption. Compared to Natural Sciences majors, students in Literature, Languages, Arts, and Humanities all show lower usage rates, with the differences being statistically significant at high usage frequencies (columns 3 and 4). Social Sciences majors exhibit adoption rates similar in magnitude to Natural Sciences majors across all usage frequency thresholds.\r\n\r\nOur findings on heterogeneity in AI adoption align with patterns documented in other settings. The gender gap in AI adoption at Middlebury—10.3 pp higher for males—is consistent with evidence across multiple studies. A meta-analysis of 18 studies by Otis et al., (2024) finds that males are 10–20 percentage points more likely to use generative AI than females. This gender gap in AI adoption appears in all educational studies (Nam, , 2023; Carvajal et al., , 2024; Stöhr et al., , 2024; Ravšelj et al., , 2025). Our finding that students with below-median GPAs are more likely to use AI aligns with Carvajal et al., (2024), who document higher adoption rates among students with lower admission grades. This pattern of greater adoption among lower-achieving students suggests that AI could narrow achievement gaps if it enhances learning and skill development, but could widen these gaps if it undermines the acquisition of fundamental skills.\r\n\r\nThe differences in adoption rates across academic fields at Middlebury mirror patterns documented in other educational settings. Stöhr et al., (2024) find that technology and engineering students exhibit significantly higher ChatGPT usage compared to students in humanities. Similarly, Nam, (2023) report that 62 percent of business majors and 59 percent of STEM majors have used AI tools for coursework, compared to 52 percent of humanities majors. Ravšelj et al., (2025) document comparable disciplinary differences, with applied sciences students showing substantially higher usage rates than arts and humanities students.\r\n\r\nThese academic differences persist into the workforce. Bick et al., (2025) find stark variation by college major: STEM graduates have the highest AI adoption rates (46.0 percent), followed by Business/Economics graduates (40.0 percent), while Liberal Arts graduates show substantially lower rates (22.4 percent). Humlum and Vestergaard, (2025) document similar patterns by occupation, finding that roles requiring strong writing and technical skills—such as marketing specialists and journalists—have the highest adoption rates. These consistent patterns across educational and professional contexts suggest that field-specific factors, particularly the applicability of AI tools to different types of tasks, shape adoption in systematic ways.\r\n\r\n3.3Timing of Generative AI Adoption\r\nThe speed of technology diffusion is a critical determinant of its economic and social impact (David, , 1990; Hall and Khan, , 2003; Stokey, , 2021). In educational contexts, rapid technology adoption can create or exacerbate inequalities between early and late adopters, particularly if the technology confers significant learning advantages (World Bank, , 2016). To track the timing of AI adoption among Middlebury students, we asked them when they first began using generative AI for academic purposes, with options ranging from “This semester (Fall 2024)” to “Before Spring 2023” (as a reference, ChatGPT’s public launch was in November 2022).\r\n\r\nStudents adopted generative AI at an extraordinary pace. Figure 2 shows that the cumulative adoption rate grew dramatically from less than 10 percent before Spring 2023 to slightly above 80 percent by Fall 2024. The pace of adoption has accelerated over time, likely reflecting improvements in AI capabilities. Among current users, 25.7 percent adopted AI in Fall 2024 alone, compared to 19.9 percent in Spring 2024 and 16.3 percent in Fall 2023.\r\n\r\nThe adoption rate among Middlebury students far exceeds that observed in other populations and represents one of the fastest technology adoption episodes ever documented. For comparison, Bick et al., (2025) show that it took over 20 years for computers to reach an 80 percent adoption rate in the U.S. working-age population, and about 15 years for internet adoption to reach similar levels. Even generative AI adoption in the broader population has been markedly slower: Pew Research found that just 23 percent of U.S. adults had ever used ChatGPT as of February 2024, up from 18 percent in July 2023 (McClain, , 2024). The dramatically faster adoption rate we document—reaching over 80 percent in less than two years—suggests academic settings may uniquely accelerate AI diffusion.\r\n\r\nTo identify early versus late adopters of generative AI, in Table LABEL:tab:ai_adopt_correlates and Appendix Figure A2, we analyze how adoption timing varies across student characteristics. The rate of adoption varies substantially across student characteristics. Male students led adoption, with a 8.9 pp higher probability of using AI before Spring 2023 compared to females (column 1, \r\np\r\n<\r\n0.01\r\n)—a gender gap that persists across all periods (columns 2–5). Black and Asian students also adopted earlier than white students, though these differences reach statistical significance only in later periods (columns 4–5). Field of study is a strong predictor of adoption timing. For example, students in Languages consistently lagged behind Natural and Social Sciences majors, with significantly lower adoption rates across nearly all time periods.\r\n\r\n3.4Choice of Generative AI Models\r\nMajor AI companies operate on a freemium model, where free versions coexist with premium subscriptions that offer higher usage limits and access to more advanced models. This tiered structure creates potential for a new form of educational disparities: if paid versions confer substantial academic advantages, students who cannot afford subscriptions may be systematically disadvantaged. To investigate these potential disparities, we presented respondents with a list of options including both free and paid versions of popular models. We also collected information on monthly subscription expenditures, with response options ranging from no active subscription to spending more than $40 monthly (standard subscriptions to major models like ChatGPT cost $20 per month at the time).\r\n\r\nOpenAI’s ChatGPT dominates AI usage among Middlebury AI users, with the free version capturing the largest market share. Figure 3 shows that 89.3 percent of AI users rely on the free version of ChatGPT, making it dramatically more popular than any alternative. Google Gemini (13.5 percent) and Microsoft Copilot (7.7 percent) are distant competitors, while other platforms each capture less than 5 percent of users. This dominance mirrors patterns in other academic settings, where Stöhr et al., (2024) find substantially higher familiarity with ChatGPT compared to alternative platforms, and aligns with broader workforce trends documented by Bick et al., (2025).8\r\n\r\nDespite near-universal AI adoption, only 11.3 percent of AI users pay for any AI service.9 This figure is remarkably similar to the 8 percent found in Ravšelj et al., (2025)’s multi-country survey. This low payment rate suggests that for most students, the premium features of paid versions—primarily higher usage limits and access to more advanced models—do not justify the subscription cost. However, payment patterns reveal significant disparities: males and Asian students are substantially more likely to purchase AI subscriptions (Appendix Table LABEL:tab:ai_model_correlates and Appendix Figure A3), potentially reflecting differences in usage intensity across demographic groups.\r\n\r\n4Generative AI and the Production of Learning\r\n4.1The Use of Generative AI across Academic Tasks\r\nHow is generative AI transforming the traditional inputs to student learning? The educational production function includes inputs like time spent studying (e.g., Stinebrickner and Stinebrickner, , 2008), faculty instruction (e.g., Fairlie et al., , 2014), peer interactions (e.g., Sacerdote, , 2001), and academic support services (e.g., Angrist et al., , 2009). AI tools have the potential to complement or substitute for these traditional inputs. For example, using AI to explain concepts might substitute for faculty office hours, while using it for proofreading might reduce time needed for academic support services.\r\n\r\nTo understand the role of AI in students’ learning production function, we collected information about students’ AI usage across ten common academic tasks: proofreading, generating ideas, writing essays, editing essay drafts, coding assistance, creating images, explaining concepts, composing emails, summarizing materials, and finding information. For each task, students indicated their frequency of AI use on a five-point scale ranging from never to daily. We supplemented this quantitative data with open-ended responses about how AI influences their academic work process.\r\n\r\nStudents use generative AI for a wide range of academic tasks, with the highest adoption rates for learning support and text-processing activities. Figure 4, Panel A shows that explaining concepts is the most common use case, with 80.3 percent of AI users using it for this task. Summarizing texts follows as the second most common task (74.0 percent), followed by finding information and generating ideas (63.1 and 61.9 percent). Writing assistance tasks like proofreading and editing essays are also common, used by 54.1 and 47.3 percent of AI users, respectively. Technical applications like coding help are significant (34.4 percent), considering that many academic degrees involve no programming. Notably, while 23.5 percent of AI users report using it for writing essays, this represents a relatively low adoption rate compared to other academic uses, suggesting students may be more hesitant to use AI for primary content creation. The lowest adoption rate is for creating images at 20.4 percent, likely reflecting fewer academic use cases for this capability.\r\n\r\nAn important limitation of our survey is that it relies on self-reported usage, which may introduce measurement error. For example, students might underreport uses they perceive as academically inappropriate—such as essay writing—while overreporting those viewed as legitimate learning tools (Ling and Imas, , 2025). To assess the validity of our self-reported measures, we compare our findings with Handa et al., 2025a, who analyze actual Claude usage patterns among users with university email addresses. Several caveats apply: their data captures conversation-level interactions rather than student-level usage, and most students in our sample use ChatGPT rather than Claude (Section 3.4). Despite these limitations, the comparison provides a useful benchmark for evaluating our survey responses.\r\n\r\nReassuringly, our results are consistent with Anthropic’s data. Both studies identify explaining concepts and technical problem-solving as primary use cases. Handa et al., 2025a report that the second largest use case (33.5 percent of conversations) involves “technical explanations or solutions for academic assignments,” while we find that 80.3 percent of AI users use it for explaining concepts—a difference likely attributable to our student-level versus their conversation-level measurement. Similarly, the most common usage category in Claude involves “designing practice questions, editing essays, or summarizing academic material” (39.3 percent of conversations), aligning with our findings that 74.0 percent of AI users use it for summarizing texts and 47.3 percent for editing essays. The disciplinary patterns also converge: Handa et al., 2025a finds that computer science, natural sciences, and mathematics conversations are overrepresented, which mirrors our finding that Natural Science majors show significantly higher AI adoption rates (Figure 1). Overall, these convergent findings from self-reported survey data and actual usage logs suggest that our results capture genuine patterns of student AI engagement rather than merely reflecting social desirability in responses.\r\n\r\n4.2Automation versus Augmentation\r\nAre students using generative AI primarily to augment their learning or to automate their coursework? This distinction is crucial for understanding AI’s impacts (Autor and Thompson, , 2025)—augmentation may enhance students’ learning processes while maintaining their active engagement and critical thinking, whereas automation produces fully-formed outputs that could be submitted with minimal student input, potentially harming learning.\r\n\r\nTo examine this empirically, Figure 4, Panel B categorizes the ten measured tasks based on whether they augment or automate academic tasks. We classify tasks as augmentation when they enhance human capabilities (explaining concepts, finding information, proofreading, and editing drafts) and as automation when they directly produce outputs (writing essays, creating images, composing emails, summarizing texts, generating ideas, and coding assistance). We then calculate the percentage of AI users who employ each category at various frequencies.10\r\n\r\nStudents use generative AI for both augmentation and automation, but with markedly different frequency. While 61.2 percent of AI users report using AI for augmentation tasks, 41.9 percent report using it for automation—thus, there is a substantial 19.3 pp difference. This gap is mainly driven by occasional use (19.7 percent for augmentation versus 12.7 percent for automation) and frequent use (16.3 versus 9.0 percent). Most strikingly, students are more than twice as likely to use AI daily for augmentation (5.0 percent) compared to automation (2.5 percent). The higher frequency of augmentation use also suggests that students find augmentation uses more valuable for their day-to-day academic activities. Our findings align closely with patterns observed in actual AI usage data: Handa et al., 2025b analyze real conversation logs from Claude and find that 57 percent of workplace AI interactions involve augmentation while 43 percent involve automation.\r\n\r\nQualitative evidence from open-ended responses provides additional insights into students’ motivations for using AI (see Appendix C for additional results and validation of the open-ended measure). Students’ descriptions align closely with the augmentation-automation framework. For augmentation, many characterize AI as an “on-demand tutor,” particularly valuable when traditional resources like office hours are unavailable. Non-native English speakers frequently mention using AI for proofreading to overcome language barriers, while students from technical majors describe using it to debug code and understand error messages. For automation, time savings emerged as the dominant motivation, with 21.7 percent of open-ended responses explicitly mentioning efficiency benefits (Appendix Figure C3). Students describe turning to AI during periods of overwhelming workload or looming deadlines, using it to generate initial drafts or complete routine assignments. That students automate tasks mostly when time-pressed helps explain why automation is less common than augmentation.11\r\n\r\nOur finding that students favor augmentation over automation extends beyond Middlebury. In Appendix Figure A4, we re-analyze data from Ravšelj et al., (2025)—a multi-country survey of higher education students—to examine how this balance varies across institutional quality.12 We classify universities into quintiles based on their Times Higher Education World University Rankings. Overall, students who use AI worldwide show similar adoption rates for augmentation (64.6 percent) and automation (63.1 percent) tasks—a much smaller gap than at Middlebury (Panel A). However, this aggregate pattern masks heterogeneity by institutional quality. Students at top-quintile universities show a modest preference for augmentation over automation, with this gap narrowing monotonically down the institution quality distribution. By the bottom quintile, augmentation and automation usage are virtually identical (Panel B).\r\n\r\n4.3Heterogeneity in Augmentation versus Automation Usage\r\nUnderstanding whether augmentation and automation patterns vary across student populations may help to design targeted support policies. To assess this, we construct four measures of augmentation and automation usage. First, we create binary indicators for whether students use AI for any augmentation or automation task. Second, we calculate the proportion of tasks in each category for which students employ AI. Third, using the Likert-scale responses, we create intensity measures that capture how frequently students use AI for augmentation and automation purposes. Finally, we compute the difference between augmentation and automation variables, as a measure that directly compare students’ relative preference for augmentation versus automation. Table LABEL:tab:ai_autom_regs presents regression estimates using these measures as outcomes.\r\n\r\nThe balance between augmentation and automation varies substantially by student characteristics. Males show higher adoption of both augmentation and automation tasks, with a similar magnitude in each case—they are 6.0 pp more likely to use augmentation prompts and 5.5 pp more likely to use automation prompts compared to females (columns 1 and 4, \r\np\r\n<\r\n0.05\r\n). Black students have a substantially higher augmentation intensity than white students (0.871 points higher, equivalent to 38.1 percent of the outcome mean, \r\np\r\n<\r\n0.01\r\n), but show no statistically significant differences in automation usage. This results in Black students having the strongest preference for augmentation over automation, with a 19.5 pp higher share of augmentation tasks relative to automation (column 7, \r\np\r\n<\r\n0.01\r\n). Asian students show high usage of both kinds relative to white students. Differences across field of study effects are sizable: humanities, languages, and literature majors tend to show lower usage of both augmentation and automation tools compared to natural science majors.\r\n\r\n5Institutional Policies and AI Adoption\r\n5.1The Role of Institutional Policies in Shaping Student Behavior\r\nInstitutional policies are central to shaping the adoption and diffusion of new technologies (Acemoglu, , 2025). In the context of generative AI in higher education, understanding how policies affect student behavior is essential for guiding evidence-based decision-making. To examine this, we asked students to report their likelihood of using AI under various policy scenarios, ranging from complete prohibition to unrestricted use. This analysis is particularly relevant in light of the ongoing debate about how universities should regulate generative AI use in academic settings (Nolan, , 2023), and the widespread variation in institutional policies across colleges (Xiao et al., , 2023).\r\n\r\nInstitutional policies substantially influence students’ reported likelihood of using generative AI. Figure 5 shows that when generative AI use is unrestricted, 52.4 percent of students report being likely or extremely likely to use it. This likelihood decreases modestly when policies require citation (40.9 percent) or when no explicit policy exists (42.3 percent). However, explicit prohibition creates a dramatic shift: only 13.4 percent of students report they would be likely or extremely likely to use AI when it is banned, while 72.9 percent say they would be unlikely or extremely unlikely to do so (note that these figures include both current AI users and students who do not use generative AI for academic purposes). These results suggest that institutional policies can significantly influence generative AI usage patterns, though a small fraction of students report they would likely use AI even when explicitly prohibited.\r\n\r\nThe magnitude of policy effects in our study aligns closely with findings from other contexts. Carvajal et al., (2024) estimate that banning AI reduces usage by 37.2 pp among females and 20.6 pp among males, for an overall drop of about 28.9 pp. In our survey, a ban leads to a 37.8 pp decline in usage, with a larger decrease among females (49.6 pp) than males (40.1 pp). These parallel results underscore how institutional policies can unintentionally produce disparate effects across gender. Notably, these differential policy effects extend beyond gender. Other demographic and academic characteristics—such as race and field of study—also moderate how students respond to policy restrictions (Appendix Table LABEL:tab:ai_policy_het), underscoring that institutional policies can produce non-neutral impacts along multiple dimensions.\r\n\r\n5.2Understanding of Institutional Policies and Resources\r\nInformation gaps and inattention can significantly affect technology adoption decisions (Duflo et al., , 2011; Hanna et al., , 2014). For example, imperfect information about rules, available resources, or proper usage guidelines could lead to underadoption of beneficial technologies or inadvertent policy violations. To test for the existence of information gaps, we examine three dimensions of policy understanding. First, we asked students whether they find AI policies in their current classes clear. Second, we measured awareness of free access to premium AI tools through the college—a resource that could reduce inequality but only if students know it exists. Third, we assessed whether students know how to properly cite AI when required, a mechanical skill necessary for academic integrity. We supplemented these measures with open-ended feedback about the college’s AI policies and support services.\r\n\r\nStudent understanding of institutional policies is high, though significant gaps remain. Figure 6, Panel A shows that most students (79.1 percent) understand when and where they are allowed to use AI in their classes, but a nontrivial minority (19.2 percent) find AI policies unclear. Moreover, critical knowledge gaps persist elsewhere. Only 10.1 percent know they have free access to Microsoft Copilot through the college (Panel B), and just 32.6 percent understand how to properly cite AI use (Panel C). These gaps vary systematically: females show better policy understanding than males (81.5 versus 75.8 percent), and non-white students demonstrate higher awareness across all three dimensions compared to white students.13\r\n\r\nThe qualitative evidence from open-ended responses reinforces these information frictions and reveals implementation challenges (Appendix D). Students express frustration with vague guidelines, requesting specific examples of acceptable versus unacceptable use cases. Many advocate for formal training, noting that simply knowing policies exist differs from understanding how to effectively integrate AI into their workflow. A particularly striking theme is students’ perception of blanket prohibitions as both ineffective and unfair, as they create a prisoner’s dilemma situation where compliant students are disadvantaged relative to those who secretly violate restrictions. Many responses call for a balanced approach—permitting AI use that supports learning while restricting uses that replace it—though the boundary between the two remains contested.\r\n\r\n6Beliefs About AI’s Educational Impact and Peer Usage\r\n6.1Student Beliefs of AI’s Impact on Educational Outcomes\r\nTechnology adoption decisions are shaped by beliefs about potential returns (Foster and Rosenzweig, , 2010). Students’ perceptions of how AI affects their learning may influence whether they adopt these tools and how they integrate them into their academic workflows. To elicit these beliefs, we asked students to evaluate AI’s impact across four dimensions of their academic performance: understanding of course materials, overall learning ability, time management, and course grades. For each dimension, students rated AI’s effect on a five-point scale ranging from “significantly reduces” to “significantly improves.” These subjective assessments provide insight into the perceived value of AI tools from the student perspective, which may differ from their actual effects on learning outcomes.\r\n\r\nStudents tend to believe that generative AI is beneficial for their academic performance, though the perceived benefits vary across dimensions. Figure 7 shows that the majority of students (70.2 percent) believe that generative AI improves their understanding of course materials, and 60.1 percent report improvement in their ability to grasp concepts, retain information, or learn new skills. Similarly, 59.4 percent report that AI improves their ability to complete assignments on time. Notably, while students believe AI helps their learning and assignment completion, they are less confident about its impact on course grades—41.1 percent believe it improves their grades, while 55.4 percent report no effect and 3.5 percent report negative effects. This pattern suggests that while students perceive generative AI as improving their learning process and workflow—through better understanding, skill development, and timely completion of work—these benefits do not necessarily translate into better course grades.14\r\n\r\nBeliefs about AI’s benefits strongly predict adoption. In Figure 8, we plot the relationship between the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis) for different subgroups of students (e.g., males, white students, public-school students, etc.). Across all four measured academic dimensions, there is a strong and statistically significant relationship between perceiving positive AI effects and AI adoption. The relationship is strongest for beliefs about the ability to improve course grades (Panel D): a 10-pp increase in the belief that AI improves grades is associated with a 5.6 pp increase in AI adoption (\r\np\r\n<\r\n0.01\r\n). Similar positive relationships exist for beliefs about learning ability (4.3 pp, \r\np\r\n<\r\n0.05\r\n), understanding of course materials (4.5 pp, \r\np\r\n<\r\n0.01\r\n), and timely assignment completion (5.3 pp, \r\np\r\n<\r\n0.05\r\n). These findings suggest that student beliefs about AI’s academic benefits—irrespective of the actual benefits—may play a crucial role in shaping adoption decisions.\r\n\r\nThese positive beliefs align with findings from other contexts. Ravšelj et al., (2025) report that the majority of students in their sample believe that ChatGPT improves their general knowledge (68.8 percent) and specific knowledge (62.7 percent)—remarkably similar to our finding that most students believe AI improves understanding of course materials (68.5 percent) and learning ability (57.6 percent). Similarly, 57.4 percent of students in Ravšelj et al., (2025)’s sample believe that ChatGPT helps meet assignment deadlines, while 59.4 percent in our data report AI improves timely assignment completion. Stöhr et al., (2024) provide complementary evidence from Swedish universities, where 47.7 percent of students believe that AI makes them more effective learners, yet only 17.3 percent believe these tools improve their grades—mirroring our finding that perceived learning benefits from AI use exceed perceived effects on course performance.\r\n\r\n6.2Student Beliefs About Peer Use of Generative AI\r\nStudents’ beliefs about their peers’ AI usage may influence their own adoption through multiple channels, including social norms (e.g., Giaccherini et al., , 2019), social learning (e.g., Foster and Rosenzweig, , 1995; Beaman et al., , 2021), peer effects (e.g., Bailey et al., , 2022), and competitive pressure to avoid falling behind (e.g., Goehring et al., , 2024). To measure these beliefs, we asked students to estimate what fraction of their peers use generative AI for different purposes and would use under different policy environments. Figure 9 presents the distribution of these beliefs. Panels A-C show students’ estimates of peer AI usage for schoolwork, leisure, and any purpose respectively. Panels D-F display students’ beliefs about AI usage under three policy environments: when classes have no explicit AI policy, when classes allow AI use, and when classes prohibit AI use.\r\n\r\nStudents systematically underestimate their peers’ AI usage. On average, they believe 65.2 percent of peers regularly use generative AI for schoolwork (Panel A), while our survey reveals an actual usage rate of 82.5 percent—a 17.3 pp gap. This underestimation appears consistent across educational contexts: Stöhr et al., (2024) find that only 38.7 percent of Swedish students believe AI chatbot use is common among peers, despite 63 percent actual usage. When we examine beliefs about policy-contingent behavior, students estimate that 62.7 percent of peers use AI when no explicit policy exists (Panel D), rising to 72.2 percent under permissive policies (Panel E) and falling to 43.5 percent under prohibition (Panel F). These beliefs align directionally with self-reported intentions, though with notable magnitude differences (Appendix Figure A5). For instance, while students believe 43.5 percent of their peers use AI in classes that prohibit it, only 28.4 percent report they themselves would be likely to do so.15\r\n\r\nNotably, students’ perceptions of peer AI adoption are closely linked to their own adoption behavior. Appendix Figure A6 shows a strong positive association between beliefs about peer usage and actual usage rates across student groups. This behavior is consistent with several psychological mechanisms, including the “false consensus effect” (Ross et al., , 1977), selection neglect (Jehiel, , 2018), or interpersonal projection bias (Bushong and Gagnon-Bartsch, , 2024). Still, virtually all groups systematically underestimate peer usage—actual usage rates exceed believed usage for every demographic and academic group we examine. This figure also shows that underestimation varies substantially across student characteristics: males use AI at 88.7 percent but believe only 66.6 percent of peers do so (22.1 pp gap), while females show a smaller gap (77.6 percent actual versus 65.1 percent believed, 13.3 pp gap).\r\n\r\nTaken together, students’ systematic underestimation of peer AI usage coupled with the strong relationship between beliefs and adoption suggests that misperceptions about social norms may shape technology diffusion. If AI enhances learning, then correcting these misperceptions through information provision could accelerate beneficial adoption; conversely, if AI undermines skill development, then students’ underestimation of peer usage may serve as an unintentional safeguard against harmful overadoption.\r\n\r\n7Discussion\r\nThis paper presents the first systematic evidence on generative AI adoption at a highly selective U.S. college. Using novel survey data, we document exceptionally rapid and widespread adoption, substantial shifts in the educational production function through augmentation and automation, and the significant roles of students’ beliefs and institutional policies in shaping AI use.\r\n\r\nOur results offer three implications for institutional policy and ongoing debates about AI in education. First, we identify low-cost opportunities to improve institutional policy effectiveness through targeted information provision. The significant gaps we document in students’ understanding of institutional AI policies, citation practices, and available AI resources suggest that simple interventions—such as clear guidelines, illustrative examples of acceptable uses, and AI literacy programs—can reduce unintentional academic integrity violations and support beneficial AI integration. Qualitative feedback strongly indicates student demand for more explicit guidance on responsible AI use.\r\n\r\nSecond, our evidence challenges alarmist narratives that conflate widespread AI adoption with universal academic dishonesty based on anecdotal accounts.16 Although AI use is indeed near-universal, we find clear evidence that students primarily employ AI as a tool for strategic task management: to enhance learning (augmentation) and selectively automate tasks when facing high time opportunity cost—not solely to circumvent academic effort. This distinction matters: by normalizing academic dishonesty as inevitable and universal, these narratives may shift social norms and encourage students who would otherwise use AI responsibly to engage in prohibited behaviors, believing “everyone else is doing it.”\r\n\r\nThird, our findings caution against policy extremes of either blanket prohibition or unrestricted AI use (Merchant, , 2024; McDonald et al., , 2025). Blanket prohibitions disproportionately harm students who benefit most from AI’s augmentation functions—particularly lower-achieving students—while also creating uneven compliance, placing conscientious students at a disadvantage relative to rule-breakers. Conversely, unrestricted AI use based solely on revealed preference arguments ignores important market failures in educational settings. Students often hold overly optimistic beliefs about AI’s learning benefits despite mixed empirical evidence, potentially leading to unintended negative learning outcomes. Most concerningly, permissive policies risk creating competitive dynamics where students feel compelled to adopt AI not for its learning benefits but simply to avoid falling behind in an educational “arms race” (Goehring et al., , 2024).\r\n\r\nFigures and Tables\r\nFigure 1:The Adoption of Generative AI among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the fraction of students who report using AI during the academic semester, categorized by demographic characteristics, high school type, academic cohort, GPA, and field of study. Usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily).\r\n\r\nThe category “All students” provides the baseline usage rate for the full sample. Gender categories are based on self-identification, with non-binary responses excluded due to a small sample size. “Private HS” refers to students who attended private high schools, while “Public HS” includes public institutions. “Cohort” denotes the student’s academic year, ranging from first-year (“Freshman”) to fourth-year and beyond (“Senior”). GPA categories (“GPA \r\n>\r\n p50” and “GPA \r\n<\r\n p50”) split students into groups above or below the median first-year GPA, as self-reported on a 4.0 scale. See Appendix B.1 for the classification of majors into fields of study.\r\n\r\nFigure 2:The Evolution of Generative AI Adoption among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the cumulative percent of students who reported using generative AI tools for academic purposes over time. The data is based on retrospective self-reports collected in our December 2024 survey, where students were asked “When did you first start using any form of Generative AI for academic purposes?” Response options ranged from “Before Spring 2023” to “This semester (Fall 2024).” The \r\nx\r\n-axis represents academic semesters, while the \r\ny\r\naxis represents the cumulative adoption rate. Vertical lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure 3:Adoption of Generative AI Models Among College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the adoption rates of various AI models as of Fall 2024. The horizontal axis shows the percent of students who reported using each tool, and the vertical axis lists the tools in descending order of adoption rates.\r\n\r\nFigure 4:Academic Uses of Generative AI\r\nPanel A. Across Common Academic Tasks\r\nRefer to caption\r\nPanel B. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of AI students who use generative AI for different academic tasks. For each task, usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). The number at the end of each bar represents the total percent of students who use AI for that purpose at any frequency. Tasks are ordered by total usage, from highest to lowest. Results are based on responses to the question: “For academic purposes, which of the following tasks do you typically use generative AI for?” Sample includes all students who reported using AI during the academic semester.\r\n\r\nFigure 5:Student Reported Likelihood of Using AI under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of students who report different likelihoods of using AI under various policy scenarios. For each policy, responses are categorized on a five-point scale from “Extremely unlikely to use AI” to “Extremely likely to use AI.” The sample includes all survey respondents. The question asked was: “How likely are you to use generative AI in a class with each of the following AI policies?”\r\n\r\nFigure 6:Understanding of Generative AI Policies and Resources\r\nPanel A. Understanding of AI Policies\r\nRefer to caption\r\nPanel B. Awareness of Copilot Access\r\nRefer to caption\r\nPanel C. Knowledge of Citation Requirements\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows students’ understanding of institutional AI policies and resources. Panel A displays the percent of students who report understanding AI policies in their classes, those who report having no explicit policy, and those who find policies unclear, broken down by demographic characteristics. Panel B shows the percent of students who are aware of their free access to Microsoft Copilot through Middlebury College. Panel C presents the percent of students who report knowing how to properly cite AI use in their academic work when required. For Panels B and C, horizontal lines represent 95 percent confidence intervals. Sample includes all survey respondents.\r\n\r\nFigure 7:Student Beliefs about the Impact of AI on their Academic Performance\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of Middlebury students who believe that AI improves, reduces, or has no effect on different aspects of their academic experience. For each outcome, responses are categorized into three groups: “Improves” combines “significantly improves” and “somewhat improves” responses, “Reduces” combines “significantly reduces” and “somewhat reduces” responses, and “No effect” represents neutral responses. Sample includes all students who report using AI during the academic semester. “Don’t know” responses are excluded.\r\n\r\nFigure 8:Relationship Between AI Adoption and Beliefs About AI’s Academic Benefits\r\nPanel A. Learning ability (e.g., ability to grasp concepts, learn new skills, etc.)\r\nRefer to caption\r\nPanel B. Understanding of\r\ncourse materials\r\nRefer to caption\r\nPanel C. Ability to complete\r\nassignments on time\r\nRefer to caption\r\nPanel D. Course grades\r\n \r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the relationship between AI adoption rates and beliefs that AI improves various academic outcomes across different student groups. Each panel plots the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis). Points represent different student groups categorized by demographics (circles), academic characteristics (triangles), and field of study (squares). The dashed line shows the linear fit across all groups. Groups with fewer than 10 students are excluded.\r\n\r\nFigure 9:Student Beliefs about Generative AI Usage at Middlebury College\r\nPanel A. For Schoolwork\r\nRefer to caption\r\nPanel B. For Leisure\r\nRefer to caption\r\nPanel C. For Any Purpose\r\nRefer to caption\r\nPanel D. Classes with No Policy\r\nRefer to caption\r\nPanel E. Classes that Allow AI\r\nRefer to caption\r\nPanel F. Classes that Disallow AI\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the distribution of students’ beliefs about generative AI usage among their peers at Middlebury College. Panels A-C display students’ estimates of the percent of their peers who regularly use AI for schoolwork, leisure activities, and any purpose, respectively. Panels D-F show students’ beliefs about AI usage in classes with different AI policies: those without an explicit policy (Panel D), those that allow AI use (Panel E), and those that prohibit AI use (Panel F). Each panel shows a histogram with bins of width ten percentage points (e.g., responses between 1-10 fall in the 10 bin, 11-20 in the 20 bin, etc.). The red dashed line indicates the mean response. Sample excludes respondents with missing values or who selected the default response for all six categories (which equals zero).\r\n\r\nTable 1:Summary Statistics of Survey Participants\r\nSurvey Sample\tAdmin records\r\nUnweighted\tWeighted\t\r\n(1)\t(2)\t(3)\r\nPanel A. Demographics\r\nMale\t0.446\t0.433\t0.463\r\nFemale\t0.508\t0.516\t0.533\r\nWhite\t0.618\t0.603\t0.538\r\nBlack\t0.036\t0.033\t0.052\r\nHispanic\t0.099\t0.106\t0.124\r\nAsian\t0.155\t0.162\t0.073\r\nPrivate high school\t0.420\t0.399\t–\r\nPublic high school\t0.543\t0.556\t–\r\nPanel B. Academic Characteristics    GPA 3.740 3.736 3.670 Hours spent on academics per week 17.899 17.889 – Freshman 0.311 0.355 0.255 Sophomore 0.273 0.272 0.257 Junior 0.202 0.179 0.201 Senior 0.213 0.194 0.287   Panel C. Field of Study    Arts 0.011 0.021 0.023 Humanities 0.052 0.068 0.073 Languages 0.021 0.020 0.025 Literature 0.035 0.025 0.025 Natural Sciences 0.218 0.249 0.244 Social Sciences 0.353 0.247 0.243 Has not declared major 0.311 0.371 0.364\r\n\r\nN (# degrees) 43 43 49 N (# students) 634 2,760 2,760\r\n\r\n\\justify\r\nNotes: This table presents summary statistics from our survey of college students. Panel A reports demographic characteristics, including the proportion of participants identifying as male, female, white, Black, Hispanic, Asian, or who attended a private or public high school. Panel B provides academic characteristics, such as GPA (only available for non-freshmen), average weekly hours spent on academics, and academic year distribution (Freshman, Sophomore, Junior, and Senior). Note that in column 1–2, GPA refers to self-reported first-year GPA while in column 3 it is the overall GPA during Spring 2024. Panel C summarizes the distribution of participants across different fields of study. Major groups are mutually exclusive.\r\n\r\n0.8250.5890.3680.106 R\r\n−\r\nsquared0.0770.0890.1160.058 N (Students)616616616616 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI usage frequency threshold and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column uses a different threshold for AI usage frequency, categorized as: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). Column 1 defines usage as any nonzero frequency; column 2 includes at least occasional use; column 3 includes frequent or higher use; and column 4 captures only very frequent use.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.0890.1990.3570.5510.801 R\r\n−\r\nsquared0.0450.0610.0880.0800.069 N (Students)633633633633633 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI adoption date and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column presents results for a different threshold of AI adoption. Column 1 shows the probability of adopting AI before Spring 2023; column 2 by Spring 2023; column 3 by Fall 2023; column 4 by Spring 2024; and column 5 by Fall 2024. The dependent variable in each regression is a binary indicator equal to one if the student had adopted AI by the specified time period.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.9120.6122.2850.9130.4191.8010.1930.484 R\r\n−\r\nsquared0.0360.0730.1260.0530.1170.1290.0420.063 N (Students)515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and their use of generative AI for academic tasks. In columns 1 and 4, the outcome is a dummy that equals one if a student reports using AI with any frequency for at least one augmentation or automation task, respectively. In columns 2 and 5, the outcome is the share of tasks within each category for which the student reports any use. In columns 3 and 6, the outcome is a continuous measure capturing average usage frequency for each task category, based on raw Likert-style responses. In columns 7 and 8, the outcome is the difference in average task share and usage intensity between augmentation and automation, respectively. Regressions are weighted and report robust standard errors clustered at the student level. ∗∗∗ \r\np\r\n<\r\n0.01\r\n, ∗∗ \r\np\r\n<\r\n0.05\r\n, ∗ \r\np\r\n<\r\n0.10\r\n.\r\n\r\nReferences\r\nAcemoglu, (2025)\r\nAcemoglu, D. (2025).Nobel lecture: Institutions, technology, and prosperity.American Economic Review, 115(6):1709–1748.\r\nAngrist et al., (2009)\r\nAngrist, J., Lang, D., and Oreopoulos, P. (2009).Incentives and services for college achievement: Evidence from a randomized trial.American Economic Journal: Applied Economics, 1(1):136–163.\r\nAutor and Thompson, (2025)\r\nAutor, D. and Thompson, N. (2025).Expertise.\r\nBailey et al., (2022)\r\nBailey, M., Johnston, D., Kuchler, T., Stroebel, J., and Wong, A. (2022).Peer effects in product adoption.American Economic Journal: Applied Economics, 14(3):488–526.\r\nBastani et al., (2025)\r\nBastani, H., Bastani, O., Sungu, A., Ge, H., Kabakcı, Ö., and Mariman, R. (2025).Generative AI without guardrails can harm learning: Evidence from high school mathematics.Proceedings of the National Academy of Sciences, 122(26):e2422633122.\r\nBeaman et al., (2021)\r\nBeaman, L., BenYishay, A., Magruder, J., and Mobarak, A. M. (2021).Can network theory-based targeting increase technology adoption?American Economic Review, 111(6):1918–1943.\r\nBick et al., (2025)\r\nBick, A., Blandin, A., and Deming, D. J. (2025).The rapid adoption of generative AI.Technical report, National Bureau of Economic Research.\r\nBonney et al., (2024)\r\nBonney, K., Breaux, C., Buffington, C., Dinlersoz, E., Foster, L. S., Goldschlag, N., Haltiwanger, J. C., Kroff, Z., and Savage, K. (2024).Tracking Firm Use of AI in Real Time: A Snapshot from the Business Trends and Outlook Survey.\r\nBonsu and Baffour-Koduah, (2023)\r\nBonsu, E. M. and Baffour-Koduah, D. (2023).From the consumers’ side: Determining students’ perception and intention to use chatgpt in ghanaian higher education.Journal of Education, Society & Multiculturalism, 4(1):1–29.\r\nBresnahan and Trajtenberg, (1995)\r\nBresnahan, T. F. and Trajtenberg, M. (1995).General purpose technologies ‘engines of growth’?Journal of econometrics, 65(1):83–108.\r\nBrynjolfsson and Hitt, (2000)\r\nBrynjolfsson, E. and Hitt, L. M. (2000).Beyond computation: Information technology, organizational transformation and business performance.Journal of Economic Perspectives, 14(4):23–48.\r\nBrynjolfsson et al., (2025)\r\nBrynjolfsson, E., Li, D., and Raymond, L. (2025).Generative AI at Work.The Quarterly Journal of Economics, 140(2):889–942.\r\nBushong and Gagnon-Bartsch, (2024)\r\nBushong, B. and Gagnon-Bartsch, T. (2024).Failures in forecasting: An experiment on interpersonal projection bias.Management Science, 70(12):8735–8752.\r\nCarvajal et al., (2024)\r\nCarvajal, D., Franco, C., and Isaksson, S. (2024).Will artificial intelligence get in the way of achieving gender equality?Discussion Paper 03, NHH Norwegian School of Economics.\r\n(15)\r\nComin, D. and Hobijn, B. (2010a).An exploration of technology diffusion.American Economic Review, 100(5):2031–2059.\r\n(16)\r\nComin, D. and Hobijn, B. (2010b).Technology Diffusion and Postwar Growth, volume 25, pages 209–246.University of Chicago Press.\r\nCui et al., (2024)\r\nCui, Z. K., Demirer, M., Jaffe, S., Musolff, L., Peng, S., and Salz, T. (2024).The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers.\r\nDavid, (1990)\r\nDavid, P. A. (1990).The dynamo and the computer: an historical perspective on the modern productivity paradox.The American economic review, 80(2):355–361.\r\nDell’Acqua et al., (2023)\r\nDell’Acqua, F., McFowland, E., Mollick, E. R., Lifshitz-Assaf, H., Kellogg, K., Rajendran, S., Krayer, L., Candelon, F., and Lakhani, K. R. (2023).Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality.\r\nDuflo et al., (2011)\r\nDuflo, E., Kremer, M., and Robinson, J. (2011).Nudging Farmers to Use Fertilizer: Theory and Experimental Evidence from Kenya.American Economic Review, 101(6):2350–2390.\r\nEloundou et al., (2024)\r\nEloundou, T., Manning, S., Mishkin, P., and Rock, D. (2024).GPTs are GPTs: Labor market impact potential of LLMs.Science, 384(6702):1306–1308.\r\nFairlie et al., (2014)\r\nFairlie, R. W., Hoffmann, F., and Oreopoulos, P. (2014).A community college instructor like me: Race and ethnicity interactions in the classroom.American Economic Review, 104(8):2567–2591.\r\nFelten et al., (2021)\r\nFelten, E. W., Raj, M., and Seamans, R. (2021).Occupational, Industry, and Geographic Exposure to Artificial Intelligence: A Novel Dataset and Its Potential Uses.\r\nFelten et al., (2023)\r\nFelten, E. W., Raj, M., and Seamans, R. (2023).How will Language Modelers like ChatGPT Affect Occupations and Industries?\r\nFoster and Rosenzweig, (1995)\r\nFoster, A. D. and Rosenzweig, M. R. (1995).Learning by doing and learning from others: Human capital and technical change in agriculture.Journal of political Economy, 103(6):1176–1209.\r\nFoster and Rosenzweig, (2010)\r\nFoster, A. D. and Rosenzweig, M. R. (2010).Microeconomics of technology adoption.Annu. Rev. Econ., 2(1):395–424.\r\nGallup, (2024)\r\nGallup (2024).AI in the Workplace: Answering 3 Big Questions.\r\nGiaccherini et al., (2019)\r\nGiaccherini, M., Herberich, D. H., Jimenez-Gomez, D., List, J. A., Ponti, G., and Price, M. K. (2019).The behavioralist goes door-to-door: Understanding household technological diffusion using a theory-driven natural field experiment.Technical report, National Bureau of Economic Research.\r\nGoehring et al., (2024)\r\nGoehring, G., Mezzanotti, F., and Ravid, S. A. A. (2024).Technology adoption and career concerns: Evidence from the adoption of digital technology in motion pictures.Technical report, National Bureau of Economic Research.\r\nGriliches, (1957)\r\nGriliches, Z. (1957).Hybrid corn: An exploration in the economics of technological change.Econometrica, 25(4):501–522.\r\nHall and Khan, (2003)\r\nHall, B. H. and Khan, B. (2003).Adoption of new technology.\r\n(32)\r\nHanda, K., Bent, D., Tamkin, A., McCain, M., Durmus, E., Stern, M., Schiraldi, M., Huang, S., Ritchie, S., Syverud, S., Jagadish, K., Vo, M., Bell, M., and Ganguli, D. (2025a).Anthropic education report: How university students use claude.\r\n(33)\r\nHanda, K., Tamkin, A., McCain, M., Huang, S., Durmus, E., Heck, S., Mueller, J., Hong, J., Ritchie, S., Belonax, T., et al. (2025b).Which economic tasks are performed with ai? evidence from millions of claude conversations.arXiv preprint arXiv:2503.04761.\r\nHanna et al., (2014)\r\nHanna, R., Mullainathan, S., and Schwartzstein, J. (2014).Learning Through Noticing: Theory and Evidence from a Field Experiment.The Quarterly Journal of Economics, 129(3):1311–1353.\r\nHartley et al., (2025)\r\nHartley, J., Jolevski, F., Melo, V., and Moore, B. (2025).The Labor Market Effects of Generative Artificial Intelligence.\r\nHumlum and Vestergaard, (2025)\r\nHumlum, A. and Vestergaard, E. (2025).Large Language Models, Small Labor Market Effects.\r\nHutto and Gilbert, (2014)\r\nHutto, C. and Gilbert, E. (2014).Vader: A parsimonious rule-based model for sentiment analysis of social media text.In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216–225.\r\nJehiel, (2018)\r\nJehiel, P. (2018).Investment strategy and selection bias: An equilibrium perspective on overoptimism.American Economic Review, 108(6):1582–1597.\r\nKelly et al., (2023)\r\nKelly, A., Sullivan, M., and Strampel, K. (2023).Generative artificial intelligence: University student awareness, experience, and confidence in use across disciplines.Journal of University Teaching and Learning Practice, 20(6):1–16.\r\nKharazian, (2025)\r\nKharazian, A. (2025).Ramp ai index: Monthly measurement of ai adoption by american businesses.Technical report, Ramp Economics Lab.\r\nLehmann et al., (2024)\r\nLehmann, M., Cornelius, P. B., and Sting, F. J. (2024).AI Meets the Classroom: When Does ChatGPT Harm Learning?\r\nLing and Imas, (2025)\r\nLing, Y. and Imas, A. (2025).Underreporting of AI use: The role of social desirability bias.Available at SSRN.\r\nLiu and Wang, (2024)\r\nLiu, Y. and Wang, H. (2024).Who on earth is using generative ai?Policy Research Working Paper WPS10870, World Bank Group, Washington, D.C.\r\nMcClain, (2024)\r\nMcClain, C. (2024).Americans’ use of ChatGPT is ticking up, but few trust its election information.\r\nMcDonald et al., (2025)\r\nMcDonald, N., Johri, A., Ali, A., and Collier, A. H. (2025).Generative artificial intelligence in higher education: Evidence from an analysis of institutional policies and guidelines.Computers in Human Behavior: Artificial Humans, 3:100121.\r\nMcElheran et al., (2023)\r\nMcElheran, K., Li, J. F., Brynjolfsson, E., Kroff, Z., Dinlersoz, E., Foster, L. S., and Zolas, N. (2023).AI Adoption in America: Who, What, and Where.\r\nMerchant, (2024)\r\nMerchant, B. (2024).The new luddites aren’t backing down.The Atlantic.Accessed: 2025-06-16.\r\nNam, (2023)\r\nNam, J. (2023).56% of college students have used ai on assignments or exams.Technical report.Edited by Lyss Welding. Accessed June 11, 2025.\r\nNolan, (2023)\r\nNolan, B. (2023).Here are the schools and colleges that have banned chatgpt.Business Insider.Accessed: 2025-01-27.\r\nNoy and Zhang, (2023)\r\nNoy, S. and Zhang, W. (2023).Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence.\r\nOtis et al., (2024)\r\nOtis, N. G., Delecourt, S., Cranney, K., and Koning, R. (2024).Global Evidence on Gender Gaps and Generative AI.Harvard Business School.\r\nPeng et al., (2023)\r\nPeng, S., Kalliamvakou, E., Cihon, P., and Demirer, M. (2023).The Impact of AI on Developer Productivity: Evidence from GitHub Copilot.\r\nRavšelj et al., (2025)\r\nRavšelj, D., Keržič, D., Tomaževič, N., Umek, L., Brezovar, N., A. Iahad, N., Abdulla, A. A., Akopyan, A., Aldana Segura, M. W., AlHumaid, J., Allam, M. F., Alló, M., Andoh, R. P. K., Andronic, O., Arthur, Y. D., Aydın, F., Badran, A., Balbontín-Alvarado, R., Ben Saad, H., Bencsik, A., Benning, I., Besimi, A., Bezerra, D. D. S., Buizza, C., Burro, R., Bwalya, A., Cachero, C., Castillo-Briceno, P., Castro, H., Chai, C. S., Charalambous, C., Chiu, T. K. F., Clipa, O., Colombari, R., Corral Escobedo, L. J. H., Costa, E., Cre\\textcommabelowtulescu, R. G., Crispino, M., Cucari, N., Dalton, F., Demir Kaya, M., Dumić-Čule, I., Dwidienawati, D., Ebardo, R., Egbenya, D. L., Faris, M. E., Fečko, M., Ferrinho, P., Florea, A., Fong, C. Y., Francis, Z., Ghilardi, A., González-Fernández, B., Hau, D., Hossain, M. S., Hug, T., Inasius, F., Ismail, M. J., Jahić, H., Jessa, M. O., Kapanadze, M., Kar, S. K., Kateeb, E. T., Kaya, F., Khadri, H. O., Kikuchi, M., Kobets, V. M., Kostova, K. M., Krasmane, E., Lau, J., Law, W. H. C., Lazăr, F., Lazović-Pita, L., Lee, V. W. Y., Li, J., López-Aguilar, D. V., Luca, A., Luciano, R. G., Machin-Mastromatteo, J. D., Madi, M., Manguele, A. L., Manrique, R. F., Mapulanga, T., Marimon, F., Marinova, G. I., Mas-Machuca, M., Mejía-Rodríguez, O., Meletiou-Mavrotheris, M., Méndez-Prado, S. M., Meza-Cano, J. M., Mirķe, E., Mishra, A., Mital, O., Mollica, C., Morariu, D. I., Mospan, N., Mukuka, A., Navarro Jiménez, S. G., Nikaj, I., Nisheva, M. M., Nisiforou, E., Njiku, J., Nomnian, S., Nuredini-Mehmedi, L., Nyamekye, E., Obadić, A., Okela, A. H., Olenik-Shemesh, D., Ostoj, I., Peralta-Rizzo, K. J., Peštek, A., Pilav-Velić, A., Pires, D. R. M., Rabin, E., Raccanello, D., Ramie, A., Rashid, M. M. U., Reuter, R. A. P., Reyes, V., Rodrigues, A. S., Rodway, P., Ručinská, S., Sadzaglishvili, S., Salem, A. A. M. S., Savić, G., Schepman, A., Shahpo, S. M., Snouber, A., Soler, E., Sonyel, B., Stefanova, E., Stone, A., Strzelecki, A., Tanaka, T., Tapia Cortes, C., Teira-Fachado, A., Tilga, H., Titko, J., Tolmach, M., Turmudi, D., Varela-Candamio, L., Vekiri, I., Vicentini, G., Woyo, E., Yorulmaz, Ö., Yunus, S. A. S., Zamfir, A.-M., Zhou, M., and Aristovnik, A. (2025).Higher education students’ perceptions of ChatGPT: A global study of early reactions.PLOS ONE, 20(2):e0315011.\r\nRogers, (1962)\r\nRogers, E. M. (1962).Diffusion of Innovations.Free Press, New York.\r\nRoss et al., (1977)\r\nRoss, L., Greene, D., and House, P. (1977).The “false consensus effect”: An egocentric bias in social perception and attribution processes.Journal of experimental social psychology, 13(3):279–301.\r\nSacerdote, (2001)\r\nSacerdote, B. (2001).Peer Effects with Random Assignment: Results for Dartmouth Roommates.The Quarterly Journal of Economics, 116(2):681–704.\r\nStinebrickner and Stinebrickner, (2008)\r\nStinebrickner, R. and Stinebrickner, T. R. (2008).The causal effect of studying on academic performance.The BE Journal of Economic Analysis & Policy, 8(1).\r\nStöhr et al., (2024)\r\nStöhr, C., Ou, A. W., and Malmström, H. (2024).Perceptions and usage of AI chatbots among students in higher education across genders, academic levels and fields of study.Computers and Education: Artificial Intelligence, 7:100259.\r\nStokey, (2021)\r\nStokey, N. L. (2021).Technology diffusion.Review of Economic Dynamics, 42:15–36.\r\nWorld Bank, (2016)\r\nWorld Bank (2016).World development report 2016: Digital dividends.World Bank Publications.\r\nWu et al., (2022)\r\nWu, M.-J., Zhao, K., and Fils-Aime, F. (2022).Response rates of online surveys in published research: A meta-analysis.Computers in Human Behavior Reports, 7.\r\nXiao et al., (2023)\r\nXiao, P., Chen, Y., and Bao, W. (2023).Waiting, banning, and embracing: An empirical analysis of adapting policies for generative ai in higher education.arXiv preprint arXiv:2305.18617.\r\nAppendix\r\n\r\nAppendix AAppendix Figures and Tables\r\nFigure A1:Generative AI Usage Survey Design Overview\r\nKey\t\r\nSurvey Structure\r\nSurvey Sections\r\nUsage Data Collection\r\nBeliefs & Perceptions\r\nAI Usage Survey at Middlebury College\r\nDecember 2024\r\nParticipant Recruitment\r\nCampus-wide email invitations, framed as general technology use survey\r\nParticipation Incentives\r\nAmazon gift card lottery ranging from $50-$500\r\nSection 1: Demographics & Academic Information\r\nStudent Characteristics\r\nGender, race/ethnicity, high school type\r\nAcademic Profile\r\nYear, major, self-reported GPA, study hours\r\nSection 2: generative AI Usage Patterns\r\nAdoption Metrics\r\nFrequency, timing of first use, specific AI models used\r\nAcademic Applications\r\nTasks performed with AI, payment for premium services\r\nSection 3: Perceptions & Institutional Policies\r\nImpact Perceptions\r\nEffects on learning, grades, time management\r\nPolicy Responses\r\nUsage likelihood under different policies\r\nPeer Usage Beliefs\r\nEstimates of AI adoption among peers for schoolwork and leisure\r\nOpen-Ended Response Collection\r\nMotivations for AI use, policy feedback, suggestions for improvement\r\nNote: This figure illustrates the structure of the AI usage survey conducted at Middlebury College in December 2024. The survey collected information across three main sections: (1) demographic and academic background, (2) patterns of generative AI usage including adoption timing, frequency, and specific applications, and (3) perceptions of AI’s impact on learning and responses to institutional policies.\r\n\r\nFigure A2:Cumulative Generative AI Use by Student Characteristic\r\nPanel A. By Gender\r\nRefer to caption\r\nPanel B. By Race\r\nRefer to caption\r\nPanel C. By High School Type\r\nRefer to caption\r\nPanel D. By Freshman Year GPA\r\nRefer to caption\r\nPanel E. By Cohort\r\nRefer to caption\r\nPanel F. By Field of Study\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents cumulative AI use based on different student characteristics. Each panel displays the cumulative distribution of AI use based on a specific characteristic: gender, race, school type, first-year GPA, cohort, or field of study. The cumulative percent of students is plotted against usage categories. The legends and colors correspond to subgroups within each demographic variable.\r\n\r\nFigure A3:Percent of Students Who Pay for Generative AI Tools\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of AI users who pay for AI tools (through any platform) across different demographic groups. Horizontal lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure A4:Academic Uses of ChatGPT: Evidence from Global Survey\r\nPanel A. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\nPanel B. Across the College Quality Distribution\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of students who use ChatGPT for different academic tasks based on data from Ravšelj et al., (2025). Panel A displays usage patterns across tasks categorized as augmenting (proofreading, translating, study assistance, research assistance) versus automating (academic writing, professional writing, creative writing, brainstorming, summarizing, calculating, coding assistance, personal assistance) student effort. Panel B shows usage patterns across university quality quintiles based on World University Rankings, with universities ranked in the top 20% (top quintile) showing slightly higher rates of augmentation relative to automation compared to bottom quintile institutions. The analysis includes universities with at least 30 student responses and excludes observations with missing usage data.\r\n\r\nFigure A5:Student Beliefs about AI Usage versus Actual Usage under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure compares students’ beliefs about AI usage with actual usage rates under different AI policies. For each policy type, blue bars show the percent of students who report being “Neutral,” “Likely,” or “Very Likely” to use AI, while red bars show students’ mean beliefs about what percent of their peers use AI in classes with that policy. The sample includes all survey respondents for both actual usage and beliefs measures. “AI Use is Unrestricted” refers to classes with no restrictions on AI use, “No Explicit AI Policy” refers to classes without a stated policy, and “AI Use is Prohibited” refers to classes that ban AI use entirely.\r\n\r\nFigure A6:Relationship Between Beliefs About AI Usage and Actual AI Usage\r\nRefer to caption\\justify\r\nNotes: This figure shows the relationship between students’ beliefs about AI usage among their peers and actual AI usage rates across different demographic groups. Each point represents a different group of students (by demographics, academic characteristics, or field of study). The \r\ny\r\n-axis shows the percent of students in each group who report using AI for academic purposes. The \r\nx\r\n-axis shows the mean belief within each group about what percent of Middlebury students use AI. The dashed line shows the linear fit. Sample includes all survey respondents with at least ten observations per group. Students’ beliefs about AI usage are positively correlated with actual usage patterns, suggesting that students have relatively accurate perceptions of AI adoption among their peers.\r\n\r\nGPA > p50 80.3 26.3 17.4 23.2 13.4 GPA < p50 87.1 18.7 26.3 32.2 9.9 Freshman 81.3 24.3 24.1 24.7 8.1 Sophomore 84.4 25.6 22.6 23.7 12.4 Junior 86.2 21.6 23.1 28.6 12.8 Senior 78.7 21.0 17.4 29.9 10.4\r\n\r\nPanel C. Field of Study    Arts 73.3 21.0 0.0 52.3 0.0 Humanities 74.8 39.3 21.8 12.1 1.7 Languages 57.4 27.0 30.3 0.0 0.0 Literature 48.6 10.0 33.7 5.0 0.0 Natural Sciences 91.1 22.0 21.9 33.8 13.3 Social Sciences 84.6 17.9 20.4 29.1 17.2\r\n\r\n\\justify\r\nNotes: This table presents the percent of students in each demographic group who report using AI at different frequencies during the academic semester. Each cell shows the percent of students within that group. Column 1 reports the total percent who use AI at any frequency. Columns 2 to 5 represent increasing usage frequencies: rarely (1–2 times per semester), occasionally (monthly), frequently (weekly), and very frequently (multiple times per week). Panel A reports percentages by demographic characteristics. Panel B shows percentages by academic characteristics. Panel C presents percentages by field of study.\r\n\r\n0.9730.1350.0770.1100.114 R\r\n−\r\nsquared0.0120.0510.0270.0240.138 N (Students)516516516516516 \\justifyNotes: This table assesses the relationship between AI model adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI model usage (columns 1-4) or payment for AI services (column 5), and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division.\r\n\r\nEach column presents results for a different model or payment outcome. Column 1 shows usage of OpenAI’s ChatGPT, column 2 Google Gemini, column 3 Microsoft Copilot, column 4 any other AI model, and column 5 whether the student pays for any generative AI service.\r\n\r\nThe omitted categories are Natural Sciences for academic division, white students for race/ethnicity, and freshmen for cohort. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.7510.6760.6610.2710.479 R\r\n−\r\nsquared0.0870.1130.0760.0680.041 N (Students)599599599599599 \\justifyNotes: This table examines how student characteristics relate to self-reported likelihood of using generative AI under different policy scenarios. Each column presents results for different policy scenarios. In columns 1-4, the dependent variable equals one if the student reports being“neutral,” “likely” or “extremely likely” to use AI under the specified policy, and zero if they report being “unlikely,” or “extremely unlikely.” Column 5 represents the impact of moving from unrestricted use to complete prohibition.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.5760.6850.3760.4890.564 R\r\n−\r\nsquared0.1020.1190.0840.0370.087 N (Students)561565543560560 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. Each column presents results for beliefs about different academic outcomes: learning ability (e.g., ability to grasp concepts, retain information, or learn new skills) in column 1, understanding of course materials in column 2, course grades in column 3, ability to complete assignments on time in column 4, and time spent on academics in column 5. The dependent variable in each regression equals one if the student believes AI somewhat improves” or significantly improves” the outcome, and zero if they believe it has no effect, reduces, or significantly reduces the outcome. “Don’t know” responses are excluded.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\nAppendix BEmpirical Appendix\r\nB.1Field of Study Classifications\r\nThis appendix details the classification of majors into broad fields of study used in Figure 1:\r\n\r\n• Natural Sciences: Includes Biology, Biochemistry, Chemistry, Computer Science, Earth and Climate Sciences/Geology, Environmental Studies, Mathematics, Molecular Biology & Biochemistry, Neuroscience, Physics, and Statistics.\r\n• Social Sciences: Includes Anthropology, Economics, Education, Geography, International & Global Studies, International Politics & Economics, Political Science, Psychology, and Sociology.\r\n• Humanities: Includes American Studies, Black Studies, Classics, History, Architectural Studies, Art History & Museum Studies, History of Art & Architecture, Philosophy, Religion, and Classical Studies.\r\n• Literature: Includes Comparative Literature, English/English & American Literatures, and Literary Studies.\r\n• Languages: Includes Arabic, Chinese, French & Francophone Studies, German, Japanese Studies, Russian, and Spanish.\r\n• Arts: Includes Film & Media Culture, Music, Studio Art, and Theatre.\r\nB.2Task-Specific Use of Generative AI\r\nThe aggregate patterns documented in Section 4 may mask important heterogeneity across student groups. Different students may adopt AI tools at varying rates depending on their academic needs and field-specific norms. To investigate this heterogeneity systematically, Appendix Table LABEL:tab:ai_purpose_correlates presents regression estimates examining how student characteristics predict AI usage for each of our ten measured academic tasks.\r\n\r\nUsage patterns differ markedly by student characteristics. Male students consistently exhibit higher adoption rates across most applications, with particularly pronounced differences for finding information (16.4 pp; \r\np\r\n<\r\n0.01\r\n), summarizing texts (12.5 pp; \r\np\r\n<\r\n0.01\r\n), and creating images (16.9 pp; \r\np\r\n<\r\n0.01\r\n). Black students show substantially higher usage for information-gathering tasks, being 23.8 pp more likely to use AI for finding information compared to white students (\r\np\r\n<\r\n0.05\r\n). They also exhibit dramatically higher adoption of writing assistance tools, with 27.6 pp higher usage for editing text (\r\np\r\n<\r\n0.05\r\n) and 23.6 pp for writing emails (\r\np\r\n<\r\n0.10\r\n). Latino students demonstrate significantly higher usage for generating ideas (24.0 pp; \r\np\r\n<\r\n0.01\r\n) and writing emails (20.7 pp; \r\np\r\n<\r\n0.01\r\n), while Asian students show markedly higher adoption for writing emails (24.5 pp; \r\np\r\n<\r\n0.01\r\n) and explaining concepts (10.9 pp, \r\np\r\n<\r\n0.05\r\n). Students from public high schools report lower usage rates for several applications, particularly for concept explanation (7.3 pp lower; \r\np\r\n<\r\n0.10\r\n) and writing emails (7.4 pp lower; \r\np\r\n<\r\n0.10\r\n), but higher usage for proofreading (8.7 pp higher; \r\np\r\n<\r\n0.10\r\n).\r\n\r\nField of study emerges as a particularly strong predictor of usage patterns. Arts majors show significantly lower adoption across multiple tasks compared to Natural Science students, with gaps of 49.7 pp for finding information (\r\np\r\n<\r\n0.01\r\n), 44.0 pp for generating ideas (\r\np\r\n<\r\n0.05\r\n), and 62.1 pp for coding assistance (\r\np\r\n<\r\n0.01\r\n). Similarly, humanities majors are 36.8 pp less likely to use AI for generating ideas (\r\np\r\n<\r\n0.01\r\n) and 30.8 pp less likely for coding assistance (\r\np\r\n<\r\n0.01\r\n). Languages majors exhibit the most pronounced differences, being 52.4 pp less likely to use AI for summarizing texts (\r\np\r\n<\r\n0.01\r\n) and 46.9 pp less likely for generating ideas (\r\np\r\n<\r\n0.01\r\n). By contrast, social science students show higher usage for explaining concepts (8.0 pp; \r\np\r\n<\r\n0.10\r\n) and summarizing texts (14.7 pp; \r\np\r\n<\r\n0.01\r\n).\r\n\r\n0.8030.7400.6310.6190.5410.4730.3710.3440.2350.204 R\r\n−\r\nsquared0.0780.0870.0770.1160.0280.0540.0830.1350.0400.094 N (Students)515515515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and use of AI for specific academic tasks. Each column shows the result for a different academic task. The omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Regressions are weighted and use heteroskedasticity-robust standard errors clustered at the student level. ∗∗∗, ∗∗, and ∗ indicate significance at the 1%, 5%, and 10% levels, respectively.\r\n\r\nAppendix CQualitative Evidence on Student Perspectives on AI Use\r\nIn this section, we analyze student responses to an open-ended question about their use of generative AI. The survey question asked: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” This was an optional question that 48.3 percent of survey respondents answered. Appendix Figure C1 presents a word cloud of the most frequent words in student responses.\r\n\r\nC.1Validating the Open-Ended Response Measure\r\nWe begin by validating our open-ended response measure to show that it contains meaningful signal. To validate this measure, we employ sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner), a widely-used lexicon and rule-based sentiment analysis tool (Hutto and Gilbert, , 2014). We calculate sentiment scores for each response and examine whether these scores correlate with students’ actual AI adoption behaviors. Intuitively, students who express more positive sentiment toward AI in their open-ended responses should be more likely to have adopted AI tools in practice. We test this hypothesis by examining two measures of AI adoption: whether students have ever used generative AI and whether they currently use AI for academic purposes. Appendix Figure C2 presents binned scatterplots showing the relationship between AI adoption rates and standardized sentiment scores.\r\n\r\nSentiment towards generative AI strongly predicts AI adoption. For both outcomes, students with negative sentiment scores (below zero) show substantially lower adoption rates—around 70-85 percent for ever using AI and 70-75 percent for academic use. In contrast, students with positive sentiment scores display markedly higher adoption rates, reaching nearly 100 percent for general use and 95-100 percent for academic purposes among those with the most positive sentiment. The relationship is particularly pronounced for academic AI use (Panel B), where the coefficient of \r\nβ\r\n^\r\n=\r\n0.168\r\n is twice as large as for general adoption.17 These systematic patterns confirm that our open-ended responses capture meaningful variation in student attitudes that corresponds to real behavioral differences.\r\n\r\nC.2How Students use Generative AI\r\nTo analyze these responses systematically, we classified each response using keywords based on their content. For example, if a student mentioned using AI to save time, we tagged the response with the keyword “time-saver.” If a student expressed concerns about AI’s impact on learning, we tagged it with “negative learning.” Responses could receive multiple keywords if they discussed several themes. Appendix Figure C3 shows the frequency of keywords in our classification. The responses reveal how students use AI tools, what motivates this use, and what causes some to avoid or limit their use.\r\n\r\nThe most common use of AI is as an explanatory tool. Nearly 30 percent of responses mentioned using AI to understand course material. Students frequently ask AI to break down complex concepts from readings and lectures, particularly when they find the material difficult to understand. For example, one student reported: “I can ask AI to explain concepts to me that I have a hard time grasping. […] I can keep asking ’simplify’ or ’break down even more.”’ Students also use AI to summarize dense academic readings, which they argue helps them manage heavy reading loads.\r\n\r\nStudents employ AI throughout different stages of the writing process. Some use AI to generate initial drafts that serve as starting points. One student explained: “Helps me get started with a base for most of my essays. It feels easier to edit something already written and make it my own than to write from scratch.” Others use AI more narrowly for brainstorming when stuck on specific problems. As one student noted: “I use it if I am feeling stuck to push me to the right direction (whether a mathematical problem or an essay idea).” Many also report using AI as an editing tool to improve grammar, sentence structure, and overall writing flow. This is especially the case for non-native English speakers. As one student explained: “English is not my first language and it frustrates me sometimes that I cannot find the best way to phrase a certain idea and AI is a useful tool to have to find alternate expressions.”\r\n\r\nStudents frequently mentioned using AI for specific academic tasks. In courses that require coding, students often use AI for debugging code and understanding programming concepts. Students also employ AI for administrative tasks like formatting citations and drafting routine emails. Finally, many students use AI as an enhanced search engine. One student reported: “It has significantly reduced the time it takes to conduct research on new topics and ideas, and helps me by giving me a thorough selection of sources to use for projects of any kind.”\r\n\r\nC.3Why Students Adopt AI\r\nTime savings was the most commonly cited reason for using AI. Nearly 30 percent of responses mentioned using AI to complete work more efficiently. Students often viewed AI as a way to manage demanding course loads. Many students particularly embrace AI assistance for tasks viewed as mechanical or administrative. A student noted they use AI for “Writing emails quick and creating resume/ cover letter templates.” But AI assistance goes beyond grunt work. Some students use it to “spend less time doing assignments and homework.” This is particularly true if students don’t view the work as central to their academic experience. As one student explained: “when I come across work I deem as ineffective, I want to spend as little time as possible doing it.”\r\n\r\nHaving an on-demand tutor for academic support was another key motivation. One-quarter of responses described using AI as an “explainer” when other resources were unavailable or inconvenient. As one student noted, “I use it as a last resort (if there are no office hours, after looking up videos, etc.) if I need extra help. I’d like to think that the way I use it is similar to going to office hours or TA hours.”\r\n\r\nPeer influence also drove AI adoption. Some students reported feeling pressure to use AI to remain competitive with their classmates. One student explained: “I noticed others use it, are getting better grades than me, and they say they learn better with the help of AI, so I gave it a try.” Others worried about being at a competitive disadvantage: “Other people were using it and told me about it. I felt like I would be at a disadvantage if I wasn’t also using it.”\r\n\r\nC.4Concerns and Limitations\r\nStudents expressed several concerns about AI use in academic work. The most frequent worry was about negative impacts on learning. One student who initially used AI extensively reported: “In the past, I have simply plugged and chugged homework assignments into ChatGPT and submitted it. Those assignments feedback from teachers was positive and I was getting good grades, but I definitely felt that my own learning outcomes to be significantly worse.” Other students viewed AI use as fundamentally incompatible with their educational goals. As one humanities student explained: “my task is as a humanities student is to think, not calculate; why should I let AI do the thinking for me? It would defeat the purpose of pursuing my education.”\r\n\r\nMany students described ethical concerns about AI use. Responses suggested uncertainty about appropriate boundaries. One student noted: “I never use it to explicitly write something because that feels like overt cheating, but sometimes I hesitate when it completely solves Econ problems. I understand how it does it, and it helps me to learn, but it still sometimes feels a little morally gray.” Another expressed similar ambivalence: “I tend to only use it when [it] will save me time in a moral way.”\r\n\r\nStudents also emphasized the importance of maintaining ownership of their work. Many expressed pride in producing original work and hesitation about diluting that ownership through AI use. As one student explained: “I don’t have interest in using generative AI for my academic work because I want my work to reflect my own ideas.” Another noted: “It usually would not even occur to me to turn to AI to substitute writing because I want to take credit for my work, and using AI seems to diminish that.”\r\n\r\nTechnical limitations deterred some students from using AI tools. Students reported concerns about inaccurate outputs (“hallucinations”) and poor output quality, particularly for creative writing or complex analytical tasks. For example, one student noted that “In my poetry class we were instructed to use it to come up with poems and they were awful, so that kinda turned me away from using it to do my work for me.”\r\n\r\nC.5Discussion\r\nWe conclude with two overarching themes that emerged from the responses.\r\n\r\nFirst, students vary substantially in how they incorporate AI into their academic lives. Crucially, this heterogeneity largely depends on what students perceive as “appropriate” uses of AI. For activities that they perceive as core to their academic journey—like writing essays or solving problems—many students hesitate to use AI. A student articulated this clearly: “Most of my work is writing or reading. If I’m not doing the writing, what is the purpose of me taking the class?” Yet, students draw different boundaries between central tasks and grunt work. Some use AI extensively, viewing their role as akin to a manager that provides high-level direction while AI handles implementation. Others restrict AI use to specific tasks like brainstorming, editing, or drafting emails. Still others avoid AI entirely for academic work, often for ethical reasons. Even among AI users, adoption patterns reflect individual trade-offs between time savings, learning goals, and academic integrity concerns.\r\n\r\nSecond, there appears to be a fundamental tension between efficiency and learning. The time-saving benefits are easy to observe, quantifiable, and tangible. But these time savings are unlikely to be a free lunch. Some benefits may come at the cost of spending less time with material that requires deeper engagement to digest. As one student noted: “There may be a negative effect in that it eliminates much of the ’struggle’ in learning.” Yet, having an on-demand tutor that explains concepts in relatable ways can also improve learning. As one student explained: “It can explain concepts to me in a way that is tailored to my learning style.” This suggests that the impact of AI use on learning outcomes depends not on whether students use AI—almost all do, to some extent—but rather on how they employ these tools.\r\n\r\nFigure C1:Word Cloud of Student Motivations for Generative AI Use\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 147 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?’\r\n\r\nFigure C2:Relationship Between AI Sentiment and AI Adoption\r\nPanel A. Ever Used Generative AI\r\nRefer to caption\r\nPanel B. Uses AI for Academic Purposes\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents the relationship between AI sentiment and AI adoption. Panel A shows the proportion of respondents who have ever used generative AI, while Panel B shows the proportion who use AI for academic purposes. Each point represents the mean adoption rate for respondents within sentiment score bins of width 0.2. Sentiment scores are standardized compound scores computed using Hutto and Gilbert, (2014)’s VADER algorithm applied to responses to an open-ended question about generative AI. Positive values indicate positive sentiment and negative values indicate negative sentiment. The dashed lines show OLS best-fit lines estimated on the microdata, with coefficients and standard errors (in parentheses) displayed in the top-left corner of each panel.\r\n\r\nFigure C3:Frequency of Keywords in Student Motivations for AI Use\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to AI use. The responses come from the question “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” Color coding indicates the category of each theme. Usage type refers to how students use AI tools. Motivation captures what drove students to try AI. Concerns include mentions of course policies and academic integrity, individual reservations about AI use, worries about AI’s impact on education, and AI’s technical limitations.\r\n\r\nAppendix DQualitative Evidence on Student Views of AI Policies\r\nIn this section, we analyze student responses to an open-ended question about Middlebury’s AI policies. The survey asked: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Appendix Figure D1 presents a word cloud of the most frequent words in student responses. To analyze these responses systematically, we classified each response using keywords based on their content. Appendix Figure D2 shows the frequency of keywords in our classification.\r\n\r\nD.1Polarized Views on Generative AI Policy Approaches\r\nStudents expressed markedly different views about appropriate AI policies, revealing fundamental disagreement about the path forward. Some strongly advocated for embracing AI technology. As one student argued, “The tool is there, there is supply and there is demand. Don’t fight another war on drugs. Don’t live in a fake reality”. Others called for significant restrictions, arguing that “the use of generative AI is dishonest and corrosive” and that it “prohibits these organic processes and divorces students from true learning”.\r\n\r\nHowever, the most common position advocated for a balanced approach that would allow beneficial uses while restricting harmful ones. Students distinguished between uses that enhance learning (like concept explanation) and those that substitute for learning (like generating entire essays). One student articulated this nuanced view particularly well: “AI also can really be helpful at explaining a textbook problem that doesn’t make sense, or guiding slightly with homework, or creating study materials, or editing/tightening up your prose. All of those things are good, and universities should figure out how to maximize AI use for those reasons and to minimize students just feeding their problem sets into ChatGPT”\r\n\r\nA recurring theme was the futility of blanket bans. Many students emphasized that prohibition would be ineffective, with one noting “I don’t think anyone really cares what the policy of any given class is. If professors want people to not use it, they need to structure assessments in a way that will discourage use.” Another compared AI bans to restricting internet use, arguing “AI policies seem to be totally irrelevant. It’s like telling people they can’t use the internet as a resource for the class.” This ineffectiveness of bans creates fairness concerns. As one student explained: “I think if it is banned in a class, that should be enforced (and right now it absolutely is not)… As with any form of cheating, those who don’t cheat are put at a disadvantage.” Another student expressed similar frustration: “I find it discouraging when I hear classmates saying they use AI for things such as essays when they use it in dishonest ways.”\r\n\r\nD.2Need for Clear Guidelines\r\nThe most frequently expressed concern was the need for clear guidelines about generative AI use. Several students reported confusion about what constitutes acceptable use, with one noting “I think it should be more clear whether we can use it and how and how to cite it since most professors rarely mention it at all”. Other students emphasized the importance of professors explaining their policies upfront and their rationale, with one stating “I think that Professor’s should be very specific about what is allowed and their reasoning behind their policy”.\r\n\r\nMany students advocated for standardization across classes, observing that “Sometimes its confusing when one class allows it and another doesn’t and the other encourages it and so on so if there was a school wide or department wide policy that could help”. Yet there is also disagreement regarding standardization. Some students preferred leaving the decision to individual professors, arguing that “GenAI is more effective in some classes/majors than others. Making sure professors understand how students use GenAI and how useful GenAI is in their class (given the course structure, nature of assignments/material, etc.) is very important for the class policy.”\r\n\r\nD.3Training and Support Services\r\nStudents strongly emphasized the need for training in appropriate generative AI use. Many suggested that the college should provide guidance on using AI tools effectively while maintaining academic integrity. One student proposed “a workshop that teaches you to effectively use GenAI without violating the honor code”. Students also expressed interest in learning how to leverage AI to enhance their learning experience rather than circumvent it. As one student explained: “I think it could be useful to develop some sort of training. How do we use AI in a way that actually benefits our learning? I tried out some things on my own but I feel that I need more guidance.”\r\n\r\nThis desire for training was often linked to workplace preparedness. Students recognized that AI proficiency would be valuable in their careers, with one noting “As the world uses more and more AI, I think it is an important tool that students should know how to leverage”. Another emphasized: “The moment us students leave campus, we will be using it in the professional world, and when used in combination with one’s own skills, it is merely a tool to maximize efficiency”.\r\n\r\nD.4Discussion\r\nThe responses reveal several key tensions in AI policy implementation. First, students express a desire to use AI in ways that enhance rather than substitute for learning, yet they recognize that blanket bans are ineffective and worry about being disadvantaged if they follow restrictions while others do not.\r\n\r\nSecond, while students desire clear guidelines, they also want flexibility to accommodate legitimate uses that vary by discipline and assignment type. Different courses and majors may find different AI uses appropriate based on their learning objectives and assessment types.\r\n\r\nThird, there is tension between faculty autonomy in setting course policies and students’ desire for consistent institutional standards. While some support letting professors determine appropriate AI use for their specific courses, others argue that varying policies across classes create confusion and enforcement challenges.\r\n\r\nFinally, students report significant variation in faculty attitudes toward AI. Some students perceived faculty fear or misunderstanding of AI tools, noting that categorical bans often reflect a lack of understanding about AI’s capabilities and limitations rather than pedagogical considerations.\r\n\r\nFigure D1:Word Cloud of Student Feedback on Generative AI Policies\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 133 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?”\r\n\r\nFigure D2:Frequency of Keywords in Student Feedback on AI Policies\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to Middlebury’s AI policies. The responses come from the question “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Color coding indicates the category of each theme. Policy Approach captures suggestions about how AI should be regulated at the college. Adoption Views reflect positions on whether and how AI should be integrated into academic work. Training Needs indicates requests for guidance and support. Other Concerns include issues of workplace relevance and fairness.\r\n\r\nSee pages 1 of results/ai-survey-midd.pdf See pages 2- of results//ai-survey-midd.pdf",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== story.txt ===\nI remember the moment that sparked this project. It was during a walk across Middlebury’s campus late one fall afternoon—just after office hours. A student had come in, clearly bright but struggling, and told me they’d been using ChatGPT late at night to help with assignments because they were too anxious to speak up in class. That confession stuck with me.\r\n\r\nLater, over coffee with Germán, we started wondering—how many students were turning to generative AI as a quiet lifeline? Not to cheat, but to learn? That simple question became the seed for this entire study. What we found wasn’t just statistics. It was a story of adaptation, of students reshaping how they learn, and of institutions racing to catch up.\"\n\n==================================================\n\n=== Generative AI in Higher Education E.txt ===\nGenerative AI in Higher Education: Evidence from an Elite College†\r\nZara Contractor\r\nGermán Reyes\r\nAbstract\r\nGenerative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT’s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI’s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI’s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\r\n\r\n1Introduction\r\nThe rapid advancement of generative artificial intelligence (AI) is transforming higher education at an unprecedented pace. The launch of ChatGPT and similar tools has introduced technologies capable of performing tasks central to academic assessment and learning—writing essays, solving complex problems, and explaining intricate concepts—instantly and at near-zero marginal cost. Yet systematic evidence on the nature and implications of this new technology remains scarce. How widespread is generative AI adoption among students, and what factors drive it? Do students primarily use AI to augment their learning or to automate coursework, potentially harming human capital development? Could disparities in access to premium AI resources amplify existing educational inequalities?\r\n\r\nThese questions are at the heart of ongoing debates about the role of AI in education, which have relied heavily on anecdotal evidence and speculation rather than rigorous evidence. As a result, universities have implemented policies that vary dramatically, from outright bans to permissive adoption, often without clear evidence on their effectiveness or unintended consequences (Nolan, , 2023; Xiao et al., , 2023; McDonald et al., , 2025).\r\n\r\nThis paper addresses this gap by systematically examining generative AI adoption at Middlebury College, a highly selective liberal arts college in Vermont. With approximately 2,800 undergraduate students, Middlebury offers 49 majors across the arts, humanities, languages, social sciences, and natural sciences. Over the past decade, Middlebury has consistently ranked among the top 10 liberal arts colleges in U.S. News & World Report, with an average ranking of 8.5 between 2015 and 2025. The college’s selectivity is comparable to that of many elite research universities—Middlebury’s acceptance rate of approximately 10 percent is similar to that of Boston University (11 percent), Georgetown University (13 percent), and the University of Virginia (16 percent).\r\n\r\nOur analysis draws on survey data collected from the student population between December 2024 and February 2025. The survey collected detailed information about AI usage, including the frequency and purpose of use, perceived impacts, and responses to institutional policies. To minimize selection bias, we framed the survey broadly as examining technology use and provided incentives for participation, achieving a 22.9 percent response rate with 634 responses representing a broad cross-section of the student population.\r\n\r\nWe document five main findings. First, generative AI is approaching near-universal adoption at an unprecedented speed. Over 80 percent of students use AI for academic purposes, up from less than 10 percent before Spring 2023. This represents one of the fastest technology adoption episodes ever documented, dramatically exceeding the 40 percent adoption rate among U.S. workers (Bick et al., , 2025) and the 23 percent among all U.S. adults (McClain, , 2024). These levels are consistent with international findings of 50–70 percent adoption in university contexts (Nam, , 2023; Stöhr et al., , 2024; Ravšelj et al., , 2025).\r\n\r\nSecond, AI adoption is markedly unequal across academic disciplines and demographic groups. Field of study is the strongest predictor of adoption, likely reflecting how well AI capabilities align with the academic tasks required across fields. Adoption ranges from 91.1 percent in Natural Sciences majors (including mathematics and computer science) to significantly lower levels in Literature (48.6 percent) and Languages (57.4 percent). Additionally, demographic disparities exist. For example, males adopt AI at higher rates than females (88.7 versus 78.4 percent), a pattern that is consistent with documented AI gender gaps in other contexts (Otis et al., , 2024). Most notably, lower-achieving students have a higher adoption rate than their higher-achieving peers (87.1 versus 80.3 percent). As a result, AI could serve as an equalizing force if it enhances student learning—helping struggling students catch up to their peers—but could also widen achievement gaps if AI undermines skill development.\r\n\r\nThird, generative AI transforms the students’ learning production function by both augmenting student capabilities and automating academic tasks. We classify academic tasks as augmentation when AI enhances human capabilities while maintaining student engagement (e.g., explaining concepts, proofreading) versus automation when AI directly produces outputs with minimal cognitive involvement (e.g., writing essays, creating images), and average usage rates across tasks within each category. We find that 61.2 percent of AI users employ these tool for augmentation purposes, while 41.9 percent use it for automation. Qualitative evidence reinforces these patterns: students describe AI as an “on-demand tutor” for augmentation purposes, particularly valuable when traditional resources like office hours are unavailable, while automation use centers on time savings during periods of overwhelming workload. These self-reported patterns align closely with actual usage data from Claude conversation logs (Handa et al., 2025a,).\r\n\r\nFourth, institutional policies can significantly influence AI adoption, though their effectiveness can be undermined by informational frictions. We find that explicit prohibitions dramatically reduce self-reported intended use 39 percentage points (pp). Importantly, the effects of this policy vary substantially across student groups, with females showing larger reductions in usage under prohibition than males, suggesting that one-size-fits-all policies may inadvertently create disparate impacts across groups. Moreover, we also document substantial informational gaps about institutional AI policies. Many students (19.2 percent) do not understand AI policy rules, few know about institution-provided premium AI resources (10.1 percent) or proper AI citation practices (32.6 percent)—a skill that is necessary for academic integrity.\r\n\r\nFifth, students believe that AI has a positive learning impact and these beliefs strongly predict their own AI usage. Most students believe that AI improves their understanding of course materials (70.2 percent) and learning ability (60.1 percent), though fewer believe it improves grades (41.1 percent). Perhaps unsurprisingly, students who believe that AI improves their academic outcomes are significantly more likely to use these tools. For example, a ten-percentage-point increase in the belief that AI improves learning corresponds to a 4.9 percentage-point increase in adoption, highlighting how perceptions can affect technology diffusion in educational settings.\r\n\r\nTaken together, our findings reveal that generative AI has already reshaped students’ college experience. The substantial information gaps that we document—regarding both permitted uses and available resources—suggest straightforward opportunities for policy intervention. Yet the observed heterogeneity across disciplines and student groups indicates that uniform policies risk unintended consequences: blanket prohibitions may disproportionately disadvantage students who benefit most from AI augmentation, while unrestricted use may encourage automation practices detrimental to skill development. Thus, effective policy requires distinguishing clearly between AI uses that enhance learning and those that undermine it.\r\n\r\nWhile our findings are specific to the population we study—students at an elite liberal arts college—this setting offers valuable into AI’s broader societal impact. The rapid uptake of generative AI in higher education may reflect a generalizable principle regarding the diffusion of generative AI: adoption may be fastest in contexts where AI consolidates fragmented tools into a unified platform. Students have long had access to various tools for the academic tasks for which they now use generative AI.1 What distinguishes AI is its ability to provide all these services through a single, instantly accessible interface at virtually zero marginal cost. This consolidation likely drives students’ perception of enhanced learning efficiency and contributes to the fast adoption. This principle may extend well beyond education: industries with fragmented, specialized tools could experience similar trajectories of rapid AI adoption, even if AI itself does not introduce fundamentally new capabilities.\r\n\r\nOur findings contribute to a rapidly growing literature examining the adoption and impacts of generative AI. Recent work has focused on AI’s effects on worker productivity (Noy and Zhang, , 2023; Dell’Acqua et al., , 2023; Peng et al., , 2023; Cui et al., , 2024; Brynjolfsson et al., , 2025) and its potential to transform occupations (Felten et al., , 2021, 2023; Eloundou et al., , 2024). While several papers document AI adoption in workplace settings (Bick et al., , 2025; Hartley et al., , 2025; Humlum and Vestergaard, , 2025) and at the firm level (McElheran et al., , 2023; Bonney et al., , 2024; Kharazian, , 2025), we examine adoption in higher education—a critical setting where future high-skilled workers develop human capital.\r\n\r\nWe add to an emerging literature examining generative AI in education. Some studies focus on the learning impacts of generative AI, finding mixed impacts depending on the setting (e.g., Lehmann et al., , 2024; Bastani et al., , 2025). A complementary strand of the literature documents adoption patterns across diverse educational contexts, including universities in Australia (Kelly et al., , 2023), Ghana (Bonsu and Baffour-Koduah, , 2023), Norway (Carvajal et al., , 2024), Sweden (Stöhr et al., , 2024), and multi-country studies (Ravšelj et al., , 2025). To the best of our knowledge, this paper provides the first systematic evidence on generative AI adoption in U.S. higher education. Importantly, we focus specifically on how students use AI for academic purposes—unlike studies that measure more general usage including personal lives (e.g., Ravšelj et al., , 2025)—and go beyond documenting adoption rates to examine how students integrate AI into their academic workflows, distinguishing between augmentation and automation, the role of student beliefs about AI’s educational impacts, and how institutional policies shape usage patterns.\r\n\r\nOur findings also contribute to the literature on technology diffusion. Unlike the S-shaped adoption curves documented for most technologies (Griliches, , 1957; Rogers, , 1962), we observe extremely rapid adoption of generative AI, with 80 percent uptake within two years of ChatGPT’s release. This pattern contrasts sharply with the adoption trajectories of historical General Purpose Technologies (GPTs) like electricity, which required decades to achieve widespread use (David, , 1990; Bresnahan and Trajtenberg, , 1995). The rapid diffusion we document aligns with evidence that newer technologies diffuse faster than older ones (Comin and Hobijn, 2010a,; Comin and Hobijn, 2010b,). Two factors likely explain AI’s exceptional adoption speed compared to other GPTs. First, unlike electricity or steam power, generative AI requires minimal physical infrastructure—students access it through existing devices—and is available at no cost to consumers, eliminating the financial barriers that typically slow technology adoption. Second, as noted above, AI consolidates multiple specialized tools into a single platform, making its benefits immediately apparent without requiring specialized training or organizational restructuring, unlike previous GPTs (Brynjolfsson and Hitt, , 2000).\r\n\r\n2Data: Novel Student Survey\r\n2.1Recruitment and Structure\r\nWe conducted the survey from December 2024 to February 2025. All Middlebury College students were contacted via email and received a reminder a few weeks and two months after the initial invitation. To minimize selection bias, the recruitment materials described the survey broadly as a study on students’ use of technology in their academic and personal lives. To incentivize participation, students who completed the survey qualified for entry into a lottery for Amazon gift cards ranging in value from $50 to $500. The full survey instrument is provided in Appendix LABEL:app:survey.\r\n\r\nThe survey contains three main sections (see Appendix Figure A1 for the survey flow). First, we gather demographic and academic information, including gender, race/ethnicity, type of high school attended (private or public), current academic year, and declared or intended major.2 We also collect data on academic inputs and performance through students’ self-reported typical weekly hours spent studying and their first-year GPA.3\r\n\r\nSecond, we measure students’ experience with generative AI tools. We begin by asking whether students have ever used generative AI tools like ChatGPT or Claude. For those who have, we collect information about their usage patterns, including frequency of use during the academic semester, which specific AI models they use, and whether they pay for AI tools. We also gather data on how students use AI for different academic tasks, including writing assistance, learning support, and coding.\r\n\r\nThe final part of the survey elicits students’ beliefs about generative AI’s adoption and impacts. We ask students about their perceptions of AI’s effects on their academic experience across multiple dimensions, including learning, grades, time management, and understanding of course material. We then collect information on the role of institutional policies, asking students how different policy environments influence their likelihood of using AI. We also elicit students’ beliefs about AI usage among their peers, including their estimates of the fraction of Middlebury students who use AI for schoolwork and leisure. The survey concludes with two open-ended questions that allow students to share their experiences with AI in academic settings and provide feedback on the college’s AI policies and support services.\r\n\r\n2.2Sample and Summary Statistics\r\nOut of Middlebury’s 2,760 enrolled students, 739 began the survey. We exclude 105 respondents who left the survey before reaching the generative AI usage module, leaving us with an analysis sample of 634 students (22.9 percent of the student body). This response rate is comparable to that of similar surveys (Wu et al., , 2022). To make our sample more representative of Middlebury’s student body, we construct poststratification weights based on the distribution of declared majors from administrative records.4 Specifically, we weight observations by the ratio of each major’s share in the student population to its share in our survey responses.5\r\n\r\nTable Figures and Tables presents summary statistics for our sample. Column 1 reports unweighted survey averages, column 2 shows averages after applying poststratification weights, and column 3 provides population benchmarks from administrative records where available. Our unweighted sample consists of 44.6 percent male and 50.8 percent females. The racial/ethnic composition includes 61.8 percent white, 15.5 percent Asian, 9.9 percent Hispanic, and 3.6 percent Black students. The majority of students (54.3 percent) attended a public school, while 42.0 percent attended a private high school. Our sample represents 43 different majors across seven fields of study, with 31.1 percent of respondents not yet having declared a major. In our analysis, we group these undeclared students by their intended field of study as reported in the survey.\r\n\r\nComparing our unweighted sample to administrative records reveals notable differences. Our sample overrepresents white students (61.8 versus 53.8 percent) and Asian students (15.5 versus 7.3 percent), while underrepresenting Black students (3.6 versus 5.2 percent) and Hispanic students (9.9 versus 12.4 percent). First-year students are overrepresented (31.1 versus 25.5 percent), while senior students are underrepresented (21.3 versus 28.7 percent). Our weighting procedure partially addresses these discrepancies—particularly in the distribution of academic fields, where the weighted figures closely approximate administrative records—but some demographic differences persist. Despite these differences, our weighted sample provides reasonably close approximations to the college population on most dimensions, particularly for academic characteristics and field distributions.\r\n\r\n3Generative AI Usage Patterns Among Students\r\n3.1Adoption of Generative AI\r\nGenerative AI has achieved a remarkably high adoption rate among Middlebury College students. Figure 1 presents the distribution of AI usage frequency during the academic semester, categorized into four levels: “Rarely” (a few times per semester), “Occasionally” (a few times per month), “Frequently” (a few times per week), and “Very Frequently” (daily or almost daily). Overall, 82.5 percent of students report using generative AI for academic purposes, with substantial variation in usage intensity: 23.5 percent use it rarely, 22.2 percent occasionally, 26.2 percent frequently, and 10.6 percent very frequently.\r\n\r\nThe adoption rate at Middlebury aligns closely with patterns observed across other higher education institutions. A BestColleges survey of U.S. undergraduate and graduate students in fall 2023 found that 56 percent had used AI on assignments or exams (Nam, , 2023). Ravšelj et al., (2025) surveyed higher education students globally between late 2023 and early 2024, finding that 71.4 percent had ever used ChatGPT—though this figure encompasses all usage rather than academic use specifically. Similarly, Stöhr et al., (2024) document that 63 percent of Swedish university students had used ChatGPT by spring 2023, with 35.4 percent reporting regular use and 27.6 percent rare use. Among Norwegian university students, Carvajal et al., (2024) find that 68.9 percent use AI tools occasionally or more frequently.\r\n\r\nThese adoption rates in higher education far exceed those in the general population and workforce. Pew Research finds that only 23 percent of U.S. adults have ever used ChatGPT (McClain, , 2024), while Gallup reports that just one-third of U.S. workers have used AI for work (Gallup, , 2024). Bick et al., (2025) and Hartley et al., (2025) estimate that about 40 percent of the U.S. working-age population used generative AI for work as of late 2024 and early 2025. Both studies document substantial heterogeneity across industries, with information services showing the highest adoption rates at 56–62 percent—still considerably below the 80 percent adoption rate at Middlebury. Even among workers in AI-exposed occupations, Humlum and Vestergaard, (2025) find adoption rates of only 41 percent.6\r\n\r\n3.2Adoption by Student Characteristics and Field of Study\r\nAI adoption varies considerably across demographic groups and academic disciplines (Figure 1 and Appendix Table LABEL:tab:ai_freq_het). Males report higher usage rates than females (88.7 versus 78.4 percent). Usage patterns differ markedly by race/ethnicity: Black students (92.3 percent) and Asian students (91.3 percent) exhibit the highest adoption rates, while white students (80.2 percent) and Hispanic students (77.9 percent) report lower usage. Students from private high schools use AI more frequently than those from public schools (84.1 versus 80.4 percent). Notably, students with below-median GPAs report higher usage rates than their higher-achieving peers (87.1 versus 80.3 percent). Adoption varies widely by field of study: Natural Sciences leads with 91.1 percent usage, followed by Social Sciences at 84.6 percent, while Languages (57.4 percent) and Literature (48.6 percent) show substantially lower adoption rates.\r\n\r\nTo examine how student characteristics jointly relate to AI adoption, we estimate multivariate OLS regressions that include all observed characteristics simultaneously.7 Table LABEL:tab:ai_usage_correlates presents regression estimates using four usage thresholds as outcomes. Each column represents the probability of meeting progressively higher frequency thresholds: any AI use (column 1), at least monthly use (column 2), at least weekly use (column 3), and daily use (column 4).\r\n\r\nThe regression results confirm the patterns observed in the descriptive statistics. Holding other characteristics constant, males are 10.3 pp more likely than females to use AI (column 1, \r\np\r\n<\r\n0.05\r\n), with this gender gap typically widening at higher usage frequencies (columns 2–4, all \r\np\r\n<\r\n0.01\r\n). Black and Asian students show substantially higher adoption rates than white students, at 11.8 and 10.7 pp respectively (column 1, both \r\np\r\n<\r\n0.01\r\n). Students from public high schools are 3.0 pp less likely to use AI than those from private schools, but this difference is not statistically significant. Field of study emerges as the strongest predictor of adoption. Compared to Natural Sciences majors, students in Literature, Languages, Arts, and Humanities all show lower usage rates, with the differences being statistically significant at high usage frequencies (columns 3 and 4). Social Sciences majors exhibit adoption rates similar in magnitude to Natural Sciences majors across all usage frequency thresholds.\r\n\r\nOur findings on heterogeneity in AI adoption align with patterns documented in other settings. The gender gap in AI adoption at Middlebury—10.3 pp higher for males—is consistent with evidence across multiple studies. A meta-analysis of 18 studies by Otis et al., (2024) finds that males are 10–20 percentage points more likely to use generative AI than females. This gender gap in AI adoption appears in all educational studies (Nam, , 2023; Carvajal et al., , 2024; Stöhr et al., , 2024; Ravšelj et al., , 2025). Our finding that students with below-median GPAs are more likely to use AI aligns with Carvajal et al., (2024), who document higher adoption rates among students with lower admission grades. This pattern of greater adoption among lower-achieving students suggests that AI could narrow achievement gaps if it enhances learning and skill development, but could widen these gaps if it undermines the acquisition of fundamental skills.\r\n\r\nThe differences in adoption rates across academic fields at Middlebury mirror patterns documented in other educational settings. Stöhr et al., (2024) find that technology and engineering students exhibit significantly higher ChatGPT usage compared to students in humanities. Similarly, Nam, (2023) report that 62 percent of business majors and 59 percent of STEM majors have used AI tools for coursework, compared to 52 percent of humanities majors. Ravšelj et al., (2025) document comparable disciplinary differences, with applied sciences students showing substantially higher usage rates than arts and humanities students.\r\n\r\nThese academic differences persist into the workforce. Bick et al., (2025) find stark variation by college major: STEM graduates have the highest AI adoption rates (46.0 percent), followed by Business/Economics graduates (40.0 percent), while Liberal Arts graduates show substantially lower rates (22.4 percent). Humlum and Vestergaard, (2025) document similar patterns by occupation, finding that roles requiring strong writing and technical skills—such as marketing specialists and journalists—have the highest adoption rates. These consistent patterns across educational and professional contexts suggest that field-specific factors, particularly the applicability of AI tools to different types of tasks, shape adoption in systematic ways.\r\n\r\n3.3Timing of Generative AI Adoption\r\nThe speed of technology diffusion is a critical determinant of its economic and social impact (David, , 1990; Hall and Khan, , 2003; Stokey, , 2021). In educational contexts, rapid technology adoption can create or exacerbate inequalities between early and late adopters, particularly if the technology confers significant learning advantages (World Bank, , 2016). To track the timing of AI adoption among Middlebury students, we asked them when they first began using generative AI for academic purposes, with options ranging from “This semester (Fall 2024)” to “Before Spring 2023” (as a reference, ChatGPT’s public launch was in November 2022).\r\n\r\nStudents adopted generative AI at an extraordinary pace. Figure 2 shows that the cumulative adoption rate grew dramatically from less than 10 percent before Spring 2023 to slightly above 80 percent by Fall 2024. The pace of adoption has accelerated over time, likely reflecting improvements in AI capabilities. Among current users, 25.7 percent adopted AI in Fall 2024 alone, compared to 19.9 percent in Spring 2024 and 16.3 percent in Fall 2023.\r\n\r\nThe adoption rate among Middlebury students far exceeds that observed in other populations and represents one of the fastest technology adoption episodes ever documented. For comparison, Bick et al., (2025) show that it took over 20 years for computers to reach an 80 percent adoption rate in the U.S. working-age population, and about 15 years for internet adoption to reach similar levels. Even generative AI adoption in the broader population has been markedly slower: Pew Research found that just 23 percent of U.S. adults had ever used ChatGPT as of February 2024, up from 18 percent in July 2023 (McClain, , 2024). The dramatically faster adoption rate we document—reaching over 80 percent in less than two years—suggests academic settings may uniquely accelerate AI diffusion.\r\n\r\nTo identify early versus late adopters of generative AI, in Table LABEL:tab:ai_adopt_correlates and Appendix Figure A2, we analyze how adoption timing varies across student characteristics. The rate of adoption varies substantially across student characteristics. Male students led adoption, with a 8.9 pp higher probability of using AI before Spring 2023 compared to females (column 1, \r\np\r\n<\r\n0.01\r\n)—a gender gap that persists across all periods (columns 2–5). Black and Asian students also adopted earlier than white students, though these differences reach statistical significance only in later periods (columns 4–5). Field of study is a strong predictor of adoption timing. For example, students in Languages consistently lagged behind Natural and Social Sciences majors, with significantly lower adoption rates across nearly all time periods.\r\n\r\n3.4Choice of Generative AI Models\r\nMajor AI companies operate on a freemium model, where free versions coexist with premium subscriptions that offer higher usage limits and access to more advanced models. This tiered structure creates potential for a new form of educational disparities: if paid versions confer substantial academic advantages, students who cannot afford subscriptions may be systematically disadvantaged. To investigate these potential disparities, we presented respondents with a list of options including both free and paid versions of popular models. We also collected information on monthly subscription expenditures, with response options ranging from no active subscription to spending more than $40 monthly (standard subscriptions to major models like ChatGPT cost $20 per month at the time).\r\n\r\nOpenAI’s ChatGPT dominates AI usage among Middlebury AI users, with the free version capturing the largest market share. Figure 3 shows that 89.3 percent of AI users rely on the free version of ChatGPT, making it dramatically more popular than any alternative. Google Gemini (13.5 percent) and Microsoft Copilot (7.7 percent) are distant competitors, while other platforms each capture less than 5 percent of users. This dominance mirrors patterns in other academic settings, where Stöhr et al., (2024) find substantially higher familiarity with ChatGPT compared to alternative platforms, and aligns with broader workforce trends documented by Bick et al., (2025).8\r\n\r\nDespite near-universal AI adoption, only 11.3 percent of AI users pay for any AI service.9 This figure is remarkably similar to the 8 percent found in Ravšelj et al., (2025)’s multi-country survey. This low payment rate suggests that for most students, the premium features of paid versions—primarily higher usage limits and access to more advanced models—do not justify the subscription cost. However, payment patterns reveal significant disparities: males and Asian students are substantially more likely to purchase AI subscriptions (Appendix Table LABEL:tab:ai_model_correlates and Appendix Figure A3), potentially reflecting differences in usage intensity across demographic groups.\r\n\r\n4Generative AI and the Production of Learning\r\n4.1The Use of Generative AI across Academic Tasks\r\nHow is generative AI transforming the traditional inputs to student learning? The educational production function includes inputs like time spent studying (e.g., Stinebrickner and Stinebrickner, , 2008), faculty instruction (e.g., Fairlie et al., , 2014), peer interactions (e.g., Sacerdote, , 2001), and academic support services (e.g., Angrist et al., , 2009). AI tools have the potential to complement or substitute for these traditional inputs. For example, using AI to explain concepts might substitute for faculty office hours, while using it for proofreading might reduce time needed for academic support services.\r\n\r\nTo understand the role of AI in students’ learning production function, we collected information about students’ AI usage across ten common academic tasks: proofreading, generating ideas, writing essays, editing essay drafts, coding assistance, creating images, explaining concepts, composing emails, summarizing materials, and finding information. For each task, students indicated their frequency of AI use on a five-point scale ranging from never to daily. We supplemented this quantitative data with open-ended responses about how AI influences their academic work process.\r\n\r\nStudents use generative AI for a wide range of academic tasks, with the highest adoption rates for learning support and text-processing activities. Figure 4, Panel A shows that explaining concepts is the most common use case, with 80.3 percent of AI users using it for this task. Summarizing texts follows as the second most common task (74.0 percent), followed by finding information and generating ideas (63.1 and 61.9 percent). Writing assistance tasks like proofreading and editing essays are also common, used by 54.1 and 47.3 percent of AI users, respectively. Technical applications like coding help are significant (34.4 percent), considering that many academic degrees involve no programming. Notably, while 23.5 percent of AI users report using it for writing essays, this represents a relatively low adoption rate compared to other academic uses, suggesting students may be more hesitant to use AI for primary content creation. The lowest adoption rate is for creating images at 20.4 percent, likely reflecting fewer academic use cases for this capability.\r\n\r\nAn important limitation of our survey is that it relies on self-reported usage, which may introduce measurement error. For example, students might underreport uses they perceive as academically inappropriate—such as essay writing—while overreporting those viewed as legitimate learning tools (Ling and Imas, , 2025). To assess the validity of our self-reported measures, we compare our findings with Handa et al., 2025a, who analyze actual Claude usage patterns among users with university email addresses. Several caveats apply: their data captures conversation-level interactions rather than student-level usage, and most students in our sample use ChatGPT rather than Claude (Section 3.4). Despite these limitations, the comparison provides a useful benchmark for evaluating our survey responses.\r\n\r\nReassuringly, our results are consistent with Anthropic’s data. Both studies identify explaining concepts and technical problem-solving as primary use cases. Handa et al., 2025a report that the second largest use case (33.5 percent of conversations) involves “technical explanations or solutions for academic assignments,” while we find that 80.3 percent of AI users use it for explaining concepts—a difference likely attributable to our student-level versus their conversation-level measurement. Similarly, the most common usage category in Claude involves “designing practice questions, editing essays, or summarizing academic material” (39.3 percent of conversations), aligning with our findings that 74.0 percent of AI users use it for summarizing texts and 47.3 percent for editing essays. The disciplinary patterns also converge: Handa et al., 2025a finds that computer science, natural sciences, and mathematics conversations are overrepresented, which mirrors our finding that Natural Science majors show significantly higher AI adoption rates (Figure 1). Overall, these convergent findings from self-reported survey data and actual usage logs suggest that our results capture genuine patterns of student AI engagement rather than merely reflecting social desirability in responses.\r\n\r\n4.2Automation versus Augmentation\r\nAre students using generative AI primarily to augment their learning or to automate their coursework? This distinction is crucial for understanding AI’s impacts (Autor and Thompson, , 2025)—augmentation may enhance students’ learning processes while maintaining their active engagement and critical thinking, whereas automation produces fully-formed outputs that could be submitted with minimal student input, potentially harming learning.\r\n\r\nTo examine this empirically, Figure 4, Panel B categorizes the ten measured tasks based on whether they augment or automate academic tasks. We classify tasks as augmentation when they enhance human capabilities (explaining concepts, finding information, proofreading, and editing drafts) and as automation when they directly produce outputs (writing essays, creating images, composing emails, summarizing texts, generating ideas, and coding assistance). We then calculate the percentage of AI users who employ each category at various frequencies.10\r\n\r\nStudents use generative AI for both augmentation and automation, but with markedly different frequency. While 61.2 percent of AI users report using AI for augmentation tasks, 41.9 percent report using it for automation—thus, there is a substantial 19.3 pp difference. This gap is mainly driven by occasional use (19.7 percent for augmentation versus 12.7 percent for automation) and frequent use (16.3 versus 9.0 percent). Most strikingly, students are more than twice as likely to use AI daily for augmentation (5.0 percent) compared to automation (2.5 percent). The higher frequency of augmentation use also suggests that students find augmentation uses more valuable for their day-to-day academic activities. Our findings align closely with patterns observed in actual AI usage data: Handa et al., 2025b analyze real conversation logs from Claude and find that 57 percent of workplace AI interactions involve augmentation while 43 percent involve automation.\r\n\r\nQualitative evidence from open-ended responses provides additional insights into students’ motivations for using AI (see Appendix C for additional results and validation of the open-ended measure). Students’ descriptions align closely with the augmentation-automation framework. For augmentation, many characterize AI as an “on-demand tutor,” particularly valuable when traditional resources like office hours are unavailable. Non-native English speakers frequently mention using AI for proofreading to overcome language barriers, while students from technical majors describe using it to debug code and understand error messages. For automation, time savings emerged as the dominant motivation, with 21.7 percent of open-ended responses explicitly mentioning efficiency benefits (Appendix Figure C3). Students describe turning to AI during periods of overwhelming workload or looming deadlines, using it to generate initial drafts or complete routine assignments. That students automate tasks mostly when time-pressed helps explain why automation is less common than augmentation.11\r\n\r\nOur finding that students favor augmentation over automation extends beyond Middlebury. In Appendix Figure A4, we re-analyze data from Ravšelj et al., (2025)—a multi-country survey of higher education students—to examine how this balance varies across institutional quality.12 We classify universities into quintiles based on their Times Higher Education World University Rankings. Overall, students who use AI worldwide show similar adoption rates for augmentation (64.6 percent) and automation (63.1 percent) tasks—a much smaller gap than at Middlebury (Panel A). However, this aggregate pattern masks heterogeneity by institutional quality. Students at top-quintile universities show a modest preference for augmentation over automation, with this gap narrowing monotonically down the institution quality distribution. By the bottom quintile, augmentation and automation usage are virtually identical (Panel B).\r\n\r\n4.3Heterogeneity in Augmentation versus Automation Usage\r\nUnderstanding whether augmentation and automation patterns vary across student populations may help to design targeted support policies. To assess this, we construct four measures of augmentation and automation usage. First, we create binary indicators for whether students use AI for any augmentation or automation task. Second, we calculate the proportion of tasks in each category for which students employ AI. Third, using the Likert-scale responses, we create intensity measures that capture how frequently students use AI for augmentation and automation purposes. Finally, we compute the difference between augmentation and automation variables, as a measure that directly compare students’ relative preference for augmentation versus automation. Table LABEL:tab:ai_autom_regs presents regression estimates using these measures as outcomes.\r\n\r\nThe balance between augmentation and automation varies substantially by student characteristics. Males show higher adoption of both augmentation and automation tasks, with a similar magnitude in each case—they are 6.0 pp more likely to use augmentation prompts and 5.5 pp more likely to use automation prompts compared to females (columns 1 and 4, \r\np\r\n<\r\n0.05\r\n). Black students have a substantially higher augmentation intensity than white students (0.871 points higher, equivalent to 38.1 percent of the outcome mean, \r\np\r\n<\r\n0.01\r\n), but show no statistically significant differences in automation usage. This results in Black students having the strongest preference for augmentation over automation, with a 19.5 pp higher share of augmentation tasks relative to automation (column 7, \r\np\r\n<\r\n0.01\r\n). Asian students show high usage of both kinds relative to white students. Differences across field of study effects are sizable: humanities, languages, and literature majors tend to show lower usage of both augmentation and automation tools compared to natural science majors.\r\n\r\n5Institutional Policies and AI Adoption\r\n5.1The Role of Institutional Policies in Shaping Student Behavior\r\nInstitutional policies are central to shaping the adoption and diffusion of new technologies (Acemoglu, , 2025). In the context of generative AI in higher education, understanding how policies affect student behavior is essential for guiding evidence-based decision-making. To examine this, we asked students to report their likelihood of using AI under various policy scenarios, ranging from complete prohibition to unrestricted use. This analysis is particularly relevant in light of the ongoing debate about how universities should regulate generative AI use in academic settings (Nolan, , 2023), and the widespread variation in institutional policies across colleges (Xiao et al., , 2023).\r\n\r\nInstitutional policies substantially influence students’ reported likelihood of using generative AI. Figure 5 shows that when generative AI use is unrestricted, 52.4 percent of students report being likely or extremely likely to use it. This likelihood decreases modestly when policies require citation (40.9 percent) or when no explicit policy exists (42.3 percent). However, explicit prohibition creates a dramatic shift: only 13.4 percent of students report they would be likely or extremely likely to use AI when it is banned, while 72.9 percent say they would be unlikely or extremely unlikely to do so (note that these figures include both current AI users and students who do not use generative AI for academic purposes). These results suggest that institutional policies can significantly influence generative AI usage patterns, though a small fraction of students report they would likely use AI even when explicitly prohibited.\r\n\r\nThe magnitude of policy effects in our study aligns closely with findings from other contexts. Carvajal et al., (2024) estimate that banning AI reduces usage by 37.2 pp among females and 20.6 pp among males, for an overall drop of about 28.9 pp. In our survey, a ban leads to a 37.8 pp decline in usage, with a larger decrease among females (49.6 pp) than males (40.1 pp). These parallel results underscore how institutional policies can unintentionally produce disparate effects across gender. Notably, these differential policy effects extend beyond gender. Other demographic and academic characteristics—such as race and field of study—also moderate how students respond to policy restrictions (Appendix Table LABEL:tab:ai_policy_het), underscoring that institutional policies can produce non-neutral impacts along multiple dimensions.\r\n\r\n5.2Understanding of Institutional Policies and Resources\r\nInformation gaps and inattention can significantly affect technology adoption decisions (Duflo et al., , 2011; Hanna et al., , 2014). For example, imperfect information about rules, available resources, or proper usage guidelines could lead to underadoption of beneficial technologies or inadvertent policy violations. To test for the existence of information gaps, we examine three dimensions of policy understanding. First, we asked students whether they find AI policies in their current classes clear. Second, we measured awareness of free access to premium AI tools through the college—a resource that could reduce inequality but only if students know it exists. Third, we assessed whether students know how to properly cite AI when required, a mechanical skill necessary for academic integrity. We supplemented these measures with open-ended feedback about the college’s AI policies and support services.\r\n\r\nStudent understanding of institutional policies is high, though significant gaps remain. Figure 6, Panel A shows that most students (79.1 percent) understand when and where they are allowed to use AI in their classes, but a nontrivial minority (19.2 percent) find AI policies unclear. Moreover, critical knowledge gaps persist elsewhere. Only 10.1 percent know they have free access to Microsoft Copilot through the college (Panel B), and just 32.6 percent understand how to properly cite AI use (Panel C). These gaps vary systematically: females show better policy understanding than males (81.5 versus 75.8 percent), and non-white students demonstrate higher awareness across all three dimensions compared to white students.13\r\n\r\nThe qualitative evidence from open-ended responses reinforces these information frictions and reveals implementation challenges (Appendix D). Students express frustration with vague guidelines, requesting specific examples of acceptable versus unacceptable use cases. Many advocate for formal training, noting that simply knowing policies exist differs from understanding how to effectively integrate AI into their workflow. A particularly striking theme is students’ perception of blanket prohibitions as both ineffective and unfair, as they create a prisoner’s dilemma situation where compliant students are disadvantaged relative to those who secretly violate restrictions. Many responses call for a balanced approach—permitting AI use that supports learning while restricting uses that replace it—though the boundary between the two remains contested.\r\n\r\n6Beliefs About AI’s Educational Impact and Peer Usage\r\n6.1Student Beliefs of AI’s Impact on Educational Outcomes\r\nTechnology adoption decisions are shaped by beliefs about potential returns (Foster and Rosenzweig, , 2010). Students’ perceptions of how AI affects their learning may influence whether they adopt these tools and how they integrate them into their academic workflows. To elicit these beliefs, we asked students to evaluate AI’s impact across four dimensions of their academic performance: understanding of course materials, overall learning ability, time management, and course grades. For each dimension, students rated AI’s effect on a five-point scale ranging from “significantly reduces” to “significantly improves.” These subjective assessments provide insight into the perceived value of AI tools from the student perspective, which may differ from their actual effects on learning outcomes.\r\n\r\nStudents tend to believe that generative AI is beneficial for their academic performance, though the perceived benefits vary across dimensions. Figure 7 shows that the majority of students (70.2 percent) believe that generative AI improves their understanding of course materials, and 60.1 percent report improvement in their ability to grasp concepts, retain information, or learn new skills. Similarly, 59.4 percent report that AI improves their ability to complete assignments on time. Notably, while students believe AI helps their learning and assignment completion, they are less confident about its impact on course grades—41.1 percent believe it improves their grades, while 55.4 percent report no effect and 3.5 percent report negative effects. This pattern suggests that while students perceive generative AI as improving their learning process and workflow—through better understanding, skill development, and timely completion of work—these benefits do not necessarily translate into better course grades.14\r\n\r\nBeliefs about AI’s benefits strongly predict adoption. In Figure 8, we plot the relationship between the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis) for different subgroups of students (e.g., males, white students, public-school students, etc.). Across all four measured academic dimensions, there is a strong and statistically significant relationship between perceiving positive AI effects and AI adoption. The relationship is strongest for beliefs about the ability to improve course grades (Panel D): a 10-pp increase in the belief that AI improves grades is associated with a 5.6 pp increase in AI adoption (\r\np\r\n<\r\n0.01\r\n). Similar positive relationships exist for beliefs about learning ability (4.3 pp, \r\np\r\n<\r\n0.05\r\n), understanding of course materials (4.5 pp, \r\np\r\n<\r\n0.01\r\n), and timely assignment completion (5.3 pp, \r\np\r\n<\r\n0.05\r\n). These findings suggest that student beliefs about AI’s academic benefits—irrespective of the actual benefits—may play a crucial role in shaping adoption decisions.\r\n\r\nThese positive beliefs align with findings from other contexts. Ravšelj et al., (2025) report that the majority of students in their sample believe that ChatGPT improves their general knowledge (68.8 percent) and specific knowledge (62.7 percent)—remarkably similar to our finding that most students believe AI improves understanding of course materials (68.5 percent) and learning ability (57.6 percent). Similarly, 57.4 percent of students in Ravšelj et al., (2025)’s sample believe that ChatGPT helps meet assignment deadlines, while 59.4 percent in our data report AI improves timely assignment completion. Stöhr et al., (2024) provide complementary evidence from Swedish universities, where 47.7 percent of students believe that AI makes them more effective learners, yet only 17.3 percent believe these tools improve their grades—mirroring our finding that perceived learning benefits from AI use exceed perceived effects on course performance.\r\n\r\n6.2Student Beliefs About Peer Use of Generative AI\r\nStudents’ beliefs about their peers’ AI usage may influence their own adoption through multiple channels, including social norms (e.g., Giaccherini et al., , 2019), social learning (e.g., Foster and Rosenzweig, , 1995; Beaman et al., , 2021), peer effects (e.g., Bailey et al., , 2022), and competitive pressure to avoid falling behind (e.g., Goehring et al., , 2024). To measure these beliefs, we asked students to estimate what fraction of their peers use generative AI for different purposes and would use under different policy environments. Figure 9 presents the distribution of these beliefs. Panels A-C show students’ estimates of peer AI usage for schoolwork, leisure, and any purpose respectively. Panels D-F display students’ beliefs about AI usage under three policy environments: when classes have no explicit AI policy, when classes allow AI use, and when classes prohibit AI use.\r\n\r\nStudents systematically underestimate their peers’ AI usage. On average, they believe 65.2 percent of peers regularly use generative AI for schoolwork (Panel A), while our survey reveals an actual usage rate of 82.5 percent—a 17.3 pp gap. This underestimation appears consistent across educational contexts: Stöhr et al., (2024) find that only 38.7 percent of Swedish students believe AI chatbot use is common among peers, despite 63 percent actual usage. When we examine beliefs about policy-contingent behavior, students estimate that 62.7 percent of peers use AI when no explicit policy exists (Panel D), rising to 72.2 percent under permissive policies (Panel E) and falling to 43.5 percent under prohibition (Panel F). These beliefs align directionally with self-reported intentions, though with notable magnitude differences (Appendix Figure A5). For instance, while students believe 43.5 percent of their peers use AI in classes that prohibit it, only 28.4 percent report they themselves would be likely to do so.15\r\n\r\nNotably, students’ perceptions of peer AI adoption are closely linked to their own adoption behavior. Appendix Figure A6 shows a strong positive association between beliefs about peer usage and actual usage rates across student groups. This behavior is consistent with several psychological mechanisms, including the “false consensus effect” (Ross et al., , 1977), selection neglect (Jehiel, , 2018), or interpersonal projection bias (Bushong and Gagnon-Bartsch, , 2024). Still, virtually all groups systematically underestimate peer usage—actual usage rates exceed believed usage for every demographic and academic group we examine. This figure also shows that underestimation varies substantially across student characteristics: males use AI at 88.7 percent but believe only 66.6 percent of peers do so (22.1 pp gap), while females show a smaller gap (77.6 percent actual versus 65.1 percent believed, 13.3 pp gap).\r\n\r\nTaken together, students’ systematic underestimation of peer AI usage coupled with the strong relationship between beliefs and adoption suggests that misperceptions about social norms may shape technology diffusion. If AI enhances learning, then correcting these misperceptions through information provision could accelerate beneficial adoption; conversely, if AI undermines skill development, then students’ underestimation of peer usage may serve as an unintentional safeguard against harmful overadoption.\r\n\r\n7Discussion\r\nThis paper presents the first systematic evidence on generative AI adoption at a highly selective U.S. college. Using novel survey data, we document exceptionally rapid and widespread adoption, substantial shifts in the educational production function through augmentation and automation, and the significant roles of students’ beliefs and institutional policies in shaping AI use.\r\n\r\nOur results offer three implications for institutional policy and ongoing debates about AI in education. First, we identify low-cost opportunities to improve institutional policy effectiveness through targeted information provision. The significant gaps we document in students’ understanding of institutional AI policies, citation practices, and available AI resources suggest that simple interventions—such as clear guidelines, illustrative examples of acceptable uses, and AI literacy programs—can reduce unintentional academic integrity violations and support beneficial AI integration. Qualitative feedback strongly indicates student demand for more explicit guidance on responsible AI use.\r\n\r\nSecond, our evidence challenges alarmist narratives that conflate widespread AI adoption with universal academic dishonesty based on anecdotal accounts.16 Although AI use is indeed near-universal, we find clear evidence that students primarily employ AI as a tool for strategic task management: to enhance learning (augmentation) and selectively automate tasks when facing high time opportunity cost—not solely to circumvent academic effort. This distinction matters: by normalizing academic dishonesty as inevitable and universal, these narratives may shift social norms and encourage students who would otherwise use AI responsibly to engage in prohibited behaviors, believing “everyone else is doing it.”\r\n\r\nThird, our findings caution against policy extremes of either blanket prohibition or unrestricted AI use (Merchant, , 2024; McDonald et al., , 2025). Blanket prohibitions disproportionately harm students who benefit most from AI’s augmentation functions—particularly lower-achieving students—while also creating uneven compliance, placing conscientious students at a disadvantage relative to rule-breakers. Conversely, unrestricted AI use based solely on revealed preference arguments ignores important market failures in educational settings. Students often hold overly optimistic beliefs about AI’s learning benefits despite mixed empirical evidence, potentially leading to unintended negative learning outcomes. Most concerningly, permissive policies risk creating competitive dynamics where students feel compelled to adopt AI not for its learning benefits but simply to avoid falling behind in an educational “arms race” (Goehring et al., , 2024).\r\n\r\nFigures and Tables\r\nFigure 1:The Adoption of Generative AI among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the fraction of students who report using AI during the academic semester, categorized by demographic characteristics, high school type, academic cohort, GPA, and field of study. Usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily).\r\n\r\nThe category “All students” provides the baseline usage rate for the full sample. Gender categories are based on self-identification, with non-binary responses excluded due to a small sample size. “Private HS” refers to students who attended private high schools, while “Public HS” includes public institutions. “Cohort” denotes the student’s academic year, ranging from first-year (“Freshman”) to fourth-year and beyond (“Senior”). GPA categories (“GPA \r\n>\r\n p50” and “GPA \r\n<\r\n p50”) split students into groups above or below the median first-year GPA, as self-reported on a 4.0 scale. See Appendix B.1 for the classification of majors into fields of study.\r\n\r\nFigure 2:The Evolution of Generative AI Adoption among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the cumulative percent of students who reported using generative AI tools for academic purposes over time. The data is based on retrospective self-reports collected in our December 2024 survey, where students were asked “When did you first start using any form of Generative AI for academic purposes?” Response options ranged from “Before Spring 2023” to “This semester (Fall 2024).” The \r\nx\r\n-axis represents academic semesters, while the \r\ny\r\naxis represents the cumulative adoption rate. Vertical lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure 3:Adoption of Generative AI Models Among College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the adoption rates of various AI models as of Fall 2024. The horizontal axis shows the percent of students who reported using each tool, and the vertical axis lists the tools in descending order of adoption rates.\r\n\r\nFigure 4:Academic Uses of Generative AI\r\nPanel A. Across Common Academic Tasks\r\nRefer to caption\r\nPanel B. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of AI students who use generative AI for different academic tasks. For each task, usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). The number at the end of each bar represents the total percent of students who use AI for that purpose at any frequency. Tasks are ordered by total usage, from highest to lowest. Results are based on responses to the question: “For academic purposes, which of the following tasks do you typically use generative AI for?” Sample includes all students who reported using AI during the academic semester.\r\n\r\nFigure 5:Student Reported Likelihood of Using AI under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of students who report different likelihoods of using AI under various policy scenarios. For each policy, responses are categorized on a five-point scale from “Extremely unlikely to use AI” to “Extremely likely to use AI.” The sample includes all survey respondents. The question asked was: “How likely are you to use generative AI in a class with each of the following AI policies?”\r\n\r\nFigure 6:Understanding of Generative AI Policies and Resources\r\nPanel A. Understanding of AI Policies\r\nRefer to caption\r\nPanel B. Awareness of Copilot Access\r\nRefer to caption\r\nPanel C. Knowledge of Citation Requirements\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows students’ understanding of institutional AI policies and resources. Panel A displays the percent of students who report understanding AI policies in their classes, those who report having no explicit policy, and those who find policies unclear, broken down by demographic characteristics. Panel B shows the percent of students who are aware of their free access to Microsoft Copilot through Middlebury College. Panel C presents the percent of students who report knowing how to properly cite AI use in their academic work when required. For Panels B and C, horizontal lines represent 95 percent confidence intervals. Sample includes all survey respondents.\r\n\r\nFigure 7:Student Beliefs about the Impact of AI on their Academic Performance\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of Middlebury students who believe that AI improves, reduces, or has no effect on different aspects of their academic experience. For each outcome, responses are categorized into three groups: “Improves” combines “significantly improves” and “somewhat improves” responses, “Reduces” combines “significantly reduces” and “somewhat reduces” responses, and “No effect” represents neutral responses. Sample includes all students who report using AI during the academic semester. “Don’t know” responses are excluded.\r\n\r\nFigure 8:Relationship Between AI Adoption and Beliefs About AI’s Academic Benefits\r\nPanel A. Learning ability (e.g., ability to grasp concepts, learn new skills, etc.)\r\nRefer to caption\r\nPanel B. Understanding of\r\ncourse materials\r\nRefer to caption\r\nPanel C. Ability to complete\r\nassignments on time\r\nRefer to caption\r\nPanel D. Course grades\r\n \r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the relationship between AI adoption rates and beliefs that AI improves various academic outcomes across different student groups. Each panel plots the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis). Points represent different student groups categorized by demographics (circles), academic characteristics (triangles), and field of study (squares). The dashed line shows the linear fit across all groups. Groups with fewer than 10 students are excluded.\r\n\r\nFigure 9:Student Beliefs about Generative AI Usage at Middlebury College\r\nPanel A. For Schoolwork\r\nRefer to caption\r\nPanel B. For Leisure\r\nRefer to caption\r\nPanel C. For Any Purpose\r\nRefer to caption\r\nPanel D. Classes with No Policy\r\nRefer to caption\r\nPanel E. Classes that Allow AI\r\nRefer to caption\r\nPanel F. Classes that Disallow AI\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the distribution of students’ beliefs about generative AI usage among their peers at Middlebury College. Panels A-C display students’ estimates of the percent of their peers who regularly use AI for schoolwork, leisure activities, and any purpose, respectively. Panels D-F show students’ beliefs about AI usage in classes with different AI policies: those without an explicit policy (Panel D), those that allow AI use (Panel E), and those that prohibit AI use (Panel F). Each panel shows a histogram with bins of width ten percentage points (e.g., responses between 1-10 fall in the 10 bin, 11-20 in the 20 bin, etc.). The red dashed line indicates the mean response. Sample excludes respondents with missing values or who selected the default response for all six categories (which equals zero).\r\n\r\nTable 1:Summary Statistics of Survey Participants\r\nSurvey Sample\tAdmin records\r\nUnweighted\tWeighted\t\r\n(1)\t(2)\t(3)\r\nPanel A. Demographics\r\nMale\t0.446\t0.433\t0.463\r\nFemale\t0.508\t0.516\t0.533\r\nWhite\t0.618\t0.603\t0.538\r\nBlack\t0.036\t0.033\t0.052\r\nHispanic\t0.099\t0.106\t0.124\r\nAsian\t0.155\t0.162\t0.073\r\nPrivate high school\t0.420\t0.399\t–\r\nPublic high school\t0.543\t0.556\t–\r\nPanel B. Academic Characteristics    GPA 3.740 3.736 3.670 Hours spent on academics per week 17.899 17.889 – Freshman 0.311 0.355 0.255 Sophomore 0.273 0.272 0.257 Junior 0.202 0.179 0.201 Senior 0.213 0.194 0.287   Panel C. Field of Study    Arts 0.011 0.021 0.023 Humanities 0.052 0.068 0.073 Languages 0.021 0.020 0.025 Literature 0.035 0.025 0.025 Natural Sciences 0.218 0.249 0.244 Social Sciences 0.353 0.247 0.243 Has not declared major 0.311 0.371 0.364\r\n\r\nN (# degrees) 43 43 49 N (# students) 634 2,760 2,760\r\n\r\n\\justify\r\nNotes: This table presents summary statistics from our survey of college students. Panel A reports demographic characteristics, including the proportion of participants identifying as male, female, white, Black, Hispanic, Asian, or who attended a private or public high school. Panel B provides academic characteristics, such as GPA (only available for non-freshmen), average weekly hours spent on academics, and academic year distribution (Freshman, Sophomore, Junior, and Senior). Note that in column 1–2, GPA refers to self-reported first-year GPA while in column 3 it is the overall GPA during Spring 2024. Panel C summarizes the distribution of participants across different fields of study. Major groups are mutually exclusive.\r\n\r\n0.8250.5890.3680.106 R\r\n−\r\nsquared0.0770.0890.1160.058 N (Students)616616616616 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI usage frequency threshold and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column uses a different threshold for AI usage frequency, categorized as: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). Column 1 defines usage as any nonzero frequency; column 2 includes at least occasional use; column 3 includes frequent or higher use; and column 4 captures only very frequent use.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.0890.1990.3570.5510.801 R\r\n−\r\nsquared0.0450.0610.0880.0800.069 N (Students)633633633633633 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI adoption date and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column presents results for a different threshold of AI adoption. Column 1 shows the probability of adopting AI before Spring 2023; column 2 by Spring 2023; column 3 by Fall 2023; column 4 by Spring 2024; and column 5 by Fall 2024. The dependent variable in each regression is a binary indicator equal to one if the student had adopted AI by the specified time period.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.9120.6122.2850.9130.4191.8010.1930.484 R\r\n−\r\nsquared0.0360.0730.1260.0530.1170.1290.0420.063 N (Students)515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and their use of generative AI for academic tasks. In columns 1 and 4, the outcome is a dummy that equals one if a student reports using AI with any frequency for at least one augmentation or automation task, respectively. In columns 2 and 5, the outcome is the share of tasks within each category for which the student reports any use. In columns 3 and 6, the outcome is a continuous measure capturing average usage frequency for each task category, based on raw Likert-style responses. In columns 7 and 8, the outcome is the difference in average task share and usage intensity between augmentation and automation, respectively. Regressions are weighted and report robust standard errors clustered at the student level. ∗∗∗ \r\np\r\n<\r\n0.01\r\n, ∗∗ \r\np\r\n<\r\n0.05\r\n, ∗ \r\np\r\n<\r\n0.10\r\n.\r\n\r\nReferences\r\nAcemoglu, (2025)\r\nAcemoglu, D. (2025).Nobel lecture: Institutions, technology, and prosperity.American Economic Review, 115(6):1709–1748.\r\nAngrist et al., (2009)\r\nAngrist, J., Lang, D., and Oreopoulos, P. (2009).Incentives and services for college achievement: Evidence from a randomized trial.American Economic Journal: Applied Economics, 1(1):136–163.\r\nAutor and Thompson, (2025)\r\nAutor, D. and Thompson, N. (2025).Expertise.\r\nBailey et al., (2022)\r\nBailey, M., Johnston, D., Kuchler, T., Stroebel, J., and Wong, A. (2022).Peer effects in product adoption.American Economic Journal: Applied Economics, 14(3):488–526.\r\nBastani et al., (2025)\r\nBastani, H., Bastani, O., Sungu, A., Ge, H., Kabakcı, Ö., and Mariman, R. (2025).Generative AI without guardrails can harm learning: Evidence from high school mathematics.Proceedings of the National Academy of Sciences, 122(26):e2422633122.\r\nBeaman et al., (2021)\r\nBeaman, L., BenYishay, A., Magruder, J., and Mobarak, A. M. (2021).Can network theory-based targeting increase technology adoption?American Economic Review, 111(6):1918–1943.\r\nBick et al., (2025)\r\nBick, A., Blandin, A., and Deming, D. J. (2025).The rapid adoption of generative AI.Technical report, National Bureau of Economic Research.\r\nBonney et al., (2024)\r\nBonney, K., Breaux, C., Buffington, C., Dinlersoz, E., Foster, L. S., Goldschlag, N., Haltiwanger, J. C., Kroff, Z., and Savage, K. (2024).Tracking Firm Use of AI in Real Time: A Snapshot from the Business Trends and Outlook Survey.\r\nBonsu and Baffour-Koduah, (2023)\r\nBonsu, E. M. and Baffour-Koduah, D. (2023).From the consumers’ side: Determining students’ perception and intention to use chatgpt in ghanaian higher education.Journal of Education, Society & Multiculturalism, 4(1):1–29.\r\nBresnahan and Trajtenberg, (1995)\r\nBresnahan, T. F. and Trajtenberg, M. (1995).General purpose technologies ‘engines of growth’?Journal of econometrics, 65(1):83–108.\r\nBrynjolfsson and Hitt, (2000)\r\nBrynjolfsson, E. and Hitt, L. M. (2000).Beyond computation: Information technology, organizational transformation and business performance.Journal of Economic Perspectives, 14(4):23–48.\r\nBrynjolfsson et al., (2025)\r\nBrynjolfsson, E., Li, D., and Raymond, L. (2025).Generative AI at Work.The Quarterly Journal of Economics, 140(2):889–942.\r\nBushong and Gagnon-Bartsch, (2024)\r\nBushong, B. and Gagnon-Bartsch, T. (2024).Failures in forecasting: An experiment on interpersonal projection bias.Management Science, 70(12):8735–8752.\r\nCarvajal et al., (2024)\r\nCarvajal, D., Franco, C., and Isaksson, S. (2024).Will artificial intelligence get in the way of achieving gender equality?Discussion Paper 03, NHH Norwegian School of Economics.\r\n(15)\r\nComin, D. and Hobijn, B. (2010a).An exploration of technology diffusion.American Economic Review, 100(5):2031–2059.\r\n(16)\r\nComin, D. and Hobijn, B. (2010b).Technology Diffusion and Postwar Growth, volume 25, pages 209–246.University of Chicago Press.\r\nCui et al., (2024)\r\nCui, Z. K., Demirer, M., Jaffe, S., Musolff, L., Peng, S., and Salz, T. (2024).The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers.\r\nDavid, (1990)\r\nDavid, P. A. (1990).The dynamo and the computer: an historical perspective on the modern productivity paradox.The American economic review, 80(2):355–361.\r\nDell’Acqua et al., (2023)\r\nDell’Acqua, F., McFowland, E., Mollick, E. R., Lifshitz-Assaf, H., Kellogg, K., Rajendran, S., Krayer, L., Candelon, F., and Lakhani, K. R. (2023).Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality.\r\nDuflo et al., (2011)\r\nDuflo, E., Kremer, M., and Robinson, J. (2011).Nudging Farmers to Use Fertilizer: Theory and Experimental Evidence from Kenya.American Economic Review, 101(6):2350–2390.\r\nEloundou et al., (2024)\r\nEloundou, T., Manning, S., Mishkin, P., and Rock, D. (2024).GPTs are GPTs: Labor market impact potential of LLMs.Science, 384(6702):1306–1308.\r\nFairlie et al., (2014)\r\nFairlie, R. W., Hoffmann, F., and Oreopoulos, P. (2014).A community college instructor like me: Race and ethnicity interactions in the classroom.American Economic Review, 104(8):2567–2591.\r\nFelten et al., (2021)\r\nFelten, E. W., Raj, M., and Seamans, R. (2021).Occupational, Industry, and Geographic Exposure to Artificial Intelligence: A Novel Dataset and Its Potential Uses.\r\nFelten et al., (2023)\r\nFelten, E. W., Raj, M., and Seamans, R. (2023).How will Language Modelers like ChatGPT Affect Occupations and Industries?\r\nFoster and Rosenzweig, (1995)\r\nFoster, A. D. and Rosenzweig, M. R. (1995).Learning by doing and learning from others: Human capital and technical change in agriculture.Journal of political Economy, 103(6):1176–1209.\r\nFoster and Rosenzweig, (2010)\r\nFoster, A. D. and Rosenzweig, M. R. (2010).Microeconomics of technology adoption.Annu. Rev. Econ., 2(1):395–424.\r\nGallup, (2024)\r\nGallup (2024).AI in the Workplace: Answering 3 Big Questions.\r\nGiaccherini et al., (2019)\r\nGiaccherini, M., Herberich, D. H., Jimenez-Gomez, D., List, J. A., Ponti, G., and Price, M. K. (2019).The behavioralist goes door-to-door: Understanding household technological diffusion using a theory-driven natural field experiment.Technical report, National Bureau of Economic Research.\r\nGoehring et al., (2024)\r\nGoehring, G., Mezzanotti, F., and Ravid, S. A. A. (2024).Technology adoption and career concerns: Evidence from the adoption of digital technology in motion pictures.Technical report, National Bureau of Economic Research.\r\nGriliches, (1957)\r\nGriliches, Z. (1957).Hybrid corn: An exploration in the economics of technological change.Econometrica, 25(4):501–522.\r\nHall and Khan, (2003)\r\nHall, B. H. and Khan, B. (2003).Adoption of new technology.\r\n(32)\r\nHanda, K., Bent, D., Tamkin, A., McCain, M., Durmus, E., Stern, M., Schiraldi, M., Huang, S., Ritchie, S., Syverud, S., Jagadish, K., Vo, M., Bell, M., and Ganguli, D. (2025a).Anthropic education report: How university students use claude.\r\n(33)\r\nHanda, K., Tamkin, A., McCain, M., Huang, S., Durmus, E., Heck, S., Mueller, J., Hong, J., Ritchie, S., Belonax, T., et al. (2025b).Which economic tasks are performed with ai? evidence from millions of claude conversations.arXiv preprint arXiv:2503.04761.\r\nHanna et al., (2014)\r\nHanna, R., Mullainathan, S., and Schwartzstein, J. (2014).Learning Through Noticing: Theory and Evidence from a Field Experiment.The Quarterly Journal of Economics, 129(3):1311–1353.\r\nHartley et al., (2025)\r\nHartley, J., Jolevski, F., Melo, V., and Moore, B. (2025).The Labor Market Effects of Generative Artificial Intelligence.\r\nHumlum and Vestergaard, (2025)\r\nHumlum, A. and Vestergaard, E. (2025).Large Language Models, Small Labor Market Effects.\r\nHutto and Gilbert, (2014)\r\nHutto, C. and Gilbert, E. (2014).Vader: A parsimonious rule-based model for sentiment analysis of social media text.In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216–225.\r\nJehiel, (2018)\r\nJehiel, P. (2018).Investment strategy and selection bias: An equilibrium perspective on overoptimism.American Economic Review, 108(6):1582–1597.\r\nKelly et al., (2023)\r\nKelly, A., Sullivan, M., and Strampel, K. (2023).Generative artificial intelligence: University student awareness, experience, and confidence in use across disciplines.Journal of University Teaching and Learning Practice, 20(6):1–16.\r\nKharazian, (2025)\r\nKharazian, A. (2025).Ramp ai index: Monthly measurement of ai adoption by american businesses.Technical report, Ramp Economics Lab.\r\nLehmann et al., (2024)\r\nLehmann, M., Cornelius, P. B., and Sting, F. J. (2024).AI Meets the Classroom: When Does ChatGPT Harm Learning?\r\nLing and Imas, (2025)\r\nLing, Y. and Imas, A. (2025).Underreporting of AI use: The role of social desirability bias.Available at SSRN.\r\nLiu and Wang, (2024)\r\nLiu, Y. and Wang, H. (2024).Who on earth is using generative ai?Policy Research Working Paper WPS10870, World Bank Group, Washington, D.C.\r\nMcClain, (2024)\r\nMcClain, C. (2024).Americans’ use of ChatGPT is ticking up, but few trust its election information.\r\nMcDonald et al., (2025)\r\nMcDonald, N., Johri, A., Ali, A., and Collier, A. H. (2025).Generative artificial intelligence in higher education: Evidence from an analysis of institutional policies and guidelines.Computers in Human Behavior: Artificial Humans, 3:100121.\r\nMcElheran et al., (2023)\r\nMcElheran, K., Li, J. F., Brynjolfsson, E., Kroff, Z., Dinlersoz, E., Foster, L. S., and Zolas, N. (2023).AI Adoption in America: Who, What, and Where.\r\nMerchant, (2024)\r\nMerchant, B. (2024).The new luddites aren’t backing down.The Atlantic.Accessed: 2025-06-16.\r\nNam, (2023)\r\nNam, J. (2023).56% of college students have used ai on assignments or exams.Technical report.Edited by Lyss Welding. Accessed June 11, 2025.\r\nNolan, (2023)\r\nNolan, B. (2023).Here are the schools and colleges that have banned chatgpt.Business Insider.Accessed: 2025-01-27.\r\nNoy and Zhang, (2023)\r\nNoy, S. and Zhang, W. (2023).Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence.\r\nOtis et al., (2024)\r\nOtis, N. G., Delecourt, S., Cranney, K., and Koning, R. (2024).Global Evidence on Gender Gaps and Generative AI.Harvard Business School.\r\nPeng et al., (2023)\r\nPeng, S., Kalliamvakou, E., Cihon, P., and Demirer, M. (2023).The Impact of AI on Developer Productivity: Evidence from GitHub Copilot.\r\nRavšelj et al., (2025)\r\nRavšelj, D., Keržič, D., Tomaževič, N., Umek, L., Brezovar, N., A. Iahad, N., Abdulla, A. A., Akopyan, A., Aldana Segura, M. W., AlHumaid, J., Allam, M. F., Alló, M., Andoh, R. P. K., Andronic, O., Arthur, Y. D., Aydın, F., Badran, A., Balbontín-Alvarado, R., Ben Saad, H., Bencsik, A., Benning, I., Besimi, A., Bezerra, D. D. S., Buizza, C., Burro, R., Bwalya, A., Cachero, C., Castillo-Briceno, P., Castro, H., Chai, C. S., Charalambous, C., Chiu, T. K. F., Clipa, O., Colombari, R., Corral Escobedo, L. J. H., Costa, E., Cre\\textcommabelowtulescu, R. G., Crispino, M., Cucari, N., Dalton, F., Demir Kaya, M., Dumić-Čule, I., Dwidienawati, D., Ebardo, R., Egbenya, D. L., Faris, M. E., Fečko, M., Ferrinho, P., Florea, A., Fong, C. Y., Francis, Z., Ghilardi, A., González-Fernández, B., Hau, D., Hossain, M. S., Hug, T., Inasius, F., Ismail, M. J., Jahić, H., Jessa, M. O., Kapanadze, M., Kar, S. K., Kateeb, E. T., Kaya, F., Khadri, H. O., Kikuchi, M., Kobets, V. M., Kostova, K. M., Krasmane, E., Lau, J., Law, W. H. C., Lazăr, F., Lazović-Pita, L., Lee, V. W. Y., Li, J., López-Aguilar, D. V., Luca, A., Luciano, R. G., Machin-Mastromatteo, J. D., Madi, M., Manguele, A. L., Manrique, R. F., Mapulanga, T., Marimon, F., Marinova, G. I., Mas-Machuca, M., Mejía-Rodríguez, O., Meletiou-Mavrotheris, M., Méndez-Prado, S. M., Meza-Cano, J. M., Mirķe, E., Mishra, A., Mital, O., Mollica, C., Morariu, D. I., Mospan, N., Mukuka, A., Navarro Jiménez, S. G., Nikaj, I., Nisheva, M. M., Nisiforou, E., Njiku, J., Nomnian, S., Nuredini-Mehmedi, L., Nyamekye, E., Obadić, A., Okela, A. H., Olenik-Shemesh, D., Ostoj, I., Peralta-Rizzo, K. J., Peštek, A., Pilav-Velić, A., Pires, D. R. M., Rabin, E., Raccanello, D., Ramie, A., Rashid, M. M. U., Reuter, R. A. P., Reyes, V., Rodrigues, A. S., Rodway, P., Ručinská, S., Sadzaglishvili, S., Salem, A. A. M. S., Savić, G., Schepman, A., Shahpo, S. M., Snouber, A., Soler, E., Sonyel, B., Stefanova, E., Stone, A., Strzelecki, A., Tanaka, T., Tapia Cortes, C., Teira-Fachado, A., Tilga, H., Titko, J., Tolmach, M., Turmudi, D., Varela-Candamio, L., Vekiri, I., Vicentini, G., Woyo, E., Yorulmaz, Ö., Yunus, S. A. S., Zamfir, A.-M., Zhou, M., and Aristovnik, A. (2025).Higher education students’ perceptions of ChatGPT: A global study of early reactions.PLOS ONE, 20(2):e0315011.\r\nRogers, (1962)\r\nRogers, E. M. (1962).Diffusion of Innovations.Free Press, New York.\r\nRoss et al., (1977)\r\nRoss, L., Greene, D., and House, P. (1977).The “false consensus effect”: An egocentric bias in social perception and attribution processes.Journal of experimental social psychology, 13(3):279–301.\r\nSacerdote, (2001)\r\nSacerdote, B. (2001).Peer Effects with Random Assignment: Results for Dartmouth Roommates.The Quarterly Journal of Economics, 116(2):681–704.\r\nStinebrickner and Stinebrickner, (2008)\r\nStinebrickner, R. and Stinebrickner, T. R. (2008).The causal effect of studying on academic performance.The BE Journal of Economic Analysis & Policy, 8(1).\r\nStöhr et al., (2024)\r\nStöhr, C., Ou, A. W., and Malmström, H. (2024).Perceptions and usage of AI chatbots among students in higher education across genders, academic levels and fields of study.Computers and Education: Artificial Intelligence, 7:100259.\r\nStokey, (2021)\r\nStokey, N. L. (2021).Technology diffusion.Review of Economic Dynamics, 42:15–36.\r\nWorld Bank, (2016)\r\nWorld Bank (2016).World development report 2016: Digital dividends.World Bank Publications.\r\nWu et al., (2022)\r\nWu, M.-J., Zhao, K., and Fils-Aime, F. (2022).Response rates of online surveys in published research: A meta-analysis.Computers in Human Behavior Reports, 7.\r\nXiao et al., (2023)\r\nXiao, P., Chen, Y., and Bao, W. (2023).Waiting, banning, and embracing: An empirical analysis of adapting policies for generative ai in higher education.arXiv preprint arXiv:2305.18617.\r\nAppendix\r\n\r\nAppendix AAppendix Figures and Tables\r\nFigure A1:Generative AI Usage Survey Design Overview\r\nKey\t\r\nSurvey Structure\r\nSurvey Sections\r\nUsage Data Collection\r\nBeliefs & Perceptions\r\nAI Usage Survey at Middlebury College\r\nDecember 2024\r\nParticipant Recruitment\r\nCampus-wide email invitations, framed as general technology use survey\r\nParticipation Incentives\r\nAmazon gift card lottery ranging from $50-$500\r\nSection 1: Demographics & Academic Information\r\nStudent Characteristics\r\nGender, race/ethnicity, high school type\r\nAcademic Profile\r\nYear, major, self-reported GPA, study hours\r\nSection 2: generative AI Usage Patterns\r\nAdoption Metrics\r\nFrequency, timing of first use, specific AI models used\r\nAcademic Applications\r\nTasks performed with AI, payment for premium services\r\nSection 3: Perceptions & Institutional Policies\r\nImpact Perceptions\r\nEffects on learning, grades, time management\r\nPolicy Responses\r\nUsage likelihood under different policies\r\nPeer Usage Beliefs\r\nEstimates of AI adoption among peers for schoolwork and leisure\r\nOpen-Ended Response Collection\r\nMotivations for AI use, policy feedback, suggestions for improvement\r\nNote: This figure illustrates the structure of the AI usage survey conducted at Middlebury College in December 2024. The survey collected information across three main sections: (1) demographic and academic background, (2) patterns of generative AI usage including adoption timing, frequency, and specific applications, and (3) perceptions of AI’s impact on learning and responses to institutional policies.\r\n\r\nFigure A2:Cumulative Generative AI Use by Student Characteristic\r\nPanel A. By Gender\r\nRefer to caption\r\nPanel B. By Race\r\nRefer to caption\r\nPanel C. By High School Type\r\nRefer to caption\r\nPanel D. By Freshman Year GPA\r\nRefer to caption\r\nPanel E. By Cohort\r\nRefer to caption\r\nPanel F. By Field of Study\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents cumulative AI use based on different student characteristics. Each panel displays the cumulative distribution of AI use based on a specific characteristic: gender, race, school type, first-year GPA, cohort, or field of study. The cumulative percent of students is plotted against usage categories. The legends and colors correspond to subgroups within each demographic variable.\r\n\r\nFigure A3:Percent of Students Who Pay for Generative AI Tools\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of AI users who pay for AI tools (through any platform) across different demographic groups. Horizontal lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure A4:Academic Uses of ChatGPT: Evidence from Global Survey\r\nPanel A. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\nPanel B. Across the College Quality Distribution\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of students who use ChatGPT for different academic tasks based on data from Ravšelj et al., (2025). Panel A displays usage patterns across tasks categorized as augmenting (proofreading, translating, study assistance, research assistance) versus automating (academic writing, professional writing, creative writing, brainstorming, summarizing, calculating, coding assistance, personal assistance) student effort. Panel B shows usage patterns across university quality quintiles based on World University Rankings, with universities ranked in the top 20% (top quintile) showing slightly higher rates of augmentation relative to automation compared to bottom quintile institutions. The analysis includes universities with at least 30 student responses and excludes observations with missing usage data.\r\n\r\nFigure A5:Student Beliefs about AI Usage versus Actual Usage under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure compares students’ beliefs about AI usage with actual usage rates under different AI policies. For each policy type, blue bars show the percent of students who report being “Neutral,” “Likely,” or “Very Likely” to use AI, while red bars show students’ mean beliefs about what percent of their peers use AI in classes with that policy. The sample includes all survey respondents for both actual usage and beliefs measures. “AI Use is Unrestricted” refers to classes with no restrictions on AI use, “No Explicit AI Policy” refers to classes without a stated policy, and “AI Use is Prohibited” refers to classes that ban AI use entirely.\r\n\r\nFigure A6:Relationship Between Beliefs About AI Usage and Actual AI Usage\r\nRefer to caption\\justify\r\nNotes: This figure shows the relationship between students’ beliefs about AI usage among their peers and actual AI usage rates across different demographic groups. Each point represents a different group of students (by demographics, academic characteristics, or field of study). The \r\ny\r\n-axis shows the percent of students in each group who report using AI for academic purposes. The \r\nx\r\n-axis shows the mean belief within each group about what percent of Middlebury students use AI. The dashed line shows the linear fit. Sample includes all survey respondents with at least ten observations per group. Students’ beliefs about AI usage are positively correlated with actual usage patterns, suggesting that students have relatively accurate perceptions of AI adoption among their peers.\r\n\r\nGPA > p50 80.3 26.3 17.4 23.2 13.4 GPA < p50 87.1 18.7 26.3 32.2 9.9 Freshman 81.3 24.3 24.1 24.7 8.1 Sophomore 84.4 25.6 22.6 23.7 12.4 Junior 86.2 21.6 23.1 28.6 12.8 Senior 78.7 21.0 17.4 29.9 10.4\r\n\r\nPanel C. Field of Study    Arts 73.3 21.0 0.0 52.3 0.0 Humanities 74.8 39.3 21.8 12.1 1.7 Languages 57.4 27.0 30.3 0.0 0.0 Literature 48.6 10.0 33.7 5.0 0.0 Natural Sciences 91.1 22.0 21.9 33.8 13.3 Social Sciences 84.6 17.9 20.4 29.1 17.2\r\n\r\n\\justify\r\nNotes: This table presents the percent of students in each demographic group who report using AI at different frequencies during the academic semester. Each cell shows the percent of students within that group. Column 1 reports the total percent who use AI at any frequency. Columns 2 to 5 represent increasing usage frequencies: rarely (1–2 times per semester), occasionally (monthly), frequently (weekly), and very frequently (multiple times per week). Panel A reports percentages by demographic characteristics. Panel B shows percentages by academic characteristics. Panel C presents percentages by field of study.\r\n\r\n0.9730.1350.0770.1100.114 R\r\n−\r\nsquared0.0120.0510.0270.0240.138 N (Students)516516516516516 \\justifyNotes: This table assesses the relationship between AI model adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI model usage (columns 1-4) or payment for AI services (column 5), and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division.\r\n\r\nEach column presents results for a different model or payment outcome. Column 1 shows usage of OpenAI’s ChatGPT, column 2 Google Gemini, column 3 Microsoft Copilot, column 4 any other AI model, and column 5 whether the student pays for any generative AI service.\r\n\r\nThe omitted categories are Natural Sciences for academic division, white students for race/ethnicity, and freshmen for cohort. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.7510.6760.6610.2710.479 R\r\n−\r\nsquared0.0870.1130.0760.0680.041 N (Students)599599599599599 \\justifyNotes: This table examines how student characteristics relate to self-reported likelihood of using generative AI under different policy scenarios. Each column presents results for different policy scenarios. In columns 1-4, the dependent variable equals one if the student reports being“neutral,” “likely” or “extremely likely” to use AI under the specified policy, and zero if they report being “unlikely,” or “extremely unlikely.” Column 5 represents the impact of moving from unrestricted use to complete prohibition.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.5760.6850.3760.4890.564 R\r\n−\r\nsquared0.1020.1190.0840.0370.087 N (Students)561565543560560 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. Each column presents results for beliefs about different academic outcomes: learning ability (e.g., ability to grasp concepts, retain information, or learn new skills) in column 1, understanding of course materials in column 2, course grades in column 3, ability to complete assignments on time in column 4, and time spent on academics in column 5. The dependent variable in each regression equals one if the student believes AI somewhat improves” or significantly improves” the outcome, and zero if they believe it has no effect, reduces, or significantly reduces the outcome. “Don’t know” responses are excluded.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\nAppendix BEmpirical Appendix\r\nB.1Field of Study Classifications\r\nThis appendix details the classification of majors into broad fields of study used in Figure 1:\r\n\r\n• Natural Sciences: Includes Biology, Biochemistry, Chemistry, Computer Science, Earth and Climate Sciences/Geology, Environmental Studies, Mathematics, Molecular Biology & Biochemistry, Neuroscience, Physics, and Statistics.\r\n• Social Sciences: Includes Anthropology, Economics, Education, Geography, International & Global Studies, International Politics & Economics, Political Science, Psychology, and Sociology.\r\n• Humanities: Includes American Studies, Black Studies, Classics, History, Architectural Studies, Art History & Museum Studies, History of Art & Architecture, Philosophy, Religion, and Classical Studies.\r\n• Literature: Includes Comparative Literature, English/English & American Literatures, and Literary Studies.\r\n• Languages: Includes Arabic, Chinese, French & Francophone Studies, German, Japanese Studies, Russian, and Spanish.\r\n• Arts: Includes Film & Media Culture, Music, Studio Art, and Theatre.\r\nB.2Task-Specific Use of Generative AI\r\nThe aggregate patterns documented in Section 4 may mask important heterogeneity across student groups. Different students may adopt AI tools at varying rates depending on their academic needs and field-specific norms. To investigate this heterogeneity systematically, Appendix Table LABEL:tab:ai_purpose_correlates presents regression estimates examining how student characteristics predict AI usage for each of our ten measured academic tasks.\r\n\r\nUsage patterns differ markedly by student characteristics. Male students consistently exhibit higher adoption rates across most applications, with particularly pronounced differences for finding information (16.4 pp; \r\np\r\n<\r\n0.01\r\n), summarizing texts (12.5 pp; \r\np\r\n<\r\n0.01\r\n), and creating images (16.9 pp; \r\np\r\n<\r\n0.01\r\n). Black students show substantially higher usage for information-gathering tasks, being 23.8 pp more likely to use AI for finding information compared to white students (\r\np\r\n<\r\n0.05\r\n). They also exhibit dramatically higher adoption of writing assistance tools, with 27.6 pp higher usage for editing text (\r\np\r\n<\r\n0.05\r\n) and 23.6 pp for writing emails (\r\np\r\n<\r\n0.10\r\n). Latino students demonstrate significantly higher usage for generating ideas (24.0 pp; \r\np\r\n<\r\n0.01\r\n) and writing emails (20.7 pp; \r\np\r\n<\r\n0.01\r\n), while Asian students show markedly higher adoption for writing emails (24.5 pp; \r\np\r\n<\r\n0.01\r\n) and explaining concepts (10.9 pp, \r\np\r\n<\r\n0.05\r\n). Students from public high schools report lower usage rates for several applications, particularly for concept explanation (7.3 pp lower; \r\np\r\n<\r\n0.10\r\n) and writing emails (7.4 pp lower; \r\np\r\n<\r\n0.10\r\n), but higher usage for proofreading (8.7 pp higher; \r\np\r\n<\r\n0.10\r\n).\r\n\r\nField of study emerges as a particularly strong predictor of usage patterns. Arts majors show significantly lower adoption across multiple tasks compared to Natural Science students, with gaps of 49.7 pp for finding information (\r\np\r\n<\r\n0.01\r\n), 44.0 pp for generating ideas (\r\np\r\n<\r\n0.05\r\n), and 62.1 pp for coding assistance (\r\np\r\n<\r\n0.01\r\n). Similarly, humanities majors are 36.8 pp less likely to use AI for generating ideas (\r\np\r\n<\r\n0.01\r\n) and 30.8 pp less likely for coding assistance (\r\np\r\n<\r\n0.01\r\n). Languages majors exhibit the most pronounced differences, being 52.4 pp less likely to use AI for summarizing texts (\r\np\r\n<\r\n0.01\r\n) and 46.9 pp less likely for generating ideas (\r\np\r\n<\r\n0.01\r\n). By contrast, social science students show higher usage for explaining concepts (8.0 pp; \r\np\r\n<\r\n0.10\r\n) and summarizing texts (14.7 pp; \r\np\r\n<\r\n0.01\r\n).\r\n\r\n0.8030.7400.6310.6190.5410.4730.3710.3440.2350.204 R\r\n−\r\nsquared0.0780.0870.0770.1160.0280.0540.0830.1350.0400.094 N (Students)515515515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and use of AI for specific academic tasks. Each column shows the result for a different academic task. The omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Regressions are weighted and use heteroskedasticity-robust standard errors clustered at the student level. ∗∗∗, ∗∗, and ∗ indicate significance at the 1%, 5%, and 10% levels, respectively.\r\n\r\nAppendix CQualitative Evidence on Student Perspectives on AI Use\r\nIn this section, we analyze student responses to an open-ended question about their use of generative AI. The survey question asked: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” This was an optional question that 48.3 percent of survey respondents answered. Appendix Figure C1 presents a word cloud of the most frequent words in student responses.\r\n\r\nC.1Validating the Open-Ended Response Measure\r\nWe begin by validating our open-ended response measure to show that it contains meaningful signal. To validate this measure, we employ sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner), a widely-used lexicon and rule-based sentiment analysis tool (Hutto and Gilbert, , 2014). We calculate sentiment scores for each response and examine whether these scores correlate with students’ actual AI adoption behaviors. Intuitively, students who express more positive sentiment toward AI in their open-ended responses should be more likely to have adopted AI tools in practice. We test this hypothesis by examining two measures of AI adoption: whether students have ever used generative AI and whether they currently use AI for academic purposes. Appendix Figure C2 presents binned scatterplots showing the relationship between AI adoption rates and standardized sentiment scores.\r\n\r\nSentiment towards generative AI strongly predicts AI adoption. For both outcomes, students with negative sentiment scores (below zero) show substantially lower adoption rates—around 70-85 percent for ever using AI and 70-75 percent for academic use. In contrast, students with positive sentiment scores display markedly higher adoption rates, reaching nearly 100 percent for general use and 95-100 percent for academic purposes among those with the most positive sentiment. The relationship is particularly pronounced for academic AI use (Panel B), where the coefficient of \r\nβ\r\n^\r\n=\r\n0.168\r\n is twice as large as for general adoption.17 These systematic patterns confirm that our open-ended responses capture meaningful variation in student attitudes that corresponds to real behavioral differences.\r\n\r\nC.2How Students use Generative AI\r\nTo analyze these responses systematically, we classified each response using keywords based on their content. For example, if a student mentioned using AI to save time, we tagged the response with the keyword “time-saver.” If a student expressed concerns about AI’s impact on learning, we tagged it with “negative learning.” Responses could receive multiple keywords if they discussed several themes. Appendix Figure C3 shows the frequency of keywords in our classification. The responses reveal how students use AI tools, what motivates this use, and what causes some to avoid or limit their use.\r\n\r\nThe most common use of AI is as an explanatory tool. Nearly 30 percent of responses mentioned using AI to understand course material. Students frequently ask AI to break down complex concepts from readings and lectures, particularly when they find the material difficult to understand. For example, one student reported: “I can ask AI to explain concepts to me that I have a hard time grasping. […] I can keep asking ’simplify’ or ’break down even more.”’ Students also use AI to summarize dense academic readings, which they argue helps them manage heavy reading loads.\r\n\r\nStudents employ AI throughout different stages of the writing process. Some use AI to generate initial drafts that serve as starting points. One student explained: “Helps me get started with a base for most of my essays. It feels easier to edit something already written and make it my own than to write from scratch.” Others use AI more narrowly for brainstorming when stuck on specific problems. As one student noted: “I use it if I am feeling stuck to push me to the right direction (whether a mathematical problem or an essay idea).” Many also report using AI as an editing tool to improve grammar, sentence structure, and overall writing flow. This is especially the case for non-native English speakers. As one student explained: “English is not my first language and it frustrates me sometimes that I cannot find the best way to phrase a certain idea and AI is a useful tool to have to find alternate expressions.”\r\n\r\nStudents frequently mentioned using AI for specific academic tasks. In courses that require coding, students often use AI for debugging code and understanding programming concepts. Students also employ AI for administrative tasks like formatting citations and drafting routine emails. Finally, many students use AI as an enhanced search engine. One student reported: “It has significantly reduced the time it takes to conduct research on new topics and ideas, and helps me by giving me a thorough selection of sources to use for projects of any kind.”\r\n\r\nC.3Why Students Adopt AI\r\nTime savings was the most commonly cited reason for using AI. Nearly 30 percent of responses mentioned using AI to complete work more efficiently. Students often viewed AI as a way to manage demanding course loads. Many students particularly embrace AI assistance for tasks viewed as mechanical or administrative. A student noted they use AI for “Writing emails quick and creating resume/ cover letter templates.” But AI assistance goes beyond grunt work. Some students use it to “spend less time doing assignments and homework.” This is particularly true if students don’t view the work as central to their academic experience. As one student explained: “when I come across work I deem as ineffective, I want to spend as little time as possible doing it.”\r\n\r\nHaving an on-demand tutor for academic support was another key motivation. One-quarter of responses described using AI as an “explainer” when other resources were unavailable or inconvenient. As one student noted, “I use it as a last resort (if there are no office hours, after looking up videos, etc.) if I need extra help. I’d like to think that the way I use it is similar to going to office hours or TA hours.”\r\n\r\nPeer influence also drove AI adoption. Some students reported feeling pressure to use AI to remain competitive with their classmates. One student explained: “I noticed others use it, are getting better grades than me, and they say they learn better with the help of AI, so I gave it a try.” Others worried about being at a competitive disadvantage: “Other people were using it and told me about it. I felt like I would be at a disadvantage if I wasn’t also using it.”\r\n\r\nC.4Concerns and Limitations\r\nStudents expressed several concerns about AI use in academic work. The most frequent worry was about negative impacts on learning. One student who initially used AI extensively reported: “In the past, I have simply plugged and chugged homework assignments into ChatGPT and submitted it. Those assignments feedback from teachers was positive and I was getting good grades, but I definitely felt that my own learning outcomes to be significantly worse.” Other students viewed AI use as fundamentally incompatible with their educational goals. As one humanities student explained: “my task is as a humanities student is to think, not calculate; why should I let AI do the thinking for me? It would defeat the purpose of pursuing my education.”\r\n\r\nMany students described ethical concerns about AI use. Responses suggested uncertainty about appropriate boundaries. One student noted: “I never use it to explicitly write something because that feels like overt cheating, but sometimes I hesitate when it completely solves Econ problems. I understand how it does it, and it helps me to learn, but it still sometimes feels a little morally gray.” Another expressed similar ambivalence: “I tend to only use it when [it] will save me time in a moral way.”\r\n\r\nStudents also emphasized the importance of maintaining ownership of their work. Many expressed pride in producing original work and hesitation about diluting that ownership through AI use. As one student explained: “I don’t have interest in using generative AI for my academic work because I want my work to reflect my own ideas.” Another noted: “It usually would not even occur to me to turn to AI to substitute writing because I want to take credit for my work, and using AI seems to diminish that.”\r\n\r\nTechnical limitations deterred some students from using AI tools. Students reported concerns about inaccurate outputs (“hallucinations”) and poor output quality, particularly for creative writing or complex analytical tasks. For example, one student noted that “In my poetry class we were instructed to use it to come up with poems and they were awful, so that kinda turned me away from using it to do my work for me.”\r\n\r\nC.5Discussion\r\nWe conclude with two overarching themes that emerged from the responses.\r\n\r\nFirst, students vary substantially in how they incorporate AI into their academic lives. Crucially, this heterogeneity largely depends on what students perceive as “appropriate” uses of AI. For activities that they perceive as core to their academic journey—like writing essays or solving problems—many students hesitate to use AI. A student articulated this clearly: “Most of my work is writing or reading. If I’m not doing the writing, what is the purpose of me taking the class?” Yet, students draw different boundaries between central tasks and grunt work. Some use AI extensively, viewing their role as akin to a manager that provides high-level direction while AI handles implementation. Others restrict AI use to specific tasks like brainstorming, editing, or drafting emails. Still others avoid AI entirely for academic work, often for ethical reasons. Even among AI users, adoption patterns reflect individual trade-offs between time savings, learning goals, and academic integrity concerns.\r\n\r\nSecond, there appears to be a fundamental tension between efficiency and learning. The time-saving benefits are easy to observe, quantifiable, and tangible. But these time savings are unlikely to be a free lunch. Some benefits may come at the cost of spending less time with material that requires deeper engagement to digest. As one student noted: “There may be a negative effect in that it eliminates much of the ’struggle’ in learning.” Yet, having an on-demand tutor that explains concepts in relatable ways can also improve learning. As one student explained: “It can explain concepts to me in a way that is tailored to my learning style.” This suggests that the impact of AI use on learning outcomes depends not on whether students use AI—almost all do, to some extent—but rather on how they employ these tools.\r\n\r\nFigure C1:Word Cloud of Student Motivations for Generative AI Use\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 147 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?’\r\n\r\nFigure C2:Relationship Between AI Sentiment and AI Adoption\r\nPanel A. Ever Used Generative AI\r\nRefer to caption\r\nPanel B. Uses AI for Academic Purposes\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents the relationship between AI sentiment and AI adoption. Panel A shows the proportion of respondents who have ever used generative AI, while Panel B shows the proportion who use AI for academic purposes. Each point represents the mean adoption rate for respondents within sentiment score bins of width 0.2. Sentiment scores are standardized compound scores computed using Hutto and Gilbert, (2014)’s VADER algorithm applied to responses to an open-ended question about generative AI. Positive values indicate positive sentiment and negative values indicate negative sentiment. The dashed lines show OLS best-fit lines estimated on the microdata, with coefficients and standard errors (in parentheses) displayed in the top-left corner of each panel.\r\n\r\nFigure C3:Frequency of Keywords in Student Motivations for AI Use\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to AI use. The responses come from the question “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” Color coding indicates the category of each theme. Usage type refers to how students use AI tools. Motivation captures what drove students to try AI. Concerns include mentions of course policies and academic integrity, individual reservations about AI use, worries about AI’s impact on education, and AI’s technical limitations.\r\n\r\nAppendix DQualitative Evidence on Student Views of AI Policies\r\nIn this section, we analyze student responses to an open-ended question about Middlebury’s AI policies. The survey asked: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Appendix Figure D1 presents a word cloud of the most frequent words in student responses. To analyze these responses systematically, we classified each response using keywords based on their content. Appendix Figure D2 shows the frequency of keywords in our classification.\r\n\r\nD.1Polarized Views on Generative AI Policy Approaches\r\nStudents expressed markedly different views about appropriate AI policies, revealing fundamental disagreement about the path forward. Some strongly advocated for embracing AI technology. As one student argued, “The tool is there, there is supply and there is demand. Don’t fight another war on drugs. Don’t live in a fake reality”. Others called for significant restrictions, arguing that “the use of generative AI is dishonest and corrosive” and that it “prohibits these organic processes and divorces students from true learning”.\r\n\r\nHowever, the most common position advocated for a balanced approach that would allow beneficial uses while restricting harmful ones. Students distinguished between uses that enhance learning (like concept explanation) and those that substitute for learning (like generating entire essays). One student articulated this nuanced view particularly well: “AI also can really be helpful at explaining a textbook problem that doesn’t make sense, or guiding slightly with homework, or creating study materials, or editing/tightening up your prose. All of those things are good, and universities should figure out how to maximize AI use for those reasons and to minimize students just feeding their problem sets into ChatGPT”\r\n\r\nA recurring theme was the futility of blanket bans. Many students emphasized that prohibition would be ineffective, with one noting “I don’t think anyone really cares what the policy of any given class is. If professors want people to not use it, they need to structure assessments in a way that will discourage use.” Another compared AI bans to restricting internet use, arguing “AI policies seem to be totally irrelevant. It’s like telling people they can’t use the internet as a resource for the class.” This ineffectiveness of bans creates fairness concerns. As one student explained: “I think if it is banned in a class, that should be enforced (and right now it absolutely is not)… As with any form of cheating, those who don’t cheat are put at a disadvantage.” Another student expressed similar frustration: “I find it discouraging when I hear classmates saying they use AI for things such as essays when they use it in dishonest ways.”\r\n\r\nD.2Need for Clear Guidelines\r\nThe most frequently expressed concern was the need for clear guidelines about generative AI use. Several students reported confusion about what constitutes acceptable use, with one noting “I think it should be more clear whether we can use it and how and how to cite it since most professors rarely mention it at all”. Other students emphasized the importance of professors explaining their policies upfront and their rationale, with one stating “I think that Professor’s should be very specific about what is allowed and their reasoning behind their policy”.\r\n\r\nMany students advocated for standardization across classes, observing that “Sometimes its confusing when one class allows it and another doesn’t and the other encourages it and so on so if there was a school wide or department wide policy that could help”. Yet there is also disagreement regarding standardization. Some students preferred leaving the decision to individual professors, arguing that “GenAI is more effective in some classes/majors than others. Making sure professors understand how students use GenAI and how useful GenAI is in their class (given the course structure, nature of assignments/material, etc.) is very important for the class policy.”\r\n\r\nD.3Training and Support Services\r\nStudents strongly emphasized the need for training in appropriate generative AI use. Many suggested that the college should provide guidance on using AI tools effectively while maintaining academic integrity. One student proposed “a workshop that teaches you to effectively use GenAI without violating the honor code”. Students also expressed interest in learning how to leverage AI to enhance their learning experience rather than circumvent it. As one student explained: “I think it could be useful to develop some sort of training. How do we use AI in a way that actually benefits our learning? I tried out some things on my own but I feel that I need more guidance.”\r\n\r\nThis desire for training was often linked to workplace preparedness. Students recognized that AI proficiency would be valuable in their careers, with one noting “As the world uses more and more AI, I think it is an important tool that students should know how to leverage”. Another emphasized: “The moment us students leave campus, we will be using it in the professional world, and when used in combination with one’s own skills, it is merely a tool to maximize efficiency”.\r\n\r\nD.4Discussion\r\nThe responses reveal several key tensions in AI policy implementation. First, students express a desire to use AI in ways that enhance rather than substitute for learning, yet they recognize that blanket bans are ineffective and worry about being disadvantaged if they follow restrictions while others do not.\r\n\r\nSecond, while students desire clear guidelines, they also want flexibility to accommodate legitimate uses that vary by discipline and assignment type. Different courses and majors may find different AI uses appropriate based on their learning objectives and assessment types.\r\n\r\nThird, there is tension between faculty autonomy in setting course policies and students’ desire for consistent institutional standards. While some support letting professors determine appropriate AI use for their specific courses, others argue that varying policies across classes create confusion and enforcement challenges.\r\n\r\nFinally, students report significant variation in faculty attitudes toward AI. Some students perceived faculty fear or misunderstanding of AI tools, noting that categorical bans often reflect a lack of understanding about AI’s capabilities and limitations rather than pedagogical considerations.\r\n\r\nFigure D1:Word Cloud of Student Feedback on Generative AI Policies\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 133 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?”\r\n\r\nFigure D2:Frequency of Keywords in Student Feedback on AI Policies\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to Middlebury’s AI policies. The responses come from the question “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Color coding indicates the category of each theme. Policy Approach captures suggestions about how AI should be regulated at the college. Adoption Views reflect positions on whether and how AI should be integrated into academic work. Training Needs indicates requests for guidance and support. Other Concerns include issues of workplace relevance and fairness.\r\n\r\nSee pages 1 of results/ai-survey-midd.pdf See pages 2- of results//ai-survey-midd.pdf\n",
    "totalFiles": 2
  },
  {
    "id": "extraction_1754348974954_m1odholhg",
    "timestamp": "2025-08-04T23:09:34.954Z",
    "files": [
      {
        "name": "story.txt",
        "size": 730,
        "type": "text/plain",
        "extractedText": "I remember the moment that sparked this project. It was during a walk across Middlebury’s campus late one fall afternoon—just after office hours. A student had come in, clearly bright but struggling, and told me they’d been using ChatGPT late at night to help with assignments because they were too anxious to speak up in class. That confession stuck with me.\r\n\r\nLater, over coffee with Germán, we started wondering—how many students were turning to generative AI as a quiet lifeline? Not to cheat, but to learn? That simple question became the seed for this entire study. What we found wasn’t just statistics. It was a story of adaptation, of students reshaping how they learn, and of institutions racing to catch up.\"",
        "extractionStatus": "success"
      },
      {
        "name": "Generative AI in Higher Education E.txt",
        "size": 117470,
        "type": "text/plain",
        "extractedText": "Generative AI in Higher Education: Evidence from an Elite College†\r\nZara Contractor\r\nGermán Reyes\r\nAbstract\r\nGenerative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT’s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI’s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI’s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\r\n\r\n1Introduction\r\nThe rapid advancement of generative artificial intelligence (AI) is transforming higher education at an unprecedented pace. The launch of ChatGPT and similar tools has introduced technologies capable of performing tasks central to academic assessment and learning—writing essays, solving complex problems, and explaining intricate concepts—instantly and at near-zero marginal cost. Yet systematic evidence on the nature and implications of this new technology remains scarce. How widespread is generative AI adoption among students, and what factors drive it? Do students primarily use AI to augment their learning or to automate coursework, potentially harming human capital development? Could disparities in access to premium AI resources amplify existing educational inequalities?\r\n\r\nThese questions are at the heart of ongoing debates about the role of AI in education, which have relied heavily on anecdotal evidence and speculation rather than rigorous evidence. As a result, universities have implemented policies that vary dramatically, from outright bans to permissive adoption, often without clear evidence on their effectiveness or unintended consequences (Nolan, , 2023; Xiao et al., , 2023; McDonald et al., , 2025).\r\n\r\nThis paper addresses this gap by systematically examining generative AI adoption at Middlebury College, a highly selective liberal arts college in Vermont. With approximately 2,800 undergraduate students, Middlebury offers 49 majors across the arts, humanities, languages, social sciences, and natural sciences. Over the past decade, Middlebury has consistently ranked among the top 10 liberal arts colleges in U.S. News & World Report, with an average ranking of 8.5 between 2015 and 2025. The college’s selectivity is comparable to that of many elite research universities—Middlebury’s acceptance rate of approximately 10 percent is similar to that of Boston University (11 percent), Georgetown University (13 percent), and the University of Virginia (16 percent).\r\n\r\nOur analysis draws on survey data collected from the student population between December 2024 and February 2025. The survey collected detailed information about AI usage, including the frequency and purpose of use, perceived impacts, and responses to institutional policies. To minimize selection bias, we framed the survey broadly as examining technology use and provided incentives for participation, achieving a 22.9 percent response rate with 634 responses representing a broad cross-section of the student population.\r\n\r\nWe document five main findings. First, generative AI is approaching near-universal adoption at an unprecedented speed. Over 80 percent of students use AI for academic purposes, up from less than 10 percent before Spring 2023. This represents one of the fastest technology adoption episodes ever documented, dramatically exceeding the 40 percent adoption rate among U.S. workers (Bick et al., , 2025) and the 23 percent among all U.S. adults (McClain, , 2024). These levels are consistent with international findings of 50–70 percent adoption in university contexts (Nam, , 2023; Stöhr et al., , 2024; Ravšelj et al., , 2025).\r\n\r\nSecond, AI adoption is markedly unequal across academic disciplines and demographic groups. Field of study is the strongest predictor of adoption, likely reflecting how well AI capabilities align with the academic tasks required across fields. Adoption ranges from 91.1 percent in Natural Sciences majors (including mathematics and computer science) to significantly lower levels in Literature (48.6 percent) and Languages (57.4 percent). Additionally, demographic disparities exist. For example, males adopt AI at higher rates than females (88.7 versus 78.4 percent), a pattern that is consistent with documented AI gender gaps in other contexts (Otis et al., , 2024). Most notably, lower-achieving students have a higher adoption rate than their higher-achieving peers (87.1 versus 80.3 percent). As a result, AI could serve as an equalizing force if it enhances student learning—helping struggling students catch up to their peers—but could also widen achievement gaps if AI undermines skill development.\r\n\r\nThird, generative AI transforms the students’ learning production function by both augmenting student capabilities and automating academic tasks. We classify academic tasks as augmentation when AI enhances human capabilities while maintaining student engagement (e.g., explaining concepts, proofreading) versus automation when AI directly produces outputs with minimal cognitive involvement (e.g., writing essays, creating images), and average usage rates across tasks within each category. We find that 61.2 percent of AI users employ these tool for augmentation purposes, while 41.9 percent use it for automation. Qualitative evidence reinforces these patterns: students describe AI as an “on-demand tutor” for augmentation purposes, particularly valuable when traditional resources like office hours are unavailable, while automation use centers on time savings during periods of overwhelming workload. These self-reported patterns align closely with actual usage data from Claude conversation logs (Handa et al., 2025a,).\r\n\r\nFourth, institutional policies can significantly influence AI adoption, though their effectiveness can be undermined by informational frictions. We find that explicit prohibitions dramatically reduce self-reported intended use 39 percentage points (pp). Importantly, the effects of this policy vary substantially across student groups, with females showing larger reductions in usage under prohibition than males, suggesting that one-size-fits-all policies may inadvertently create disparate impacts across groups. Moreover, we also document substantial informational gaps about institutional AI policies. Many students (19.2 percent) do not understand AI policy rules, few know about institution-provided premium AI resources (10.1 percent) or proper AI citation practices (32.6 percent)—a skill that is necessary for academic integrity.\r\n\r\nFifth, students believe that AI has a positive learning impact and these beliefs strongly predict their own AI usage. Most students believe that AI improves their understanding of course materials (70.2 percent) and learning ability (60.1 percent), though fewer believe it improves grades (41.1 percent). Perhaps unsurprisingly, students who believe that AI improves their academic outcomes are significantly more likely to use these tools. For example, a ten-percentage-point increase in the belief that AI improves learning corresponds to a 4.9 percentage-point increase in adoption, highlighting how perceptions can affect technology diffusion in educational settings.\r\n\r\nTaken together, our findings reveal that generative AI has already reshaped students’ college experience. The substantial information gaps that we document—regarding both permitted uses and available resources—suggest straightforward opportunities for policy intervention. Yet the observed heterogeneity across disciplines and student groups indicates that uniform policies risk unintended consequences: blanket prohibitions may disproportionately disadvantage students who benefit most from AI augmentation, while unrestricted use may encourage automation practices detrimental to skill development. Thus, effective policy requires distinguishing clearly between AI uses that enhance learning and those that undermine it.\r\n\r\nWhile our findings are specific to the population we study—students at an elite liberal arts college—this setting offers valuable into AI’s broader societal impact. The rapid uptake of generative AI in higher education may reflect a generalizable principle regarding the diffusion of generative AI: adoption may be fastest in contexts where AI consolidates fragmented tools into a unified platform. Students have long had access to various tools for the academic tasks for which they now use generative AI.1 What distinguishes AI is its ability to provide all these services through a single, instantly accessible interface at virtually zero marginal cost. This consolidation likely drives students’ perception of enhanced learning efficiency and contributes to the fast adoption. This principle may extend well beyond education: industries with fragmented, specialized tools could experience similar trajectories of rapid AI adoption, even if AI itself does not introduce fundamentally new capabilities.\r\n\r\nOur findings contribute to a rapidly growing literature examining the adoption and impacts of generative AI. Recent work has focused on AI’s effects on worker productivity (Noy and Zhang, , 2023; Dell’Acqua et al., , 2023; Peng et al., , 2023; Cui et al., , 2024; Brynjolfsson et al., , 2025) and its potential to transform occupations (Felten et al., , 2021, 2023; Eloundou et al., , 2024). While several papers document AI adoption in workplace settings (Bick et al., , 2025; Hartley et al., , 2025; Humlum and Vestergaard, , 2025) and at the firm level (McElheran et al., , 2023; Bonney et al., , 2024; Kharazian, , 2025), we examine adoption in higher education—a critical setting where future high-skilled workers develop human capital.\r\n\r\nWe add to an emerging literature examining generative AI in education. Some studies focus on the learning impacts of generative AI, finding mixed impacts depending on the setting (e.g., Lehmann et al., , 2024; Bastani et al., , 2025). A complementary strand of the literature documents adoption patterns across diverse educational contexts, including universities in Australia (Kelly et al., , 2023), Ghana (Bonsu and Baffour-Koduah, , 2023), Norway (Carvajal et al., , 2024), Sweden (Stöhr et al., , 2024), and multi-country studies (Ravšelj et al., , 2025). To the best of our knowledge, this paper provides the first systematic evidence on generative AI adoption in U.S. higher education. Importantly, we focus specifically on how students use AI for academic purposes—unlike studies that measure more general usage including personal lives (e.g., Ravšelj et al., , 2025)—and go beyond documenting adoption rates to examine how students integrate AI into their academic workflows, distinguishing between augmentation and automation, the role of student beliefs about AI’s educational impacts, and how institutional policies shape usage patterns.\r\n\r\nOur findings also contribute to the literature on technology diffusion. Unlike the S-shaped adoption curves documented for most technologies (Griliches, , 1957; Rogers, , 1962), we observe extremely rapid adoption of generative AI, with 80 percent uptake within two years of ChatGPT’s release. This pattern contrasts sharply with the adoption trajectories of historical General Purpose Technologies (GPTs) like electricity, which required decades to achieve widespread use (David, , 1990; Bresnahan and Trajtenberg, , 1995). The rapid diffusion we document aligns with evidence that newer technologies diffuse faster than older ones (Comin and Hobijn, 2010a,; Comin and Hobijn, 2010b,). Two factors likely explain AI’s exceptional adoption speed compared to other GPTs. First, unlike electricity or steam power, generative AI requires minimal physical infrastructure—students access it through existing devices—and is available at no cost to consumers, eliminating the financial barriers that typically slow technology adoption. Second, as noted above, AI consolidates multiple specialized tools into a single platform, making its benefits immediately apparent without requiring specialized training or organizational restructuring, unlike previous GPTs (Brynjolfsson and Hitt, , 2000).\r\n\r\n2Data: Novel Student Survey\r\n2.1Recruitment and Structure\r\nWe conducted the survey from December 2024 to February 2025. All Middlebury College students were contacted via email and received a reminder a few weeks and two months after the initial invitation. To minimize selection bias, the recruitment materials described the survey broadly as a study on students’ use of technology in their academic and personal lives. To incentivize participation, students who completed the survey qualified for entry into a lottery for Amazon gift cards ranging in value from $50 to $500. The full survey instrument is provided in Appendix LABEL:app:survey.\r\n\r\nThe survey contains three main sections (see Appendix Figure A1 for the survey flow). First, we gather demographic and academic information, including gender, race/ethnicity, type of high school attended (private or public), current academic year, and declared or intended major.2 We also collect data on academic inputs and performance through students’ self-reported typical weekly hours spent studying and their first-year GPA.3\r\n\r\nSecond, we measure students’ experience with generative AI tools. We begin by asking whether students have ever used generative AI tools like ChatGPT or Claude. For those who have, we collect information about their usage patterns, including frequency of use during the academic semester, which specific AI models they use, and whether they pay for AI tools. We also gather data on how students use AI for different academic tasks, including writing assistance, learning support, and coding.\r\n\r\nThe final part of the survey elicits students’ beliefs about generative AI’s adoption and impacts. We ask students about their perceptions of AI’s effects on their academic experience across multiple dimensions, including learning, grades, time management, and understanding of course material. We then collect information on the role of institutional policies, asking students how different policy environments influence their likelihood of using AI. We also elicit students’ beliefs about AI usage among their peers, including their estimates of the fraction of Middlebury students who use AI for schoolwork and leisure. The survey concludes with two open-ended questions that allow students to share their experiences with AI in academic settings and provide feedback on the college’s AI policies and support services.\r\n\r\n2.2Sample and Summary Statistics\r\nOut of Middlebury’s 2,760 enrolled students, 739 began the survey. We exclude 105 respondents who left the survey before reaching the generative AI usage module, leaving us with an analysis sample of 634 students (22.9 percent of the student body). This response rate is comparable to that of similar surveys (Wu et al., , 2022). To make our sample more representative of Middlebury’s student body, we construct poststratification weights based on the distribution of declared majors from administrative records.4 Specifically, we weight observations by the ratio of each major’s share in the student population to its share in our survey responses.5\r\n\r\nTable Figures and Tables presents summary statistics for our sample. Column 1 reports unweighted survey averages, column 2 shows averages after applying poststratification weights, and column 3 provides population benchmarks from administrative records where available. Our unweighted sample consists of 44.6 percent male and 50.8 percent females. The racial/ethnic composition includes 61.8 percent white, 15.5 percent Asian, 9.9 percent Hispanic, and 3.6 percent Black students. The majority of students (54.3 percent) attended a public school, while 42.0 percent attended a private high school. Our sample represents 43 different majors across seven fields of study, with 31.1 percent of respondents not yet having declared a major. In our analysis, we group these undeclared students by their intended field of study as reported in the survey.\r\n\r\nComparing our unweighted sample to administrative records reveals notable differences. Our sample overrepresents white students (61.8 versus 53.8 percent) and Asian students (15.5 versus 7.3 percent), while underrepresenting Black students (3.6 versus 5.2 percent) and Hispanic students (9.9 versus 12.4 percent). First-year students are overrepresented (31.1 versus 25.5 percent), while senior students are underrepresented (21.3 versus 28.7 percent). Our weighting procedure partially addresses these discrepancies—particularly in the distribution of academic fields, where the weighted figures closely approximate administrative records—but some demographic differences persist. Despite these differences, our weighted sample provides reasonably close approximations to the college population on most dimensions, particularly for academic characteristics and field distributions.\r\n\r\n3Generative AI Usage Patterns Among Students\r\n3.1Adoption of Generative AI\r\nGenerative AI has achieved a remarkably high adoption rate among Middlebury College students. Figure 1 presents the distribution of AI usage frequency during the academic semester, categorized into four levels: “Rarely” (a few times per semester), “Occasionally” (a few times per month), “Frequently” (a few times per week), and “Very Frequently” (daily or almost daily). Overall, 82.5 percent of students report using generative AI for academic purposes, with substantial variation in usage intensity: 23.5 percent use it rarely, 22.2 percent occasionally, 26.2 percent frequently, and 10.6 percent very frequently.\r\n\r\nThe adoption rate at Middlebury aligns closely with patterns observed across other higher education institutions. A BestColleges survey of U.S. undergraduate and graduate students in fall 2023 found that 56 percent had used AI on assignments or exams (Nam, , 2023). Ravšelj et al., (2025) surveyed higher education students globally between late 2023 and early 2024, finding that 71.4 percent had ever used ChatGPT—though this figure encompasses all usage rather than academic use specifically. Similarly, Stöhr et al., (2024) document that 63 percent of Swedish university students had used ChatGPT by spring 2023, with 35.4 percent reporting regular use and 27.6 percent rare use. Among Norwegian university students, Carvajal et al., (2024) find that 68.9 percent use AI tools occasionally or more frequently.\r\n\r\nThese adoption rates in higher education far exceed those in the general population and workforce. Pew Research finds that only 23 percent of U.S. adults have ever used ChatGPT (McClain, , 2024), while Gallup reports that just one-third of U.S. workers have used AI for work (Gallup, , 2024). Bick et al., (2025) and Hartley et al., (2025) estimate that about 40 percent of the U.S. working-age population used generative AI for work as of late 2024 and early 2025. Both studies document substantial heterogeneity across industries, with information services showing the highest adoption rates at 56–62 percent—still considerably below the 80 percent adoption rate at Middlebury. Even among workers in AI-exposed occupations, Humlum and Vestergaard, (2025) find adoption rates of only 41 percent.6\r\n\r\n3.2Adoption by Student Characteristics and Field of Study\r\nAI adoption varies considerably across demographic groups and academic disciplines (Figure 1 and Appendix Table LABEL:tab:ai_freq_het). Males report higher usage rates than females (88.7 versus 78.4 percent). Usage patterns differ markedly by race/ethnicity: Black students (92.3 percent) and Asian students (91.3 percent) exhibit the highest adoption rates, while white students (80.2 percent) and Hispanic students (77.9 percent) report lower usage. Students from private high schools use AI more frequently than those from public schools (84.1 versus 80.4 percent). Notably, students with below-median GPAs report higher usage rates than their higher-achieving peers (87.1 versus 80.3 percent). Adoption varies widely by field of study: Natural Sciences leads with 91.1 percent usage, followed by Social Sciences at 84.6 percent, while Languages (57.4 percent) and Literature (48.6 percent) show substantially lower adoption rates.\r\n\r\nTo examine how student characteristics jointly relate to AI adoption, we estimate multivariate OLS regressions that include all observed characteristics simultaneously.7 Table LABEL:tab:ai_usage_correlates presents regression estimates using four usage thresholds as outcomes. Each column represents the probability of meeting progressively higher frequency thresholds: any AI use (column 1), at least monthly use (column 2), at least weekly use (column 3), and daily use (column 4).\r\n\r\nThe regression results confirm the patterns observed in the descriptive statistics. Holding other characteristics constant, males are 10.3 pp more likely than females to use AI (column 1, \r\np\r\n<\r\n0.05\r\n), with this gender gap typically widening at higher usage frequencies (columns 2–4, all \r\np\r\n<\r\n0.01\r\n). Black and Asian students show substantially higher adoption rates than white students, at 11.8 and 10.7 pp respectively (column 1, both \r\np\r\n<\r\n0.01\r\n). Students from public high schools are 3.0 pp less likely to use AI than those from private schools, but this difference is not statistically significant. Field of study emerges as the strongest predictor of adoption. Compared to Natural Sciences majors, students in Literature, Languages, Arts, and Humanities all show lower usage rates, with the differences being statistically significant at high usage frequencies (columns 3 and 4). Social Sciences majors exhibit adoption rates similar in magnitude to Natural Sciences majors across all usage frequency thresholds.\r\n\r\nOur findings on heterogeneity in AI adoption align with patterns documented in other settings. The gender gap in AI adoption at Middlebury—10.3 pp higher for males—is consistent with evidence across multiple studies. A meta-analysis of 18 studies by Otis et al., (2024) finds that males are 10–20 percentage points more likely to use generative AI than females. This gender gap in AI adoption appears in all educational studies (Nam, , 2023; Carvajal et al., , 2024; Stöhr et al., , 2024; Ravšelj et al., , 2025). Our finding that students with below-median GPAs are more likely to use AI aligns with Carvajal et al., (2024), who document higher adoption rates among students with lower admission grades. This pattern of greater adoption among lower-achieving students suggests that AI could narrow achievement gaps if it enhances learning and skill development, but could widen these gaps if it undermines the acquisition of fundamental skills.\r\n\r\nThe differences in adoption rates across academic fields at Middlebury mirror patterns documented in other educational settings. Stöhr et al., (2024) find that technology and engineering students exhibit significantly higher ChatGPT usage compared to students in humanities. Similarly, Nam, (2023) report that 62 percent of business majors and 59 percent of STEM majors have used AI tools for coursework, compared to 52 percent of humanities majors. Ravšelj et al., (2025) document comparable disciplinary differences, with applied sciences students showing substantially higher usage rates than arts and humanities students.\r\n\r\nThese academic differences persist into the workforce. Bick et al., (2025) find stark variation by college major: STEM graduates have the highest AI adoption rates (46.0 percent), followed by Business/Economics graduates (40.0 percent), while Liberal Arts graduates show substantially lower rates (22.4 percent). Humlum and Vestergaard, (2025) document similar patterns by occupation, finding that roles requiring strong writing and technical skills—such as marketing specialists and journalists—have the highest adoption rates. These consistent patterns across educational and professional contexts suggest that field-specific factors, particularly the applicability of AI tools to different types of tasks, shape adoption in systematic ways.\r\n\r\n3.3Timing of Generative AI Adoption\r\nThe speed of technology diffusion is a critical determinant of its economic and social impact (David, , 1990; Hall and Khan, , 2003; Stokey, , 2021). In educational contexts, rapid technology adoption can create or exacerbate inequalities between early and late adopters, particularly if the technology confers significant learning advantages (World Bank, , 2016). To track the timing of AI adoption among Middlebury students, we asked them when they first began using generative AI for academic purposes, with options ranging from “This semester (Fall 2024)” to “Before Spring 2023” (as a reference, ChatGPT’s public launch was in November 2022).\r\n\r\nStudents adopted generative AI at an extraordinary pace. Figure 2 shows that the cumulative adoption rate grew dramatically from less than 10 percent before Spring 2023 to slightly above 80 percent by Fall 2024. The pace of adoption has accelerated over time, likely reflecting improvements in AI capabilities. Among current users, 25.7 percent adopted AI in Fall 2024 alone, compared to 19.9 percent in Spring 2024 and 16.3 percent in Fall 2023.\r\n\r\nThe adoption rate among Middlebury students far exceeds that observed in other populations and represents one of the fastest technology adoption episodes ever documented. For comparison, Bick et al., (2025) show that it took over 20 years for computers to reach an 80 percent adoption rate in the U.S. working-age population, and about 15 years for internet adoption to reach similar levels. Even generative AI adoption in the broader population has been markedly slower: Pew Research found that just 23 percent of U.S. adults had ever used ChatGPT as of February 2024, up from 18 percent in July 2023 (McClain, , 2024). The dramatically faster adoption rate we document—reaching over 80 percent in less than two years—suggests academic settings may uniquely accelerate AI diffusion.\r\n\r\nTo identify early versus late adopters of generative AI, in Table LABEL:tab:ai_adopt_correlates and Appendix Figure A2, we analyze how adoption timing varies across student characteristics. The rate of adoption varies substantially across student characteristics. Male students led adoption, with a 8.9 pp higher probability of using AI before Spring 2023 compared to females (column 1, \r\np\r\n<\r\n0.01\r\n)—a gender gap that persists across all periods (columns 2–5). Black and Asian students also adopted earlier than white students, though these differences reach statistical significance only in later periods (columns 4–5). Field of study is a strong predictor of adoption timing. For example, students in Languages consistently lagged behind Natural and Social Sciences majors, with significantly lower adoption rates across nearly all time periods.\r\n\r\n3.4Choice of Generative AI Models\r\nMajor AI companies operate on a freemium model, where free versions coexist with premium subscriptions that offer higher usage limits and access to more advanced models. This tiered structure creates potential for a new form of educational disparities: if paid versions confer substantial academic advantages, students who cannot afford subscriptions may be systematically disadvantaged. To investigate these potential disparities, we presented respondents with a list of options including both free and paid versions of popular models. We also collected information on monthly subscription expenditures, with response options ranging from no active subscription to spending more than $40 monthly (standard subscriptions to major models like ChatGPT cost $20 per month at the time).\r\n\r\nOpenAI’s ChatGPT dominates AI usage among Middlebury AI users, with the free version capturing the largest market share. Figure 3 shows that 89.3 percent of AI users rely on the free version of ChatGPT, making it dramatically more popular than any alternative. Google Gemini (13.5 percent) and Microsoft Copilot (7.7 percent) are distant competitors, while other platforms each capture less than 5 percent of users. This dominance mirrors patterns in other academic settings, where Stöhr et al., (2024) find substantially higher familiarity with ChatGPT compared to alternative platforms, and aligns with broader workforce trends documented by Bick et al., (2025).8\r\n\r\nDespite near-universal AI adoption, only 11.3 percent of AI users pay for any AI service.9 This figure is remarkably similar to the 8 percent found in Ravšelj et al., (2025)’s multi-country survey. This low payment rate suggests that for most students, the premium features of paid versions—primarily higher usage limits and access to more advanced models—do not justify the subscription cost. However, payment patterns reveal significant disparities: males and Asian students are substantially more likely to purchase AI subscriptions (Appendix Table LABEL:tab:ai_model_correlates and Appendix Figure A3), potentially reflecting differences in usage intensity across demographic groups.\r\n\r\n4Generative AI and the Production of Learning\r\n4.1The Use of Generative AI across Academic Tasks\r\nHow is generative AI transforming the traditional inputs to student learning? The educational production function includes inputs like time spent studying (e.g., Stinebrickner and Stinebrickner, , 2008), faculty instruction (e.g., Fairlie et al., , 2014), peer interactions (e.g., Sacerdote, , 2001), and academic support services (e.g., Angrist et al., , 2009). AI tools have the potential to complement or substitute for these traditional inputs. For example, using AI to explain concepts might substitute for faculty office hours, while using it for proofreading might reduce time needed for academic support services.\r\n\r\nTo understand the role of AI in students’ learning production function, we collected information about students’ AI usage across ten common academic tasks: proofreading, generating ideas, writing essays, editing essay drafts, coding assistance, creating images, explaining concepts, composing emails, summarizing materials, and finding information. For each task, students indicated their frequency of AI use on a five-point scale ranging from never to daily. We supplemented this quantitative data with open-ended responses about how AI influences their academic work process.\r\n\r\nStudents use generative AI for a wide range of academic tasks, with the highest adoption rates for learning support and text-processing activities. Figure 4, Panel A shows that explaining concepts is the most common use case, with 80.3 percent of AI users using it for this task. Summarizing texts follows as the second most common task (74.0 percent), followed by finding information and generating ideas (63.1 and 61.9 percent). Writing assistance tasks like proofreading and editing essays are also common, used by 54.1 and 47.3 percent of AI users, respectively. Technical applications like coding help are significant (34.4 percent), considering that many academic degrees involve no programming. Notably, while 23.5 percent of AI users report using it for writing essays, this represents a relatively low adoption rate compared to other academic uses, suggesting students may be more hesitant to use AI for primary content creation. The lowest adoption rate is for creating images at 20.4 percent, likely reflecting fewer academic use cases for this capability.\r\n\r\nAn important limitation of our survey is that it relies on self-reported usage, which may introduce measurement error. For example, students might underreport uses they perceive as academically inappropriate—such as essay writing—while overreporting those viewed as legitimate learning tools (Ling and Imas, , 2025). To assess the validity of our self-reported measures, we compare our findings with Handa et al., 2025a, who analyze actual Claude usage patterns among users with university email addresses. Several caveats apply: their data captures conversation-level interactions rather than student-level usage, and most students in our sample use ChatGPT rather than Claude (Section 3.4). Despite these limitations, the comparison provides a useful benchmark for evaluating our survey responses.\r\n\r\nReassuringly, our results are consistent with Anthropic’s data. Both studies identify explaining concepts and technical problem-solving as primary use cases. Handa et al., 2025a report that the second largest use case (33.5 percent of conversations) involves “technical explanations or solutions for academic assignments,” while we find that 80.3 percent of AI users use it for explaining concepts—a difference likely attributable to our student-level versus their conversation-level measurement. Similarly, the most common usage category in Claude involves “designing practice questions, editing essays, or summarizing academic material” (39.3 percent of conversations), aligning with our findings that 74.0 percent of AI users use it for summarizing texts and 47.3 percent for editing essays. The disciplinary patterns also converge: Handa et al., 2025a finds that computer science, natural sciences, and mathematics conversations are overrepresented, which mirrors our finding that Natural Science majors show significantly higher AI adoption rates (Figure 1). Overall, these convergent findings from self-reported survey data and actual usage logs suggest that our results capture genuine patterns of student AI engagement rather than merely reflecting social desirability in responses.\r\n\r\n4.2Automation versus Augmentation\r\nAre students using generative AI primarily to augment their learning or to automate their coursework? This distinction is crucial for understanding AI’s impacts (Autor and Thompson, , 2025)—augmentation may enhance students’ learning processes while maintaining their active engagement and critical thinking, whereas automation produces fully-formed outputs that could be submitted with minimal student input, potentially harming learning.\r\n\r\nTo examine this empirically, Figure 4, Panel B categorizes the ten measured tasks based on whether they augment or automate academic tasks. We classify tasks as augmentation when they enhance human capabilities (explaining concepts, finding information, proofreading, and editing drafts) and as automation when they directly produce outputs (writing essays, creating images, composing emails, summarizing texts, generating ideas, and coding assistance). We then calculate the percentage of AI users who employ each category at various frequencies.10\r\n\r\nStudents use generative AI for both augmentation and automation, but with markedly different frequency. While 61.2 percent of AI users report using AI for augmentation tasks, 41.9 percent report using it for automation—thus, there is a substantial 19.3 pp difference. This gap is mainly driven by occasional use (19.7 percent for augmentation versus 12.7 percent for automation) and frequent use (16.3 versus 9.0 percent). Most strikingly, students are more than twice as likely to use AI daily for augmentation (5.0 percent) compared to automation (2.5 percent). The higher frequency of augmentation use also suggests that students find augmentation uses more valuable for their day-to-day academic activities. Our findings align closely with patterns observed in actual AI usage data: Handa et al., 2025b analyze real conversation logs from Claude and find that 57 percent of workplace AI interactions involve augmentation while 43 percent involve automation.\r\n\r\nQualitative evidence from open-ended responses provides additional insights into students’ motivations for using AI (see Appendix C for additional results and validation of the open-ended measure). Students’ descriptions align closely with the augmentation-automation framework. For augmentation, many characterize AI as an “on-demand tutor,” particularly valuable when traditional resources like office hours are unavailable. Non-native English speakers frequently mention using AI for proofreading to overcome language barriers, while students from technical majors describe using it to debug code and understand error messages. For automation, time savings emerged as the dominant motivation, with 21.7 percent of open-ended responses explicitly mentioning efficiency benefits (Appendix Figure C3). Students describe turning to AI during periods of overwhelming workload or looming deadlines, using it to generate initial drafts or complete routine assignments. That students automate tasks mostly when time-pressed helps explain why automation is less common than augmentation.11\r\n\r\nOur finding that students favor augmentation over automation extends beyond Middlebury. In Appendix Figure A4, we re-analyze data from Ravšelj et al., (2025)—a multi-country survey of higher education students—to examine how this balance varies across institutional quality.12 We classify universities into quintiles based on their Times Higher Education World University Rankings. Overall, students who use AI worldwide show similar adoption rates for augmentation (64.6 percent) and automation (63.1 percent) tasks—a much smaller gap than at Middlebury (Panel A). However, this aggregate pattern masks heterogeneity by institutional quality. Students at top-quintile universities show a modest preference for augmentation over automation, with this gap narrowing monotonically down the institution quality distribution. By the bottom quintile, augmentation and automation usage are virtually identical (Panel B).\r\n\r\n4.3Heterogeneity in Augmentation versus Automation Usage\r\nUnderstanding whether augmentation and automation patterns vary across student populations may help to design targeted support policies. To assess this, we construct four measures of augmentation and automation usage. First, we create binary indicators for whether students use AI for any augmentation or automation task. Second, we calculate the proportion of tasks in each category for which students employ AI. Third, using the Likert-scale responses, we create intensity measures that capture how frequently students use AI for augmentation and automation purposes. Finally, we compute the difference between augmentation and automation variables, as a measure that directly compare students’ relative preference for augmentation versus automation. Table LABEL:tab:ai_autom_regs presents regression estimates using these measures as outcomes.\r\n\r\nThe balance between augmentation and automation varies substantially by student characteristics. Males show higher adoption of both augmentation and automation tasks, with a similar magnitude in each case—they are 6.0 pp more likely to use augmentation prompts and 5.5 pp more likely to use automation prompts compared to females (columns 1 and 4, \r\np\r\n<\r\n0.05\r\n). Black students have a substantially higher augmentation intensity than white students (0.871 points higher, equivalent to 38.1 percent of the outcome mean, \r\np\r\n<\r\n0.01\r\n), but show no statistically significant differences in automation usage. This results in Black students having the strongest preference for augmentation over automation, with a 19.5 pp higher share of augmentation tasks relative to automation (column 7, \r\np\r\n<\r\n0.01\r\n). Asian students show high usage of both kinds relative to white students. Differences across field of study effects are sizable: humanities, languages, and literature majors tend to show lower usage of both augmentation and automation tools compared to natural science majors.\r\n\r\n5Institutional Policies and AI Adoption\r\n5.1The Role of Institutional Policies in Shaping Student Behavior\r\nInstitutional policies are central to shaping the adoption and diffusion of new technologies (Acemoglu, , 2025). In the context of generative AI in higher education, understanding how policies affect student behavior is essential for guiding evidence-based decision-making. To examine this, we asked students to report their likelihood of using AI under various policy scenarios, ranging from complete prohibition to unrestricted use. This analysis is particularly relevant in light of the ongoing debate about how universities should regulate generative AI use in academic settings (Nolan, , 2023), and the widespread variation in institutional policies across colleges (Xiao et al., , 2023).\r\n\r\nInstitutional policies substantially influence students’ reported likelihood of using generative AI. Figure 5 shows that when generative AI use is unrestricted, 52.4 percent of students report being likely or extremely likely to use it. This likelihood decreases modestly when policies require citation (40.9 percent) or when no explicit policy exists (42.3 percent). However, explicit prohibition creates a dramatic shift: only 13.4 percent of students report they would be likely or extremely likely to use AI when it is banned, while 72.9 percent say they would be unlikely or extremely unlikely to do so (note that these figures include both current AI users and students who do not use generative AI for academic purposes). These results suggest that institutional policies can significantly influence generative AI usage patterns, though a small fraction of students report they would likely use AI even when explicitly prohibited.\r\n\r\nThe magnitude of policy effects in our study aligns closely with findings from other contexts. Carvajal et al., (2024) estimate that banning AI reduces usage by 37.2 pp among females and 20.6 pp among males, for an overall drop of about 28.9 pp. In our survey, a ban leads to a 37.8 pp decline in usage, with a larger decrease among females (49.6 pp) than males (40.1 pp). These parallel results underscore how institutional policies can unintentionally produce disparate effects across gender. Notably, these differential policy effects extend beyond gender. Other demographic and academic characteristics—such as race and field of study—also moderate how students respond to policy restrictions (Appendix Table LABEL:tab:ai_policy_het), underscoring that institutional policies can produce non-neutral impacts along multiple dimensions.\r\n\r\n5.2Understanding of Institutional Policies and Resources\r\nInformation gaps and inattention can significantly affect technology adoption decisions (Duflo et al., , 2011; Hanna et al., , 2014). For example, imperfect information about rules, available resources, or proper usage guidelines could lead to underadoption of beneficial technologies or inadvertent policy violations. To test for the existence of information gaps, we examine three dimensions of policy understanding. First, we asked students whether they find AI policies in their current classes clear. Second, we measured awareness of free access to premium AI tools through the college—a resource that could reduce inequality but only if students know it exists. Third, we assessed whether students know how to properly cite AI when required, a mechanical skill necessary for academic integrity. We supplemented these measures with open-ended feedback about the college’s AI policies and support services.\r\n\r\nStudent understanding of institutional policies is high, though significant gaps remain. Figure 6, Panel A shows that most students (79.1 percent) understand when and where they are allowed to use AI in their classes, but a nontrivial minority (19.2 percent) find AI policies unclear. Moreover, critical knowledge gaps persist elsewhere. Only 10.1 percent know they have free access to Microsoft Copilot through the college (Panel B), and just 32.6 percent understand how to properly cite AI use (Panel C). These gaps vary systematically: females show better policy understanding than males (81.5 versus 75.8 percent), and non-white students demonstrate higher awareness across all three dimensions compared to white students.13\r\n\r\nThe qualitative evidence from open-ended responses reinforces these information frictions and reveals implementation challenges (Appendix D). Students express frustration with vague guidelines, requesting specific examples of acceptable versus unacceptable use cases. Many advocate for formal training, noting that simply knowing policies exist differs from understanding how to effectively integrate AI into their workflow. A particularly striking theme is students’ perception of blanket prohibitions as both ineffective and unfair, as they create a prisoner’s dilemma situation where compliant students are disadvantaged relative to those who secretly violate restrictions. Many responses call for a balanced approach—permitting AI use that supports learning while restricting uses that replace it—though the boundary between the two remains contested.\r\n\r\n6Beliefs About AI’s Educational Impact and Peer Usage\r\n6.1Student Beliefs of AI’s Impact on Educational Outcomes\r\nTechnology adoption decisions are shaped by beliefs about potential returns (Foster and Rosenzweig, , 2010). Students’ perceptions of how AI affects their learning may influence whether they adopt these tools and how they integrate them into their academic workflows. To elicit these beliefs, we asked students to evaluate AI’s impact across four dimensions of their academic performance: understanding of course materials, overall learning ability, time management, and course grades. For each dimension, students rated AI’s effect on a five-point scale ranging from “significantly reduces” to “significantly improves.” These subjective assessments provide insight into the perceived value of AI tools from the student perspective, which may differ from their actual effects on learning outcomes.\r\n\r\nStudents tend to believe that generative AI is beneficial for their academic performance, though the perceived benefits vary across dimensions. Figure 7 shows that the majority of students (70.2 percent) believe that generative AI improves their understanding of course materials, and 60.1 percent report improvement in their ability to grasp concepts, retain information, or learn new skills. Similarly, 59.4 percent report that AI improves their ability to complete assignments on time. Notably, while students believe AI helps their learning and assignment completion, they are less confident about its impact on course grades—41.1 percent believe it improves their grades, while 55.4 percent report no effect and 3.5 percent report negative effects. This pattern suggests that while students perceive generative AI as improving their learning process and workflow—through better understanding, skill development, and timely completion of work—these benefits do not necessarily translate into better course grades.14\r\n\r\nBeliefs about AI’s benefits strongly predict adoption. In Figure 8, we plot the relationship between the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis) for different subgroups of students (e.g., males, white students, public-school students, etc.). Across all four measured academic dimensions, there is a strong and statistically significant relationship between perceiving positive AI effects and AI adoption. The relationship is strongest for beliefs about the ability to improve course grades (Panel D): a 10-pp increase in the belief that AI improves grades is associated with a 5.6 pp increase in AI adoption (\r\np\r\n<\r\n0.01\r\n). Similar positive relationships exist for beliefs about learning ability (4.3 pp, \r\np\r\n<\r\n0.05\r\n), understanding of course materials (4.5 pp, \r\np\r\n<\r\n0.01\r\n), and timely assignment completion (5.3 pp, \r\np\r\n<\r\n0.05\r\n). These findings suggest that student beliefs about AI’s academic benefits—irrespective of the actual benefits—may play a crucial role in shaping adoption decisions.\r\n\r\nThese positive beliefs align with findings from other contexts. Ravšelj et al., (2025) report that the majority of students in their sample believe that ChatGPT improves their general knowledge (68.8 percent) and specific knowledge (62.7 percent)—remarkably similar to our finding that most students believe AI improves understanding of course materials (68.5 percent) and learning ability (57.6 percent). Similarly, 57.4 percent of students in Ravšelj et al., (2025)’s sample believe that ChatGPT helps meet assignment deadlines, while 59.4 percent in our data report AI improves timely assignment completion. Stöhr et al., (2024) provide complementary evidence from Swedish universities, where 47.7 percent of students believe that AI makes them more effective learners, yet only 17.3 percent believe these tools improve their grades—mirroring our finding that perceived learning benefits from AI use exceed perceived effects on course performance.\r\n\r\n6.2Student Beliefs About Peer Use of Generative AI\r\nStudents’ beliefs about their peers’ AI usage may influence their own adoption through multiple channels, including social norms (e.g., Giaccherini et al., , 2019), social learning (e.g., Foster and Rosenzweig, , 1995; Beaman et al., , 2021), peer effects (e.g., Bailey et al., , 2022), and competitive pressure to avoid falling behind (e.g., Goehring et al., , 2024). To measure these beliefs, we asked students to estimate what fraction of their peers use generative AI for different purposes and would use under different policy environments. Figure 9 presents the distribution of these beliefs. Panels A-C show students’ estimates of peer AI usage for schoolwork, leisure, and any purpose respectively. Panels D-F display students’ beliefs about AI usage under three policy environments: when classes have no explicit AI policy, when classes allow AI use, and when classes prohibit AI use.\r\n\r\nStudents systematically underestimate their peers’ AI usage. On average, they believe 65.2 percent of peers regularly use generative AI for schoolwork (Panel A), while our survey reveals an actual usage rate of 82.5 percent—a 17.3 pp gap. This underestimation appears consistent across educational contexts: Stöhr et al., (2024) find that only 38.7 percent of Swedish students believe AI chatbot use is common among peers, despite 63 percent actual usage. When we examine beliefs about policy-contingent behavior, students estimate that 62.7 percent of peers use AI when no explicit policy exists (Panel D), rising to 72.2 percent under permissive policies (Panel E) and falling to 43.5 percent under prohibition (Panel F). These beliefs align directionally with self-reported intentions, though with notable magnitude differences (Appendix Figure A5). For instance, while students believe 43.5 percent of their peers use AI in classes that prohibit it, only 28.4 percent report they themselves would be likely to do so.15\r\n\r\nNotably, students’ perceptions of peer AI adoption are closely linked to their own adoption behavior. Appendix Figure A6 shows a strong positive association between beliefs about peer usage and actual usage rates across student groups. This behavior is consistent with several psychological mechanisms, including the “false consensus effect” (Ross et al., , 1977), selection neglect (Jehiel, , 2018), or interpersonal projection bias (Bushong and Gagnon-Bartsch, , 2024). Still, virtually all groups systematically underestimate peer usage—actual usage rates exceed believed usage for every demographic and academic group we examine. This figure also shows that underestimation varies substantially across student characteristics: males use AI at 88.7 percent but believe only 66.6 percent of peers do so (22.1 pp gap), while females show a smaller gap (77.6 percent actual versus 65.1 percent believed, 13.3 pp gap).\r\n\r\nTaken together, students’ systematic underestimation of peer AI usage coupled with the strong relationship between beliefs and adoption suggests that misperceptions about social norms may shape technology diffusion. If AI enhances learning, then correcting these misperceptions through information provision could accelerate beneficial adoption; conversely, if AI undermines skill development, then students’ underestimation of peer usage may serve as an unintentional safeguard against harmful overadoption.\r\n\r\n7Discussion\r\nThis paper presents the first systematic evidence on generative AI adoption at a highly selective U.S. college. Using novel survey data, we document exceptionally rapid and widespread adoption, substantial shifts in the educational production function through augmentation and automation, and the significant roles of students’ beliefs and institutional policies in shaping AI use.\r\n\r\nOur results offer three implications for institutional policy and ongoing debates about AI in education. First, we identify low-cost opportunities to improve institutional policy effectiveness through targeted information provision. The significant gaps we document in students’ understanding of institutional AI policies, citation practices, and available AI resources suggest that simple interventions—such as clear guidelines, illustrative examples of acceptable uses, and AI literacy programs—can reduce unintentional academic integrity violations and support beneficial AI integration. Qualitative feedback strongly indicates student demand for more explicit guidance on responsible AI use.\r\n\r\nSecond, our evidence challenges alarmist narratives that conflate widespread AI adoption with universal academic dishonesty based on anecdotal accounts.16 Although AI use is indeed near-universal, we find clear evidence that students primarily employ AI as a tool for strategic task management: to enhance learning (augmentation) and selectively automate tasks when facing high time opportunity cost—not solely to circumvent academic effort. This distinction matters: by normalizing academic dishonesty as inevitable and universal, these narratives may shift social norms and encourage students who would otherwise use AI responsibly to engage in prohibited behaviors, believing “everyone else is doing it.”\r\n\r\nThird, our findings caution against policy extremes of either blanket prohibition or unrestricted AI use (Merchant, , 2024; McDonald et al., , 2025). Blanket prohibitions disproportionately harm students who benefit most from AI’s augmentation functions—particularly lower-achieving students—while also creating uneven compliance, placing conscientious students at a disadvantage relative to rule-breakers. Conversely, unrestricted AI use based solely on revealed preference arguments ignores important market failures in educational settings. Students often hold overly optimistic beliefs about AI’s learning benefits despite mixed empirical evidence, potentially leading to unintended negative learning outcomes. Most concerningly, permissive policies risk creating competitive dynamics where students feel compelled to adopt AI not for its learning benefits but simply to avoid falling behind in an educational “arms race” (Goehring et al., , 2024).\r\n\r\nFigures and Tables\r\nFigure 1:The Adoption of Generative AI among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the fraction of students who report using AI during the academic semester, categorized by demographic characteristics, high school type, academic cohort, GPA, and field of study. Usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily).\r\n\r\nThe category “All students” provides the baseline usage rate for the full sample. Gender categories are based on self-identification, with non-binary responses excluded due to a small sample size. “Private HS” refers to students who attended private high schools, while “Public HS” includes public institutions. “Cohort” denotes the student’s academic year, ranging from first-year (“Freshman”) to fourth-year and beyond (“Senior”). GPA categories (“GPA \r\n>\r\n p50” and “GPA \r\n<\r\n p50”) split students into groups above or below the median first-year GPA, as self-reported on a 4.0 scale. See Appendix B.1 for the classification of majors into fields of study.\r\n\r\nFigure 2:The Evolution of Generative AI Adoption among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the cumulative percent of students who reported using generative AI tools for academic purposes over time. The data is based on retrospective self-reports collected in our December 2024 survey, where students were asked “When did you first start using any form of Generative AI for academic purposes?” Response options ranged from “Before Spring 2023” to “This semester (Fall 2024).” The \r\nx\r\n-axis represents academic semesters, while the \r\ny\r\naxis represents the cumulative adoption rate. Vertical lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure 3:Adoption of Generative AI Models Among College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the adoption rates of various AI models as of Fall 2024. The horizontal axis shows the percent of students who reported using each tool, and the vertical axis lists the tools in descending order of adoption rates.\r\n\r\nFigure 4:Academic Uses of Generative AI\r\nPanel A. Across Common Academic Tasks\r\nRefer to caption\r\nPanel B. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of AI students who use generative AI for different academic tasks. For each task, usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). The number at the end of each bar represents the total percent of students who use AI for that purpose at any frequency. Tasks are ordered by total usage, from highest to lowest. Results are based on responses to the question: “For academic purposes, which of the following tasks do you typically use generative AI for?” Sample includes all students who reported using AI during the academic semester.\r\n\r\nFigure 5:Student Reported Likelihood of Using AI under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of students who report different likelihoods of using AI under various policy scenarios. For each policy, responses are categorized on a five-point scale from “Extremely unlikely to use AI” to “Extremely likely to use AI.” The sample includes all survey respondents. The question asked was: “How likely are you to use generative AI in a class with each of the following AI policies?”\r\n\r\nFigure 6:Understanding of Generative AI Policies and Resources\r\nPanel A. Understanding of AI Policies\r\nRefer to caption\r\nPanel B. Awareness of Copilot Access\r\nRefer to caption\r\nPanel C. Knowledge of Citation Requirements\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows students’ understanding of institutional AI policies and resources. Panel A displays the percent of students who report understanding AI policies in their classes, those who report having no explicit policy, and those who find policies unclear, broken down by demographic characteristics. Panel B shows the percent of students who are aware of their free access to Microsoft Copilot through Middlebury College. Panel C presents the percent of students who report knowing how to properly cite AI use in their academic work when required. For Panels B and C, horizontal lines represent 95 percent confidence intervals. Sample includes all survey respondents.\r\n\r\nFigure 7:Student Beliefs about the Impact of AI on their Academic Performance\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of Middlebury students who believe that AI improves, reduces, or has no effect on different aspects of their academic experience. For each outcome, responses are categorized into three groups: “Improves” combines “significantly improves” and “somewhat improves” responses, “Reduces” combines “significantly reduces” and “somewhat reduces” responses, and “No effect” represents neutral responses. Sample includes all students who report using AI during the academic semester. “Don’t know” responses are excluded.\r\n\r\nFigure 8:Relationship Between AI Adoption and Beliefs About AI’s Academic Benefits\r\nPanel A. Learning ability (e.g., ability to grasp concepts, learn new skills, etc.)\r\nRefer to caption\r\nPanel B. Understanding of\r\ncourse materials\r\nRefer to caption\r\nPanel C. Ability to complete\r\nassignments on time\r\nRefer to caption\r\nPanel D. Course grades\r\n \r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the relationship between AI adoption rates and beliefs that AI improves various academic outcomes across different student groups. Each panel plots the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis). Points represent different student groups categorized by demographics (circles), academic characteristics (triangles), and field of study (squares). The dashed line shows the linear fit across all groups. Groups with fewer than 10 students are excluded.\r\n\r\nFigure 9:Student Beliefs about Generative AI Usage at Middlebury College\r\nPanel A. For Schoolwork\r\nRefer to caption\r\nPanel B. For Leisure\r\nRefer to caption\r\nPanel C. For Any Purpose\r\nRefer to caption\r\nPanel D. Classes with No Policy\r\nRefer to caption\r\nPanel E. Classes that Allow AI\r\nRefer to caption\r\nPanel F. Classes that Disallow AI\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the distribution of students’ beliefs about generative AI usage among their peers at Middlebury College. Panels A-C display students’ estimates of the percent of their peers who regularly use AI for schoolwork, leisure activities, and any purpose, respectively. Panels D-F show students’ beliefs about AI usage in classes with different AI policies: those without an explicit policy (Panel D), those that allow AI use (Panel E), and those that prohibit AI use (Panel F). Each panel shows a histogram with bins of width ten percentage points (e.g., responses between 1-10 fall in the 10 bin, 11-20 in the 20 bin, etc.). The red dashed line indicates the mean response. Sample excludes respondents with missing values or who selected the default response for all six categories (which equals zero).\r\n\r\nTable 1:Summary Statistics of Survey Participants\r\nSurvey Sample\tAdmin records\r\nUnweighted\tWeighted\t\r\n(1)\t(2)\t(3)\r\nPanel A. Demographics\r\nMale\t0.446\t0.433\t0.463\r\nFemale\t0.508\t0.516\t0.533\r\nWhite\t0.618\t0.603\t0.538\r\nBlack\t0.036\t0.033\t0.052\r\nHispanic\t0.099\t0.106\t0.124\r\nAsian\t0.155\t0.162\t0.073\r\nPrivate high school\t0.420\t0.399\t–\r\nPublic high school\t0.543\t0.556\t–\r\nPanel B. Academic Characteristics    GPA 3.740 3.736 3.670 Hours spent on academics per week 17.899 17.889 – Freshman 0.311 0.355 0.255 Sophomore 0.273 0.272 0.257 Junior 0.202 0.179 0.201 Senior 0.213 0.194 0.287   Panel C. Field of Study    Arts 0.011 0.021 0.023 Humanities 0.052 0.068 0.073 Languages 0.021 0.020 0.025 Literature 0.035 0.025 0.025 Natural Sciences 0.218 0.249 0.244 Social Sciences 0.353 0.247 0.243 Has not declared major 0.311 0.371 0.364\r\n\r\nN (# degrees) 43 43 49 N (# students) 634 2,760 2,760\r\n\r\n\\justify\r\nNotes: This table presents summary statistics from our survey of college students. Panel A reports demographic characteristics, including the proportion of participants identifying as male, female, white, Black, Hispanic, Asian, or who attended a private or public high school. Panel B provides academic characteristics, such as GPA (only available for non-freshmen), average weekly hours spent on academics, and academic year distribution (Freshman, Sophomore, Junior, and Senior). Note that in column 1–2, GPA refers to self-reported first-year GPA while in column 3 it is the overall GPA during Spring 2024. Panel C summarizes the distribution of participants across different fields of study. Major groups are mutually exclusive.\r\n\r\n0.8250.5890.3680.106 R\r\n−\r\nsquared0.0770.0890.1160.058 N (Students)616616616616 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI usage frequency threshold and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column uses a different threshold for AI usage frequency, categorized as: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). Column 1 defines usage as any nonzero frequency; column 2 includes at least occasional use; column 3 includes frequent or higher use; and column 4 captures only very frequent use.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.0890.1990.3570.5510.801 R\r\n−\r\nsquared0.0450.0610.0880.0800.069 N (Students)633633633633633 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI adoption date and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column presents results for a different threshold of AI adoption. Column 1 shows the probability of adopting AI before Spring 2023; column 2 by Spring 2023; column 3 by Fall 2023; column 4 by Spring 2024; and column 5 by Fall 2024. The dependent variable in each regression is a binary indicator equal to one if the student had adopted AI by the specified time period.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.9120.6122.2850.9130.4191.8010.1930.484 R\r\n−\r\nsquared0.0360.0730.1260.0530.1170.1290.0420.063 N (Students)515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and their use of generative AI for academic tasks. In columns 1 and 4, the outcome is a dummy that equals one if a student reports using AI with any frequency for at least one augmentation or automation task, respectively. In columns 2 and 5, the outcome is the share of tasks within each category for which the student reports any use. In columns 3 and 6, the outcome is a continuous measure capturing average usage frequency for each task category, based on raw Likert-style responses. In columns 7 and 8, the outcome is the difference in average task share and usage intensity between augmentation and automation, respectively. Regressions are weighted and report robust standard errors clustered at the student level. ∗∗∗ \r\np\r\n<\r\n0.01\r\n, ∗∗ \r\np\r\n<\r\n0.05\r\n, ∗ \r\np\r\n<\r\n0.10\r\n.\r\n\r\nReferences\r\nAcemoglu, (2025)\r\nAcemoglu, D. (2025).Nobel lecture: Institutions, technology, and prosperity.American Economic Review, 115(6):1709–1748.\r\nAngrist et al., (2009)\r\nAngrist, J., Lang, D., and Oreopoulos, P. (2009).Incentives and services for college achievement: Evidence from a randomized trial.American Economic Journal: Applied Economics, 1(1):136–163.\r\nAutor and Thompson, (2025)\r\nAutor, D. and Thompson, N. (2025).Expertise.\r\nBailey et al., (2022)\r\nBailey, M., Johnston, D., Kuchler, T., Stroebel, J., and Wong, A. (2022).Peer effects in product adoption.American Economic Journal: Applied Economics, 14(3):488–526.\r\nBastani et al., (2025)\r\nBastani, H., Bastani, O., Sungu, A., Ge, H., Kabakcı, Ö., and Mariman, R. (2025).Generative AI without guardrails can harm learning: Evidence from high school mathematics.Proceedings of the National Academy of Sciences, 122(26):e2422633122.\r\nBeaman et al., (2021)\r\nBeaman, L., BenYishay, A., Magruder, J., and Mobarak, A. M. (2021).Can network theory-based targeting increase technology adoption?American Economic Review, 111(6):1918–1943.\r\nBick et al., (2025)\r\nBick, A., Blandin, A., and Deming, D. J. (2025).The rapid adoption of generative AI.Technical report, National Bureau of Economic Research.\r\nBonney et al., (2024)\r\nBonney, K., Breaux, C., Buffington, C., Dinlersoz, E., Foster, L. S., Goldschlag, N., Haltiwanger, J. C., Kroff, Z., and Savage, K. (2024).Tracking Firm Use of AI in Real Time: A Snapshot from the Business Trends and Outlook Survey.\r\nBonsu and Baffour-Koduah, (2023)\r\nBonsu, E. M. and Baffour-Koduah, D. (2023).From the consumers’ side: Determining students’ perception and intention to use chatgpt in ghanaian higher education.Journal of Education, Society & Multiculturalism, 4(1):1–29.\r\nBresnahan and Trajtenberg, (1995)\r\nBresnahan, T. F. and Trajtenberg, M. (1995).General purpose technologies ‘engines of growth’?Journal of econometrics, 65(1):83–108.\r\nBrynjolfsson and Hitt, (2000)\r\nBrynjolfsson, E. and Hitt, L. M. (2000).Beyond computation: Information technology, organizational transformation and business performance.Journal of Economic Perspectives, 14(4):23–48.\r\nBrynjolfsson et al., (2025)\r\nBrynjolfsson, E., Li, D., and Raymond, L. (2025).Generative AI at Work.The Quarterly Journal of Economics, 140(2):889–942.\r\nBushong and Gagnon-Bartsch, (2024)\r\nBushong, B. and Gagnon-Bartsch, T. (2024).Failures in forecasting: An experiment on interpersonal projection bias.Management Science, 70(12):8735–8752.\r\nCarvajal et al., (2024)\r\nCarvajal, D., Franco, C., and Isaksson, S. (2024).Will artificial intelligence get in the way of achieving gender equality?Discussion Paper 03, NHH Norwegian School of Economics.\r\n(15)\r\nComin, D. and Hobijn, B. (2010a).An exploration of technology diffusion.American Economic Review, 100(5):2031–2059.\r\n(16)\r\nComin, D. and Hobijn, B. (2010b).Technology Diffusion and Postwar Growth, volume 25, pages 209–246.University of Chicago Press.\r\nCui et al., (2024)\r\nCui, Z. K., Demirer, M., Jaffe, S., Musolff, L., Peng, S., and Salz, T. (2024).The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers.\r\nDavid, (1990)\r\nDavid, P. A. (1990).The dynamo and the computer: an historical perspective on the modern productivity paradox.The American economic review, 80(2):355–361.\r\nDell’Acqua et al., (2023)\r\nDell’Acqua, F., McFowland, E., Mollick, E. R., Lifshitz-Assaf, H., Kellogg, K., Rajendran, S., Krayer, L., Candelon, F., and Lakhani, K. R. (2023).Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality.\r\nDuflo et al., (2011)\r\nDuflo, E., Kremer, M., and Robinson, J. (2011).Nudging Farmers to Use Fertilizer: Theory and Experimental Evidence from Kenya.American Economic Review, 101(6):2350–2390.\r\nEloundou et al., (2024)\r\nEloundou, T., Manning, S., Mishkin, P., and Rock, D. (2024).GPTs are GPTs: Labor market impact potential of LLMs.Science, 384(6702):1306–1308.\r\nFairlie et al., (2014)\r\nFairlie, R. W., Hoffmann, F., and Oreopoulos, P. (2014).A community college instructor like me: Race and ethnicity interactions in the classroom.American Economic Review, 104(8):2567–2591.\r\nFelten et al., (2021)\r\nFelten, E. W., Raj, M., and Seamans, R. (2021).Occupational, Industry, and Geographic Exposure to Artificial Intelligence: A Novel Dataset and Its Potential Uses.\r\nFelten et al., (2023)\r\nFelten, E. W., Raj, M., and Seamans, R. (2023).How will Language Modelers like ChatGPT Affect Occupations and Industries?\r\nFoster and Rosenzweig, (1995)\r\nFoster, A. D. and Rosenzweig, M. R. (1995).Learning by doing and learning from others: Human capital and technical change in agriculture.Journal of political Economy, 103(6):1176–1209.\r\nFoster and Rosenzweig, (2010)\r\nFoster, A. D. and Rosenzweig, M. R. (2010).Microeconomics of technology adoption.Annu. Rev. Econ., 2(1):395–424.\r\nGallup, (2024)\r\nGallup (2024).AI in the Workplace: Answering 3 Big Questions.\r\nGiaccherini et al., (2019)\r\nGiaccherini, M., Herberich, D. H., Jimenez-Gomez, D., List, J. A., Ponti, G., and Price, M. K. (2019).The behavioralist goes door-to-door: Understanding household technological diffusion using a theory-driven natural field experiment.Technical report, National Bureau of Economic Research.\r\nGoehring et al., (2024)\r\nGoehring, G., Mezzanotti, F., and Ravid, S. A. A. (2024).Technology adoption and career concerns: Evidence from the adoption of digital technology in motion pictures.Technical report, National Bureau of Economic Research.\r\nGriliches, (1957)\r\nGriliches, Z. (1957).Hybrid corn: An exploration in the economics of technological change.Econometrica, 25(4):501–522.\r\nHall and Khan, (2003)\r\nHall, B. H. and Khan, B. (2003).Adoption of new technology.\r\n(32)\r\nHanda, K., Bent, D., Tamkin, A., McCain, M., Durmus, E., Stern, M., Schiraldi, M., Huang, S., Ritchie, S., Syverud, S., Jagadish, K., Vo, M., Bell, M., and Ganguli, D. (2025a).Anthropic education report: How university students use claude.\r\n(33)\r\nHanda, K., Tamkin, A., McCain, M., Huang, S., Durmus, E., Heck, S., Mueller, J., Hong, J., Ritchie, S., Belonax, T., et al. (2025b).Which economic tasks are performed with ai? evidence from millions of claude conversations.arXiv preprint arXiv:2503.04761.\r\nHanna et al., (2014)\r\nHanna, R., Mullainathan, S., and Schwartzstein, J. (2014).Learning Through Noticing: Theory and Evidence from a Field Experiment.The Quarterly Journal of Economics, 129(3):1311–1353.\r\nHartley et al., (2025)\r\nHartley, J., Jolevski, F., Melo, V., and Moore, B. (2025).The Labor Market Effects of Generative Artificial Intelligence.\r\nHumlum and Vestergaard, (2025)\r\nHumlum, A. and Vestergaard, E. (2025).Large Language Models, Small Labor Market Effects.\r\nHutto and Gilbert, (2014)\r\nHutto, C. and Gilbert, E. (2014).Vader: A parsimonious rule-based model for sentiment analysis of social media text.In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216–225.\r\nJehiel, (2018)\r\nJehiel, P. (2018).Investment strategy and selection bias: An equilibrium perspective on overoptimism.American Economic Review, 108(6):1582–1597.\r\nKelly et al., (2023)\r\nKelly, A., Sullivan, M., and Strampel, K. (2023).Generative artificial intelligence: University student awareness, experience, and confidence in use across disciplines.Journal of University Teaching and Learning Practice, 20(6):1–16.\r\nKharazian, (2025)\r\nKharazian, A. (2025).Ramp ai index: Monthly measurement of ai adoption by american businesses.Technical report, Ramp Economics Lab.\r\nLehmann et al., (2024)\r\nLehmann, M., Cornelius, P. B., and Sting, F. J. (2024).AI Meets the Classroom: When Does ChatGPT Harm Learning?\r\nLing and Imas, (2025)\r\nLing, Y. and Imas, A. (2025).Underreporting of AI use: The role of social desirability bias.Available at SSRN.\r\nLiu and Wang, (2024)\r\nLiu, Y. and Wang, H. (2024).Who on earth is using generative ai?Policy Research Working Paper WPS10870, World Bank Group, Washington, D.C.\r\nMcClain, (2024)\r\nMcClain, C. (2024).Americans’ use of ChatGPT is ticking up, but few trust its election information.\r\nMcDonald et al., (2025)\r\nMcDonald, N., Johri, A., Ali, A., and Collier, A. H. (2025).Generative artificial intelligence in higher education: Evidence from an analysis of institutional policies and guidelines.Computers in Human Behavior: Artificial Humans, 3:100121.\r\nMcElheran et al., (2023)\r\nMcElheran, K., Li, J. F., Brynjolfsson, E., Kroff, Z., Dinlersoz, E., Foster, L. S., and Zolas, N. (2023).AI Adoption in America: Who, What, and Where.\r\nMerchant, (2024)\r\nMerchant, B. (2024).The new luddites aren’t backing down.The Atlantic.Accessed: 2025-06-16.\r\nNam, (2023)\r\nNam, J. (2023).56% of college students have used ai on assignments or exams.Technical report.Edited by Lyss Welding. Accessed June 11, 2025.\r\nNolan, (2023)\r\nNolan, B. (2023).Here are the schools and colleges that have banned chatgpt.Business Insider.Accessed: 2025-01-27.\r\nNoy and Zhang, (2023)\r\nNoy, S. and Zhang, W. (2023).Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence.\r\nOtis et al., (2024)\r\nOtis, N. G., Delecourt, S., Cranney, K., and Koning, R. (2024).Global Evidence on Gender Gaps and Generative AI.Harvard Business School.\r\nPeng et al., (2023)\r\nPeng, S., Kalliamvakou, E., Cihon, P., and Demirer, M. (2023).The Impact of AI on Developer Productivity: Evidence from GitHub Copilot.\r\nRavšelj et al., (2025)\r\nRavšelj, D., Keržič, D., Tomaževič, N., Umek, L., Brezovar, N., A. Iahad, N., Abdulla, A. A., Akopyan, A., Aldana Segura, M. W., AlHumaid, J., Allam, M. F., Alló, M., Andoh, R. P. K., Andronic, O., Arthur, Y. D., Aydın, F., Badran, A., Balbontín-Alvarado, R., Ben Saad, H., Bencsik, A., Benning, I., Besimi, A., Bezerra, D. D. S., Buizza, C., Burro, R., Bwalya, A., Cachero, C., Castillo-Briceno, P., Castro, H., Chai, C. S., Charalambous, C., Chiu, T. K. F., Clipa, O., Colombari, R., Corral Escobedo, L. J. H., Costa, E., Cre\\textcommabelowtulescu, R. G., Crispino, M., Cucari, N., Dalton, F., Demir Kaya, M., Dumić-Čule, I., Dwidienawati, D., Ebardo, R., Egbenya, D. L., Faris, M. E., Fečko, M., Ferrinho, P., Florea, A., Fong, C. Y., Francis, Z., Ghilardi, A., González-Fernández, B., Hau, D., Hossain, M. S., Hug, T., Inasius, F., Ismail, M. J., Jahić, H., Jessa, M. O., Kapanadze, M., Kar, S. K., Kateeb, E. T., Kaya, F., Khadri, H. O., Kikuchi, M., Kobets, V. M., Kostova, K. M., Krasmane, E., Lau, J., Law, W. H. C., Lazăr, F., Lazović-Pita, L., Lee, V. W. Y., Li, J., López-Aguilar, D. V., Luca, A., Luciano, R. G., Machin-Mastromatteo, J. D., Madi, M., Manguele, A. L., Manrique, R. F., Mapulanga, T., Marimon, F., Marinova, G. I., Mas-Machuca, M., Mejía-Rodríguez, O., Meletiou-Mavrotheris, M., Méndez-Prado, S. M., Meza-Cano, J. M., Mirķe, E., Mishra, A., Mital, O., Mollica, C., Morariu, D. I., Mospan, N., Mukuka, A., Navarro Jiménez, S. G., Nikaj, I., Nisheva, M. M., Nisiforou, E., Njiku, J., Nomnian, S., Nuredini-Mehmedi, L., Nyamekye, E., Obadić, A., Okela, A. H., Olenik-Shemesh, D., Ostoj, I., Peralta-Rizzo, K. J., Peštek, A., Pilav-Velić, A., Pires, D. R. M., Rabin, E., Raccanello, D., Ramie, A., Rashid, M. M. U., Reuter, R. A. P., Reyes, V., Rodrigues, A. S., Rodway, P., Ručinská, S., Sadzaglishvili, S., Salem, A. A. M. S., Savić, G., Schepman, A., Shahpo, S. M., Snouber, A., Soler, E., Sonyel, B., Stefanova, E., Stone, A., Strzelecki, A., Tanaka, T., Tapia Cortes, C., Teira-Fachado, A., Tilga, H., Titko, J., Tolmach, M., Turmudi, D., Varela-Candamio, L., Vekiri, I., Vicentini, G., Woyo, E., Yorulmaz, Ö., Yunus, S. A. S., Zamfir, A.-M., Zhou, M., and Aristovnik, A. (2025).Higher education students’ perceptions of ChatGPT: A global study of early reactions.PLOS ONE, 20(2):e0315011.\r\nRogers, (1962)\r\nRogers, E. M. (1962).Diffusion of Innovations.Free Press, New York.\r\nRoss et al., (1977)\r\nRoss, L., Greene, D., and House, P. (1977).The “false consensus effect”: An egocentric bias in social perception and attribution processes.Journal of experimental social psychology, 13(3):279–301.\r\nSacerdote, (2001)\r\nSacerdote, B. (2001).Peer Effects with Random Assignment: Results for Dartmouth Roommates.The Quarterly Journal of Economics, 116(2):681–704.\r\nStinebrickner and Stinebrickner, (2008)\r\nStinebrickner, R. and Stinebrickner, T. R. (2008).The causal effect of studying on academic performance.The BE Journal of Economic Analysis & Policy, 8(1).\r\nStöhr et al., (2024)\r\nStöhr, C., Ou, A. W., and Malmström, H. (2024).Perceptions and usage of AI chatbots among students in higher education across genders, academic levels and fields of study.Computers and Education: Artificial Intelligence, 7:100259.\r\nStokey, (2021)\r\nStokey, N. L. (2021).Technology diffusion.Review of Economic Dynamics, 42:15–36.\r\nWorld Bank, (2016)\r\nWorld Bank (2016).World development report 2016: Digital dividends.World Bank Publications.\r\nWu et al., (2022)\r\nWu, M.-J., Zhao, K., and Fils-Aime, F. (2022).Response rates of online surveys in published research: A meta-analysis.Computers in Human Behavior Reports, 7.\r\nXiao et al., (2023)\r\nXiao, P., Chen, Y., and Bao, W. (2023).Waiting, banning, and embracing: An empirical analysis of adapting policies for generative ai in higher education.arXiv preprint arXiv:2305.18617.\r\nAppendix\r\n\r\nAppendix AAppendix Figures and Tables\r\nFigure A1:Generative AI Usage Survey Design Overview\r\nKey\t\r\nSurvey Structure\r\nSurvey Sections\r\nUsage Data Collection\r\nBeliefs & Perceptions\r\nAI Usage Survey at Middlebury College\r\nDecember 2024\r\nParticipant Recruitment\r\nCampus-wide email invitations, framed as general technology use survey\r\nParticipation Incentives\r\nAmazon gift card lottery ranging from $50-$500\r\nSection 1: Demographics & Academic Information\r\nStudent Characteristics\r\nGender, race/ethnicity, high school type\r\nAcademic Profile\r\nYear, major, self-reported GPA, study hours\r\nSection 2: generative AI Usage Patterns\r\nAdoption Metrics\r\nFrequency, timing of first use, specific AI models used\r\nAcademic Applications\r\nTasks performed with AI, payment for premium services\r\nSection 3: Perceptions & Institutional Policies\r\nImpact Perceptions\r\nEffects on learning, grades, time management\r\nPolicy Responses\r\nUsage likelihood under different policies\r\nPeer Usage Beliefs\r\nEstimates of AI adoption among peers for schoolwork and leisure\r\nOpen-Ended Response Collection\r\nMotivations for AI use, policy feedback, suggestions for improvement\r\nNote: This figure illustrates the structure of the AI usage survey conducted at Middlebury College in December 2024. The survey collected information across three main sections: (1) demographic and academic background, (2) patterns of generative AI usage including adoption timing, frequency, and specific applications, and (3) perceptions of AI’s impact on learning and responses to institutional policies.\r\n\r\nFigure A2:Cumulative Generative AI Use by Student Characteristic\r\nPanel A. By Gender\r\nRefer to caption\r\nPanel B. By Race\r\nRefer to caption\r\nPanel C. By High School Type\r\nRefer to caption\r\nPanel D. By Freshman Year GPA\r\nRefer to caption\r\nPanel E. By Cohort\r\nRefer to caption\r\nPanel F. By Field of Study\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents cumulative AI use based on different student characteristics. Each panel displays the cumulative distribution of AI use based on a specific characteristic: gender, race, school type, first-year GPA, cohort, or field of study. The cumulative percent of students is plotted against usage categories. The legends and colors correspond to subgroups within each demographic variable.\r\n\r\nFigure A3:Percent of Students Who Pay for Generative AI Tools\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of AI users who pay for AI tools (through any platform) across different demographic groups. Horizontal lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure A4:Academic Uses of ChatGPT: Evidence from Global Survey\r\nPanel A. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\nPanel B. Across the College Quality Distribution\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of students who use ChatGPT for different academic tasks based on data from Ravšelj et al., (2025). Panel A displays usage patterns across tasks categorized as augmenting (proofreading, translating, study assistance, research assistance) versus automating (academic writing, professional writing, creative writing, brainstorming, summarizing, calculating, coding assistance, personal assistance) student effort. Panel B shows usage patterns across university quality quintiles based on World University Rankings, with universities ranked in the top 20% (top quintile) showing slightly higher rates of augmentation relative to automation compared to bottom quintile institutions. The analysis includes universities with at least 30 student responses and excludes observations with missing usage data.\r\n\r\nFigure A5:Student Beliefs about AI Usage versus Actual Usage under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure compares students’ beliefs about AI usage with actual usage rates under different AI policies. For each policy type, blue bars show the percent of students who report being “Neutral,” “Likely,” or “Very Likely” to use AI, while red bars show students’ mean beliefs about what percent of their peers use AI in classes with that policy. The sample includes all survey respondents for both actual usage and beliefs measures. “AI Use is Unrestricted” refers to classes with no restrictions on AI use, “No Explicit AI Policy” refers to classes without a stated policy, and “AI Use is Prohibited” refers to classes that ban AI use entirely.\r\n\r\nFigure A6:Relationship Between Beliefs About AI Usage and Actual AI Usage\r\nRefer to caption\\justify\r\nNotes: This figure shows the relationship between students’ beliefs about AI usage among their peers and actual AI usage rates across different demographic groups. Each point represents a different group of students (by demographics, academic characteristics, or field of study). The \r\ny\r\n-axis shows the percent of students in each group who report using AI for academic purposes. The \r\nx\r\n-axis shows the mean belief within each group about what percent of Middlebury students use AI. The dashed line shows the linear fit. Sample includes all survey respondents with at least ten observations per group. Students’ beliefs about AI usage are positively correlated with actual usage patterns, suggesting that students have relatively accurate perceptions of AI adoption among their peers.\r\n\r\nGPA > p50 80.3 26.3 17.4 23.2 13.4 GPA < p50 87.1 18.7 26.3 32.2 9.9 Freshman 81.3 24.3 24.1 24.7 8.1 Sophomore 84.4 25.6 22.6 23.7 12.4 Junior 86.2 21.6 23.1 28.6 12.8 Senior 78.7 21.0 17.4 29.9 10.4\r\n\r\nPanel C. Field of Study    Arts 73.3 21.0 0.0 52.3 0.0 Humanities 74.8 39.3 21.8 12.1 1.7 Languages 57.4 27.0 30.3 0.0 0.0 Literature 48.6 10.0 33.7 5.0 0.0 Natural Sciences 91.1 22.0 21.9 33.8 13.3 Social Sciences 84.6 17.9 20.4 29.1 17.2\r\n\r\n\\justify\r\nNotes: This table presents the percent of students in each demographic group who report using AI at different frequencies during the academic semester. Each cell shows the percent of students within that group. Column 1 reports the total percent who use AI at any frequency. Columns 2 to 5 represent increasing usage frequencies: rarely (1–2 times per semester), occasionally (monthly), frequently (weekly), and very frequently (multiple times per week). Panel A reports percentages by demographic characteristics. Panel B shows percentages by academic characteristics. Panel C presents percentages by field of study.\r\n\r\n0.9730.1350.0770.1100.114 R\r\n−\r\nsquared0.0120.0510.0270.0240.138 N (Students)516516516516516 \\justifyNotes: This table assesses the relationship between AI model adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI model usage (columns 1-4) or payment for AI services (column 5), and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division.\r\n\r\nEach column presents results for a different model or payment outcome. Column 1 shows usage of OpenAI’s ChatGPT, column 2 Google Gemini, column 3 Microsoft Copilot, column 4 any other AI model, and column 5 whether the student pays for any generative AI service.\r\n\r\nThe omitted categories are Natural Sciences for academic division, white students for race/ethnicity, and freshmen for cohort. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.7510.6760.6610.2710.479 R\r\n−\r\nsquared0.0870.1130.0760.0680.041 N (Students)599599599599599 \\justifyNotes: This table examines how student characteristics relate to self-reported likelihood of using generative AI under different policy scenarios. Each column presents results for different policy scenarios. In columns 1-4, the dependent variable equals one if the student reports being“neutral,” “likely” or “extremely likely” to use AI under the specified policy, and zero if they report being “unlikely,” or “extremely unlikely.” Column 5 represents the impact of moving from unrestricted use to complete prohibition.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.5760.6850.3760.4890.564 R\r\n−\r\nsquared0.1020.1190.0840.0370.087 N (Students)561565543560560 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. Each column presents results for beliefs about different academic outcomes: learning ability (e.g., ability to grasp concepts, retain information, or learn new skills) in column 1, understanding of course materials in column 2, course grades in column 3, ability to complete assignments on time in column 4, and time spent on academics in column 5. The dependent variable in each regression equals one if the student believes AI somewhat improves” or significantly improves” the outcome, and zero if they believe it has no effect, reduces, or significantly reduces the outcome. “Don’t know” responses are excluded.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\nAppendix BEmpirical Appendix\r\nB.1Field of Study Classifications\r\nThis appendix details the classification of majors into broad fields of study used in Figure 1:\r\n\r\n• Natural Sciences: Includes Biology, Biochemistry, Chemistry, Computer Science, Earth and Climate Sciences/Geology, Environmental Studies, Mathematics, Molecular Biology & Biochemistry, Neuroscience, Physics, and Statistics.\r\n• Social Sciences: Includes Anthropology, Economics, Education, Geography, International & Global Studies, International Politics & Economics, Political Science, Psychology, and Sociology.\r\n• Humanities: Includes American Studies, Black Studies, Classics, History, Architectural Studies, Art History & Museum Studies, History of Art & Architecture, Philosophy, Religion, and Classical Studies.\r\n• Literature: Includes Comparative Literature, English/English & American Literatures, and Literary Studies.\r\n• Languages: Includes Arabic, Chinese, French & Francophone Studies, German, Japanese Studies, Russian, and Spanish.\r\n• Arts: Includes Film & Media Culture, Music, Studio Art, and Theatre.\r\nB.2Task-Specific Use of Generative AI\r\nThe aggregate patterns documented in Section 4 may mask important heterogeneity across student groups. Different students may adopt AI tools at varying rates depending on their academic needs and field-specific norms. To investigate this heterogeneity systematically, Appendix Table LABEL:tab:ai_purpose_correlates presents regression estimates examining how student characteristics predict AI usage for each of our ten measured academic tasks.\r\n\r\nUsage patterns differ markedly by student characteristics. Male students consistently exhibit higher adoption rates across most applications, with particularly pronounced differences for finding information (16.4 pp; \r\np\r\n<\r\n0.01\r\n), summarizing texts (12.5 pp; \r\np\r\n<\r\n0.01\r\n), and creating images (16.9 pp; \r\np\r\n<\r\n0.01\r\n). Black students show substantially higher usage for information-gathering tasks, being 23.8 pp more likely to use AI for finding information compared to white students (\r\np\r\n<\r\n0.05\r\n). They also exhibit dramatically higher adoption of writing assistance tools, with 27.6 pp higher usage for editing text (\r\np\r\n<\r\n0.05\r\n) and 23.6 pp for writing emails (\r\np\r\n<\r\n0.10\r\n). Latino students demonstrate significantly higher usage for generating ideas (24.0 pp; \r\np\r\n<\r\n0.01\r\n) and writing emails (20.7 pp; \r\np\r\n<\r\n0.01\r\n), while Asian students show markedly higher adoption for writing emails (24.5 pp; \r\np\r\n<\r\n0.01\r\n) and explaining concepts (10.9 pp, \r\np\r\n<\r\n0.05\r\n). Students from public high schools report lower usage rates for several applications, particularly for concept explanation (7.3 pp lower; \r\np\r\n<\r\n0.10\r\n) and writing emails (7.4 pp lower; \r\np\r\n<\r\n0.10\r\n), but higher usage for proofreading (8.7 pp higher; \r\np\r\n<\r\n0.10\r\n).\r\n\r\nField of study emerges as a particularly strong predictor of usage patterns. Arts majors show significantly lower adoption across multiple tasks compared to Natural Science students, with gaps of 49.7 pp for finding information (\r\np\r\n<\r\n0.01\r\n), 44.0 pp for generating ideas (\r\np\r\n<\r\n0.05\r\n), and 62.1 pp for coding assistance (\r\np\r\n<\r\n0.01\r\n). Similarly, humanities majors are 36.8 pp less likely to use AI for generating ideas (\r\np\r\n<\r\n0.01\r\n) and 30.8 pp less likely for coding assistance (\r\np\r\n<\r\n0.01\r\n). Languages majors exhibit the most pronounced differences, being 52.4 pp less likely to use AI for summarizing texts (\r\np\r\n<\r\n0.01\r\n) and 46.9 pp less likely for generating ideas (\r\np\r\n<\r\n0.01\r\n). By contrast, social science students show higher usage for explaining concepts (8.0 pp; \r\np\r\n<\r\n0.10\r\n) and summarizing texts (14.7 pp; \r\np\r\n<\r\n0.01\r\n).\r\n\r\n0.8030.7400.6310.6190.5410.4730.3710.3440.2350.204 R\r\n−\r\nsquared0.0780.0870.0770.1160.0280.0540.0830.1350.0400.094 N (Students)515515515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and use of AI for specific academic tasks. Each column shows the result for a different academic task. The omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Regressions are weighted and use heteroskedasticity-robust standard errors clustered at the student level. ∗∗∗, ∗∗, and ∗ indicate significance at the 1%, 5%, and 10% levels, respectively.\r\n\r\nAppendix CQualitative Evidence on Student Perspectives on AI Use\r\nIn this section, we analyze student responses to an open-ended question about their use of generative AI. The survey question asked: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” This was an optional question that 48.3 percent of survey respondents answered. Appendix Figure C1 presents a word cloud of the most frequent words in student responses.\r\n\r\nC.1Validating the Open-Ended Response Measure\r\nWe begin by validating our open-ended response measure to show that it contains meaningful signal. To validate this measure, we employ sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner), a widely-used lexicon and rule-based sentiment analysis tool (Hutto and Gilbert, , 2014). We calculate sentiment scores for each response and examine whether these scores correlate with students’ actual AI adoption behaviors. Intuitively, students who express more positive sentiment toward AI in their open-ended responses should be more likely to have adopted AI tools in practice. We test this hypothesis by examining two measures of AI adoption: whether students have ever used generative AI and whether they currently use AI for academic purposes. Appendix Figure C2 presents binned scatterplots showing the relationship between AI adoption rates and standardized sentiment scores.\r\n\r\nSentiment towards generative AI strongly predicts AI adoption. For both outcomes, students with negative sentiment scores (below zero) show substantially lower adoption rates—around 70-85 percent for ever using AI and 70-75 percent for academic use. In contrast, students with positive sentiment scores display markedly higher adoption rates, reaching nearly 100 percent for general use and 95-100 percent for academic purposes among those with the most positive sentiment. The relationship is particularly pronounced for academic AI use (Panel B), where the coefficient of \r\nβ\r\n^\r\n=\r\n0.168\r\n is twice as large as for general adoption.17 These systematic patterns confirm that our open-ended responses capture meaningful variation in student attitudes that corresponds to real behavioral differences.\r\n\r\nC.2How Students use Generative AI\r\nTo analyze these responses systematically, we classified each response using keywords based on their content. For example, if a student mentioned using AI to save time, we tagged the response with the keyword “time-saver.” If a student expressed concerns about AI’s impact on learning, we tagged it with “negative learning.” Responses could receive multiple keywords if they discussed several themes. Appendix Figure C3 shows the frequency of keywords in our classification. The responses reveal how students use AI tools, what motivates this use, and what causes some to avoid or limit their use.\r\n\r\nThe most common use of AI is as an explanatory tool. Nearly 30 percent of responses mentioned using AI to understand course material. Students frequently ask AI to break down complex concepts from readings and lectures, particularly when they find the material difficult to understand. For example, one student reported: “I can ask AI to explain concepts to me that I have a hard time grasping. […] I can keep asking ’simplify’ or ’break down even more.”’ Students also use AI to summarize dense academic readings, which they argue helps them manage heavy reading loads.\r\n\r\nStudents employ AI throughout different stages of the writing process. Some use AI to generate initial drafts that serve as starting points. One student explained: “Helps me get started with a base for most of my essays. It feels easier to edit something already written and make it my own than to write from scratch.” Others use AI more narrowly for brainstorming when stuck on specific problems. As one student noted: “I use it if I am feeling stuck to push me to the right direction (whether a mathematical problem or an essay idea).” Many also report using AI as an editing tool to improve grammar, sentence structure, and overall writing flow. This is especially the case for non-native English speakers. As one student explained: “English is not my first language and it frustrates me sometimes that I cannot find the best way to phrase a certain idea and AI is a useful tool to have to find alternate expressions.”\r\n\r\nStudents frequently mentioned using AI for specific academic tasks. In courses that require coding, students often use AI for debugging code and understanding programming concepts. Students also employ AI for administrative tasks like formatting citations and drafting routine emails. Finally, many students use AI as an enhanced search engine. One student reported: “It has significantly reduced the time it takes to conduct research on new topics and ideas, and helps me by giving me a thorough selection of sources to use for projects of any kind.”\r\n\r\nC.3Why Students Adopt AI\r\nTime savings was the most commonly cited reason for using AI. Nearly 30 percent of responses mentioned using AI to complete work more efficiently. Students often viewed AI as a way to manage demanding course loads. Many students particularly embrace AI assistance for tasks viewed as mechanical or administrative. A student noted they use AI for “Writing emails quick and creating resume/ cover letter templates.” But AI assistance goes beyond grunt work. Some students use it to “spend less time doing assignments and homework.” This is particularly true if students don’t view the work as central to their academic experience. As one student explained: “when I come across work I deem as ineffective, I want to spend as little time as possible doing it.”\r\n\r\nHaving an on-demand tutor for academic support was another key motivation. One-quarter of responses described using AI as an “explainer” when other resources were unavailable or inconvenient. As one student noted, “I use it as a last resort (if there are no office hours, after looking up videos, etc.) if I need extra help. I’d like to think that the way I use it is similar to going to office hours or TA hours.”\r\n\r\nPeer influence also drove AI adoption. Some students reported feeling pressure to use AI to remain competitive with their classmates. One student explained: “I noticed others use it, are getting better grades than me, and they say they learn better with the help of AI, so I gave it a try.” Others worried about being at a competitive disadvantage: “Other people were using it and told me about it. I felt like I would be at a disadvantage if I wasn’t also using it.”\r\n\r\nC.4Concerns and Limitations\r\nStudents expressed several concerns about AI use in academic work. The most frequent worry was about negative impacts on learning. One student who initially used AI extensively reported: “In the past, I have simply plugged and chugged homework assignments into ChatGPT and submitted it. Those assignments feedback from teachers was positive and I was getting good grades, but I definitely felt that my own learning outcomes to be significantly worse.” Other students viewed AI use as fundamentally incompatible with their educational goals. As one humanities student explained: “my task is as a humanities student is to think, not calculate; why should I let AI do the thinking for me? It would defeat the purpose of pursuing my education.”\r\n\r\nMany students described ethical concerns about AI use. Responses suggested uncertainty about appropriate boundaries. One student noted: “I never use it to explicitly write something because that feels like overt cheating, but sometimes I hesitate when it completely solves Econ problems. I understand how it does it, and it helps me to learn, but it still sometimes feels a little morally gray.” Another expressed similar ambivalence: “I tend to only use it when [it] will save me time in a moral way.”\r\n\r\nStudents also emphasized the importance of maintaining ownership of their work. Many expressed pride in producing original work and hesitation about diluting that ownership through AI use. As one student explained: “I don’t have interest in using generative AI for my academic work because I want my work to reflect my own ideas.” Another noted: “It usually would not even occur to me to turn to AI to substitute writing because I want to take credit for my work, and using AI seems to diminish that.”\r\n\r\nTechnical limitations deterred some students from using AI tools. Students reported concerns about inaccurate outputs (“hallucinations”) and poor output quality, particularly for creative writing or complex analytical tasks. For example, one student noted that “In my poetry class we were instructed to use it to come up with poems and they were awful, so that kinda turned me away from using it to do my work for me.”\r\n\r\nC.5Discussion\r\nWe conclude with two overarching themes that emerged from the responses.\r\n\r\nFirst, students vary substantially in how they incorporate AI into their academic lives. Crucially, this heterogeneity largely depends on what students perceive as “appropriate” uses of AI. For activities that they perceive as core to their academic journey—like writing essays or solving problems—many students hesitate to use AI. A student articulated this clearly: “Most of my work is writing or reading. If I’m not doing the writing, what is the purpose of me taking the class?” Yet, students draw different boundaries between central tasks and grunt work. Some use AI extensively, viewing their role as akin to a manager that provides high-level direction while AI handles implementation. Others restrict AI use to specific tasks like brainstorming, editing, or drafting emails. Still others avoid AI entirely for academic work, often for ethical reasons. Even among AI users, adoption patterns reflect individual trade-offs between time savings, learning goals, and academic integrity concerns.\r\n\r\nSecond, there appears to be a fundamental tension between efficiency and learning. The time-saving benefits are easy to observe, quantifiable, and tangible. But these time savings are unlikely to be a free lunch. Some benefits may come at the cost of spending less time with material that requires deeper engagement to digest. As one student noted: “There may be a negative effect in that it eliminates much of the ’struggle’ in learning.” Yet, having an on-demand tutor that explains concepts in relatable ways can also improve learning. As one student explained: “It can explain concepts to me in a way that is tailored to my learning style.” This suggests that the impact of AI use on learning outcomes depends not on whether students use AI—almost all do, to some extent—but rather on how they employ these tools.\r\n\r\nFigure C1:Word Cloud of Student Motivations for Generative AI Use\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 147 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?’\r\n\r\nFigure C2:Relationship Between AI Sentiment and AI Adoption\r\nPanel A. Ever Used Generative AI\r\nRefer to caption\r\nPanel B. Uses AI for Academic Purposes\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents the relationship between AI sentiment and AI adoption. Panel A shows the proportion of respondents who have ever used generative AI, while Panel B shows the proportion who use AI for academic purposes. Each point represents the mean adoption rate for respondents within sentiment score bins of width 0.2. Sentiment scores are standardized compound scores computed using Hutto and Gilbert, (2014)’s VADER algorithm applied to responses to an open-ended question about generative AI. Positive values indicate positive sentiment and negative values indicate negative sentiment. The dashed lines show OLS best-fit lines estimated on the microdata, with coefficients and standard errors (in parentheses) displayed in the top-left corner of each panel.\r\n\r\nFigure C3:Frequency of Keywords in Student Motivations for AI Use\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to AI use. The responses come from the question “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” Color coding indicates the category of each theme. Usage type refers to how students use AI tools. Motivation captures what drove students to try AI. Concerns include mentions of course policies and academic integrity, individual reservations about AI use, worries about AI’s impact on education, and AI’s technical limitations.\r\n\r\nAppendix DQualitative Evidence on Student Views of AI Policies\r\nIn this section, we analyze student responses to an open-ended question about Middlebury’s AI policies. The survey asked: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Appendix Figure D1 presents a word cloud of the most frequent words in student responses. To analyze these responses systematically, we classified each response using keywords based on their content. Appendix Figure D2 shows the frequency of keywords in our classification.\r\n\r\nD.1Polarized Views on Generative AI Policy Approaches\r\nStudents expressed markedly different views about appropriate AI policies, revealing fundamental disagreement about the path forward. Some strongly advocated for embracing AI technology. As one student argued, “The tool is there, there is supply and there is demand. Don’t fight another war on drugs. Don’t live in a fake reality”. Others called for significant restrictions, arguing that “the use of generative AI is dishonest and corrosive” and that it “prohibits these organic processes and divorces students from true learning”.\r\n\r\nHowever, the most common position advocated for a balanced approach that would allow beneficial uses while restricting harmful ones. Students distinguished between uses that enhance learning (like concept explanation) and those that substitute for learning (like generating entire essays). One student articulated this nuanced view particularly well: “AI also can really be helpful at explaining a textbook problem that doesn’t make sense, or guiding slightly with homework, or creating study materials, or editing/tightening up your prose. All of those things are good, and universities should figure out how to maximize AI use for those reasons and to minimize students just feeding their problem sets into ChatGPT”\r\n\r\nA recurring theme was the futility of blanket bans. Many students emphasized that prohibition would be ineffective, with one noting “I don’t think anyone really cares what the policy of any given class is. If professors want people to not use it, they need to structure assessments in a way that will discourage use.” Another compared AI bans to restricting internet use, arguing “AI policies seem to be totally irrelevant. It’s like telling people they can’t use the internet as a resource for the class.” This ineffectiveness of bans creates fairness concerns. As one student explained: “I think if it is banned in a class, that should be enforced (and right now it absolutely is not)… As with any form of cheating, those who don’t cheat are put at a disadvantage.” Another student expressed similar frustration: “I find it discouraging when I hear classmates saying they use AI for things such as essays when they use it in dishonest ways.”\r\n\r\nD.2Need for Clear Guidelines\r\nThe most frequently expressed concern was the need for clear guidelines about generative AI use. Several students reported confusion about what constitutes acceptable use, with one noting “I think it should be more clear whether we can use it and how and how to cite it since most professors rarely mention it at all”. Other students emphasized the importance of professors explaining their policies upfront and their rationale, with one stating “I think that Professor’s should be very specific about what is allowed and their reasoning behind their policy”.\r\n\r\nMany students advocated for standardization across classes, observing that “Sometimes its confusing when one class allows it and another doesn’t and the other encourages it and so on so if there was a school wide or department wide policy that could help”. Yet there is also disagreement regarding standardization. Some students preferred leaving the decision to individual professors, arguing that “GenAI is more effective in some classes/majors than others. Making sure professors understand how students use GenAI and how useful GenAI is in their class (given the course structure, nature of assignments/material, etc.) is very important for the class policy.”\r\n\r\nD.3Training and Support Services\r\nStudents strongly emphasized the need for training in appropriate generative AI use. Many suggested that the college should provide guidance on using AI tools effectively while maintaining academic integrity. One student proposed “a workshop that teaches you to effectively use GenAI without violating the honor code”. Students also expressed interest in learning how to leverage AI to enhance their learning experience rather than circumvent it. As one student explained: “I think it could be useful to develop some sort of training. How do we use AI in a way that actually benefits our learning? I tried out some things on my own but I feel that I need more guidance.”\r\n\r\nThis desire for training was often linked to workplace preparedness. Students recognized that AI proficiency would be valuable in their careers, with one noting “As the world uses more and more AI, I think it is an important tool that students should know how to leverage”. Another emphasized: “The moment us students leave campus, we will be using it in the professional world, and when used in combination with one’s own skills, it is merely a tool to maximize efficiency”.\r\n\r\nD.4Discussion\r\nThe responses reveal several key tensions in AI policy implementation. First, students express a desire to use AI in ways that enhance rather than substitute for learning, yet they recognize that blanket bans are ineffective and worry about being disadvantaged if they follow restrictions while others do not.\r\n\r\nSecond, while students desire clear guidelines, they also want flexibility to accommodate legitimate uses that vary by discipline and assignment type. Different courses and majors may find different AI uses appropriate based on their learning objectives and assessment types.\r\n\r\nThird, there is tension between faculty autonomy in setting course policies and students’ desire for consistent institutional standards. While some support letting professors determine appropriate AI use for their specific courses, others argue that varying policies across classes create confusion and enforcement challenges.\r\n\r\nFinally, students report significant variation in faculty attitudes toward AI. Some students perceived faculty fear or misunderstanding of AI tools, noting that categorical bans often reflect a lack of understanding about AI’s capabilities and limitations rather than pedagogical considerations.\r\n\r\nFigure D1:Word Cloud of Student Feedback on Generative AI Policies\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 133 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?”\r\n\r\nFigure D2:Frequency of Keywords in Student Feedback on AI Policies\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to Middlebury’s AI policies. The responses come from the question “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Color coding indicates the category of each theme. Policy Approach captures suggestions about how AI should be regulated at the college. Adoption Views reflect positions on whether and how AI should be integrated into academic work. Training Needs indicates requests for guidance and support. Other Concerns include issues of workplace relevance and fairness.\r\n\r\nSee pages 1 of results/ai-survey-midd.pdf See pages 2- of results//ai-survey-midd.pdf",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== story.txt ===\nI remember the moment that sparked this project. It was during a walk across Middlebury’s campus late one fall afternoon—just after office hours. A student had come in, clearly bright but struggling, and told me they’d been using ChatGPT late at night to help with assignments because they were too anxious to speak up in class. That confession stuck with me.\r\n\r\nLater, over coffee with Germán, we started wondering—how many students were turning to generative AI as a quiet lifeline? Not to cheat, but to learn? That simple question became the seed for this entire study. What we found wasn’t just statistics. It was a story of adaptation, of students reshaping how they learn, and of institutions racing to catch up.\"\n\n==================================================\n\n=== Generative AI in Higher Education E.txt ===\nGenerative AI in Higher Education: Evidence from an Elite College†\r\nZara Contractor\r\nGermán Reyes\r\nAbstract\r\nGenerative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT’s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI’s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI’s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\r\n\r\n1Introduction\r\nThe rapid advancement of generative artificial intelligence (AI) is transforming higher education at an unprecedented pace. The launch of ChatGPT and similar tools has introduced technologies capable of performing tasks central to academic assessment and learning—writing essays, solving complex problems, and explaining intricate concepts—instantly and at near-zero marginal cost. Yet systematic evidence on the nature and implications of this new technology remains scarce. How widespread is generative AI adoption among students, and what factors drive it? Do students primarily use AI to augment their learning or to automate coursework, potentially harming human capital development? Could disparities in access to premium AI resources amplify existing educational inequalities?\r\n\r\nThese questions are at the heart of ongoing debates about the role of AI in education, which have relied heavily on anecdotal evidence and speculation rather than rigorous evidence. As a result, universities have implemented policies that vary dramatically, from outright bans to permissive adoption, often without clear evidence on their effectiveness or unintended consequences (Nolan, , 2023; Xiao et al., , 2023; McDonald et al., , 2025).\r\n\r\nThis paper addresses this gap by systematically examining generative AI adoption at Middlebury College, a highly selective liberal arts college in Vermont. With approximately 2,800 undergraduate students, Middlebury offers 49 majors across the arts, humanities, languages, social sciences, and natural sciences. Over the past decade, Middlebury has consistently ranked among the top 10 liberal arts colleges in U.S. News & World Report, with an average ranking of 8.5 between 2015 and 2025. The college’s selectivity is comparable to that of many elite research universities—Middlebury’s acceptance rate of approximately 10 percent is similar to that of Boston University (11 percent), Georgetown University (13 percent), and the University of Virginia (16 percent).\r\n\r\nOur analysis draws on survey data collected from the student population between December 2024 and February 2025. The survey collected detailed information about AI usage, including the frequency and purpose of use, perceived impacts, and responses to institutional policies. To minimize selection bias, we framed the survey broadly as examining technology use and provided incentives for participation, achieving a 22.9 percent response rate with 634 responses representing a broad cross-section of the student population.\r\n\r\nWe document five main findings. First, generative AI is approaching near-universal adoption at an unprecedented speed. Over 80 percent of students use AI for academic purposes, up from less than 10 percent before Spring 2023. This represents one of the fastest technology adoption episodes ever documented, dramatically exceeding the 40 percent adoption rate among U.S. workers (Bick et al., , 2025) and the 23 percent among all U.S. adults (McClain, , 2024). These levels are consistent with international findings of 50–70 percent adoption in university contexts (Nam, , 2023; Stöhr et al., , 2024; Ravšelj et al., , 2025).\r\n\r\nSecond, AI adoption is markedly unequal across academic disciplines and demographic groups. Field of study is the strongest predictor of adoption, likely reflecting how well AI capabilities align with the academic tasks required across fields. Adoption ranges from 91.1 percent in Natural Sciences majors (including mathematics and computer science) to significantly lower levels in Literature (48.6 percent) and Languages (57.4 percent). Additionally, demographic disparities exist. For example, males adopt AI at higher rates than females (88.7 versus 78.4 percent), a pattern that is consistent with documented AI gender gaps in other contexts (Otis et al., , 2024). Most notably, lower-achieving students have a higher adoption rate than their higher-achieving peers (87.1 versus 80.3 percent). As a result, AI could serve as an equalizing force if it enhances student learning—helping struggling students catch up to their peers—but could also widen achievement gaps if AI undermines skill development.\r\n\r\nThird, generative AI transforms the students’ learning production function by both augmenting student capabilities and automating academic tasks. We classify academic tasks as augmentation when AI enhances human capabilities while maintaining student engagement (e.g., explaining concepts, proofreading) versus automation when AI directly produces outputs with minimal cognitive involvement (e.g., writing essays, creating images), and average usage rates across tasks within each category. We find that 61.2 percent of AI users employ these tool for augmentation purposes, while 41.9 percent use it for automation. Qualitative evidence reinforces these patterns: students describe AI as an “on-demand tutor” for augmentation purposes, particularly valuable when traditional resources like office hours are unavailable, while automation use centers on time savings during periods of overwhelming workload. These self-reported patterns align closely with actual usage data from Claude conversation logs (Handa et al., 2025a,).\r\n\r\nFourth, institutional policies can significantly influence AI adoption, though their effectiveness can be undermined by informational frictions. We find that explicit prohibitions dramatically reduce self-reported intended use 39 percentage points (pp). Importantly, the effects of this policy vary substantially across student groups, with females showing larger reductions in usage under prohibition than males, suggesting that one-size-fits-all policies may inadvertently create disparate impacts across groups. Moreover, we also document substantial informational gaps about institutional AI policies. Many students (19.2 percent) do not understand AI policy rules, few know about institution-provided premium AI resources (10.1 percent) or proper AI citation practices (32.6 percent)—a skill that is necessary for academic integrity.\r\n\r\nFifth, students believe that AI has a positive learning impact and these beliefs strongly predict their own AI usage. Most students believe that AI improves their understanding of course materials (70.2 percent) and learning ability (60.1 percent), though fewer believe it improves grades (41.1 percent). Perhaps unsurprisingly, students who believe that AI improves their academic outcomes are significantly more likely to use these tools. For example, a ten-percentage-point increase in the belief that AI improves learning corresponds to a 4.9 percentage-point increase in adoption, highlighting how perceptions can affect technology diffusion in educational settings.\r\n\r\nTaken together, our findings reveal that generative AI has already reshaped students’ college experience. The substantial information gaps that we document—regarding both permitted uses and available resources—suggest straightforward opportunities for policy intervention. Yet the observed heterogeneity across disciplines and student groups indicates that uniform policies risk unintended consequences: blanket prohibitions may disproportionately disadvantage students who benefit most from AI augmentation, while unrestricted use may encourage automation practices detrimental to skill development. Thus, effective policy requires distinguishing clearly between AI uses that enhance learning and those that undermine it.\r\n\r\nWhile our findings are specific to the population we study—students at an elite liberal arts college—this setting offers valuable into AI’s broader societal impact. The rapid uptake of generative AI in higher education may reflect a generalizable principle regarding the diffusion of generative AI: adoption may be fastest in contexts where AI consolidates fragmented tools into a unified platform. Students have long had access to various tools for the academic tasks for which they now use generative AI.1 What distinguishes AI is its ability to provide all these services through a single, instantly accessible interface at virtually zero marginal cost. This consolidation likely drives students’ perception of enhanced learning efficiency and contributes to the fast adoption. This principle may extend well beyond education: industries with fragmented, specialized tools could experience similar trajectories of rapid AI adoption, even if AI itself does not introduce fundamentally new capabilities.\r\n\r\nOur findings contribute to a rapidly growing literature examining the adoption and impacts of generative AI. Recent work has focused on AI’s effects on worker productivity (Noy and Zhang, , 2023; Dell’Acqua et al., , 2023; Peng et al., , 2023; Cui et al., , 2024; Brynjolfsson et al., , 2025) and its potential to transform occupations (Felten et al., , 2021, 2023; Eloundou et al., , 2024). While several papers document AI adoption in workplace settings (Bick et al., , 2025; Hartley et al., , 2025; Humlum and Vestergaard, , 2025) and at the firm level (McElheran et al., , 2023; Bonney et al., , 2024; Kharazian, , 2025), we examine adoption in higher education—a critical setting where future high-skilled workers develop human capital.\r\n\r\nWe add to an emerging literature examining generative AI in education. Some studies focus on the learning impacts of generative AI, finding mixed impacts depending on the setting (e.g., Lehmann et al., , 2024; Bastani et al., , 2025). A complementary strand of the literature documents adoption patterns across diverse educational contexts, including universities in Australia (Kelly et al., , 2023), Ghana (Bonsu and Baffour-Koduah, , 2023), Norway (Carvajal et al., , 2024), Sweden (Stöhr et al., , 2024), and multi-country studies (Ravšelj et al., , 2025). To the best of our knowledge, this paper provides the first systematic evidence on generative AI adoption in U.S. higher education. Importantly, we focus specifically on how students use AI for academic purposes—unlike studies that measure more general usage including personal lives (e.g., Ravšelj et al., , 2025)—and go beyond documenting adoption rates to examine how students integrate AI into their academic workflows, distinguishing between augmentation and automation, the role of student beliefs about AI’s educational impacts, and how institutional policies shape usage patterns.\r\n\r\nOur findings also contribute to the literature on technology diffusion. Unlike the S-shaped adoption curves documented for most technologies (Griliches, , 1957; Rogers, , 1962), we observe extremely rapid adoption of generative AI, with 80 percent uptake within two years of ChatGPT’s release. This pattern contrasts sharply with the adoption trajectories of historical General Purpose Technologies (GPTs) like electricity, which required decades to achieve widespread use (David, , 1990; Bresnahan and Trajtenberg, , 1995). The rapid diffusion we document aligns with evidence that newer technologies diffuse faster than older ones (Comin and Hobijn, 2010a,; Comin and Hobijn, 2010b,). Two factors likely explain AI’s exceptional adoption speed compared to other GPTs. First, unlike electricity or steam power, generative AI requires minimal physical infrastructure—students access it through existing devices—and is available at no cost to consumers, eliminating the financial barriers that typically slow technology adoption. Second, as noted above, AI consolidates multiple specialized tools into a single platform, making its benefits immediately apparent without requiring specialized training or organizational restructuring, unlike previous GPTs (Brynjolfsson and Hitt, , 2000).\r\n\r\n2Data: Novel Student Survey\r\n2.1Recruitment and Structure\r\nWe conducted the survey from December 2024 to February 2025. All Middlebury College students were contacted via email and received a reminder a few weeks and two months after the initial invitation. To minimize selection bias, the recruitment materials described the survey broadly as a study on students’ use of technology in their academic and personal lives. To incentivize participation, students who completed the survey qualified for entry into a lottery for Amazon gift cards ranging in value from $50 to $500. The full survey instrument is provided in Appendix LABEL:app:survey.\r\n\r\nThe survey contains three main sections (see Appendix Figure A1 for the survey flow). First, we gather demographic and academic information, including gender, race/ethnicity, type of high school attended (private or public), current academic year, and declared or intended major.2 We also collect data on academic inputs and performance through students’ self-reported typical weekly hours spent studying and their first-year GPA.3\r\n\r\nSecond, we measure students’ experience with generative AI tools. We begin by asking whether students have ever used generative AI tools like ChatGPT or Claude. For those who have, we collect information about their usage patterns, including frequency of use during the academic semester, which specific AI models they use, and whether they pay for AI tools. We also gather data on how students use AI for different academic tasks, including writing assistance, learning support, and coding.\r\n\r\nThe final part of the survey elicits students’ beliefs about generative AI’s adoption and impacts. We ask students about their perceptions of AI’s effects on their academic experience across multiple dimensions, including learning, grades, time management, and understanding of course material. We then collect information on the role of institutional policies, asking students how different policy environments influence their likelihood of using AI. We also elicit students’ beliefs about AI usage among their peers, including their estimates of the fraction of Middlebury students who use AI for schoolwork and leisure. The survey concludes with two open-ended questions that allow students to share their experiences with AI in academic settings and provide feedback on the college’s AI policies and support services.\r\n\r\n2.2Sample and Summary Statistics\r\nOut of Middlebury’s 2,760 enrolled students, 739 began the survey. We exclude 105 respondents who left the survey before reaching the generative AI usage module, leaving us with an analysis sample of 634 students (22.9 percent of the student body). This response rate is comparable to that of similar surveys (Wu et al., , 2022). To make our sample more representative of Middlebury’s student body, we construct poststratification weights based on the distribution of declared majors from administrative records.4 Specifically, we weight observations by the ratio of each major’s share in the student population to its share in our survey responses.5\r\n\r\nTable Figures and Tables presents summary statistics for our sample. Column 1 reports unweighted survey averages, column 2 shows averages after applying poststratification weights, and column 3 provides population benchmarks from administrative records where available. Our unweighted sample consists of 44.6 percent male and 50.8 percent females. The racial/ethnic composition includes 61.8 percent white, 15.5 percent Asian, 9.9 percent Hispanic, and 3.6 percent Black students. The majority of students (54.3 percent) attended a public school, while 42.0 percent attended a private high school. Our sample represents 43 different majors across seven fields of study, with 31.1 percent of respondents not yet having declared a major. In our analysis, we group these undeclared students by their intended field of study as reported in the survey.\r\n\r\nComparing our unweighted sample to administrative records reveals notable differences. Our sample overrepresents white students (61.8 versus 53.8 percent) and Asian students (15.5 versus 7.3 percent), while underrepresenting Black students (3.6 versus 5.2 percent) and Hispanic students (9.9 versus 12.4 percent). First-year students are overrepresented (31.1 versus 25.5 percent), while senior students are underrepresented (21.3 versus 28.7 percent). Our weighting procedure partially addresses these discrepancies—particularly in the distribution of academic fields, where the weighted figures closely approximate administrative records—but some demographic differences persist. Despite these differences, our weighted sample provides reasonably close approximations to the college population on most dimensions, particularly for academic characteristics and field distributions.\r\n\r\n3Generative AI Usage Patterns Among Students\r\n3.1Adoption of Generative AI\r\nGenerative AI has achieved a remarkably high adoption rate among Middlebury College students. Figure 1 presents the distribution of AI usage frequency during the academic semester, categorized into four levels: “Rarely” (a few times per semester), “Occasionally” (a few times per month), “Frequently” (a few times per week), and “Very Frequently” (daily or almost daily). Overall, 82.5 percent of students report using generative AI for academic purposes, with substantial variation in usage intensity: 23.5 percent use it rarely, 22.2 percent occasionally, 26.2 percent frequently, and 10.6 percent very frequently.\r\n\r\nThe adoption rate at Middlebury aligns closely with patterns observed across other higher education institutions. A BestColleges survey of U.S. undergraduate and graduate students in fall 2023 found that 56 percent had used AI on assignments or exams (Nam, , 2023). Ravšelj et al., (2025) surveyed higher education students globally between late 2023 and early 2024, finding that 71.4 percent had ever used ChatGPT—though this figure encompasses all usage rather than academic use specifically. Similarly, Stöhr et al., (2024) document that 63 percent of Swedish university students had used ChatGPT by spring 2023, with 35.4 percent reporting regular use and 27.6 percent rare use. Among Norwegian university students, Carvajal et al., (2024) find that 68.9 percent use AI tools occasionally or more frequently.\r\n\r\nThese adoption rates in higher education far exceed those in the general population and workforce. Pew Research finds that only 23 percent of U.S. adults have ever used ChatGPT (McClain, , 2024), while Gallup reports that just one-third of U.S. workers have used AI for work (Gallup, , 2024). Bick et al., (2025) and Hartley et al., (2025) estimate that about 40 percent of the U.S. working-age population used generative AI for work as of late 2024 and early 2025. Both studies document substantial heterogeneity across industries, with information services showing the highest adoption rates at 56–62 percent—still considerably below the 80 percent adoption rate at Middlebury. Even among workers in AI-exposed occupations, Humlum and Vestergaard, (2025) find adoption rates of only 41 percent.6\r\n\r\n3.2Adoption by Student Characteristics and Field of Study\r\nAI adoption varies considerably across demographic groups and academic disciplines (Figure 1 and Appendix Table LABEL:tab:ai_freq_het). Males report higher usage rates than females (88.7 versus 78.4 percent). Usage patterns differ markedly by race/ethnicity: Black students (92.3 percent) and Asian students (91.3 percent) exhibit the highest adoption rates, while white students (80.2 percent) and Hispanic students (77.9 percent) report lower usage. Students from private high schools use AI more frequently than those from public schools (84.1 versus 80.4 percent). Notably, students with below-median GPAs report higher usage rates than their higher-achieving peers (87.1 versus 80.3 percent). Adoption varies widely by field of study: Natural Sciences leads with 91.1 percent usage, followed by Social Sciences at 84.6 percent, while Languages (57.4 percent) and Literature (48.6 percent) show substantially lower adoption rates.\r\n\r\nTo examine how student characteristics jointly relate to AI adoption, we estimate multivariate OLS regressions that include all observed characteristics simultaneously.7 Table LABEL:tab:ai_usage_correlates presents regression estimates using four usage thresholds as outcomes. Each column represents the probability of meeting progressively higher frequency thresholds: any AI use (column 1), at least monthly use (column 2), at least weekly use (column 3), and daily use (column 4).\r\n\r\nThe regression results confirm the patterns observed in the descriptive statistics. Holding other characteristics constant, males are 10.3 pp more likely than females to use AI (column 1, \r\np\r\n<\r\n0.05\r\n), with this gender gap typically widening at higher usage frequencies (columns 2–4, all \r\np\r\n<\r\n0.01\r\n). Black and Asian students show substantially higher adoption rates than white students, at 11.8 and 10.7 pp respectively (column 1, both \r\np\r\n<\r\n0.01\r\n). Students from public high schools are 3.0 pp less likely to use AI than those from private schools, but this difference is not statistically significant. Field of study emerges as the strongest predictor of adoption. Compared to Natural Sciences majors, students in Literature, Languages, Arts, and Humanities all show lower usage rates, with the differences being statistically significant at high usage frequencies (columns 3 and 4). Social Sciences majors exhibit adoption rates similar in magnitude to Natural Sciences majors across all usage frequency thresholds.\r\n\r\nOur findings on heterogeneity in AI adoption align with patterns documented in other settings. The gender gap in AI adoption at Middlebury—10.3 pp higher for males—is consistent with evidence across multiple studies. A meta-analysis of 18 studies by Otis et al., (2024) finds that males are 10–20 percentage points more likely to use generative AI than females. This gender gap in AI adoption appears in all educational studies (Nam, , 2023; Carvajal et al., , 2024; Stöhr et al., , 2024; Ravšelj et al., , 2025). Our finding that students with below-median GPAs are more likely to use AI aligns with Carvajal et al., (2024), who document higher adoption rates among students with lower admission grades. This pattern of greater adoption among lower-achieving students suggests that AI could narrow achievement gaps if it enhances learning and skill development, but could widen these gaps if it undermines the acquisition of fundamental skills.\r\n\r\nThe differences in adoption rates across academic fields at Middlebury mirror patterns documented in other educational settings. Stöhr et al., (2024) find that technology and engineering students exhibit significantly higher ChatGPT usage compared to students in humanities. Similarly, Nam, (2023) report that 62 percent of business majors and 59 percent of STEM majors have used AI tools for coursework, compared to 52 percent of humanities majors. Ravšelj et al., (2025) document comparable disciplinary differences, with applied sciences students showing substantially higher usage rates than arts and humanities students.\r\n\r\nThese academic differences persist into the workforce. Bick et al., (2025) find stark variation by college major: STEM graduates have the highest AI adoption rates (46.0 percent), followed by Business/Economics graduates (40.0 percent), while Liberal Arts graduates show substantially lower rates (22.4 percent). Humlum and Vestergaard, (2025) document similar patterns by occupation, finding that roles requiring strong writing and technical skills—such as marketing specialists and journalists—have the highest adoption rates. These consistent patterns across educational and professional contexts suggest that field-specific factors, particularly the applicability of AI tools to different types of tasks, shape adoption in systematic ways.\r\n\r\n3.3Timing of Generative AI Adoption\r\nThe speed of technology diffusion is a critical determinant of its economic and social impact (David, , 1990; Hall and Khan, , 2003; Stokey, , 2021). In educational contexts, rapid technology adoption can create or exacerbate inequalities between early and late adopters, particularly if the technology confers significant learning advantages (World Bank, , 2016). To track the timing of AI adoption among Middlebury students, we asked them when they first began using generative AI for academic purposes, with options ranging from “This semester (Fall 2024)” to “Before Spring 2023” (as a reference, ChatGPT’s public launch was in November 2022).\r\n\r\nStudents adopted generative AI at an extraordinary pace. Figure 2 shows that the cumulative adoption rate grew dramatically from less than 10 percent before Spring 2023 to slightly above 80 percent by Fall 2024. The pace of adoption has accelerated over time, likely reflecting improvements in AI capabilities. Among current users, 25.7 percent adopted AI in Fall 2024 alone, compared to 19.9 percent in Spring 2024 and 16.3 percent in Fall 2023.\r\n\r\nThe adoption rate among Middlebury students far exceeds that observed in other populations and represents one of the fastest technology adoption episodes ever documented. For comparison, Bick et al., (2025) show that it took over 20 years for computers to reach an 80 percent adoption rate in the U.S. working-age population, and about 15 years for internet adoption to reach similar levels. Even generative AI adoption in the broader population has been markedly slower: Pew Research found that just 23 percent of U.S. adults had ever used ChatGPT as of February 2024, up from 18 percent in July 2023 (McClain, , 2024). The dramatically faster adoption rate we document—reaching over 80 percent in less than two years—suggests academic settings may uniquely accelerate AI diffusion.\r\n\r\nTo identify early versus late adopters of generative AI, in Table LABEL:tab:ai_adopt_correlates and Appendix Figure A2, we analyze how adoption timing varies across student characteristics. The rate of adoption varies substantially across student characteristics. Male students led adoption, with a 8.9 pp higher probability of using AI before Spring 2023 compared to females (column 1, \r\np\r\n<\r\n0.01\r\n)—a gender gap that persists across all periods (columns 2–5). Black and Asian students also adopted earlier than white students, though these differences reach statistical significance only in later periods (columns 4–5). Field of study is a strong predictor of adoption timing. For example, students in Languages consistently lagged behind Natural and Social Sciences majors, with significantly lower adoption rates across nearly all time periods.\r\n\r\n3.4Choice of Generative AI Models\r\nMajor AI companies operate on a freemium model, where free versions coexist with premium subscriptions that offer higher usage limits and access to more advanced models. This tiered structure creates potential for a new form of educational disparities: if paid versions confer substantial academic advantages, students who cannot afford subscriptions may be systematically disadvantaged. To investigate these potential disparities, we presented respondents with a list of options including both free and paid versions of popular models. We also collected information on monthly subscription expenditures, with response options ranging from no active subscription to spending more than $40 monthly (standard subscriptions to major models like ChatGPT cost $20 per month at the time).\r\n\r\nOpenAI’s ChatGPT dominates AI usage among Middlebury AI users, with the free version capturing the largest market share. Figure 3 shows that 89.3 percent of AI users rely on the free version of ChatGPT, making it dramatically more popular than any alternative. Google Gemini (13.5 percent) and Microsoft Copilot (7.7 percent) are distant competitors, while other platforms each capture less than 5 percent of users. This dominance mirrors patterns in other academic settings, where Stöhr et al., (2024) find substantially higher familiarity with ChatGPT compared to alternative platforms, and aligns with broader workforce trends documented by Bick et al., (2025).8\r\n\r\nDespite near-universal AI adoption, only 11.3 percent of AI users pay for any AI service.9 This figure is remarkably similar to the 8 percent found in Ravšelj et al., (2025)’s multi-country survey. This low payment rate suggests that for most students, the premium features of paid versions—primarily higher usage limits and access to more advanced models—do not justify the subscription cost. However, payment patterns reveal significant disparities: males and Asian students are substantially more likely to purchase AI subscriptions (Appendix Table LABEL:tab:ai_model_correlates and Appendix Figure A3), potentially reflecting differences in usage intensity across demographic groups.\r\n\r\n4Generative AI and the Production of Learning\r\n4.1The Use of Generative AI across Academic Tasks\r\nHow is generative AI transforming the traditional inputs to student learning? The educational production function includes inputs like time spent studying (e.g., Stinebrickner and Stinebrickner, , 2008), faculty instruction (e.g., Fairlie et al., , 2014), peer interactions (e.g., Sacerdote, , 2001), and academic support services (e.g., Angrist et al., , 2009). AI tools have the potential to complement or substitute for these traditional inputs. For example, using AI to explain concepts might substitute for faculty office hours, while using it for proofreading might reduce time needed for academic support services.\r\n\r\nTo understand the role of AI in students’ learning production function, we collected information about students’ AI usage across ten common academic tasks: proofreading, generating ideas, writing essays, editing essay drafts, coding assistance, creating images, explaining concepts, composing emails, summarizing materials, and finding information. For each task, students indicated their frequency of AI use on a five-point scale ranging from never to daily. We supplemented this quantitative data with open-ended responses about how AI influences their academic work process.\r\n\r\nStudents use generative AI for a wide range of academic tasks, with the highest adoption rates for learning support and text-processing activities. Figure 4, Panel A shows that explaining concepts is the most common use case, with 80.3 percent of AI users using it for this task. Summarizing texts follows as the second most common task (74.0 percent), followed by finding information and generating ideas (63.1 and 61.9 percent). Writing assistance tasks like proofreading and editing essays are also common, used by 54.1 and 47.3 percent of AI users, respectively. Technical applications like coding help are significant (34.4 percent), considering that many academic degrees involve no programming. Notably, while 23.5 percent of AI users report using it for writing essays, this represents a relatively low adoption rate compared to other academic uses, suggesting students may be more hesitant to use AI for primary content creation. The lowest adoption rate is for creating images at 20.4 percent, likely reflecting fewer academic use cases for this capability.\r\n\r\nAn important limitation of our survey is that it relies on self-reported usage, which may introduce measurement error. For example, students might underreport uses they perceive as academically inappropriate—such as essay writing—while overreporting those viewed as legitimate learning tools (Ling and Imas, , 2025). To assess the validity of our self-reported measures, we compare our findings with Handa et al., 2025a, who analyze actual Claude usage patterns among users with university email addresses. Several caveats apply: their data captures conversation-level interactions rather than student-level usage, and most students in our sample use ChatGPT rather than Claude (Section 3.4). Despite these limitations, the comparison provides a useful benchmark for evaluating our survey responses.\r\n\r\nReassuringly, our results are consistent with Anthropic’s data. Both studies identify explaining concepts and technical problem-solving as primary use cases. Handa et al., 2025a report that the second largest use case (33.5 percent of conversations) involves “technical explanations or solutions for academic assignments,” while we find that 80.3 percent of AI users use it for explaining concepts—a difference likely attributable to our student-level versus their conversation-level measurement. Similarly, the most common usage category in Claude involves “designing practice questions, editing essays, or summarizing academic material” (39.3 percent of conversations), aligning with our findings that 74.0 percent of AI users use it for summarizing texts and 47.3 percent for editing essays. The disciplinary patterns also converge: Handa et al., 2025a finds that computer science, natural sciences, and mathematics conversations are overrepresented, which mirrors our finding that Natural Science majors show significantly higher AI adoption rates (Figure 1). Overall, these convergent findings from self-reported survey data and actual usage logs suggest that our results capture genuine patterns of student AI engagement rather than merely reflecting social desirability in responses.\r\n\r\n4.2Automation versus Augmentation\r\nAre students using generative AI primarily to augment their learning or to automate their coursework? This distinction is crucial for understanding AI’s impacts (Autor and Thompson, , 2025)—augmentation may enhance students’ learning processes while maintaining their active engagement and critical thinking, whereas automation produces fully-formed outputs that could be submitted with minimal student input, potentially harming learning.\r\n\r\nTo examine this empirically, Figure 4, Panel B categorizes the ten measured tasks based on whether they augment or automate academic tasks. We classify tasks as augmentation when they enhance human capabilities (explaining concepts, finding information, proofreading, and editing drafts) and as automation when they directly produce outputs (writing essays, creating images, composing emails, summarizing texts, generating ideas, and coding assistance). We then calculate the percentage of AI users who employ each category at various frequencies.10\r\n\r\nStudents use generative AI for both augmentation and automation, but with markedly different frequency. While 61.2 percent of AI users report using AI for augmentation tasks, 41.9 percent report using it for automation—thus, there is a substantial 19.3 pp difference. This gap is mainly driven by occasional use (19.7 percent for augmentation versus 12.7 percent for automation) and frequent use (16.3 versus 9.0 percent). Most strikingly, students are more than twice as likely to use AI daily for augmentation (5.0 percent) compared to automation (2.5 percent). The higher frequency of augmentation use also suggests that students find augmentation uses more valuable for their day-to-day academic activities. Our findings align closely with patterns observed in actual AI usage data: Handa et al., 2025b analyze real conversation logs from Claude and find that 57 percent of workplace AI interactions involve augmentation while 43 percent involve automation.\r\n\r\nQualitative evidence from open-ended responses provides additional insights into students’ motivations for using AI (see Appendix C for additional results and validation of the open-ended measure). Students’ descriptions align closely with the augmentation-automation framework. For augmentation, many characterize AI as an “on-demand tutor,” particularly valuable when traditional resources like office hours are unavailable. Non-native English speakers frequently mention using AI for proofreading to overcome language barriers, while students from technical majors describe using it to debug code and understand error messages. For automation, time savings emerged as the dominant motivation, with 21.7 percent of open-ended responses explicitly mentioning efficiency benefits (Appendix Figure C3). Students describe turning to AI during periods of overwhelming workload or looming deadlines, using it to generate initial drafts or complete routine assignments. That students automate tasks mostly when time-pressed helps explain why automation is less common than augmentation.11\r\n\r\nOur finding that students favor augmentation over automation extends beyond Middlebury. In Appendix Figure A4, we re-analyze data from Ravšelj et al., (2025)—a multi-country survey of higher education students—to examine how this balance varies across institutional quality.12 We classify universities into quintiles based on their Times Higher Education World University Rankings. Overall, students who use AI worldwide show similar adoption rates for augmentation (64.6 percent) and automation (63.1 percent) tasks—a much smaller gap than at Middlebury (Panel A). However, this aggregate pattern masks heterogeneity by institutional quality. Students at top-quintile universities show a modest preference for augmentation over automation, with this gap narrowing monotonically down the institution quality distribution. By the bottom quintile, augmentation and automation usage are virtually identical (Panel B).\r\n\r\n4.3Heterogeneity in Augmentation versus Automation Usage\r\nUnderstanding whether augmentation and automation patterns vary across student populations may help to design targeted support policies. To assess this, we construct four measures of augmentation and automation usage. First, we create binary indicators for whether students use AI for any augmentation or automation task. Second, we calculate the proportion of tasks in each category for which students employ AI. Third, using the Likert-scale responses, we create intensity measures that capture how frequently students use AI for augmentation and automation purposes. Finally, we compute the difference between augmentation and automation variables, as a measure that directly compare students’ relative preference for augmentation versus automation. Table LABEL:tab:ai_autom_regs presents regression estimates using these measures as outcomes.\r\n\r\nThe balance between augmentation and automation varies substantially by student characteristics. Males show higher adoption of both augmentation and automation tasks, with a similar magnitude in each case—they are 6.0 pp more likely to use augmentation prompts and 5.5 pp more likely to use automation prompts compared to females (columns 1 and 4, \r\np\r\n<\r\n0.05\r\n). Black students have a substantially higher augmentation intensity than white students (0.871 points higher, equivalent to 38.1 percent of the outcome mean, \r\np\r\n<\r\n0.01\r\n), but show no statistically significant differences in automation usage. This results in Black students having the strongest preference for augmentation over automation, with a 19.5 pp higher share of augmentation tasks relative to automation (column 7, \r\np\r\n<\r\n0.01\r\n). Asian students show high usage of both kinds relative to white students. Differences across field of study effects are sizable: humanities, languages, and literature majors tend to show lower usage of both augmentation and automation tools compared to natural science majors.\r\n\r\n5Institutional Policies and AI Adoption\r\n5.1The Role of Institutional Policies in Shaping Student Behavior\r\nInstitutional policies are central to shaping the adoption and diffusion of new technologies (Acemoglu, , 2025). In the context of generative AI in higher education, understanding how policies affect student behavior is essential for guiding evidence-based decision-making. To examine this, we asked students to report their likelihood of using AI under various policy scenarios, ranging from complete prohibition to unrestricted use. This analysis is particularly relevant in light of the ongoing debate about how universities should regulate generative AI use in academic settings (Nolan, , 2023), and the widespread variation in institutional policies across colleges (Xiao et al., , 2023).\r\n\r\nInstitutional policies substantially influence students’ reported likelihood of using generative AI. Figure 5 shows that when generative AI use is unrestricted, 52.4 percent of students report being likely or extremely likely to use it. This likelihood decreases modestly when policies require citation (40.9 percent) or when no explicit policy exists (42.3 percent). However, explicit prohibition creates a dramatic shift: only 13.4 percent of students report they would be likely or extremely likely to use AI when it is banned, while 72.9 percent say they would be unlikely or extremely unlikely to do so (note that these figures include both current AI users and students who do not use generative AI for academic purposes). These results suggest that institutional policies can significantly influence generative AI usage patterns, though a small fraction of students report they would likely use AI even when explicitly prohibited.\r\n\r\nThe magnitude of policy effects in our study aligns closely with findings from other contexts. Carvajal et al., (2024) estimate that banning AI reduces usage by 37.2 pp among females and 20.6 pp among males, for an overall drop of about 28.9 pp. In our survey, a ban leads to a 37.8 pp decline in usage, with a larger decrease among females (49.6 pp) than males (40.1 pp). These parallel results underscore how institutional policies can unintentionally produce disparate effects across gender. Notably, these differential policy effects extend beyond gender. Other demographic and academic characteristics—such as race and field of study—also moderate how students respond to policy restrictions (Appendix Table LABEL:tab:ai_policy_het), underscoring that institutional policies can produce non-neutral impacts along multiple dimensions.\r\n\r\n5.2Understanding of Institutional Policies and Resources\r\nInformation gaps and inattention can significantly affect technology adoption decisions (Duflo et al., , 2011; Hanna et al., , 2014). For example, imperfect information about rules, available resources, or proper usage guidelines could lead to underadoption of beneficial technologies or inadvertent policy violations. To test for the existence of information gaps, we examine three dimensions of policy understanding. First, we asked students whether they find AI policies in their current classes clear. Second, we measured awareness of free access to premium AI tools through the college—a resource that could reduce inequality but only if students know it exists. Third, we assessed whether students know how to properly cite AI when required, a mechanical skill necessary for academic integrity. We supplemented these measures with open-ended feedback about the college’s AI policies and support services.\r\n\r\nStudent understanding of institutional policies is high, though significant gaps remain. Figure 6, Panel A shows that most students (79.1 percent) understand when and where they are allowed to use AI in their classes, but a nontrivial minority (19.2 percent) find AI policies unclear. Moreover, critical knowledge gaps persist elsewhere. Only 10.1 percent know they have free access to Microsoft Copilot through the college (Panel B), and just 32.6 percent understand how to properly cite AI use (Panel C). These gaps vary systematically: females show better policy understanding than males (81.5 versus 75.8 percent), and non-white students demonstrate higher awareness across all three dimensions compared to white students.13\r\n\r\nThe qualitative evidence from open-ended responses reinforces these information frictions and reveals implementation challenges (Appendix D). Students express frustration with vague guidelines, requesting specific examples of acceptable versus unacceptable use cases. Many advocate for formal training, noting that simply knowing policies exist differs from understanding how to effectively integrate AI into their workflow. A particularly striking theme is students’ perception of blanket prohibitions as both ineffective and unfair, as they create a prisoner’s dilemma situation where compliant students are disadvantaged relative to those who secretly violate restrictions. Many responses call for a balanced approach—permitting AI use that supports learning while restricting uses that replace it—though the boundary between the two remains contested.\r\n\r\n6Beliefs About AI’s Educational Impact and Peer Usage\r\n6.1Student Beliefs of AI’s Impact on Educational Outcomes\r\nTechnology adoption decisions are shaped by beliefs about potential returns (Foster and Rosenzweig, , 2010). Students’ perceptions of how AI affects their learning may influence whether they adopt these tools and how they integrate them into their academic workflows. To elicit these beliefs, we asked students to evaluate AI’s impact across four dimensions of their academic performance: understanding of course materials, overall learning ability, time management, and course grades. For each dimension, students rated AI’s effect on a five-point scale ranging from “significantly reduces” to “significantly improves.” These subjective assessments provide insight into the perceived value of AI tools from the student perspective, which may differ from their actual effects on learning outcomes.\r\n\r\nStudents tend to believe that generative AI is beneficial for their academic performance, though the perceived benefits vary across dimensions. Figure 7 shows that the majority of students (70.2 percent) believe that generative AI improves their understanding of course materials, and 60.1 percent report improvement in their ability to grasp concepts, retain information, or learn new skills. Similarly, 59.4 percent report that AI improves their ability to complete assignments on time. Notably, while students believe AI helps their learning and assignment completion, they are less confident about its impact on course grades—41.1 percent believe it improves their grades, while 55.4 percent report no effect and 3.5 percent report negative effects. This pattern suggests that while students perceive generative AI as improving their learning process and workflow—through better understanding, skill development, and timely completion of work—these benefits do not necessarily translate into better course grades.14\r\n\r\nBeliefs about AI’s benefits strongly predict adoption. In Figure 8, we plot the relationship between the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis) for different subgroups of students (e.g., males, white students, public-school students, etc.). Across all four measured academic dimensions, there is a strong and statistically significant relationship between perceiving positive AI effects and AI adoption. The relationship is strongest for beliefs about the ability to improve course grades (Panel D): a 10-pp increase in the belief that AI improves grades is associated with a 5.6 pp increase in AI adoption (\r\np\r\n<\r\n0.01\r\n). Similar positive relationships exist for beliefs about learning ability (4.3 pp, \r\np\r\n<\r\n0.05\r\n), understanding of course materials (4.5 pp, \r\np\r\n<\r\n0.01\r\n), and timely assignment completion (5.3 pp, \r\np\r\n<\r\n0.05\r\n). These findings suggest that student beliefs about AI’s academic benefits—irrespective of the actual benefits—may play a crucial role in shaping adoption decisions.\r\n\r\nThese positive beliefs align with findings from other contexts. Ravšelj et al., (2025) report that the majority of students in their sample believe that ChatGPT improves their general knowledge (68.8 percent) and specific knowledge (62.7 percent)—remarkably similar to our finding that most students believe AI improves understanding of course materials (68.5 percent) and learning ability (57.6 percent). Similarly, 57.4 percent of students in Ravšelj et al., (2025)’s sample believe that ChatGPT helps meet assignment deadlines, while 59.4 percent in our data report AI improves timely assignment completion. Stöhr et al., (2024) provide complementary evidence from Swedish universities, where 47.7 percent of students believe that AI makes them more effective learners, yet only 17.3 percent believe these tools improve their grades—mirroring our finding that perceived learning benefits from AI use exceed perceived effects on course performance.\r\n\r\n6.2Student Beliefs About Peer Use of Generative AI\r\nStudents’ beliefs about their peers’ AI usage may influence their own adoption through multiple channels, including social norms (e.g., Giaccherini et al., , 2019), social learning (e.g., Foster and Rosenzweig, , 1995; Beaman et al., , 2021), peer effects (e.g., Bailey et al., , 2022), and competitive pressure to avoid falling behind (e.g., Goehring et al., , 2024). To measure these beliefs, we asked students to estimate what fraction of their peers use generative AI for different purposes and would use under different policy environments. Figure 9 presents the distribution of these beliefs. Panels A-C show students’ estimates of peer AI usage for schoolwork, leisure, and any purpose respectively. Panels D-F display students’ beliefs about AI usage under three policy environments: when classes have no explicit AI policy, when classes allow AI use, and when classes prohibit AI use.\r\n\r\nStudents systematically underestimate their peers’ AI usage. On average, they believe 65.2 percent of peers regularly use generative AI for schoolwork (Panel A), while our survey reveals an actual usage rate of 82.5 percent—a 17.3 pp gap. This underestimation appears consistent across educational contexts: Stöhr et al., (2024) find that only 38.7 percent of Swedish students believe AI chatbot use is common among peers, despite 63 percent actual usage. When we examine beliefs about policy-contingent behavior, students estimate that 62.7 percent of peers use AI when no explicit policy exists (Panel D), rising to 72.2 percent under permissive policies (Panel E) and falling to 43.5 percent under prohibition (Panel F). These beliefs align directionally with self-reported intentions, though with notable magnitude differences (Appendix Figure A5). For instance, while students believe 43.5 percent of their peers use AI in classes that prohibit it, only 28.4 percent report they themselves would be likely to do so.15\r\n\r\nNotably, students’ perceptions of peer AI adoption are closely linked to their own adoption behavior. Appendix Figure A6 shows a strong positive association between beliefs about peer usage and actual usage rates across student groups. This behavior is consistent with several psychological mechanisms, including the “false consensus effect” (Ross et al., , 1977), selection neglect (Jehiel, , 2018), or interpersonal projection bias (Bushong and Gagnon-Bartsch, , 2024). Still, virtually all groups systematically underestimate peer usage—actual usage rates exceed believed usage for every demographic and academic group we examine. This figure also shows that underestimation varies substantially across student characteristics: males use AI at 88.7 percent but believe only 66.6 percent of peers do so (22.1 pp gap), while females show a smaller gap (77.6 percent actual versus 65.1 percent believed, 13.3 pp gap).\r\n\r\nTaken together, students’ systematic underestimation of peer AI usage coupled with the strong relationship between beliefs and adoption suggests that misperceptions about social norms may shape technology diffusion. If AI enhances learning, then correcting these misperceptions through information provision could accelerate beneficial adoption; conversely, if AI undermines skill development, then students’ underestimation of peer usage may serve as an unintentional safeguard against harmful overadoption.\r\n\r\n7Discussion\r\nThis paper presents the first systematic evidence on generative AI adoption at a highly selective U.S. college. Using novel survey data, we document exceptionally rapid and widespread adoption, substantial shifts in the educational production function through augmentation and automation, and the significant roles of students’ beliefs and institutional policies in shaping AI use.\r\n\r\nOur results offer three implications for institutional policy and ongoing debates about AI in education. First, we identify low-cost opportunities to improve institutional policy effectiveness through targeted information provision. The significant gaps we document in students’ understanding of institutional AI policies, citation practices, and available AI resources suggest that simple interventions—such as clear guidelines, illustrative examples of acceptable uses, and AI literacy programs—can reduce unintentional academic integrity violations and support beneficial AI integration. Qualitative feedback strongly indicates student demand for more explicit guidance on responsible AI use.\r\n\r\nSecond, our evidence challenges alarmist narratives that conflate widespread AI adoption with universal academic dishonesty based on anecdotal accounts.16 Although AI use is indeed near-universal, we find clear evidence that students primarily employ AI as a tool for strategic task management: to enhance learning (augmentation) and selectively automate tasks when facing high time opportunity cost—not solely to circumvent academic effort. This distinction matters: by normalizing academic dishonesty as inevitable and universal, these narratives may shift social norms and encourage students who would otherwise use AI responsibly to engage in prohibited behaviors, believing “everyone else is doing it.”\r\n\r\nThird, our findings caution against policy extremes of either blanket prohibition or unrestricted AI use (Merchant, , 2024; McDonald et al., , 2025). Blanket prohibitions disproportionately harm students who benefit most from AI’s augmentation functions—particularly lower-achieving students—while also creating uneven compliance, placing conscientious students at a disadvantage relative to rule-breakers. Conversely, unrestricted AI use based solely on revealed preference arguments ignores important market failures in educational settings. Students often hold overly optimistic beliefs about AI’s learning benefits despite mixed empirical evidence, potentially leading to unintended negative learning outcomes. Most concerningly, permissive policies risk creating competitive dynamics where students feel compelled to adopt AI not for its learning benefits but simply to avoid falling behind in an educational “arms race” (Goehring et al., , 2024).\r\n\r\nFigures and Tables\r\nFigure 1:The Adoption of Generative AI among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the fraction of students who report using AI during the academic semester, categorized by demographic characteristics, high school type, academic cohort, GPA, and field of study. Usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily).\r\n\r\nThe category “All students” provides the baseline usage rate for the full sample. Gender categories are based on self-identification, with non-binary responses excluded due to a small sample size. “Private HS” refers to students who attended private high schools, while “Public HS” includes public institutions. “Cohort” denotes the student’s academic year, ranging from first-year (“Freshman”) to fourth-year and beyond (“Senior”). GPA categories (“GPA \r\n>\r\n p50” and “GPA \r\n<\r\n p50”) split students into groups above or below the median first-year GPA, as self-reported on a 4.0 scale. See Appendix B.1 for the classification of majors into fields of study.\r\n\r\nFigure 2:The Evolution of Generative AI Adoption among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the cumulative percent of students who reported using generative AI tools for academic purposes over time. The data is based on retrospective self-reports collected in our December 2024 survey, where students were asked “When did you first start using any form of Generative AI for academic purposes?” Response options ranged from “Before Spring 2023” to “This semester (Fall 2024).” The \r\nx\r\n-axis represents academic semesters, while the \r\ny\r\naxis represents the cumulative adoption rate. Vertical lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure 3:Adoption of Generative AI Models Among College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the adoption rates of various AI models as of Fall 2024. The horizontal axis shows the percent of students who reported using each tool, and the vertical axis lists the tools in descending order of adoption rates.\r\n\r\nFigure 4:Academic Uses of Generative AI\r\nPanel A. Across Common Academic Tasks\r\nRefer to caption\r\nPanel B. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of AI students who use generative AI for different academic tasks. For each task, usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). The number at the end of each bar represents the total percent of students who use AI for that purpose at any frequency. Tasks are ordered by total usage, from highest to lowest. Results are based on responses to the question: “For academic purposes, which of the following tasks do you typically use generative AI for?” Sample includes all students who reported using AI during the academic semester.\r\n\r\nFigure 5:Student Reported Likelihood of Using AI under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of students who report different likelihoods of using AI under various policy scenarios. For each policy, responses are categorized on a five-point scale from “Extremely unlikely to use AI” to “Extremely likely to use AI.” The sample includes all survey respondents. The question asked was: “How likely are you to use generative AI in a class with each of the following AI policies?”\r\n\r\nFigure 6:Understanding of Generative AI Policies and Resources\r\nPanel A. Understanding of AI Policies\r\nRefer to caption\r\nPanel B. Awareness of Copilot Access\r\nRefer to caption\r\nPanel C. Knowledge of Citation Requirements\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows students’ understanding of institutional AI policies and resources. Panel A displays the percent of students who report understanding AI policies in their classes, those who report having no explicit policy, and those who find policies unclear, broken down by demographic characteristics. Panel B shows the percent of students who are aware of their free access to Microsoft Copilot through Middlebury College. Panel C presents the percent of students who report knowing how to properly cite AI use in their academic work when required. For Panels B and C, horizontal lines represent 95 percent confidence intervals. Sample includes all survey respondents.\r\n\r\nFigure 7:Student Beliefs about the Impact of AI on their Academic Performance\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of Middlebury students who believe that AI improves, reduces, or has no effect on different aspects of their academic experience. For each outcome, responses are categorized into three groups: “Improves” combines “significantly improves” and “somewhat improves” responses, “Reduces” combines “significantly reduces” and “somewhat reduces” responses, and “No effect” represents neutral responses. Sample includes all students who report using AI during the academic semester. “Don’t know” responses are excluded.\r\n\r\nFigure 8:Relationship Between AI Adoption and Beliefs About AI’s Academic Benefits\r\nPanel A. Learning ability (e.g., ability to grasp concepts, learn new skills, etc.)\r\nRefer to caption\r\nPanel B. Understanding of\r\ncourse materials\r\nRefer to caption\r\nPanel C. Ability to complete\r\nassignments on time\r\nRefer to caption\r\nPanel D. Course grades\r\n \r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the relationship between AI adoption rates and beliefs that AI improves various academic outcomes across different student groups. Each panel plots the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis). Points represent different student groups categorized by demographics (circles), academic characteristics (triangles), and field of study (squares). The dashed line shows the linear fit across all groups. Groups with fewer than 10 students are excluded.\r\n\r\nFigure 9:Student Beliefs about Generative AI Usage at Middlebury College\r\nPanel A. For Schoolwork\r\nRefer to caption\r\nPanel B. For Leisure\r\nRefer to caption\r\nPanel C. For Any Purpose\r\nRefer to caption\r\nPanel D. Classes with No Policy\r\nRefer to caption\r\nPanel E. Classes that Allow AI\r\nRefer to caption\r\nPanel F. Classes that Disallow AI\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the distribution of students’ beliefs about generative AI usage among their peers at Middlebury College. Panels A-C display students’ estimates of the percent of their peers who regularly use AI for schoolwork, leisure activities, and any purpose, respectively. Panels D-F show students’ beliefs about AI usage in classes with different AI policies: those without an explicit policy (Panel D), those that allow AI use (Panel E), and those that prohibit AI use (Panel F). Each panel shows a histogram with bins of width ten percentage points (e.g., responses between 1-10 fall in the 10 bin, 11-20 in the 20 bin, etc.). The red dashed line indicates the mean response. Sample excludes respondents with missing values or who selected the default response for all six categories (which equals zero).\r\n\r\nTable 1:Summary Statistics of Survey Participants\r\nSurvey Sample\tAdmin records\r\nUnweighted\tWeighted\t\r\n(1)\t(2)\t(3)\r\nPanel A. Demographics\r\nMale\t0.446\t0.433\t0.463\r\nFemale\t0.508\t0.516\t0.533\r\nWhite\t0.618\t0.603\t0.538\r\nBlack\t0.036\t0.033\t0.052\r\nHispanic\t0.099\t0.106\t0.124\r\nAsian\t0.155\t0.162\t0.073\r\nPrivate high school\t0.420\t0.399\t–\r\nPublic high school\t0.543\t0.556\t–\r\nPanel B. Academic Characteristics    GPA 3.740 3.736 3.670 Hours spent on academics per week 17.899 17.889 – Freshman 0.311 0.355 0.255 Sophomore 0.273 0.272 0.257 Junior 0.202 0.179 0.201 Senior 0.213 0.194 0.287   Panel C. Field of Study    Arts 0.011 0.021 0.023 Humanities 0.052 0.068 0.073 Languages 0.021 0.020 0.025 Literature 0.035 0.025 0.025 Natural Sciences 0.218 0.249 0.244 Social Sciences 0.353 0.247 0.243 Has not declared major 0.311 0.371 0.364\r\n\r\nN (# degrees) 43 43 49 N (# students) 634 2,760 2,760\r\n\r\n\\justify\r\nNotes: This table presents summary statistics from our survey of college students. Panel A reports demographic characteristics, including the proportion of participants identifying as male, female, white, Black, Hispanic, Asian, or who attended a private or public high school. Panel B provides academic characteristics, such as GPA (only available for non-freshmen), average weekly hours spent on academics, and academic year distribution (Freshman, Sophomore, Junior, and Senior). Note that in column 1–2, GPA refers to self-reported first-year GPA while in column 3 it is the overall GPA during Spring 2024. Panel C summarizes the distribution of participants across different fields of study. Major groups are mutually exclusive.\r\n\r\n0.8250.5890.3680.106 R\r\n−\r\nsquared0.0770.0890.1160.058 N (Students)616616616616 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI usage frequency threshold and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column uses a different threshold for AI usage frequency, categorized as: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). Column 1 defines usage as any nonzero frequency; column 2 includes at least occasional use; column 3 includes frequent or higher use; and column 4 captures only very frequent use.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.0890.1990.3570.5510.801 R\r\n−\r\nsquared0.0450.0610.0880.0800.069 N (Students)633633633633633 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI adoption date and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column presents results for a different threshold of AI adoption. Column 1 shows the probability of adopting AI before Spring 2023; column 2 by Spring 2023; column 3 by Fall 2023; column 4 by Spring 2024; and column 5 by Fall 2024. The dependent variable in each regression is a binary indicator equal to one if the student had adopted AI by the specified time period.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.9120.6122.2850.9130.4191.8010.1930.484 R\r\n−\r\nsquared0.0360.0730.1260.0530.1170.1290.0420.063 N (Students)515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and their use of generative AI for academic tasks. In columns 1 and 4, the outcome is a dummy that equals one if a student reports using AI with any frequency for at least one augmentation or automation task, respectively. In columns 2 and 5, the outcome is the share of tasks within each category for which the student reports any use. In columns 3 and 6, the outcome is a continuous measure capturing average usage frequency for each task category, based on raw Likert-style responses. In columns 7 and 8, the outcome is the difference in average task share and usage intensity between augmentation and automation, respectively. Regressions are weighted and report robust standard errors clustered at the student level. ∗∗∗ \r\np\r\n<\r\n0.01\r\n, ∗∗ \r\np\r\n<\r\n0.05\r\n, ∗ \r\np\r\n<\r\n0.10\r\n.\r\n\r\nReferences\r\nAcemoglu, (2025)\r\nAcemoglu, D. (2025).Nobel lecture: Institutions, technology, and prosperity.American Economic Review, 115(6):1709–1748.\r\nAngrist et al., (2009)\r\nAngrist, J., Lang, D., and Oreopoulos, P. (2009).Incentives and services for college achievement: Evidence from a randomized trial.American Economic Journal: Applied Economics, 1(1):136–163.\r\nAutor and Thompson, (2025)\r\nAutor, D. and Thompson, N. (2025).Expertise.\r\nBailey et al., (2022)\r\nBailey, M., Johnston, D., Kuchler, T., Stroebel, J., and Wong, A. (2022).Peer effects in product adoption.American Economic Journal: Applied Economics, 14(3):488–526.\r\nBastani et al., (2025)\r\nBastani, H., Bastani, O., Sungu, A., Ge, H., Kabakcı, Ö., and Mariman, R. (2025).Generative AI without guardrails can harm learning: Evidence from high school mathematics.Proceedings of the National Academy of Sciences, 122(26):e2422633122.\r\nBeaman et al., (2021)\r\nBeaman, L., BenYishay, A., Magruder, J., and Mobarak, A. M. (2021).Can network theory-based targeting increase technology adoption?American Economic Review, 111(6):1918–1943.\r\nBick et al., (2025)\r\nBick, A., Blandin, A., and Deming, D. J. (2025).The rapid adoption of generative AI.Technical report, National Bureau of Economic Research.\r\nBonney et al., (2024)\r\nBonney, K., Breaux, C., Buffington, C., Dinlersoz, E., Foster, L. S., Goldschlag, N., Haltiwanger, J. C., Kroff, Z., and Savage, K. (2024).Tracking Firm Use of AI in Real Time: A Snapshot from the Business Trends and Outlook Survey.\r\nBonsu and Baffour-Koduah, (2023)\r\nBonsu, E. M. and Baffour-Koduah, D. (2023).From the consumers’ side: Determining students’ perception and intention to use chatgpt in ghanaian higher education.Journal of Education, Society & Multiculturalism, 4(1):1–29.\r\nBresnahan and Trajtenberg, (1995)\r\nBresnahan, T. F. and Trajtenberg, M. (1995).General purpose technologies ‘engines of growth’?Journal of econometrics, 65(1):83–108.\r\nBrynjolfsson and Hitt, (2000)\r\nBrynjolfsson, E. and Hitt, L. M. (2000).Beyond computation: Information technology, organizational transformation and business performance.Journal of Economic Perspectives, 14(4):23–48.\r\nBrynjolfsson et al., (2025)\r\nBrynjolfsson, E., Li, D., and Raymond, L. (2025).Generative AI at Work.The Quarterly Journal of Economics, 140(2):889–942.\r\nBushong and Gagnon-Bartsch, (2024)\r\nBushong, B. and Gagnon-Bartsch, T. (2024).Failures in forecasting: An experiment on interpersonal projection bias.Management Science, 70(12):8735–8752.\r\nCarvajal et al., (2024)\r\nCarvajal, D., Franco, C., and Isaksson, S. (2024).Will artificial intelligence get in the way of achieving gender equality?Discussion Paper 03, NHH Norwegian School of Economics.\r\n(15)\r\nComin, D. and Hobijn, B. (2010a).An exploration of technology diffusion.American Economic Review, 100(5):2031–2059.\r\n(16)\r\nComin, D. and Hobijn, B. (2010b).Technology Diffusion and Postwar Growth, volume 25, pages 209–246.University of Chicago Press.\r\nCui et al., (2024)\r\nCui, Z. K., Demirer, M., Jaffe, S., Musolff, L., Peng, S., and Salz, T. (2024).The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers.\r\nDavid, (1990)\r\nDavid, P. A. (1990).The dynamo and the computer: an historical perspective on the modern productivity paradox.The American economic review, 80(2):355–361.\r\nDell’Acqua et al., (2023)\r\nDell’Acqua, F., McFowland, E., Mollick, E. R., Lifshitz-Assaf, H., Kellogg, K., Rajendran, S., Krayer, L., Candelon, F., and Lakhani, K. R. (2023).Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality.\r\nDuflo et al., (2011)\r\nDuflo, E., Kremer, M., and Robinson, J. (2011).Nudging Farmers to Use Fertilizer: Theory and Experimental Evidence from Kenya.American Economic Review, 101(6):2350–2390.\r\nEloundou et al., (2024)\r\nEloundou, T., Manning, S., Mishkin, P., and Rock, D. (2024).GPTs are GPTs: Labor market impact potential of LLMs.Science, 384(6702):1306–1308.\r\nFairlie et al., (2014)\r\nFairlie, R. W., Hoffmann, F., and Oreopoulos, P. (2014).A community college instructor like me: Race and ethnicity interactions in the classroom.American Economic Review, 104(8):2567–2591.\r\nFelten et al., (2021)\r\nFelten, E. W., Raj, M., and Seamans, R. (2021).Occupational, Industry, and Geographic Exposure to Artificial Intelligence: A Novel Dataset and Its Potential Uses.\r\nFelten et al., (2023)\r\nFelten, E. W., Raj, M., and Seamans, R. (2023).How will Language Modelers like ChatGPT Affect Occupations and Industries?\r\nFoster and Rosenzweig, (1995)\r\nFoster, A. D. and Rosenzweig, M. R. (1995).Learning by doing and learning from others: Human capital and technical change in agriculture.Journal of political Economy, 103(6):1176–1209.\r\nFoster and Rosenzweig, (2010)\r\nFoster, A. D. and Rosenzweig, M. R. (2010).Microeconomics of technology adoption.Annu. Rev. Econ., 2(1):395–424.\r\nGallup, (2024)\r\nGallup (2024).AI in the Workplace: Answering 3 Big Questions.\r\nGiaccherini et al., (2019)\r\nGiaccherini, M., Herberich, D. H., Jimenez-Gomez, D., List, J. A., Ponti, G., and Price, M. K. (2019).The behavioralist goes door-to-door: Understanding household technological diffusion using a theory-driven natural field experiment.Technical report, National Bureau of Economic Research.\r\nGoehring et al., (2024)\r\nGoehring, G., Mezzanotti, F., and Ravid, S. A. A. (2024).Technology adoption and career concerns: Evidence from the adoption of digital technology in motion pictures.Technical report, National Bureau of Economic Research.\r\nGriliches, (1957)\r\nGriliches, Z. (1957).Hybrid corn: An exploration in the economics of technological change.Econometrica, 25(4):501–522.\r\nHall and Khan, (2003)\r\nHall, B. H. and Khan, B. (2003).Adoption of new technology.\r\n(32)\r\nHanda, K., Bent, D., Tamkin, A., McCain, M., Durmus, E., Stern, M., Schiraldi, M., Huang, S., Ritchie, S., Syverud, S., Jagadish, K., Vo, M., Bell, M., and Ganguli, D. (2025a).Anthropic education report: How university students use claude.\r\n(33)\r\nHanda, K., Tamkin, A., McCain, M., Huang, S., Durmus, E., Heck, S., Mueller, J., Hong, J., Ritchie, S., Belonax, T., et al. (2025b).Which economic tasks are performed with ai? evidence from millions of claude conversations.arXiv preprint arXiv:2503.04761.\r\nHanna et al., (2014)\r\nHanna, R., Mullainathan, S., and Schwartzstein, J. (2014).Learning Through Noticing: Theory and Evidence from a Field Experiment.The Quarterly Journal of Economics, 129(3):1311–1353.\r\nHartley et al., (2025)\r\nHartley, J., Jolevski, F., Melo, V., and Moore, B. (2025).The Labor Market Effects of Generative Artificial Intelligence.\r\nHumlum and Vestergaard, (2025)\r\nHumlum, A. and Vestergaard, E. (2025).Large Language Models, Small Labor Market Effects.\r\nHutto and Gilbert, (2014)\r\nHutto, C. and Gilbert, E. (2014).Vader: A parsimonious rule-based model for sentiment analysis of social media text.In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216–225.\r\nJehiel, (2018)\r\nJehiel, P. (2018).Investment strategy and selection bias: An equilibrium perspective on overoptimism.American Economic Review, 108(6):1582–1597.\r\nKelly et al., (2023)\r\nKelly, A., Sullivan, M., and Strampel, K. (2023).Generative artificial intelligence: University student awareness, experience, and confidence in use across disciplines.Journal of University Teaching and Learning Practice, 20(6):1–16.\r\nKharazian, (2025)\r\nKharazian, A. (2025).Ramp ai index: Monthly measurement of ai adoption by american businesses.Technical report, Ramp Economics Lab.\r\nLehmann et al., (2024)\r\nLehmann, M., Cornelius, P. B., and Sting, F. J. (2024).AI Meets the Classroom: When Does ChatGPT Harm Learning?\r\nLing and Imas, (2025)\r\nLing, Y. and Imas, A. (2025).Underreporting of AI use: The role of social desirability bias.Available at SSRN.\r\nLiu and Wang, (2024)\r\nLiu, Y. and Wang, H. (2024).Who on earth is using generative ai?Policy Research Working Paper WPS10870, World Bank Group, Washington, D.C.\r\nMcClain, (2024)\r\nMcClain, C. (2024).Americans’ use of ChatGPT is ticking up, but few trust its election information.\r\nMcDonald et al., (2025)\r\nMcDonald, N., Johri, A., Ali, A., and Collier, A. H. (2025).Generative artificial intelligence in higher education: Evidence from an analysis of institutional policies and guidelines.Computers in Human Behavior: Artificial Humans, 3:100121.\r\nMcElheran et al., (2023)\r\nMcElheran, K., Li, J. F., Brynjolfsson, E., Kroff, Z., Dinlersoz, E., Foster, L. S., and Zolas, N. (2023).AI Adoption in America: Who, What, and Where.\r\nMerchant, (2024)\r\nMerchant, B. (2024).The new luddites aren’t backing down.The Atlantic.Accessed: 2025-06-16.\r\nNam, (2023)\r\nNam, J. (2023).56% of college students have used ai on assignments or exams.Technical report.Edited by Lyss Welding. Accessed June 11, 2025.\r\nNolan, (2023)\r\nNolan, B. (2023).Here are the schools and colleges that have banned chatgpt.Business Insider.Accessed: 2025-01-27.\r\nNoy and Zhang, (2023)\r\nNoy, S. and Zhang, W. (2023).Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence.\r\nOtis et al., (2024)\r\nOtis, N. G., Delecourt, S., Cranney, K., and Koning, R. (2024).Global Evidence on Gender Gaps and Generative AI.Harvard Business School.\r\nPeng et al., (2023)\r\nPeng, S., Kalliamvakou, E., Cihon, P., and Demirer, M. (2023).The Impact of AI on Developer Productivity: Evidence from GitHub Copilot.\r\nRavšelj et al., (2025)\r\nRavšelj, D., Keržič, D., Tomaževič, N., Umek, L., Brezovar, N., A. Iahad, N., Abdulla, A. A., Akopyan, A., Aldana Segura, M. W., AlHumaid, J., Allam, M. F., Alló, M., Andoh, R. P. K., Andronic, O., Arthur, Y. D., Aydın, F., Badran, A., Balbontín-Alvarado, R., Ben Saad, H., Bencsik, A., Benning, I., Besimi, A., Bezerra, D. D. S., Buizza, C., Burro, R., Bwalya, A., Cachero, C., Castillo-Briceno, P., Castro, H., Chai, C. S., Charalambous, C., Chiu, T. K. F., Clipa, O., Colombari, R., Corral Escobedo, L. J. H., Costa, E., Cre\\textcommabelowtulescu, R. G., Crispino, M., Cucari, N., Dalton, F., Demir Kaya, M., Dumić-Čule, I., Dwidienawati, D., Ebardo, R., Egbenya, D. L., Faris, M. E., Fečko, M., Ferrinho, P., Florea, A., Fong, C. Y., Francis, Z., Ghilardi, A., González-Fernández, B., Hau, D., Hossain, M. S., Hug, T., Inasius, F., Ismail, M. J., Jahić, H., Jessa, M. O., Kapanadze, M., Kar, S. K., Kateeb, E. T., Kaya, F., Khadri, H. O., Kikuchi, M., Kobets, V. M., Kostova, K. M., Krasmane, E., Lau, J., Law, W. H. C., Lazăr, F., Lazović-Pita, L., Lee, V. W. Y., Li, J., López-Aguilar, D. V., Luca, A., Luciano, R. G., Machin-Mastromatteo, J. D., Madi, M., Manguele, A. L., Manrique, R. F., Mapulanga, T., Marimon, F., Marinova, G. I., Mas-Machuca, M., Mejía-Rodríguez, O., Meletiou-Mavrotheris, M., Méndez-Prado, S. M., Meza-Cano, J. M., Mirķe, E., Mishra, A., Mital, O., Mollica, C., Morariu, D. I., Mospan, N., Mukuka, A., Navarro Jiménez, S. G., Nikaj, I., Nisheva, M. M., Nisiforou, E., Njiku, J., Nomnian, S., Nuredini-Mehmedi, L., Nyamekye, E., Obadić, A., Okela, A. H., Olenik-Shemesh, D., Ostoj, I., Peralta-Rizzo, K. J., Peštek, A., Pilav-Velić, A., Pires, D. R. M., Rabin, E., Raccanello, D., Ramie, A., Rashid, M. M. U., Reuter, R. A. P., Reyes, V., Rodrigues, A. S., Rodway, P., Ručinská, S., Sadzaglishvili, S., Salem, A. A. M. S., Savić, G., Schepman, A., Shahpo, S. M., Snouber, A., Soler, E., Sonyel, B., Stefanova, E., Stone, A., Strzelecki, A., Tanaka, T., Tapia Cortes, C., Teira-Fachado, A., Tilga, H., Titko, J., Tolmach, M., Turmudi, D., Varela-Candamio, L., Vekiri, I., Vicentini, G., Woyo, E., Yorulmaz, Ö., Yunus, S. A. S., Zamfir, A.-M., Zhou, M., and Aristovnik, A. (2025).Higher education students’ perceptions of ChatGPT: A global study of early reactions.PLOS ONE, 20(2):e0315011.\r\nRogers, (1962)\r\nRogers, E. M. (1962).Diffusion of Innovations.Free Press, New York.\r\nRoss et al., (1977)\r\nRoss, L., Greene, D., and House, P. (1977).The “false consensus effect”: An egocentric bias in social perception and attribution processes.Journal of experimental social psychology, 13(3):279–301.\r\nSacerdote, (2001)\r\nSacerdote, B. (2001).Peer Effects with Random Assignment: Results for Dartmouth Roommates.The Quarterly Journal of Economics, 116(2):681–704.\r\nStinebrickner and Stinebrickner, (2008)\r\nStinebrickner, R. and Stinebrickner, T. R. (2008).The causal effect of studying on academic performance.The BE Journal of Economic Analysis & Policy, 8(1).\r\nStöhr et al., (2024)\r\nStöhr, C., Ou, A. W., and Malmström, H. (2024).Perceptions and usage of AI chatbots among students in higher education across genders, academic levels and fields of study.Computers and Education: Artificial Intelligence, 7:100259.\r\nStokey, (2021)\r\nStokey, N. L. (2021).Technology diffusion.Review of Economic Dynamics, 42:15–36.\r\nWorld Bank, (2016)\r\nWorld Bank (2016).World development report 2016: Digital dividends.World Bank Publications.\r\nWu et al., (2022)\r\nWu, M.-J., Zhao, K., and Fils-Aime, F. (2022).Response rates of online surveys in published research: A meta-analysis.Computers in Human Behavior Reports, 7.\r\nXiao et al., (2023)\r\nXiao, P., Chen, Y., and Bao, W. (2023).Waiting, banning, and embracing: An empirical analysis of adapting policies for generative ai in higher education.arXiv preprint arXiv:2305.18617.\r\nAppendix\r\n\r\nAppendix AAppendix Figures and Tables\r\nFigure A1:Generative AI Usage Survey Design Overview\r\nKey\t\r\nSurvey Structure\r\nSurvey Sections\r\nUsage Data Collection\r\nBeliefs & Perceptions\r\nAI Usage Survey at Middlebury College\r\nDecember 2024\r\nParticipant Recruitment\r\nCampus-wide email invitations, framed as general technology use survey\r\nParticipation Incentives\r\nAmazon gift card lottery ranging from $50-$500\r\nSection 1: Demographics & Academic Information\r\nStudent Characteristics\r\nGender, race/ethnicity, high school type\r\nAcademic Profile\r\nYear, major, self-reported GPA, study hours\r\nSection 2: generative AI Usage Patterns\r\nAdoption Metrics\r\nFrequency, timing of first use, specific AI models used\r\nAcademic Applications\r\nTasks performed with AI, payment for premium services\r\nSection 3: Perceptions & Institutional Policies\r\nImpact Perceptions\r\nEffects on learning, grades, time management\r\nPolicy Responses\r\nUsage likelihood under different policies\r\nPeer Usage Beliefs\r\nEstimates of AI adoption among peers for schoolwork and leisure\r\nOpen-Ended Response Collection\r\nMotivations for AI use, policy feedback, suggestions for improvement\r\nNote: This figure illustrates the structure of the AI usage survey conducted at Middlebury College in December 2024. The survey collected information across three main sections: (1) demographic and academic background, (2) patterns of generative AI usage including adoption timing, frequency, and specific applications, and (3) perceptions of AI’s impact on learning and responses to institutional policies.\r\n\r\nFigure A2:Cumulative Generative AI Use by Student Characteristic\r\nPanel A. By Gender\r\nRefer to caption\r\nPanel B. By Race\r\nRefer to caption\r\nPanel C. By High School Type\r\nRefer to caption\r\nPanel D. By Freshman Year GPA\r\nRefer to caption\r\nPanel E. By Cohort\r\nRefer to caption\r\nPanel F. By Field of Study\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents cumulative AI use based on different student characteristics. Each panel displays the cumulative distribution of AI use based on a specific characteristic: gender, race, school type, first-year GPA, cohort, or field of study. The cumulative percent of students is plotted against usage categories. The legends and colors correspond to subgroups within each demographic variable.\r\n\r\nFigure A3:Percent of Students Who Pay for Generative AI Tools\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of AI users who pay for AI tools (through any platform) across different demographic groups. Horizontal lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure A4:Academic Uses of ChatGPT: Evidence from Global Survey\r\nPanel A. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\nPanel B. Across the College Quality Distribution\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of students who use ChatGPT for different academic tasks based on data from Ravšelj et al., (2025). Panel A displays usage patterns across tasks categorized as augmenting (proofreading, translating, study assistance, research assistance) versus automating (academic writing, professional writing, creative writing, brainstorming, summarizing, calculating, coding assistance, personal assistance) student effort. Panel B shows usage patterns across university quality quintiles based on World University Rankings, with universities ranked in the top 20% (top quintile) showing slightly higher rates of augmentation relative to automation compared to bottom quintile institutions. The analysis includes universities with at least 30 student responses and excludes observations with missing usage data.\r\n\r\nFigure A5:Student Beliefs about AI Usage versus Actual Usage under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure compares students’ beliefs about AI usage with actual usage rates under different AI policies. For each policy type, blue bars show the percent of students who report being “Neutral,” “Likely,” or “Very Likely” to use AI, while red bars show students’ mean beliefs about what percent of their peers use AI in classes with that policy. The sample includes all survey respondents for both actual usage and beliefs measures. “AI Use is Unrestricted” refers to classes with no restrictions on AI use, “No Explicit AI Policy” refers to classes without a stated policy, and “AI Use is Prohibited” refers to classes that ban AI use entirely.\r\n\r\nFigure A6:Relationship Between Beliefs About AI Usage and Actual AI Usage\r\nRefer to caption\\justify\r\nNotes: This figure shows the relationship between students’ beliefs about AI usage among their peers and actual AI usage rates across different demographic groups. Each point represents a different group of students (by demographics, academic characteristics, or field of study). The \r\ny\r\n-axis shows the percent of students in each group who report using AI for academic purposes. The \r\nx\r\n-axis shows the mean belief within each group about what percent of Middlebury students use AI. The dashed line shows the linear fit. Sample includes all survey respondents with at least ten observations per group. Students’ beliefs about AI usage are positively correlated with actual usage patterns, suggesting that students have relatively accurate perceptions of AI adoption among their peers.\r\n\r\nGPA > p50 80.3 26.3 17.4 23.2 13.4 GPA < p50 87.1 18.7 26.3 32.2 9.9 Freshman 81.3 24.3 24.1 24.7 8.1 Sophomore 84.4 25.6 22.6 23.7 12.4 Junior 86.2 21.6 23.1 28.6 12.8 Senior 78.7 21.0 17.4 29.9 10.4\r\n\r\nPanel C. Field of Study    Arts 73.3 21.0 0.0 52.3 0.0 Humanities 74.8 39.3 21.8 12.1 1.7 Languages 57.4 27.0 30.3 0.0 0.0 Literature 48.6 10.0 33.7 5.0 0.0 Natural Sciences 91.1 22.0 21.9 33.8 13.3 Social Sciences 84.6 17.9 20.4 29.1 17.2\r\n\r\n\\justify\r\nNotes: This table presents the percent of students in each demographic group who report using AI at different frequencies during the academic semester. Each cell shows the percent of students within that group. Column 1 reports the total percent who use AI at any frequency. Columns 2 to 5 represent increasing usage frequencies: rarely (1–2 times per semester), occasionally (monthly), frequently (weekly), and very frequently (multiple times per week). Panel A reports percentages by demographic characteristics. Panel B shows percentages by academic characteristics. Panel C presents percentages by field of study.\r\n\r\n0.9730.1350.0770.1100.114 R\r\n−\r\nsquared0.0120.0510.0270.0240.138 N (Students)516516516516516 \\justifyNotes: This table assesses the relationship between AI model adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI model usage (columns 1-4) or payment for AI services (column 5), and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division.\r\n\r\nEach column presents results for a different model or payment outcome. Column 1 shows usage of OpenAI’s ChatGPT, column 2 Google Gemini, column 3 Microsoft Copilot, column 4 any other AI model, and column 5 whether the student pays for any generative AI service.\r\n\r\nThe omitted categories are Natural Sciences for academic division, white students for race/ethnicity, and freshmen for cohort. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.7510.6760.6610.2710.479 R\r\n−\r\nsquared0.0870.1130.0760.0680.041 N (Students)599599599599599 \\justifyNotes: This table examines how student characteristics relate to self-reported likelihood of using generative AI under different policy scenarios. Each column presents results for different policy scenarios. In columns 1-4, the dependent variable equals one if the student reports being“neutral,” “likely” or “extremely likely” to use AI under the specified policy, and zero if they report being “unlikely,” or “extremely unlikely.” Column 5 represents the impact of moving from unrestricted use to complete prohibition.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.5760.6850.3760.4890.564 R\r\n−\r\nsquared0.1020.1190.0840.0370.087 N (Students)561565543560560 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. Each column presents results for beliefs about different academic outcomes: learning ability (e.g., ability to grasp concepts, retain information, or learn new skills) in column 1, understanding of course materials in column 2, course grades in column 3, ability to complete assignments on time in column 4, and time spent on academics in column 5. The dependent variable in each regression equals one if the student believes AI somewhat improves” or significantly improves” the outcome, and zero if they believe it has no effect, reduces, or significantly reduces the outcome. “Don’t know” responses are excluded.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\nAppendix BEmpirical Appendix\r\nB.1Field of Study Classifications\r\nThis appendix details the classification of majors into broad fields of study used in Figure 1:\r\n\r\n• Natural Sciences: Includes Biology, Biochemistry, Chemistry, Computer Science, Earth and Climate Sciences/Geology, Environmental Studies, Mathematics, Molecular Biology & Biochemistry, Neuroscience, Physics, and Statistics.\r\n• Social Sciences: Includes Anthropology, Economics, Education, Geography, International & Global Studies, International Politics & Economics, Political Science, Psychology, and Sociology.\r\n• Humanities: Includes American Studies, Black Studies, Classics, History, Architectural Studies, Art History & Museum Studies, History of Art & Architecture, Philosophy, Religion, and Classical Studies.\r\n• Literature: Includes Comparative Literature, English/English & American Literatures, and Literary Studies.\r\n• Languages: Includes Arabic, Chinese, French & Francophone Studies, German, Japanese Studies, Russian, and Spanish.\r\n• Arts: Includes Film & Media Culture, Music, Studio Art, and Theatre.\r\nB.2Task-Specific Use of Generative AI\r\nThe aggregate patterns documented in Section 4 may mask important heterogeneity across student groups. Different students may adopt AI tools at varying rates depending on their academic needs and field-specific norms. To investigate this heterogeneity systematically, Appendix Table LABEL:tab:ai_purpose_correlates presents regression estimates examining how student characteristics predict AI usage for each of our ten measured academic tasks.\r\n\r\nUsage patterns differ markedly by student characteristics. Male students consistently exhibit higher adoption rates across most applications, with particularly pronounced differences for finding information (16.4 pp; \r\np\r\n<\r\n0.01\r\n), summarizing texts (12.5 pp; \r\np\r\n<\r\n0.01\r\n), and creating images (16.9 pp; \r\np\r\n<\r\n0.01\r\n). Black students show substantially higher usage for information-gathering tasks, being 23.8 pp more likely to use AI for finding information compared to white students (\r\np\r\n<\r\n0.05\r\n). They also exhibit dramatically higher adoption of writing assistance tools, with 27.6 pp higher usage for editing text (\r\np\r\n<\r\n0.05\r\n) and 23.6 pp for writing emails (\r\np\r\n<\r\n0.10\r\n). Latino students demonstrate significantly higher usage for generating ideas (24.0 pp; \r\np\r\n<\r\n0.01\r\n) and writing emails (20.7 pp; \r\np\r\n<\r\n0.01\r\n), while Asian students show markedly higher adoption for writing emails (24.5 pp; \r\np\r\n<\r\n0.01\r\n) and explaining concepts (10.9 pp, \r\np\r\n<\r\n0.05\r\n). Students from public high schools report lower usage rates for several applications, particularly for concept explanation (7.3 pp lower; \r\np\r\n<\r\n0.10\r\n) and writing emails (7.4 pp lower; \r\np\r\n<\r\n0.10\r\n), but higher usage for proofreading (8.7 pp higher; \r\np\r\n<\r\n0.10\r\n).\r\n\r\nField of study emerges as a particularly strong predictor of usage patterns. Arts majors show significantly lower adoption across multiple tasks compared to Natural Science students, with gaps of 49.7 pp for finding information (\r\np\r\n<\r\n0.01\r\n), 44.0 pp for generating ideas (\r\np\r\n<\r\n0.05\r\n), and 62.1 pp for coding assistance (\r\np\r\n<\r\n0.01\r\n). Similarly, humanities majors are 36.8 pp less likely to use AI for generating ideas (\r\np\r\n<\r\n0.01\r\n) and 30.8 pp less likely for coding assistance (\r\np\r\n<\r\n0.01\r\n). Languages majors exhibit the most pronounced differences, being 52.4 pp less likely to use AI for summarizing texts (\r\np\r\n<\r\n0.01\r\n) and 46.9 pp less likely for generating ideas (\r\np\r\n<\r\n0.01\r\n). By contrast, social science students show higher usage for explaining concepts (8.0 pp; \r\np\r\n<\r\n0.10\r\n) and summarizing texts (14.7 pp; \r\np\r\n<\r\n0.01\r\n).\r\n\r\n0.8030.7400.6310.6190.5410.4730.3710.3440.2350.204 R\r\n−\r\nsquared0.0780.0870.0770.1160.0280.0540.0830.1350.0400.094 N (Students)515515515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and use of AI for specific academic tasks. Each column shows the result for a different academic task. The omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Regressions are weighted and use heteroskedasticity-robust standard errors clustered at the student level. ∗∗∗, ∗∗, and ∗ indicate significance at the 1%, 5%, and 10% levels, respectively.\r\n\r\nAppendix CQualitative Evidence on Student Perspectives on AI Use\r\nIn this section, we analyze student responses to an open-ended question about their use of generative AI. The survey question asked: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” This was an optional question that 48.3 percent of survey respondents answered. Appendix Figure C1 presents a word cloud of the most frequent words in student responses.\r\n\r\nC.1Validating the Open-Ended Response Measure\r\nWe begin by validating our open-ended response measure to show that it contains meaningful signal. To validate this measure, we employ sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner), a widely-used lexicon and rule-based sentiment analysis tool (Hutto and Gilbert, , 2014). We calculate sentiment scores for each response and examine whether these scores correlate with students’ actual AI adoption behaviors. Intuitively, students who express more positive sentiment toward AI in their open-ended responses should be more likely to have adopted AI tools in practice. We test this hypothesis by examining two measures of AI adoption: whether students have ever used generative AI and whether they currently use AI for academic purposes. Appendix Figure C2 presents binned scatterplots showing the relationship between AI adoption rates and standardized sentiment scores.\r\n\r\nSentiment towards generative AI strongly predicts AI adoption. For both outcomes, students with negative sentiment scores (below zero) show substantially lower adoption rates—around 70-85 percent for ever using AI and 70-75 percent for academic use. In contrast, students with positive sentiment scores display markedly higher adoption rates, reaching nearly 100 percent for general use and 95-100 percent for academic purposes among those with the most positive sentiment. The relationship is particularly pronounced for academic AI use (Panel B), where the coefficient of \r\nβ\r\n^\r\n=\r\n0.168\r\n is twice as large as for general adoption.17 These systematic patterns confirm that our open-ended responses capture meaningful variation in student attitudes that corresponds to real behavioral differences.\r\n\r\nC.2How Students use Generative AI\r\nTo analyze these responses systematically, we classified each response using keywords based on their content. For example, if a student mentioned using AI to save time, we tagged the response with the keyword “time-saver.” If a student expressed concerns about AI’s impact on learning, we tagged it with “negative learning.” Responses could receive multiple keywords if they discussed several themes. Appendix Figure C3 shows the frequency of keywords in our classification. The responses reveal how students use AI tools, what motivates this use, and what causes some to avoid or limit their use.\r\n\r\nThe most common use of AI is as an explanatory tool. Nearly 30 percent of responses mentioned using AI to understand course material. Students frequently ask AI to break down complex concepts from readings and lectures, particularly when they find the material difficult to understand. For example, one student reported: “I can ask AI to explain concepts to me that I have a hard time grasping. […] I can keep asking ’simplify’ or ’break down even more.”’ Students also use AI to summarize dense academic readings, which they argue helps them manage heavy reading loads.\r\n\r\nStudents employ AI throughout different stages of the writing process. Some use AI to generate initial drafts that serve as starting points. One student explained: “Helps me get started with a base for most of my essays. It feels easier to edit something already written and make it my own than to write from scratch.” Others use AI more narrowly for brainstorming when stuck on specific problems. As one student noted: “I use it if I am feeling stuck to push me to the right direction (whether a mathematical problem or an essay idea).” Many also report using AI as an editing tool to improve grammar, sentence structure, and overall writing flow. This is especially the case for non-native English speakers. As one student explained: “English is not my first language and it frustrates me sometimes that I cannot find the best way to phrase a certain idea and AI is a useful tool to have to find alternate expressions.”\r\n\r\nStudents frequently mentioned using AI for specific academic tasks. In courses that require coding, students often use AI for debugging code and understanding programming concepts. Students also employ AI for administrative tasks like formatting citations and drafting routine emails. Finally, many students use AI as an enhanced search engine. One student reported: “It has significantly reduced the time it takes to conduct research on new topics and ideas, and helps me by giving me a thorough selection of sources to use for projects of any kind.”\r\n\r\nC.3Why Students Adopt AI\r\nTime savings was the most commonly cited reason for using AI. Nearly 30 percent of responses mentioned using AI to complete work more efficiently. Students often viewed AI as a way to manage demanding course loads. Many students particularly embrace AI assistance for tasks viewed as mechanical or administrative. A student noted they use AI for “Writing emails quick and creating resume/ cover letter templates.” But AI assistance goes beyond grunt work. Some students use it to “spend less time doing assignments and homework.” This is particularly true if students don’t view the work as central to their academic experience. As one student explained: “when I come across work I deem as ineffective, I want to spend as little time as possible doing it.”\r\n\r\nHaving an on-demand tutor for academic support was another key motivation. One-quarter of responses described using AI as an “explainer” when other resources were unavailable or inconvenient. As one student noted, “I use it as a last resort (if there are no office hours, after looking up videos, etc.) if I need extra help. I’d like to think that the way I use it is similar to going to office hours or TA hours.”\r\n\r\nPeer influence also drove AI adoption. Some students reported feeling pressure to use AI to remain competitive with their classmates. One student explained: “I noticed others use it, are getting better grades than me, and they say they learn better with the help of AI, so I gave it a try.” Others worried about being at a competitive disadvantage: “Other people were using it and told me about it. I felt like I would be at a disadvantage if I wasn’t also using it.”\r\n\r\nC.4Concerns and Limitations\r\nStudents expressed several concerns about AI use in academic work. The most frequent worry was about negative impacts on learning. One student who initially used AI extensively reported: “In the past, I have simply plugged and chugged homework assignments into ChatGPT and submitted it. Those assignments feedback from teachers was positive and I was getting good grades, but I definitely felt that my own learning outcomes to be significantly worse.” Other students viewed AI use as fundamentally incompatible with their educational goals. As one humanities student explained: “my task is as a humanities student is to think, not calculate; why should I let AI do the thinking for me? It would defeat the purpose of pursuing my education.”\r\n\r\nMany students described ethical concerns about AI use. Responses suggested uncertainty about appropriate boundaries. One student noted: “I never use it to explicitly write something because that feels like overt cheating, but sometimes I hesitate when it completely solves Econ problems. I understand how it does it, and it helps me to learn, but it still sometimes feels a little morally gray.” Another expressed similar ambivalence: “I tend to only use it when [it] will save me time in a moral way.”\r\n\r\nStudents also emphasized the importance of maintaining ownership of their work. Many expressed pride in producing original work and hesitation about diluting that ownership through AI use. As one student explained: “I don’t have interest in using generative AI for my academic work because I want my work to reflect my own ideas.” Another noted: “It usually would not even occur to me to turn to AI to substitute writing because I want to take credit for my work, and using AI seems to diminish that.”\r\n\r\nTechnical limitations deterred some students from using AI tools. Students reported concerns about inaccurate outputs (“hallucinations”) and poor output quality, particularly for creative writing or complex analytical tasks. For example, one student noted that “In my poetry class we were instructed to use it to come up with poems and they were awful, so that kinda turned me away from using it to do my work for me.”\r\n\r\nC.5Discussion\r\nWe conclude with two overarching themes that emerged from the responses.\r\n\r\nFirst, students vary substantially in how they incorporate AI into their academic lives. Crucially, this heterogeneity largely depends on what students perceive as “appropriate” uses of AI. For activities that they perceive as core to their academic journey—like writing essays or solving problems—many students hesitate to use AI. A student articulated this clearly: “Most of my work is writing or reading. If I’m not doing the writing, what is the purpose of me taking the class?” Yet, students draw different boundaries between central tasks and grunt work. Some use AI extensively, viewing their role as akin to a manager that provides high-level direction while AI handles implementation. Others restrict AI use to specific tasks like brainstorming, editing, or drafting emails. Still others avoid AI entirely for academic work, often for ethical reasons. Even among AI users, adoption patterns reflect individual trade-offs between time savings, learning goals, and academic integrity concerns.\r\n\r\nSecond, there appears to be a fundamental tension between efficiency and learning. The time-saving benefits are easy to observe, quantifiable, and tangible. But these time savings are unlikely to be a free lunch. Some benefits may come at the cost of spending less time with material that requires deeper engagement to digest. As one student noted: “There may be a negative effect in that it eliminates much of the ’struggle’ in learning.” Yet, having an on-demand tutor that explains concepts in relatable ways can also improve learning. As one student explained: “It can explain concepts to me in a way that is tailored to my learning style.” This suggests that the impact of AI use on learning outcomes depends not on whether students use AI—almost all do, to some extent—but rather on how they employ these tools.\r\n\r\nFigure C1:Word Cloud of Student Motivations for Generative AI Use\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 147 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?’\r\n\r\nFigure C2:Relationship Between AI Sentiment and AI Adoption\r\nPanel A. Ever Used Generative AI\r\nRefer to caption\r\nPanel B. Uses AI for Academic Purposes\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents the relationship between AI sentiment and AI adoption. Panel A shows the proportion of respondents who have ever used generative AI, while Panel B shows the proportion who use AI for academic purposes. Each point represents the mean adoption rate for respondents within sentiment score bins of width 0.2. Sentiment scores are standardized compound scores computed using Hutto and Gilbert, (2014)’s VADER algorithm applied to responses to an open-ended question about generative AI. Positive values indicate positive sentiment and negative values indicate negative sentiment. The dashed lines show OLS best-fit lines estimated on the microdata, with coefficients and standard errors (in parentheses) displayed in the top-left corner of each panel.\r\n\r\nFigure C3:Frequency of Keywords in Student Motivations for AI Use\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to AI use. The responses come from the question “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” Color coding indicates the category of each theme. Usage type refers to how students use AI tools. Motivation captures what drove students to try AI. Concerns include mentions of course policies and academic integrity, individual reservations about AI use, worries about AI’s impact on education, and AI’s technical limitations.\r\n\r\nAppendix DQualitative Evidence on Student Views of AI Policies\r\nIn this section, we analyze student responses to an open-ended question about Middlebury’s AI policies. The survey asked: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Appendix Figure D1 presents a word cloud of the most frequent words in student responses. To analyze these responses systematically, we classified each response using keywords based on their content. Appendix Figure D2 shows the frequency of keywords in our classification.\r\n\r\nD.1Polarized Views on Generative AI Policy Approaches\r\nStudents expressed markedly different views about appropriate AI policies, revealing fundamental disagreement about the path forward. Some strongly advocated for embracing AI technology. As one student argued, “The tool is there, there is supply and there is demand. Don’t fight another war on drugs. Don’t live in a fake reality”. Others called for significant restrictions, arguing that “the use of generative AI is dishonest and corrosive” and that it “prohibits these organic processes and divorces students from true learning”.\r\n\r\nHowever, the most common position advocated for a balanced approach that would allow beneficial uses while restricting harmful ones. Students distinguished between uses that enhance learning (like concept explanation) and those that substitute for learning (like generating entire essays). One student articulated this nuanced view particularly well: “AI also can really be helpful at explaining a textbook problem that doesn’t make sense, or guiding slightly with homework, or creating study materials, or editing/tightening up your prose. All of those things are good, and universities should figure out how to maximize AI use for those reasons and to minimize students just feeding their problem sets into ChatGPT”\r\n\r\nA recurring theme was the futility of blanket bans. Many students emphasized that prohibition would be ineffective, with one noting “I don’t think anyone really cares what the policy of any given class is. If professors want people to not use it, they need to structure assessments in a way that will discourage use.” Another compared AI bans to restricting internet use, arguing “AI policies seem to be totally irrelevant. It’s like telling people they can’t use the internet as a resource for the class.” This ineffectiveness of bans creates fairness concerns. As one student explained: “I think if it is banned in a class, that should be enforced (and right now it absolutely is not)… As with any form of cheating, those who don’t cheat are put at a disadvantage.” Another student expressed similar frustration: “I find it discouraging when I hear classmates saying they use AI for things such as essays when they use it in dishonest ways.”\r\n\r\nD.2Need for Clear Guidelines\r\nThe most frequently expressed concern was the need for clear guidelines about generative AI use. Several students reported confusion about what constitutes acceptable use, with one noting “I think it should be more clear whether we can use it and how and how to cite it since most professors rarely mention it at all”. Other students emphasized the importance of professors explaining their policies upfront and their rationale, with one stating “I think that Professor’s should be very specific about what is allowed and their reasoning behind their policy”.\r\n\r\nMany students advocated for standardization across classes, observing that “Sometimes its confusing when one class allows it and another doesn’t and the other encourages it and so on so if there was a school wide or department wide policy that could help”. Yet there is also disagreement regarding standardization. Some students preferred leaving the decision to individual professors, arguing that “GenAI is more effective in some classes/majors than others. Making sure professors understand how students use GenAI and how useful GenAI is in their class (given the course structure, nature of assignments/material, etc.) is very important for the class policy.”\r\n\r\nD.3Training and Support Services\r\nStudents strongly emphasized the need for training in appropriate generative AI use. Many suggested that the college should provide guidance on using AI tools effectively while maintaining academic integrity. One student proposed “a workshop that teaches you to effectively use GenAI without violating the honor code”. Students also expressed interest in learning how to leverage AI to enhance their learning experience rather than circumvent it. As one student explained: “I think it could be useful to develop some sort of training. How do we use AI in a way that actually benefits our learning? I tried out some things on my own but I feel that I need more guidance.”\r\n\r\nThis desire for training was often linked to workplace preparedness. Students recognized that AI proficiency would be valuable in their careers, with one noting “As the world uses more and more AI, I think it is an important tool that students should know how to leverage”. Another emphasized: “The moment us students leave campus, we will be using it in the professional world, and when used in combination with one’s own skills, it is merely a tool to maximize efficiency”.\r\n\r\nD.4Discussion\r\nThe responses reveal several key tensions in AI policy implementation. First, students express a desire to use AI in ways that enhance rather than substitute for learning, yet they recognize that blanket bans are ineffective and worry about being disadvantaged if they follow restrictions while others do not.\r\n\r\nSecond, while students desire clear guidelines, they also want flexibility to accommodate legitimate uses that vary by discipline and assignment type. Different courses and majors may find different AI uses appropriate based on their learning objectives and assessment types.\r\n\r\nThird, there is tension between faculty autonomy in setting course policies and students’ desire for consistent institutional standards. While some support letting professors determine appropriate AI use for their specific courses, others argue that varying policies across classes create confusion and enforcement challenges.\r\n\r\nFinally, students report significant variation in faculty attitudes toward AI. Some students perceived faculty fear or misunderstanding of AI tools, noting that categorical bans often reflect a lack of understanding about AI’s capabilities and limitations rather than pedagogical considerations.\r\n\r\nFigure D1:Word Cloud of Student Feedback on Generative AI Policies\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 133 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?”\r\n\r\nFigure D2:Frequency of Keywords in Student Feedback on AI Policies\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to Middlebury’s AI policies. The responses come from the question “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Color coding indicates the category of each theme. Policy Approach captures suggestions about how AI should be regulated at the college. Adoption Views reflect positions on whether and how AI should be integrated into academic work. Training Needs indicates requests for guidance and support. Other Concerns include issues of workplace relevance and fairness.\r\n\r\nSee pages 1 of results/ai-survey-midd.pdf See pages 2- of results//ai-survey-midd.pdf\n",
    "totalFiles": 2
  },
  {
    "id": "extraction_1754349125336_wid4n0zox",
    "timestamp": "2025-08-04T23:12:05.336Z",
    "files": [
      {
        "name": "story.txt",
        "size": 730,
        "type": "text/plain",
        "extractedText": "I remember the moment that sparked this project. It was during a walk across Middlebury’s campus late one fall afternoon—just after office hours. A student had come in, clearly bright but struggling, and told me they’d been using ChatGPT late at night to help with assignments because they were too anxious to speak up in class. That confession stuck with me.\r\n\r\nLater, over coffee with Germán, we started wondering—how many students were turning to generative AI as a quiet lifeline? Not to cheat, but to learn? That simple question became the seed for this entire study. What we found wasn’t just statistics. It was a story of adaptation, of students reshaping how they learn, and of institutions racing to catch up.\"",
        "extractionStatus": "success"
      },
      {
        "name": "Generative AI in Higher Education E.txt",
        "size": 117470,
        "type": "text/plain",
        "extractedText": "Generative AI in Higher Education: Evidence from an Elite College†\r\nZara Contractor\r\nGermán Reyes\r\nAbstract\r\nGenerative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT’s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI’s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI’s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\r\n\r\n1Introduction\r\nThe rapid advancement of generative artificial intelligence (AI) is transforming higher education at an unprecedented pace. The launch of ChatGPT and similar tools has introduced technologies capable of performing tasks central to academic assessment and learning—writing essays, solving complex problems, and explaining intricate concepts—instantly and at near-zero marginal cost. Yet systematic evidence on the nature and implications of this new technology remains scarce. How widespread is generative AI adoption among students, and what factors drive it? Do students primarily use AI to augment their learning or to automate coursework, potentially harming human capital development? Could disparities in access to premium AI resources amplify existing educational inequalities?\r\n\r\nThese questions are at the heart of ongoing debates about the role of AI in education, which have relied heavily on anecdotal evidence and speculation rather than rigorous evidence. As a result, universities have implemented policies that vary dramatically, from outright bans to permissive adoption, often without clear evidence on their effectiveness or unintended consequences (Nolan, , 2023; Xiao et al., , 2023; McDonald et al., , 2025).\r\n\r\nThis paper addresses this gap by systematically examining generative AI adoption at Middlebury College, a highly selective liberal arts college in Vermont. With approximately 2,800 undergraduate students, Middlebury offers 49 majors across the arts, humanities, languages, social sciences, and natural sciences. Over the past decade, Middlebury has consistently ranked among the top 10 liberal arts colleges in U.S. News & World Report, with an average ranking of 8.5 between 2015 and 2025. The college’s selectivity is comparable to that of many elite research universities—Middlebury’s acceptance rate of approximately 10 percent is similar to that of Boston University (11 percent), Georgetown University (13 percent), and the University of Virginia (16 percent).\r\n\r\nOur analysis draws on survey data collected from the student population between December 2024 and February 2025. The survey collected detailed information about AI usage, including the frequency and purpose of use, perceived impacts, and responses to institutional policies. To minimize selection bias, we framed the survey broadly as examining technology use and provided incentives for participation, achieving a 22.9 percent response rate with 634 responses representing a broad cross-section of the student population.\r\n\r\nWe document five main findings. First, generative AI is approaching near-universal adoption at an unprecedented speed. Over 80 percent of students use AI for academic purposes, up from less than 10 percent before Spring 2023. This represents one of the fastest technology adoption episodes ever documented, dramatically exceeding the 40 percent adoption rate among U.S. workers (Bick et al., , 2025) and the 23 percent among all U.S. adults (McClain, , 2024). These levels are consistent with international findings of 50–70 percent adoption in university contexts (Nam, , 2023; Stöhr et al., , 2024; Ravšelj et al., , 2025).\r\n\r\nSecond, AI adoption is markedly unequal across academic disciplines and demographic groups. Field of study is the strongest predictor of adoption, likely reflecting how well AI capabilities align with the academic tasks required across fields. Adoption ranges from 91.1 percent in Natural Sciences majors (including mathematics and computer science) to significantly lower levels in Literature (48.6 percent) and Languages (57.4 percent). Additionally, demographic disparities exist. For example, males adopt AI at higher rates than females (88.7 versus 78.4 percent), a pattern that is consistent with documented AI gender gaps in other contexts (Otis et al., , 2024). Most notably, lower-achieving students have a higher adoption rate than their higher-achieving peers (87.1 versus 80.3 percent). As a result, AI could serve as an equalizing force if it enhances student learning—helping struggling students catch up to their peers—but could also widen achievement gaps if AI undermines skill development.\r\n\r\nThird, generative AI transforms the students’ learning production function by both augmenting student capabilities and automating academic tasks. We classify academic tasks as augmentation when AI enhances human capabilities while maintaining student engagement (e.g., explaining concepts, proofreading) versus automation when AI directly produces outputs with minimal cognitive involvement (e.g., writing essays, creating images), and average usage rates across tasks within each category. We find that 61.2 percent of AI users employ these tool for augmentation purposes, while 41.9 percent use it for automation. Qualitative evidence reinforces these patterns: students describe AI as an “on-demand tutor” for augmentation purposes, particularly valuable when traditional resources like office hours are unavailable, while automation use centers on time savings during periods of overwhelming workload. These self-reported patterns align closely with actual usage data from Claude conversation logs (Handa et al., 2025a,).\r\n\r\nFourth, institutional policies can significantly influence AI adoption, though their effectiveness can be undermined by informational frictions. We find that explicit prohibitions dramatically reduce self-reported intended use 39 percentage points (pp). Importantly, the effects of this policy vary substantially across student groups, with females showing larger reductions in usage under prohibition than males, suggesting that one-size-fits-all policies may inadvertently create disparate impacts across groups. Moreover, we also document substantial informational gaps about institutional AI policies. Many students (19.2 percent) do not understand AI policy rules, few know about institution-provided premium AI resources (10.1 percent) or proper AI citation practices (32.6 percent)—a skill that is necessary for academic integrity.\r\n\r\nFifth, students believe that AI has a positive learning impact and these beliefs strongly predict their own AI usage. Most students believe that AI improves their understanding of course materials (70.2 percent) and learning ability (60.1 percent), though fewer believe it improves grades (41.1 percent). Perhaps unsurprisingly, students who believe that AI improves their academic outcomes are significantly more likely to use these tools. For example, a ten-percentage-point increase in the belief that AI improves learning corresponds to a 4.9 percentage-point increase in adoption, highlighting how perceptions can affect technology diffusion in educational settings.\r\n\r\nTaken together, our findings reveal that generative AI has already reshaped students’ college experience. The substantial information gaps that we document—regarding both permitted uses and available resources—suggest straightforward opportunities for policy intervention. Yet the observed heterogeneity across disciplines and student groups indicates that uniform policies risk unintended consequences: blanket prohibitions may disproportionately disadvantage students who benefit most from AI augmentation, while unrestricted use may encourage automation practices detrimental to skill development. Thus, effective policy requires distinguishing clearly between AI uses that enhance learning and those that undermine it.\r\n\r\nWhile our findings are specific to the population we study—students at an elite liberal arts college—this setting offers valuable into AI’s broader societal impact. The rapid uptake of generative AI in higher education may reflect a generalizable principle regarding the diffusion of generative AI: adoption may be fastest in contexts where AI consolidates fragmented tools into a unified platform. Students have long had access to various tools for the academic tasks for which they now use generative AI.1 What distinguishes AI is its ability to provide all these services through a single, instantly accessible interface at virtually zero marginal cost. This consolidation likely drives students’ perception of enhanced learning efficiency and contributes to the fast adoption. This principle may extend well beyond education: industries with fragmented, specialized tools could experience similar trajectories of rapid AI adoption, even if AI itself does not introduce fundamentally new capabilities.\r\n\r\nOur findings contribute to a rapidly growing literature examining the adoption and impacts of generative AI. Recent work has focused on AI’s effects on worker productivity (Noy and Zhang, , 2023; Dell’Acqua et al., , 2023; Peng et al., , 2023; Cui et al., , 2024; Brynjolfsson et al., , 2025) and its potential to transform occupations (Felten et al., , 2021, 2023; Eloundou et al., , 2024). While several papers document AI adoption in workplace settings (Bick et al., , 2025; Hartley et al., , 2025; Humlum and Vestergaard, , 2025) and at the firm level (McElheran et al., , 2023; Bonney et al., , 2024; Kharazian, , 2025), we examine adoption in higher education—a critical setting where future high-skilled workers develop human capital.\r\n\r\nWe add to an emerging literature examining generative AI in education. Some studies focus on the learning impacts of generative AI, finding mixed impacts depending on the setting (e.g., Lehmann et al., , 2024; Bastani et al., , 2025). A complementary strand of the literature documents adoption patterns across diverse educational contexts, including universities in Australia (Kelly et al., , 2023), Ghana (Bonsu and Baffour-Koduah, , 2023), Norway (Carvajal et al., , 2024), Sweden (Stöhr et al., , 2024), and multi-country studies (Ravšelj et al., , 2025). To the best of our knowledge, this paper provides the first systematic evidence on generative AI adoption in U.S. higher education. Importantly, we focus specifically on how students use AI for academic purposes—unlike studies that measure more general usage including personal lives (e.g., Ravšelj et al., , 2025)—and go beyond documenting adoption rates to examine how students integrate AI into their academic workflows, distinguishing between augmentation and automation, the role of student beliefs about AI’s educational impacts, and how institutional policies shape usage patterns.\r\n\r\nOur findings also contribute to the literature on technology diffusion. Unlike the S-shaped adoption curves documented for most technologies (Griliches, , 1957; Rogers, , 1962), we observe extremely rapid adoption of generative AI, with 80 percent uptake within two years of ChatGPT’s release. This pattern contrasts sharply with the adoption trajectories of historical General Purpose Technologies (GPTs) like electricity, which required decades to achieve widespread use (David, , 1990; Bresnahan and Trajtenberg, , 1995). The rapid diffusion we document aligns with evidence that newer technologies diffuse faster than older ones (Comin and Hobijn, 2010a,; Comin and Hobijn, 2010b,). Two factors likely explain AI’s exceptional adoption speed compared to other GPTs. First, unlike electricity or steam power, generative AI requires minimal physical infrastructure—students access it through existing devices—and is available at no cost to consumers, eliminating the financial barriers that typically slow technology adoption. Second, as noted above, AI consolidates multiple specialized tools into a single platform, making its benefits immediately apparent without requiring specialized training or organizational restructuring, unlike previous GPTs (Brynjolfsson and Hitt, , 2000).\r\n\r\n2Data: Novel Student Survey\r\n2.1Recruitment and Structure\r\nWe conducted the survey from December 2024 to February 2025. All Middlebury College students were contacted via email and received a reminder a few weeks and two months after the initial invitation. To minimize selection bias, the recruitment materials described the survey broadly as a study on students’ use of technology in their academic and personal lives. To incentivize participation, students who completed the survey qualified for entry into a lottery for Amazon gift cards ranging in value from $50 to $500. The full survey instrument is provided in Appendix LABEL:app:survey.\r\n\r\nThe survey contains three main sections (see Appendix Figure A1 for the survey flow). First, we gather demographic and academic information, including gender, race/ethnicity, type of high school attended (private or public), current academic year, and declared or intended major.2 We also collect data on academic inputs and performance through students’ self-reported typical weekly hours spent studying and their first-year GPA.3\r\n\r\nSecond, we measure students’ experience with generative AI tools. We begin by asking whether students have ever used generative AI tools like ChatGPT or Claude. For those who have, we collect information about their usage patterns, including frequency of use during the academic semester, which specific AI models they use, and whether they pay for AI tools. We also gather data on how students use AI for different academic tasks, including writing assistance, learning support, and coding.\r\n\r\nThe final part of the survey elicits students’ beliefs about generative AI’s adoption and impacts. We ask students about their perceptions of AI’s effects on their academic experience across multiple dimensions, including learning, grades, time management, and understanding of course material. We then collect information on the role of institutional policies, asking students how different policy environments influence their likelihood of using AI. We also elicit students’ beliefs about AI usage among their peers, including their estimates of the fraction of Middlebury students who use AI for schoolwork and leisure. The survey concludes with two open-ended questions that allow students to share their experiences with AI in academic settings and provide feedback on the college’s AI policies and support services.\r\n\r\n2.2Sample and Summary Statistics\r\nOut of Middlebury’s 2,760 enrolled students, 739 began the survey. We exclude 105 respondents who left the survey before reaching the generative AI usage module, leaving us with an analysis sample of 634 students (22.9 percent of the student body). This response rate is comparable to that of similar surveys (Wu et al., , 2022). To make our sample more representative of Middlebury’s student body, we construct poststratification weights based on the distribution of declared majors from administrative records.4 Specifically, we weight observations by the ratio of each major’s share in the student population to its share in our survey responses.5\r\n\r\nTable Figures and Tables presents summary statistics for our sample. Column 1 reports unweighted survey averages, column 2 shows averages after applying poststratification weights, and column 3 provides population benchmarks from administrative records where available. Our unweighted sample consists of 44.6 percent male and 50.8 percent females. The racial/ethnic composition includes 61.8 percent white, 15.5 percent Asian, 9.9 percent Hispanic, and 3.6 percent Black students. The majority of students (54.3 percent) attended a public school, while 42.0 percent attended a private high school. Our sample represents 43 different majors across seven fields of study, with 31.1 percent of respondents not yet having declared a major. In our analysis, we group these undeclared students by their intended field of study as reported in the survey.\r\n\r\nComparing our unweighted sample to administrative records reveals notable differences. Our sample overrepresents white students (61.8 versus 53.8 percent) and Asian students (15.5 versus 7.3 percent), while underrepresenting Black students (3.6 versus 5.2 percent) and Hispanic students (9.9 versus 12.4 percent). First-year students are overrepresented (31.1 versus 25.5 percent), while senior students are underrepresented (21.3 versus 28.7 percent). Our weighting procedure partially addresses these discrepancies—particularly in the distribution of academic fields, where the weighted figures closely approximate administrative records—but some demographic differences persist. Despite these differences, our weighted sample provides reasonably close approximations to the college population on most dimensions, particularly for academic characteristics and field distributions.\r\n\r\n3Generative AI Usage Patterns Among Students\r\n3.1Adoption of Generative AI\r\nGenerative AI has achieved a remarkably high adoption rate among Middlebury College students. Figure 1 presents the distribution of AI usage frequency during the academic semester, categorized into four levels: “Rarely” (a few times per semester), “Occasionally” (a few times per month), “Frequently” (a few times per week), and “Very Frequently” (daily or almost daily). Overall, 82.5 percent of students report using generative AI for academic purposes, with substantial variation in usage intensity: 23.5 percent use it rarely, 22.2 percent occasionally, 26.2 percent frequently, and 10.6 percent very frequently.\r\n\r\nThe adoption rate at Middlebury aligns closely with patterns observed across other higher education institutions. A BestColleges survey of U.S. undergraduate and graduate students in fall 2023 found that 56 percent had used AI on assignments or exams (Nam, , 2023). Ravšelj et al., (2025) surveyed higher education students globally between late 2023 and early 2024, finding that 71.4 percent had ever used ChatGPT—though this figure encompasses all usage rather than academic use specifically. Similarly, Stöhr et al., (2024) document that 63 percent of Swedish university students had used ChatGPT by spring 2023, with 35.4 percent reporting regular use and 27.6 percent rare use. Among Norwegian university students, Carvajal et al., (2024) find that 68.9 percent use AI tools occasionally or more frequently.\r\n\r\nThese adoption rates in higher education far exceed those in the general population and workforce. Pew Research finds that only 23 percent of U.S. adults have ever used ChatGPT (McClain, , 2024), while Gallup reports that just one-third of U.S. workers have used AI for work (Gallup, , 2024). Bick et al., (2025) and Hartley et al., (2025) estimate that about 40 percent of the U.S. working-age population used generative AI for work as of late 2024 and early 2025. Both studies document substantial heterogeneity across industries, with information services showing the highest adoption rates at 56–62 percent—still considerably below the 80 percent adoption rate at Middlebury. Even among workers in AI-exposed occupations, Humlum and Vestergaard, (2025) find adoption rates of only 41 percent.6\r\n\r\n3.2Adoption by Student Characteristics and Field of Study\r\nAI adoption varies considerably across demographic groups and academic disciplines (Figure 1 and Appendix Table LABEL:tab:ai_freq_het). Males report higher usage rates than females (88.7 versus 78.4 percent). Usage patterns differ markedly by race/ethnicity: Black students (92.3 percent) and Asian students (91.3 percent) exhibit the highest adoption rates, while white students (80.2 percent) and Hispanic students (77.9 percent) report lower usage. Students from private high schools use AI more frequently than those from public schools (84.1 versus 80.4 percent). Notably, students with below-median GPAs report higher usage rates than their higher-achieving peers (87.1 versus 80.3 percent). Adoption varies widely by field of study: Natural Sciences leads with 91.1 percent usage, followed by Social Sciences at 84.6 percent, while Languages (57.4 percent) and Literature (48.6 percent) show substantially lower adoption rates.\r\n\r\nTo examine how student characteristics jointly relate to AI adoption, we estimate multivariate OLS regressions that include all observed characteristics simultaneously.7 Table LABEL:tab:ai_usage_correlates presents regression estimates using four usage thresholds as outcomes. Each column represents the probability of meeting progressively higher frequency thresholds: any AI use (column 1), at least monthly use (column 2), at least weekly use (column 3), and daily use (column 4).\r\n\r\nThe regression results confirm the patterns observed in the descriptive statistics. Holding other characteristics constant, males are 10.3 pp more likely than females to use AI (column 1, \r\np\r\n<\r\n0.05\r\n), with this gender gap typically widening at higher usage frequencies (columns 2–4, all \r\np\r\n<\r\n0.01\r\n). Black and Asian students show substantially higher adoption rates than white students, at 11.8 and 10.7 pp respectively (column 1, both \r\np\r\n<\r\n0.01\r\n). Students from public high schools are 3.0 pp less likely to use AI than those from private schools, but this difference is not statistically significant. Field of study emerges as the strongest predictor of adoption. Compared to Natural Sciences majors, students in Literature, Languages, Arts, and Humanities all show lower usage rates, with the differences being statistically significant at high usage frequencies (columns 3 and 4). Social Sciences majors exhibit adoption rates similar in magnitude to Natural Sciences majors across all usage frequency thresholds.\r\n\r\nOur findings on heterogeneity in AI adoption align with patterns documented in other settings. The gender gap in AI adoption at Middlebury—10.3 pp higher for males—is consistent with evidence across multiple studies. A meta-analysis of 18 studies by Otis et al., (2024) finds that males are 10–20 percentage points more likely to use generative AI than females. This gender gap in AI adoption appears in all educational studies (Nam, , 2023; Carvajal et al., , 2024; Stöhr et al., , 2024; Ravšelj et al., , 2025). Our finding that students with below-median GPAs are more likely to use AI aligns with Carvajal et al., (2024), who document higher adoption rates among students with lower admission grades. This pattern of greater adoption among lower-achieving students suggests that AI could narrow achievement gaps if it enhances learning and skill development, but could widen these gaps if it undermines the acquisition of fundamental skills.\r\n\r\nThe differences in adoption rates across academic fields at Middlebury mirror patterns documented in other educational settings. Stöhr et al., (2024) find that technology and engineering students exhibit significantly higher ChatGPT usage compared to students in humanities. Similarly, Nam, (2023) report that 62 percent of business majors and 59 percent of STEM majors have used AI tools for coursework, compared to 52 percent of humanities majors. Ravšelj et al., (2025) document comparable disciplinary differences, with applied sciences students showing substantially higher usage rates than arts and humanities students.\r\n\r\nThese academic differences persist into the workforce. Bick et al., (2025) find stark variation by college major: STEM graduates have the highest AI adoption rates (46.0 percent), followed by Business/Economics graduates (40.0 percent), while Liberal Arts graduates show substantially lower rates (22.4 percent). Humlum and Vestergaard, (2025) document similar patterns by occupation, finding that roles requiring strong writing and technical skills—such as marketing specialists and journalists—have the highest adoption rates. These consistent patterns across educational and professional contexts suggest that field-specific factors, particularly the applicability of AI tools to different types of tasks, shape adoption in systematic ways.\r\n\r\n3.3Timing of Generative AI Adoption\r\nThe speed of technology diffusion is a critical determinant of its economic and social impact (David, , 1990; Hall and Khan, , 2003; Stokey, , 2021). In educational contexts, rapid technology adoption can create or exacerbate inequalities between early and late adopters, particularly if the technology confers significant learning advantages (World Bank, , 2016). To track the timing of AI adoption among Middlebury students, we asked them when they first began using generative AI for academic purposes, with options ranging from “This semester (Fall 2024)” to “Before Spring 2023” (as a reference, ChatGPT’s public launch was in November 2022).\r\n\r\nStudents adopted generative AI at an extraordinary pace. Figure 2 shows that the cumulative adoption rate grew dramatically from less than 10 percent before Spring 2023 to slightly above 80 percent by Fall 2024. The pace of adoption has accelerated over time, likely reflecting improvements in AI capabilities. Among current users, 25.7 percent adopted AI in Fall 2024 alone, compared to 19.9 percent in Spring 2024 and 16.3 percent in Fall 2023.\r\n\r\nThe adoption rate among Middlebury students far exceeds that observed in other populations and represents one of the fastest technology adoption episodes ever documented. For comparison, Bick et al., (2025) show that it took over 20 years for computers to reach an 80 percent adoption rate in the U.S. working-age population, and about 15 years for internet adoption to reach similar levels. Even generative AI adoption in the broader population has been markedly slower: Pew Research found that just 23 percent of U.S. adults had ever used ChatGPT as of February 2024, up from 18 percent in July 2023 (McClain, , 2024). The dramatically faster adoption rate we document—reaching over 80 percent in less than two years—suggests academic settings may uniquely accelerate AI diffusion.\r\n\r\nTo identify early versus late adopters of generative AI, in Table LABEL:tab:ai_adopt_correlates and Appendix Figure A2, we analyze how adoption timing varies across student characteristics. The rate of adoption varies substantially across student characteristics. Male students led adoption, with a 8.9 pp higher probability of using AI before Spring 2023 compared to females (column 1, \r\np\r\n<\r\n0.01\r\n)—a gender gap that persists across all periods (columns 2–5). Black and Asian students also adopted earlier than white students, though these differences reach statistical significance only in later periods (columns 4–5). Field of study is a strong predictor of adoption timing. For example, students in Languages consistently lagged behind Natural and Social Sciences majors, with significantly lower adoption rates across nearly all time periods.\r\n\r\n3.4Choice of Generative AI Models\r\nMajor AI companies operate on a freemium model, where free versions coexist with premium subscriptions that offer higher usage limits and access to more advanced models. This tiered structure creates potential for a new form of educational disparities: if paid versions confer substantial academic advantages, students who cannot afford subscriptions may be systematically disadvantaged. To investigate these potential disparities, we presented respondents with a list of options including both free and paid versions of popular models. We also collected information on monthly subscription expenditures, with response options ranging from no active subscription to spending more than $40 monthly (standard subscriptions to major models like ChatGPT cost $20 per month at the time).\r\n\r\nOpenAI’s ChatGPT dominates AI usage among Middlebury AI users, with the free version capturing the largest market share. Figure 3 shows that 89.3 percent of AI users rely on the free version of ChatGPT, making it dramatically more popular than any alternative. Google Gemini (13.5 percent) and Microsoft Copilot (7.7 percent) are distant competitors, while other platforms each capture less than 5 percent of users. This dominance mirrors patterns in other academic settings, where Stöhr et al., (2024) find substantially higher familiarity with ChatGPT compared to alternative platforms, and aligns with broader workforce trends documented by Bick et al., (2025).8\r\n\r\nDespite near-universal AI adoption, only 11.3 percent of AI users pay for any AI service.9 This figure is remarkably similar to the 8 percent found in Ravšelj et al., (2025)’s multi-country survey. This low payment rate suggests that for most students, the premium features of paid versions—primarily higher usage limits and access to more advanced models—do not justify the subscription cost. However, payment patterns reveal significant disparities: males and Asian students are substantially more likely to purchase AI subscriptions (Appendix Table LABEL:tab:ai_model_correlates and Appendix Figure A3), potentially reflecting differences in usage intensity across demographic groups.\r\n\r\n4Generative AI and the Production of Learning\r\n4.1The Use of Generative AI across Academic Tasks\r\nHow is generative AI transforming the traditional inputs to student learning? The educational production function includes inputs like time spent studying (e.g., Stinebrickner and Stinebrickner, , 2008), faculty instruction (e.g., Fairlie et al., , 2014), peer interactions (e.g., Sacerdote, , 2001), and academic support services (e.g., Angrist et al., , 2009). AI tools have the potential to complement or substitute for these traditional inputs. For example, using AI to explain concepts might substitute for faculty office hours, while using it for proofreading might reduce time needed for academic support services.\r\n\r\nTo understand the role of AI in students’ learning production function, we collected information about students’ AI usage across ten common academic tasks: proofreading, generating ideas, writing essays, editing essay drafts, coding assistance, creating images, explaining concepts, composing emails, summarizing materials, and finding information. For each task, students indicated their frequency of AI use on a five-point scale ranging from never to daily. We supplemented this quantitative data with open-ended responses about how AI influences their academic work process.\r\n\r\nStudents use generative AI for a wide range of academic tasks, with the highest adoption rates for learning support and text-processing activities. Figure 4, Panel A shows that explaining concepts is the most common use case, with 80.3 percent of AI users using it for this task. Summarizing texts follows as the second most common task (74.0 percent), followed by finding information and generating ideas (63.1 and 61.9 percent). Writing assistance tasks like proofreading and editing essays are also common, used by 54.1 and 47.3 percent of AI users, respectively. Technical applications like coding help are significant (34.4 percent), considering that many academic degrees involve no programming. Notably, while 23.5 percent of AI users report using it for writing essays, this represents a relatively low adoption rate compared to other academic uses, suggesting students may be more hesitant to use AI for primary content creation. The lowest adoption rate is for creating images at 20.4 percent, likely reflecting fewer academic use cases for this capability.\r\n\r\nAn important limitation of our survey is that it relies on self-reported usage, which may introduce measurement error. For example, students might underreport uses they perceive as academically inappropriate—such as essay writing—while overreporting those viewed as legitimate learning tools (Ling and Imas, , 2025). To assess the validity of our self-reported measures, we compare our findings with Handa et al., 2025a, who analyze actual Claude usage patterns among users with university email addresses. Several caveats apply: their data captures conversation-level interactions rather than student-level usage, and most students in our sample use ChatGPT rather than Claude (Section 3.4). Despite these limitations, the comparison provides a useful benchmark for evaluating our survey responses.\r\n\r\nReassuringly, our results are consistent with Anthropic’s data. Both studies identify explaining concepts and technical problem-solving as primary use cases. Handa et al., 2025a report that the second largest use case (33.5 percent of conversations) involves “technical explanations or solutions for academic assignments,” while we find that 80.3 percent of AI users use it for explaining concepts—a difference likely attributable to our student-level versus their conversation-level measurement. Similarly, the most common usage category in Claude involves “designing practice questions, editing essays, or summarizing academic material” (39.3 percent of conversations), aligning with our findings that 74.0 percent of AI users use it for summarizing texts and 47.3 percent for editing essays. The disciplinary patterns also converge: Handa et al., 2025a finds that computer science, natural sciences, and mathematics conversations are overrepresented, which mirrors our finding that Natural Science majors show significantly higher AI adoption rates (Figure 1). Overall, these convergent findings from self-reported survey data and actual usage logs suggest that our results capture genuine patterns of student AI engagement rather than merely reflecting social desirability in responses.\r\n\r\n4.2Automation versus Augmentation\r\nAre students using generative AI primarily to augment their learning or to automate their coursework? This distinction is crucial for understanding AI’s impacts (Autor and Thompson, , 2025)—augmentation may enhance students’ learning processes while maintaining their active engagement and critical thinking, whereas automation produces fully-formed outputs that could be submitted with minimal student input, potentially harming learning.\r\n\r\nTo examine this empirically, Figure 4, Panel B categorizes the ten measured tasks based on whether they augment or automate academic tasks. We classify tasks as augmentation when they enhance human capabilities (explaining concepts, finding information, proofreading, and editing drafts) and as automation when they directly produce outputs (writing essays, creating images, composing emails, summarizing texts, generating ideas, and coding assistance). We then calculate the percentage of AI users who employ each category at various frequencies.10\r\n\r\nStudents use generative AI for both augmentation and automation, but with markedly different frequency. While 61.2 percent of AI users report using AI for augmentation tasks, 41.9 percent report using it for automation—thus, there is a substantial 19.3 pp difference. This gap is mainly driven by occasional use (19.7 percent for augmentation versus 12.7 percent for automation) and frequent use (16.3 versus 9.0 percent). Most strikingly, students are more than twice as likely to use AI daily for augmentation (5.0 percent) compared to automation (2.5 percent). The higher frequency of augmentation use also suggests that students find augmentation uses more valuable for their day-to-day academic activities. Our findings align closely with patterns observed in actual AI usage data: Handa et al., 2025b analyze real conversation logs from Claude and find that 57 percent of workplace AI interactions involve augmentation while 43 percent involve automation.\r\n\r\nQualitative evidence from open-ended responses provides additional insights into students’ motivations for using AI (see Appendix C for additional results and validation of the open-ended measure). Students’ descriptions align closely with the augmentation-automation framework. For augmentation, many characterize AI as an “on-demand tutor,” particularly valuable when traditional resources like office hours are unavailable. Non-native English speakers frequently mention using AI for proofreading to overcome language barriers, while students from technical majors describe using it to debug code and understand error messages. For automation, time savings emerged as the dominant motivation, with 21.7 percent of open-ended responses explicitly mentioning efficiency benefits (Appendix Figure C3). Students describe turning to AI during periods of overwhelming workload or looming deadlines, using it to generate initial drafts or complete routine assignments. That students automate tasks mostly when time-pressed helps explain why automation is less common than augmentation.11\r\n\r\nOur finding that students favor augmentation over automation extends beyond Middlebury. In Appendix Figure A4, we re-analyze data from Ravšelj et al., (2025)—a multi-country survey of higher education students—to examine how this balance varies across institutional quality.12 We classify universities into quintiles based on their Times Higher Education World University Rankings. Overall, students who use AI worldwide show similar adoption rates for augmentation (64.6 percent) and automation (63.1 percent) tasks—a much smaller gap than at Middlebury (Panel A). However, this aggregate pattern masks heterogeneity by institutional quality. Students at top-quintile universities show a modest preference for augmentation over automation, with this gap narrowing monotonically down the institution quality distribution. By the bottom quintile, augmentation and automation usage are virtually identical (Panel B).\r\n\r\n4.3Heterogeneity in Augmentation versus Automation Usage\r\nUnderstanding whether augmentation and automation patterns vary across student populations may help to design targeted support policies. To assess this, we construct four measures of augmentation and automation usage. First, we create binary indicators for whether students use AI for any augmentation or automation task. Second, we calculate the proportion of tasks in each category for which students employ AI. Third, using the Likert-scale responses, we create intensity measures that capture how frequently students use AI for augmentation and automation purposes. Finally, we compute the difference between augmentation and automation variables, as a measure that directly compare students’ relative preference for augmentation versus automation. Table LABEL:tab:ai_autom_regs presents regression estimates using these measures as outcomes.\r\n\r\nThe balance between augmentation and automation varies substantially by student characteristics. Males show higher adoption of both augmentation and automation tasks, with a similar magnitude in each case—they are 6.0 pp more likely to use augmentation prompts and 5.5 pp more likely to use automation prompts compared to females (columns 1 and 4, \r\np\r\n<\r\n0.05\r\n). Black students have a substantially higher augmentation intensity than white students (0.871 points higher, equivalent to 38.1 percent of the outcome mean, \r\np\r\n<\r\n0.01\r\n), but show no statistically significant differences in automation usage. This results in Black students having the strongest preference for augmentation over automation, with a 19.5 pp higher share of augmentation tasks relative to automation (column 7, \r\np\r\n<\r\n0.01\r\n). Asian students show high usage of both kinds relative to white students. Differences across field of study effects are sizable: humanities, languages, and literature majors tend to show lower usage of both augmentation and automation tools compared to natural science majors.\r\n\r\n5Institutional Policies and AI Adoption\r\n5.1The Role of Institutional Policies in Shaping Student Behavior\r\nInstitutional policies are central to shaping the adoption and diffusion of new technologies (Acemoglu, , 2025). In the context of generative AI in higher education, understanding how policies affect student behavior is essential for guiding evidence-based decision-making. To examine this, we asked students to report their likelihood of using AI under various policy scenarios, ranging from complete prohibition to unrestricted use. This analysis is particularly relevant in light of the ongoing debate about how universities should regulate generative AI use in academic settings (Nolan, , 2023), and the widespread variation in institutional policies across colleges (Xiao et al., , 2023).\r\n\r\nInstitutional policies substantially influence students’ reported likelihood of using generative AI. Figure 5 shows that when generative AI use is unrestricted, 52.4 percent of students report being likely or extremely likely to use it. This likelihood decreases modestly when policies require citation (40.9 percent) or when no explicit policy exists (42.3 percent). However, explicit prohibition creates a dramatic shift: only 13.4 percent of students report they would be likely or extremely likely to use AI when it is banned, while 72.9 percent say they would be unlikely or extremely unlikely to do so (note that these figures include both current AI users and students who do not use generative AI for academic purposes). These results suggest that institutional policies can significantly influence generative AI usage patterns, though a small fraction of students report they would likely use AI even when explicitly prohibited.\r\n\r\nThe magnitude of policy effects in our study aligns closely with findings from other contexts. Carvajal et al., (2024) estimate that banning AI reduces usage by 37.2 pp among females and 20.6 pp among males, for an overall drop of about 28.9 pp. In our survey, a ban leads to a 37.8 pp decline in usage, with a larger decrease among females (49.6 pp) than males (40.1 pp). These parallel results underscore how institutional policies can unintentionally produce disparate effects across gender. Notably, these differential policy effects extend beyond gender. Other demographic and academic characteristics—such as race and field of study—also moderate how students respond to policy restrictions (Appendix Table LABEL:tab:ai_policy_het), underscoring that institutional policies can produce non-neutral impacts along multiple dimensions.\r\n\r\n5.2Understanding of Institutional Policies and Resources\r\nInformation gaps and inattention can significantly affect technology adoption decisions (Duflo et al., , 2011; Hanna et al., , 2014). For example, imperfect information about rules, available resources, or proper usage guidelines could lead to underadoption of beneficial technologies or inadvertent policy violations. To test for the existence of information gaps, we examine three dimensions of policy understanding. First, we asked students whether they find AI policies in their current classes clear. Second, we measured awareness of free access to premium AI tools through the college—a resource that could reduce inequality but only if students know it exists. Third, we assessed whether students know how to properly cite AI when required, a mechanical skill necessary for academic integrity. We supplemented these measures with open-ended feedback about the college’s AI policies and support services.\r\n\r\nStudent understanding of institutional policies is high, though significant gaps remain. Figure 6, Panel A shows that most students (79.1 percent) understand when and where they are allowed to use AI in their classes, but a nontrivial minority (19.2 percent) find AI policies unclear. Moreover, critical knowledge gaps persist elsewhere. Only 10.1 percent know they have free access to Microsoft Copilot through the college (Panel B), and just 32.6 percent understand how to properly cite AI use (Panel C). These gaps vary systematically: females show better policy understanding than males (81.5 versus 75.8 percent), and non-white students demonstrate higher awareness across all three dimensions compared to white students.13\r\n\r\nThe qualitative evidence from open-ended responses reinforces these information frictions and reveals implementation challenges (Appendix D). Students express frustration with vague guidelines, requesting specific examples of acceptable versus unacceptable use cases. Many advocate for formal training, noting that simply knowing policies exist differs from understanding how to effectively integrate AI into their workflow. A particularly striking theme is students’ perception of blanket prohibitions as both ineffective and unfair, as they create a prisoner’s dilemma situation where compliant students are disadvantaged relative to those who secretly violate restrictions. Many responses call for a balanced approach—permitting AI use that supports learning while restricting uses that replace it—though the boundary between the two remains contested.\r\n\r\n6Beliefs About AI’s Educational Impact and Peer Usage\r\n6.1Student Beliefs of AI’s Impact on Educational Outcomes\r\nTechnology adoption decisions are shaped by beliefs about potential returns (Foster and Rosenzweig, , 2010). Students’ perceptions of how AI affects their learning may influence whether they adopt these tools and how they integrate them into their academic workflows. To elicit these beliefs, we asked students to evaluate AI’s impact across four dimensions of their academic performance: understanding of course materials, overall learning ability, time management, and course grades. For each dimension, students rated AI’s effect on a five-point scale ranging from “significantly reduces” to “significantly improves.” These subjective assessments provide insight into the perceived value of AI tools from the student perspective, which may differ from their actual effects on learning outcomes.\r\n\r\nStudents tend to believe that generative AI is beneficial for their academic performance, though the perceived benefits vary across dimensions. Figure 7 shows that the majority of students (70.2 percent) believe that generative AI improves their understanding of course materials, and 60.1 percent report improvement in their ability to grasp concepts, retain information, or learn new skills. Similarly, 59.4 percent report that AI improves their ability to complete assignments on time. Notably, while students believe AI helps their learning and assignment completion, they are less confident about its impact on course grades—41.1 percent believe it improves their grades, while 55.4 percent report no effect and 3.5 percent report negative effects. This pattern suggests that while students perceive generative AI as improving their learning process and workflow—through better understanding, skill development, and timely completion of work—these benefits do not necessarily translate into better course grades.14\r\n\r\nBeliefs about AI’s benefits strongly predict adoption. In Figure 8, we plot the relationship between the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis) for different subgroups of students (e.g., males, white students, public-school students, etc.). Across all four measured academic dimensions, there is a strong and statistically significant relationship between perceiving positive AI effects and AI adoption. The relationship is strongest for beliefs about the ability to improve course grades (Panel D): a 10-pp increase in the belief that AI improves grades is associated with a 5.6 pp increase in AI adoption (\r\np\r\n<\r\n0.01\r\n). Similar positive relationships exist for beliefs about learning ability (4.3 pp, \r\np\r\n<\r\n0.05\r\n), understanding of course materials (4.5 pp, \r\np\r\n<\r\n0.01\r\n), and timely assignment completion (5.3 pp, \r\np\r\n<\r\n0.05\r\n). These findings suggest that student beliefs about AI’s academic benefits—irrespective of the actual benefits—may play a crucial role in shaping adoption decisions.\r\n\r\nThese positive beliefs align with findings from other contexts. Ravšelj et al., (2025) report that the majority of students in their sample believe that ChatGPT improves their general knowledge (68.8 percent) and specific knowledge (62.7 percent)—remarkably similar to our finding that most students believe AI improves understanding of course materials (68.5 percent) and learning ability (57.6 percent). Similarly, 57.4 percent of students in Ravšelj et al., (2025)’s sample believe that ChatGPT helps meet assignment deadlines, while 59.4 percent in our data report AI improves timely assignment completion. Stöhr et al., (2024) provide complementary evidence from Swedish universities, where 47.7 percent of students believe that AI makes them more effective learners, yet only 17.3 percent believe these tools improve their grades—mirroring our finding that perceived learning benefits from AI use exceed perceived effects on course performance.\r\n\r\n6.2Student Beliefs About Peer Use of Generative AI\r\nStudents’ beliefs about their peers’ AI usage may influence their own adoption through multiple channels, including social norms (e.g., Giaccherini et al., , 2019), social learning (e.g., Foster and Rosenzweig, , 1995; Beaman et al., , 2021), peer effects (e.g., Bailey et al., , 2022), and competitive pressure to avoid falling behind (e.g., Goehring et al., , 2024). To measure these beliefs, we asked students to estimate what fraction of their peers use generative AI for different purposes and would use under different policy environments. Figure 9 presents the distribution of these beliefs. Panels A-C show students’ estimates of peer AI usage for schoolwork, leisure, and any purpose respectively. Panels D-F display students’ beliefs about AI usage under three policy environments: when classes have no explicit AI policy, when classes allow AI use, and when classes prohibit AI use.\r\n\r\nStudents systematically underestimate their peers’ AI usage. On average, they believe 65.2 percent of peers regularly use generative AI for schoolwork (Panel A), while our survey reveals an actual usage rate of 82.5 percent—a 17.3 pp gap. This underestimation appears consistent across educational contexts: Stöhr et al., (2024) find that only 38.7 percent of Swedish students believe AI chatbot use is common among peers, despite 63 percent actual usage. When we examine beliefs about policy-contingent behavior, students estimate that 62.7 percent of peers use AI when no explicit policy exists (Panel D), rising to 72.2 percent under permissive policies (Panel E) and falling to 43.5 percent under prohibition (Panel F). These beliefs align directionally with self-reported intentions, though with notable magnitude differences (Appendix Figure A5). For instance, while students believe 43.5 percent of their peers use AI in classes that prohibit it, only 28.4 percent report they themselves would be likely to do so.15\r\n\r\nNotably, students’ perceptions of peer AI adoption are closely linked to their own adoption behavior. Appendix Figure A6 shows a strong positive association between beliefs about peer usage and actual usage rates across student groups. This behavior is consistent with several psychological mechanisms, including the “false consensus effect” (Ross et al., , 1977), selection neglect (Jehiel, , 2018), or interpersonal projection bias (Bushong and Gagnon-Bartsch, , 2024). Still, virtually all groups systematically underestimate peer usage—actual usage rates exceed believed usage for every demographic and academic group we examine. This figure also shows that underestimation varies substantially across student characteristics: males use AI at 88.7 percent but believe only 66.6 percent of peers do so (22.1 pp gap), while females show a smaller gap (77.6 percent actual versus 65.1 percent believed, 13.3 pp gap).\r\n\r\nTaken together, students’ systematic underestimation of peer AI usage coupled with the strong relationship between beliefs and adoption suggests that misperceptions about social norms may shape technology diffusion. If AI enhances learning, then correcting these misperceptions through information provision could accelerate beneficial adoption; conversely, if AI undermines skill development, then students’ underestimation of peer usage may serve as an unintentional safeguard against harmful overadoption.\r\n\r\n7Discussion\r\nThis paper presents the first systematic evidence on generative AI adoption at a highly selective U.S. college. Using novel survey data, we document exceptionally rapid and widespread adoption, substantial shifts in the educational production function through augmentation and automation, and the significant roles of students’ beliefs and institutional policies in shaping AI use.\r\n\r\nOur results offer three implications for institutional policy and ongoing debates about AI in education. First, we identify low-cost opportunities to improve institutional policy effectiveness through targeted information provision. The significant gaps we document in students’ understanding of institutional AI policies, citation practices, and available AI resources suggest that simple interventions—such as clear guidelines, illustrative examples of acceptable uses, and AI literacy programs—can reduce unintentional academic integrity violations and support beneficial AI integration. Qualitative feedback strongly indicates student demand for more explicit guidance on responsible AI use.\r\n\r\nSecond, our evidence challenges alarmist narratives that conflate widespread AI adoption with universal academic dishonesty based on anecdotal accounts.16 Although AI use is indeed near-universal, we find clear evidence that students primarily employ AI as a tool for strategic task management: to enhance learning (augmentation) and selectively automate tasks when facing high time opportunity cost—not solely to circumvent academic effort. This distinction matters: by normalizing academic dishonesty as inevitable and universal, these narratives may shift social norms and encourage students who would otherwise use AI responsibly to engage in prohibited behaviors, believing “everyone else is doing it.”\r\n\r\nThird, our findings caution against policy extremes of either blanket prohibition or unrestricted AI use (Merchant, , 2024; McDonald et al., , 2025). Blanket prohibitions disproportionately harm students who benefit most from AI’s augmentation functions—particularly lower-achieving students—while also creating uneven compliance, placing conscientious students at a disadvantage relative to rule-breakers. Conversely, unrestricted AI use based solely on revealed preference arguments ignores important market failures in educational settings. Students often hold overly optimistic beliefs about AI’s learning benefits despite mixed empirical evidence, potentially leading to unintended negative learning outcomes. Most concerningly, permissive policies risk creating competitive dynamics where students feel compelled to adopt AI not for its learning benefits but simply to avoid falling behind in an educational “arms race” (Goehring et al., , 2024).\r\n\r\nFigures and Tables\r\nFigure 1:The Adoption of Generative AI among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the fraction of students who report using AI during the academic semester, categorized by demographic characteristics, high school type, academic cohort, GPA, and field of study. Usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily).\r\n\r\nThe category “All students” provides the baseline usage rate for the full sample. Gender categories are based on self-identification, with non-binary responses excluded due to a small sample size. “Private HS” refers to students who attended private high schools, while “Public HS” includes public institutions. “Cohort” denotes the student’s academic year, ranging from first-year (“Freshman”) to fourth-year and beyond (“Senior”). GPA categories (“GPA \r\n>\r\n p50” and “GPA \r\n<\r\n p50”) split students into groups above or below the median first-year GPA, as self-reported on a 4.0 scale. See Appendix B.1 for the classification of majors into fields of study.\r\n\r\nFigure 2:The Evolution of Generative AI Adoption among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the cumulative percent of students who reported using generative AI tools for academic purposes over time. The data is based on retrospective self-reports collected in our December 2024 survey, where students were asked “When did you first start using any form of Generative AI for academic purposes?” Response options ranged from “Before Spring 2023” to “This semester (Fall 2024).” The \r\nx\r\n-axis represents academic semesters, while the \r\ny\r\naxis represents the cumulative adoption rate. Vertical lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure 3:Adoption of Generative AI Models Among College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the adoption rates of various AI models as of Fall 2024. The horizontal axis shows the percent of students who reported using each tool, and the vertical axis lists the tools in descending order of adoption rates.\r\n\r\nFigure 4:Academic Uses of Generative AI\r\nPanel A. Across Common Academic Tasks\r\nRefer to caption\r\nPanel B. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of AI students who use generative AI for different academic tasks. For each task, usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). The number at the end of each bar represents the total percent of students who use AI for that purpose at any frequency. Tasks are ordered by total usage, from highest to lowest. Results are based on responses to the question: “For academic purposes, which of the following tasks do you typically use generative AI for?” Sample includes all students who reported using AI during the academic semester.\r\n\r\nFigure 5:Student Reported Likelihood of Using AI under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of students who report different likelihoods of using AI under various policy scenarios. For each policy, responses are categorized on a five-point scale from “Extremely unlikely to use AI” to “Extremely likely to use AI.” The sample includes all survey respondents. The question asked was: “How likely are you to use generative AI in a class with each of the following AI policies?”\r\n\r\nFigure 6:Understanding of Generative AI Policies and Resources\r\nPanel A. Understanding of AI Policies\r\nRefer to caption\r\nPanel B. Awareness of Copilot Access\r\nRefer to caption\r\nPanel C. Knowledge of Citation Requirements\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows students’ understanding of institutional AI policies and resources. Panel A displays the percent of students who report understanding AI policies in their classes, those who report having no explicit policy, and those who find policies unclear, broken down by demographic characteristics. Panel B shows the percent of students who are aware of their free access to Microsoft Copilot through Middlebury College. Panel C presents the percent of students who report knowing how to properly cite AI use in their academic work when required. For Panels B and C, horizontal lines represent 95 percent confidence intervals. Sample includes all survey respondents.\r\n\r\nFigure 7:Student Beliefs about the Impact of AI on their Academic Performance\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of Middlebury students who believe that AI improves, reduces, or has no effect on different aspects of their academic experience. For each outcome, responses are categorized into three groups: “Improves” combines “significantly improves” and “somewhat improves” responses, “Reduces” combines “significantly reduces” and “somewhat reduces” responses, and “No effect” represents neutral responses. Sample includes all students who report using AI during the academic semester. “Don’t know” responses are excluded.\r\n\r\nFigure 8:Relationship Between AI Adoption and Beliefs About AI’s Academic Benefits\r\nPanel A. Learning ability (e.g., ability to grasp concepts, learn new skills, etc.)\r\nRefer to caption\r\nPanel B. Understanding of\r\ncourse materials\r\nRefer to caption\r\nPanel C. Ability to complete\r\nassignments on time\r\nRefer to caption\r\nPanel D. Course grades\r\n \r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the relationship between AI adoption rates and beliefs that AI improves various academic outcomes across different student groups. Each panel plots the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis). Points represent different student groups categorized by demographics (circles), academic characteristics (triangles), and field of study (squares). The dashed line shows the linear fit across all groups. Groups with fewer than 10 students are excluded.\r\n\r\nFigure 9:Student Beliefs about Generative AI Usage at Middlebury College\r\nPanel A. For Schoolwork\r\nRefer to caption\r\nPanel B. For Leisure\r\nRefer to caption\r\nPanel C. For Any Purpose\r\nRefer to caption\r\nPanel D. Classes with No Policy\r\nRefer to caption\r\nPanel E. Classes that Allow AI\r\nRefer to caption\r\nPanel F. Classes that Disallow AI\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the distribution of students’ beliefs about generative AI usage among their peers at Middlebury College. Panels A-C display students’ estimates of the percent of their peers who regularly use AI for schoolwork, leisure activities, and any purpose, respectively. Panels D-F show students’ beliefs about AI usage in classes with different AI policies: those without an explicit policy (Panel D), those that allow AI use (Panel E), and those that prohibit AI use (Panel F). Each panel shows a histogram with bins of width ten percentage points (e.g., responses between 1-10 fall in the 10 bin, 11-20 in the 20 bin, etc.). The red dashed line indicates the mean response. Sample excludes respondents with missing values or who selected the default response for all six categories (which equals zero).\r\n\r\nTable 1:Summary Statistics of Survey Participants\r\nSurvey Sample\tAdmin records\r\nUnweighted\tWeighted\t\r\n(1)\t(2)\t(3)\r\nPanel A. Demographics\r\nMale\t0.446\t0.433\t0.463\r\nFemale\t0.508\t0.516\t0.533\r\nWhite\t0.618\t0.603\t0.538\r\nBlack\t0.036\t0.033\t0.052\r\nHispanic\t0.099\t0.106\t0.124\r\nAsian\t0.155\t0.162\t0.073\r\nPrivate high school\t0.420\t0.399\t–\r\nPublic high school\t0.543\t0.556\t–\r\nPanel B. Academic Characteristics    GPA 3.740 3.736 3.670 Hours spent on academics per week 17.899 17.889 – Freshman 0.311 0.355 0.255 Sophomore 0.273 0.272 0.257 Junior 0.202 0.179 0.201 Senior 0.213 0.194 0.287   Panel C. Field of Study    Arts 0.011 0.021 0.023 Humanities 0.052 0.068 0.073 Languages 0.021 0.020 0.025 Literature 0.035 0.025 0.025 Natural Sciences 0.218 0.249 0.244 Social Sciences 0.353 0.247 0.243 Has not declared major 0.311 0.371 0.364\r\n\r\nN (# degrees) 43 43 49 N (# students) 634 2,760 2,760\r\n\r\n\\justify\r\nNotes: This table presents summary statistics from our survey of college students. Panel A reports demographic characteristics, including the proportion of participants identifying as male, female, white, Black, Hispanic, Asian, or who attended a private or public high school. Panel B provides academic characteristics, such as GPA (only available for non-freshmen), average weekly hours spent on academics, and academic year distribution (Freshman, Sophomore, Junior, and Senior). Note that in column 1–2, GPA refers to self-reported first-year GPA while in column 3 it is the overall GPA during Spring 2024. Panel C summarizes the distribution of participants across different fields of study. Major groups are mutually exclusive.\r\n\r\n0.8250.5890.3680.106 R\r\n−\r\nsquared0.0770.0890.1160.058 N (Students)616616616616 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI usage frequency threshold and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column uses a different threshold for AI usage frequency, categorized as: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). Column 1 defines usage as any nonzero frequency; column 2 includes at least occasional use; column 3 includes frequent or higher use; and column 4 captures only very frequent use.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.0890.1990.3570.5510.801 R\r\n−\r\nsquared0.0450.0610.0880.0800.069 N (Students)633633633633633 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI adoption date and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column presents results for a different threshold of AI adoption. Column 1 shows the probability of adopting AI before Spring 2023; column 2 by Spring 2023; column 3 by Fall 2023; column 4 by Spring 2024; and column 5 by Fall 2024. The dependent variable in each regression is a binary indicator equal to one if the student had adopted AI by the specified time period.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.9120.6122.2850.9130.4191.8010.1930.484 R\r\n−\r\nsquared0.0360.0730.1260.0530.1170.1290.0420.063 N (Students)515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and their use of generative AI for academic tasks. In columns 1 and 4, the outcome is a dummy that equals one if a student reports using AI with any frequency for at least one augmentation or automation task, respectively. In columns 2 and 5, the outcome is the share of tasks within each category for which the student reports any use. In columns 3 and 6, the outcome is a continuous measure capturing average usage frequency for each task category, based on raw Likert-style responses. In columns 7 and 8, the outcome is the difference in average task share and usage intensity between augmentation and automation, respectively. Regressions are weighted and report robust standard errors clustered at the student level. ∗∗∗ \r\np\r\n<\r\n0.01\r\n, ∗∗ \r\np\r\n<\r\n0.05\r\n, ∗ \r\np\r\n<\r\n0.10\r\n.\r\n\r\nReferences\r\nAcemoglu, (2025)\r\nAcemoglu, D. (2025).Nobel lecture: Institutions, technology, and prosperity.American Economic Review, 115(6):1709–1748.\r\nAngrist et al., (2009)\r\nAngrist, J., Lang, D., and Oreopoulos, P. (2009).Incentives and services for college achievement: Evidence from a randomized trial.American Economic Journal: Applied Economics, 1(1):136–163.\r\nAutor and Thompson, (2025)\r\nAutor, D. and Thompson, N. (2025).Expertise.\r\nBailey et al., (2022)\r\nBailey, M., Johnston, D., Kuchler, T., Stroebel, J., and Wong, A. (2022).Peer effects in product adoption.American Economic Journal: Applied Economics, 14(3):488–526.\r\nBastani et al., (2025)\r\nBastani, H., Bastani, O., Sungu, A., Ge, H., Kabakcı, Ö., and Mariman, R. (2025).Generative AI without guardrails can harm learning: Evidence from high school mathematics.Proceedings of the National Academy of Sciences, 122(26):e2422633122.\r\nBeaman et al., (2021)\r\nBeaman, L., BenYishay, A., Magruder, J., and Mobarak, A. M. (2021).Can network theory-based targeting increase technology adoption?American Economic Review, 111(6):1918–1943.\r\nBick et al., (2025)\r\nBick, A., Blandin, A., and Deming, D. J. (2025).The rapid adoption of generative AI.Technical report, National Bureau of Economic Research.\r\nBonney et al., (2024)\r\nBonney, K., Breaux, C., Buffington, C., Dinlersoz, E., Foster, L. S., Goldschlag, N., Haltiwanger, J. C., Kroff, Z., and Savage, K. (2024).Tracking Firm Use of AI in Real Time: A Snapshot from the Business Trends and Outlook Survey.\r\nBonsu and Baffour-Koduah, (2023)\r\nBonsu, E. M. and Baffour-Koduah, D. (2023).From the consumers’ side: Determining students’ perception and intention to use chatgpt in ghanaian higher education.Journal of Education, Society & Multiculturalism, 4(1):1–29.\r\nBresnahan and Trajtenberg, (1995)\r\nBresnahan, T. F. and Trajtenberg, M. (1995).General purpose technologies ‘engines of growth’?Journal of econometrics, 65(1):83–108.\r\nBrynjolfsson and Hitt, (2000)\r\nBrynjolfsson, E. and Hitt, L. M. (2000).Beyond computation: Information technology, organizational transformation and business performance.Journal of Economic Perspectives, 14(4):23–48.\r\nBrynjolfsson et al., (2025)\r\nBrynjolfsson, E., Li, D., and Raymond, L. (2025).Generative AI at Work.The Quarterly Journal of Economics, 140(2):889–942.\r\nBushong and Gagnon-Bartsch, (2024)\r\nBushong, B. and Gagnon-Bartsch, T. (2024).Failures in forecasting: An experiment on interpersonal projection bias.Management Science, 70(12):8735–8752.\r\nCarvajal et al., (2024)\r\nCarvajal, D., Franco, C., and Isaksson, S. (2024).Will artificial intelligence get in the way of achieving gender equality?Discussion Paper 03, NHH Norwegian School of Economics.\r\n(15)\r\nComin, D. and Hobijn, B. (2010a).An exploration of technology diffusion.American Economic Review, 100(5):2031–2059.\r\n(16)\r\nComin, D. and Hobijn, B. (2010b).Technology Diffusion and Postwar Growth, volume 25, pages 209–246.University of Chicago Press.\r\nCui et al., (2024)\r\nCui, Z. K., Demirer, M., Jaffe, S., Musolff, L., Peng, S., and Salz, T. (2024).The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers.\r\nDavid, (1990)\r\nDavid, P. A. (1990).The dynamo and the computer: an historical perspective on the modern productivity paradox.The American economic review, 80(2):355–361.\r\nDell’Acqua et al., (2023)\r\nDell’Acqua, F., McFowland, E., Mollick, E. R., Lifshitz-Assaf, H., Kellogg, K., Rajendran, S., Krayer, L., Candelon, F., and Lakhani, K. R. (2023).Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality.\r\nDuflo et al., (2011)\r\nDuflo, E., Kremer, M., and Robinson, J. (2011).Nudging Farmers to Use Fertilizer: Theory and Experimental Evidence from Kenya.American Economic Review, 101(6):2350–2390.\r\nEloundou et al., (2024)\r\nEloundou, T., Manning, S., Mishkin, P., and Rock, D. (2024).GPTs are GPTs: Labor market impact potential of LLMs.Science, 384(6702):1306–1308.\r\nFairlie et al., (2014)\r\nFairlie, R. W., Hoffmann, F., and Oreopoulos, P. (2014).A community college instructor like me: Race and ethnicity interactions in the classroom.American Economic Review, 104(8):2567–2591.\r\nFelten et al., (2021)\r\nFelten, E. W., Raj, M., and Seamans, R. (2021).Occupational, Industry, and Geographic Exposure to Artificial Intelligence: A Novel Dataset and Its Potential Uses.\r\nFelten et al., (2023)\r\nFelten, E. W., Raj, M., and Seamans, R. (2023).How will Language Modelers like ChatGPT Affect Occupations and Industries?\r\nFoster and Rosenzweig, (1995)\r\nFoster, A. D. and Rosenzweig, M. R. (1995).Learning by doing and learning from others: Human capital and technical change in agriculture.Journal of political Economy, 103(6):1176–1209.\r\nFoster and Rosenzweig, (2010)\r\nFoster, A. D. and Rosenzweig, M. R. (2010).Microeconomics of technology adoption.Annu. Rev. Econ., 2(1):395–424.\r\nGallup, (2024)\r\nGallup (2024).AI in the Workplace: Answering 3 Big Questions.\r\nGiaccherini et al., (2019)\r\nGiaccherini, M., Herberich, D. H., Jimenez-Gomez, D., List, J. A., Ponti, G., and Price, M. K. (2019).The behavioralist goes door-to-door: Understanding household technological diffusion using a theory-driven natural field experiment.Technical report, National Bureau of Economic Research.\r\nGoehring et al., (2024)\r\nGoehring, G., Mezzanotti, F., and Ravid, S. A. A. (2024).Technology adoption and career concerns: Evidence from the adoption of digital technology in motion pictures.Technical report, National Bureau of Economic Research.\r\nGriliches, (1957)\r\nGriliches, Z. (1957).Hybrid corn: An exploration in the economics of technological change.Econometrica, 25(4):501–522.\r\nHall and Khan, (2003)\r\nHall, B. H. and Khan, B. (2003).Adoption of new technology.\r\n(32)\r\nHanda, K., Bent, D., Tamkin, A., McCain, M., Durmus, E., Stern, M., Schiraldi, M., Huang, S., Ritchie, S., Syverud, S., Jagadish, K., Vo, M., Bell, M., and Ganguli, D. (2025a).Anthropic education report: How university students use claude.\r\n(33)\r\nHanda, K., Tamkin, A., McCain, M., Huang, S., Durmus, E., Heck, S., Mueller, J., Hong, J., Ritchie, S., Belonax, T., et al. (2025b).Which economic tasks are performed with ai? evidence from millions of claude conversations.arXiv preprint arXiv:2503.04761.\r\nHanna et al., (2014)\r\nHanna, R., Mullainathan, S., and Schwartzstein, J. (2014).Learning Through Noticing: Theory and Evidence from a Field Experiment.The Quarterly Journal of Economics, 129(3):1311–1353.\r\nHartley et al., (2025)\r\nHartley, J., Jolevski, F., Melo, V., and Moore, B. (2025).The Labor Market Effects of Generative Artificial Intelligence.\r\nHumlum and Vestergaard, (2025)\r\nHumlum, A. and Vestergaard, E. (2025).Large Language Models, Small Labor Market Effects.\r\nHutto and Gilbert, (2014)\r\nHutto, C. and Gilbert, E. (2014).Vader: A parsimonious rule-based model for sentiment analysis of social media text.In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216–225.\r\nJehiel, (2018)\r\nJehiel, P. (2018).Investment strategy and selection bias: An equilibrium perspective on overoptimism.American Economic Review, 108(6):1582–1597.\r\nKelly et al., (2023)\r\nKelly, A., Sullivan, M., and Strampel, K. (2023).Generative artificial intelligence: University student awareness, experience, and confidence in use across disciplines.Journal of University Teaching and Learning Practice, 20(6):1–16.\r\nKharazian, (2025)\r\nKharazian, A. (2025).Ramp ai index: Monthly measurement of ai adoption by american businesses.Technical report, Ramp Economics Lab.\r\nLehmann et al., (2024)\r\nLehmann, M., Cornelius, P. B., and Sting, F. J. (2024).AI Meets the Classroom: When Does ChatGPT Harm Learning?\r\nLing and Imas, (2025)\r\nLing, Y. and Imas, A. (2025).Underreporting of AI use: The role of social desirability bias.Available at SSRN.\r\nLiu and Wang, (2024)\r\nLiu, Y. and Wang, H. (2024).Who on earth is using generative ai?Policy Research Working Paper WPS10870, World Bank Group, Washington, D.C.\r\nMcClain, (2024)\r\nMcClain, C. (2024).Americans’ use of ChatGPT is ticking up, but few trust its election information.\r\nMcDonald et al., (2025)\r\nMcDonald, N., Johri, A., Ali, A., and Collier, A. H. (2025).Generative artificial intelligence in higher education: Evidence from an analysis of institutional policies and guidelines.Computers in Human Behavior: Artificial Humans, 3:100121.\r\nMcElheran et al., (2023)\r\nMcElheran, K., Li, J. F., Brynjolfsson, E., Kroff, Z., Dinlersoz, E., Foster, L. S., and Zolas, N. (2023).AI Adoption in America: Who, What, and Where.\r\nMerchant, (2024)\r\nMerchant, B. (2024).The new luddites aren’t backing down.The Atlantic.Accessed: 2025-06-16.\r\nNam, (2023)\r\nNam, J. (2023).56% of college students have used ai on assignments or exams.Technical report.Edited by Lyss Welding. Accessed June 11, 2025.\r\nNolan, (2023)\r\nNolan, B. (2023).Here are the schools and colleges that have banned chatgpt.Business Insider.Accessed: 2025-01-27.\r\nNoy and Zhang, (2023)\r\nNoy, S. and Zhang, W. (2023).Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence.\r\nOtis et al., (2024)\r\nOtis, N. G., Delecourt, S., Cranney, K., and Koning, R. (2024).Global Evidence on Gender Gaps and Generative AI.Harvard Business School.\r\nPeng et al., (2023)\r\nPeng, S., Kalliamvakou, E., Cihon, P., and Demirer, M. (2023).The Impact of AI on Developer Productivity: Evidence from GitHub Copilot.\r\nRavšelj et al., (2025)\r\nRavšelj, D., Keržič, D., Tomaževič, N., Umek, L., Brezovar, N., A. Iahad, N., Abdulla, A. A., Akopyan, A., Aldana Segura, M. W., AlHumaid, J., Allam, M. F., Alló, M., Andoh, R. P. K., Andronic, O., Arthur, Y. D., Aydın, F., Badran, A., Balbontín-Alvarado, R., Ben Saad, H., Bencsik, A., Benning, I., Besimi, A., Bezerra, D. D. S., Buizza, C., Burro, R., Bwalya, A., Cachero, C., Castillo-Briceno, P., Castro, H., Chai, C. S., Charalambous, C., Chiu, T. K. F., Clipa, O., Colombari, R., Corral Escobedo, L. J. H., Costa, E., Cre\\textcommabelowtulescu, R. G., Crispino, M., Cucari, N., Dalton, F., Demir Kaya, M., Dumić-Čule, I., Dwidienawati, D., Ebardo, R., Egbenya, D. L., Faris, M. E., Fečko, M., Ferrinho, P., Florea, A., Fong, C. Y., Francis, Z., Ghilardi, A., González-Fernández, B., Hau, D., Hossain, M. S., Hug, T., Inasius, F., Ismail, M. J., Jahić, H., Jessa, M. O., Kapanadze, M., Kar, S. K., Kateeb, E. T., Kaya, F., Khadri, H. O., Kikuchi, M., Kobets, V. M., Kostova, K. M., Krasmane, E., Lau, J., Law, W. H. C., Lazăr, F., Lazović-Pita, L., Lee, V. W. Y., Li, J., López-Aguilar, D. V., Luca, A., Luciano, R. G., Machin-Mastromatteo, J. D., Madi, M., Manguele, A. L., Manrique, R. F., Mapulanga, T., Marimon, F., Marinova, G. I., Mas-Machuca, M., Mejía-Rodríguez, O., Meletiou-Mavrotheris, M., Méndez-Prado, S. M., Meza-Cano, J. M., Mirķe, E., Mishra, A., Mital, O., Mollica, C., Morariu, D. I., Mospan, N., Mukuka, A., Navarro Jiménez, S. G., Nikaj, I., Nisheva, M. M., Nisiforou, E., Njiku, J., Nomnian, S., Nuredini-Mehmedi, L., Nyamekye, E., Obadić, A., Okela, A. H., Olenik-Shemesh, D., Ostoj, I., Peralta-Rizzo, K. J., Peštek, A., Pilav-Velić, A., Pires, D. R. M., Rabin, E., Raccanello, D., Ramie, A., Rashid, M. M. U., Reuter, R. A. P., Reyes, V., Rodrigues, A. S., Rodway, P., Ručinská, S., Sadzaglishvili, S., Salem, A. A. M. S., Savić, G., Schepman, A., Shahpo, S. M., Snouber, A., Soler, E., Sonyel, B., Stefanova, E., Stone, A., Strzelecki, A., Tanaka, T., Tapia Cortes, C., Teira-Fachado, A., Tilga, H., Titko, J., Tolmach, M., Turmudi, D., Varela-Candamio, L., Vekiri, I., Vicentini, G., Woyo, E., Yorulmaz, Ö., Yunus, S. A. S., Zamfir, A.-M., Zhou, M., and Aristovnik, A. (2025).Higher education students’ perceptions of ChatGPT: A global study of early reactions.PLOS ONE, 20(2):e0315011.\r\nRogers, (1962)\r\nRogers, E. M. (1962).Diffusion of Innovations.Free Press, New York.\r\nRoss et al., (1977)\r\nRoss, L., Greene, D., and House, P. (1977).The “false consensus effect”: An egocentric bias in social perception and attribution processes.Journal of experimental social psychology, 13(3):279–301.\r\nSacerdote, (2001)\r\nSacerdote, B. (2001).Peer Effects with Random Assignment: Results for Dartmouth Roommates.The Quarterly Journal of Economics, 116(2):681–704.\r\nStinebrickner and Stinebrickner, (2008)\r\nStinebrickner, R. and Stinebrickner, T. R. (2008).The causal effect of studying on academic performance.The BE Journal of Economic Analysis & Policy, 8(1).\r\nStöhr et al., (2024)\r\nStöhr, C., Ou, A. W., and Malmström, H. (2024).Perceptions and usage of AI chatbots among students in higher education across genders, academic levels and fields of study.Computers and Education: Artificial Intelligence, 7:100259.\r\nStokey, (2021)\r\nStokey, N. L. (2021).Technology diffusion.Review of Economic Dynamics, 42:15–36.\r\nWorld Bank, (2016)\r\nWorld Bank (2016).World development report 2016: Digital dividends.World Bank Publications.\r\nWu et al., (2022)\r\nWu, M.-J., Zhao, K., and Fils-Aime, F. (2022).Response rates of online surveys in published research: A meta-analysis.Computers in Human Behavior Reports, 7.\r\nXiao et al., (2023)\r\nXiao, P., Chen, Y., and Bao, W. (2023).Waiting, banning, and embracing: An empirical analysis of adapting policies for generative ai in higher education.arXiv preprint arXiv:2305.18617.\r\nAppendix\r\n\r\nAppendix AAppendix Figures and Tables\r\nFigure A1:Generative AI Usage Survey Design Overview\r\nKey\t\r\nSurvey Structure\r\nSurvey Sections\r\nUsage Data Collection\r\nBeliefs & Perceptions\r\nAI Usage Survey at Middlebury College\r\nDecember 2024\r\nParticipant Recruitment\r\nCampus-wide email invitations, framed as general technology use survey\r\nParticipation Incentives\r\nAmazon gift card lottery ranging from $50-$500\r\nSection 1: Demographics & Academic Information\r\nStudent Characteristics\r\nGender, race/ethnicity, high school type\r\nAcademic Profile\r\nYear, major, self-reported GPA, study hours\r\nSection 2: generative AI Usage Patterns\r\nAdoption Metrics\r\nFrequency, timing of first use, specific AI models used\r\nAcademic Applications\r\nTasks performed with AI, payment for premium services\r\nSection 3: Perceptions & Institutional Policies\r\nImpact Perceptions\r\nEffects on learning, grades, time management\r\nPolicy Responses\r\nUsage likelihood under different policies\r\nPeer Usage Beliefs\r\nEstimates of AI adoption among peers for schoolwork and leisure\r\nOpen-Ended Response Collection\r\nMotivations for AI use, policy feedback, suggestions for improvement\r\nNote: This figure illustrates the structure of the AI usage survey conducted at Middlebury College in December 2024. The survey collected information across three main sections: (1) demographic and academic background, (2) patterns of generative AI usage including adoption timing, frequency, and specific applications, and (3) perceptions of AI’s impact on learning and responses to institutional policies.\r\n\r\nFigure A2:Cumulative Generative AI Use by Student Characteristic\r\nPanel A. By Gender\r\nRefer to caption\r\nPanel B. By Race\r\nRefer to caption\r\nPanel C. By High School Type\r\nRefer to caption\r\nPanel D. By Freshman Year GPA\r\nRefer to caption\r\nPanel E. By Cohort\r\nRefer to caption\r\nPanel F. By Field of Study\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents cumulative AI use based on different student characteristics. Each panel displays the cumulative distribution of AI use based on a specific characteristic: gender, race, school type, first-year GPA, cohort, or field of study. The cumulative percent of students is plotted against usage categories. The legends and colors correspond to subgroups within each demographic variable.\r\n\r\nFigure A3:Percent of Students Who Pay for Generative AI Tools\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of AI users who pay for AI tools (through any platform) across different demographic groups. Horizontal lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure A4:Academic Uses of ChatGPT: Evidence from Global Survey\r\nPanel A. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\nPanel B. Across the College Quality Distribution\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of students who use ChatGPT for different academic tasks based on data from Ravšelj et al., (2025). Panel A displays usage patterns across tasks categorized as augmenting (proofreading, translating, study assistance, research assistance) versus automating (academic writing, professional writing, creative writing, brainstorming, summarizing, calculating, coding assistance, personal assistance) student effort. Panel B shows usage patterns across university quality quintiles based on World University Rankings, with universities ranked in the top 20% (top quintile) showing slightly higher rates of augmentation relative to automation compared to bottom quintile institutions. The analysis includes universities with at least 30 student responses and excludes observations with missing usage data.\r\n\r\nFigure A5:Student Beliefs about AI Usage versus Actual Usage under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure compares students’ beliefs about AI usage with actual usage rates under different AI policies. For each policy type, blue bars show the percent of students who report being “Neutral,” “Likely,” or “Very Likely” to use AI, while red bars show students’ mean beliefs about what percent of their peers use AI in classes with that policy. The sample includes all survey respondents for both actual usage and beliefs measures. “AI Use is Unrestricted” refers to classes with no restrictions on AI use, “No Explicit AI Policy” refers to classes without a stated policy, and “AI Use is Prohibited” refers to classes that ban AI use entirely.\r\n\r\nFigure A6:Relationship Between Beliefs About AI Usage and Actual AI Usage\r\nRefer to caption\\justify\r\nNotes: This figure shows the relationship between students’ beliefs about AI usage among their peers and actual AI usage rates across different demographic groups. Each point represents a different group of students (by demographics, academic characteristics, or field of study). The \r\ny\r\n-axis shows the percent of students in each group who report using AI for academic purposes. The \r\nx\r\n-axis shows the mean belief within each group about what percent of Middlebury students use AI. The dashed line shows the linear fit. Sample includes all survey respondents with at least ten observations per group. Students’ beliefs about AI usage are positively correlated with actual usage patterns, suggesting that students have relatively accurate perceptions of AI adoption among their peers.\r\n\r\nGPA > p50 80.3 26.3 17.4 23.2 13.4 GPA < p50 87.1 18.7 26.3 32.2 9.9 Freshman 81.3 24.3 24.1 24.7 8.1 Sophomore 84.4 25.6 22.6 23.7 12.4 Junior 86.2 21.6 23.1 28.6 12.8 Senior 78.7 21.0 17.4 29.9 10.4\r\n\r\nPanel C. Field of Study    Arts 73.3 21.0 0.0 52.3 0.0 Humanities 74.8 39.3 21.8 12.1 1.7 Languages 57.4 27.0 30.3 0.0 0.0 Literature 48.6 10.0 33.7 5.0 0.0 Natural Sciences 91.1 22.0 21.9 33.8 13.3 Social Sciences 84.6 17.9 20.4 29.1 17.2\r\n\r\n\\justify\r\nNotes: This table presents the percent of students in each demographic group who report using AI at different frequencies during the academic semester. Each cell shows the percent of students within that group. Column 1 reports the total percent who use AI at any frequency. Columns 2 to 5 represent increasing usage frequencies: rarely (1–2 times per semester), occasionally (monthly), frequently (weekly), and very frequently (multiple times per week). Panel A reports percentages by demographic characteristics. Panel B shows percentages by academic characteristics. Panel C presents percentages by field of study.\r\n\r\n0.9730.1350.0770.1100.114 R\r\n−\r\nsquared0.0120.0510.0270.0240.138 N (Students)516516516516516 \\justifyNotes: This table assesses the relationship between AI model adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI model usage (columns 1-4) or payment for AI services (column 5), and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division.\r\n\r\nEach column presents results for a different model or payment outcome. Column 1 shows usage of OpenAI’s ChatGPT, column 2 Google Gemini, column 3 Microsoft Copilot, column 4 any other AI model, and column 5 whether the student pays for any generative AI service.\r\n\r\nThe omitted categories are Natural Sciences for academic division, white students for race/ethnicity, and freshmen for cohort. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.7510.6760.6610.2710.479 R\r\n−\r\nsquared0.0870.1130.0760.0680.041 N (Students)599599599599599 \\justifyNotes: This table examines how student characteristics relate to self-reported likelihood of using generative AI under different policy scenarios. Each column presents results for different policy scenarios. In columns 1-4, the dependent variable equals one if the student reports being“neutral,” “likely” or “extremely likely” to use AI under the specified policy, and zero if they report being “unlikely,” or “extremely unlikely.” Column 5 represents the impact of moving from unrestricted use to complete prohibition.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.5760.6850.3760.4890.564 R\r\n−\r\nsquared0.1020.1190.0840.0370.087 N (Students)561565543560560 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. Each column presents results for beliefs about different academic outcomes: learning ability (e.g., ability to grasp concepts, retain information, or learn new skills) in column 1, understanding of course materials in column 2, course grades in column 3, ability to complete assignments on time in column 4, and time spent on academics in column 5. The dependent variable in each regression equals one if the student believes AI somewhat improves” or significantly improves” the outcome, and zero if they believe it has no effect, reduces, or significantly reduces the outcome. “Don’t know” responses are excluded.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\nAppendix BEmpirical Appendix\r\nB.1Field of Study Classifications\r\nThis appendix details the classification of majors into broad fields of study used in Figure 1:\r\n\r\n• Natural Sciences: Includes Biology, Biochemistry, Chemistry, Computer Science, Earth and Climate Sciences/Geology, Environmental Studies, Mathematics, Molecular Biology & Biochemistry, Neuroscience, Physics, and Statistics.\r\n• Social Sciences: Includes Anthropology, Economics, Education, Geography, International & Global Studies, International Politics & Economics, Political Science, Psychology, and Sociology.\r\n• Humanities: Includes American Studies, Black Studies, Classics, History, Architectural Studies, Art History & Museum Studies, History of Art & Architecture, Philosophy, Religion, and Classical Studies.\r\n• Literature: Includes Comparative Literature, English/English & American Literatures, and Literary Studies.\r\n• Languages: Includes Arabic, Chinese, French & Francophone Studies, German, Japanese Studies, Russian, and Spanish.\r\n• Arts: Includes Film & Media Culture, Music, Studio Art, and Theatre.\r\nB.2Task-Specific Use of Generative AI\r\nThe aggregate patterns documented in Section 4 may mask important heterogeneity across student groups. Different students may adopt AI tools at varying rates depending on their academic needs and field-specific norms. To investigate this heterogeneity systematically, Appendix Table LABEL:tab:ai_purpose_correlates presents regression estimates examining how student characteristics predict AI usage for each of our ten measured academic tasks.\r\n\r\nUsage patterns differ markedly by student characteristics. Male students consistently exhibit higher adoption rates across most applications, with particularly pronounced differences for finding information (16.4 pp; \r\np\r\n<\r\n0.01\r\n), summarizing texts (12.5 pp; \r\np\r\n<\r\n0.01\r\n), and creating images (16.9 pp; \r\np\r\n<\r\n0.01\r\n). Black students show substantially higher usage for information-gathering tasks, being 23.8 pp more likely to use AI for finding information compared to white students (\r\np\r\n<\r\n0.05\r\n). They also exhibit dramatically higher adoption of writing assistance tools, with 27.6 pp higher usage for editing text (\r\np\r\n<\r\n0.05\r\n) and 23.6 pp for writing emails (\r\np\r\n<\r\n0.10\r\n). Latino students demonstrate significantly higher usage for generating ideas (24.0 pp; \r\np\r\n<\r\n0.01\r\n) and writing emails (20.7 pp; \r\np\r\n<\r\n0.01\r\n), while Asian students show markedly higher adoption for writing emails (24.5 pp; \r\np\r\n<\r\n0.01\r\n) and explaining concepts (10.9 pp, \r\np\r\n<\r\n0.05\r\n). Students from public high schools report lower usage rates for several applications, particularly for concept explanation (7.3 pp lower; \r\np\r\n<\r\n0.10\r\n) and writing emails (7.4 pp lower; \r\np\r\n<\r\n0.10\r\n), but higher usage for proofreading (8.7 pp higher; \r\np\r\n<\r\n0.10\r\n).\r\n\r\nField of study emerges as a particularly strong predictor of usage patterns. Arts majors show significantly lower adoption across multiple tasks compared to Natural Science students, with gaps of 49.7 pp for finding information (\r\np\r\n<\r\n0.01\r\n), 44.0 pp for generating ideas (\r\np\r\n<\r\n0.05\r\n), and 62.1 pp for coding assistance (\r\np\r\n<\r\n0.01\r\n). Similarly, humanities majors are 36.8 pp less likely to use AI for generating ideas (\r\np\r\n<\r\n0.01\r\n) and 30.8 pp less likely for coding assistance (\r\np\r\n<\r\n0.01\r\n). Languages majors exhibit the most pronounced differences, being 52.4 pp less likely to use AI for summarizing texts (\r\np\r\n<\r\n0.01\r\n) and 46.9 pp less likely for generating ideas (\r\np\r\n<\r\n0.01\r\n). By contrast, social science students show higher usage for explaining concepts (8.0 pp; \r\np\r\n<\r\n0.10\r\n) and summarizing texts (14.7 pp; \r\np\r\n<\r\n0.01\r\n).\r\n\r\n0.8030.7400.6310.6190.5410.4730.3710.3440.2350.204 R\r\n−\r\nsquared0.0780.0870.0770.1160.0280.0540.0830.1350.0400.094 N (Students)515515515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and use of AI for specific academic tasks. Each column shows the result for a different academic task. The omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Regressions are weighted and use heteroskedasticity-robust standard errors clustered at the student level. ∗∗∗, ∗∗, and ∗ indicate significance at the 1%, 5%, and 10% levels, respectively.\r\n\r\nAppendix CQualitative Evidence on Student Perspectives on AI Use\r\nIn this section, we analyze student responses to an open-ended question about their use of generative AI. The survey question asked: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” This was an optional question that 48.3 percent of survey respondents answered. Appendix Figure C1 presents a word cloud of the most frequent words in student responses.\r\n\r\nC.1Validating the Open-Ended Response Measure\r\nWe begin by validating our open-ended response measure to show that it contains meaningful signal. To validate this measure, we employ sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner), a widely-used lexicon and rule-based sentiment analysis tool (Hutto and Gilbert, , 2014). We calculate sentiment scores for each response and examine whether these scores correlate with students’ actual AI adoption behaviors. Intuitively, students who express more positive sentiment toward AI in their open-ended responses should be more likely to have adopted AI tools in practice. We test this hypothesis by examining two measures of AI adoption: whether students have ever used generative AI and whether they currently use AI for academic purposes. Appendix Figure C2 presents binned scatterplots showing the relationship between AI adoption rates and standardized sentiment scores.\r\n\r\nSentiment towards generative AI strongly predicts AI adoption. For both outcomes, students with negative sentiment scores (below zero) show substantially lower adoption rates—around 70-85 percent for ever using AI and 70-75 percent for academic use. In contrast, students with positive sentiment scores display markedly higher adoption rates, reaching nearly 100 percent for general use and 95-100 percent for academic purposes among those with the most positive sentiment. The relationship is particularly pronounced for academic AI use (Panel B), where the coefficient of \r\nβ\r\n^\r\n=\r\n0.168\r\n is twice as large as for general adoption.17 These systematic patterns confirm that our open-ended responses capture meaningful variation in student attitudes that corresponds to real behavioral differences.\r\n\r\nC.2How Students use Generative AI\r\nTo analyze these responses systematically, we classified each response using keywords based on their content. For example, if a student mentioned using AI to save time, we tagged the response with the keyword “time-saver.” If a student expressed concerns about AI’s impact on learning, we tagged it with “negative learning.” Responses could receive multiple keywords if they discussed several themes. Appendix Figure C3 shows the frequency of keywords in our classification. The responses reveal how students use AI tools, what motivates this use, and what causes some to avoid or limit their use.\r\n\r\nThe most common use of AI is as an explanatory tool. Nearly 30 percent of responses mentioned using AI to understand course material. Students frequently ask AI to break down complex concepts from readings and lectures, particularly when they find the material difficult to understand. For example, one student reported: “I can ask AI to explain concepts to me that I have a hard time grasping. […] I can keep asking ’simplify’ or ’break down even more.”’ Students also use AI to summarize dense academic readings, which they argue helps them manage heavy reading loads.\r\n\r\nStudents employ AI throughout different stages of the writing process. Some use AI to generate initial drafts that serve as starting points. One student explained: “Helps me get started with a base for most of my essays. It feels easier to edit something already written and make it my own than to write from scratch.” Others use AI more narrowly for brainstorming when stuck on specific problems. As one student noted: “I use it if I am feeling stuck to push me to the right direction (whether a mathematical problem or an essay idea).” Many also report using AI as an editing tool to improve grammar, sentence structure, and overall writing flow. This is especially the case for non-native English speakers. As one student explained: “English is not my first language and it frustrates me sometimes that I cannot find the best way to phrase a certain idea and AI is a useful tool to have to find alternate expressions.”\r\n\r\nStudents frequently mentioned using AI for specific academic tasks. In courses that require coding, students often use AI for debugging code and understanding programming concepts. Students also employ AI for administrative tasks like formatting citations and drafting routine emails. Finally, many students use AI as an enhanced search engine. One student reported: “It has significantly reduced the time it takes to conduct research on new topics and ideas, and helps me by giving me a thorough selection of sources to use for projects of any kind.”\r\n\r\nC.3Why Students Adopt AI\r\nTime savings was the most commonly cited reason for using AI. Nearly 30 percent of responses mentioned using AI to complete work more efficiently. Students often viewed AI as a way to manage demanding course loads. Many students particularly embrace AI assistance for tasks viewed as mechanical or administrative. A student noted they use AI for “Writing emails quick and creating resume/ cover letter templates.” But AI assistance goes beyond grunt work. Some students use it to “spend less time doing assignments and homework.” This is particularly true if students don’t view the work as central to their academic experience. As one student explained: “when I come across work I deem as ineffective, I want to spend as little time as possible doing it.”\r\n\r\nHaving an on-demand tutor for academic support was another key motivation. One-quarter of responses described using AI as an “explainer” when other resources were unavailable or inconvenient. As one student noted, “I use it as a last resort (if there are no office hours, after looking up videos, etc.) if I need extra help. I’d like to think that the way I use it is similar to going to office hours or TA hours.”\r\n\r\nPeer influence also drove AI adoption. Some students reported feeling pressure to use AI to remain competitive with their classmates. One student explained: “I noticed others use it, are getting better grades than me, and they say they learn better with the help of AI, so I gave it a try.” Others worried about being at a competitive disadvantage: “Other people were using it and told me about it. I felt like I would be at a disadvantage if I wasn’t also using it.”\r\n\r\nC.4Concerns and Limitations\r\nStudents expressed several concerns about AI use in academic work. The most frequent worry was about negative impacts on learning. One student who initially used AI extensively reported: “In the past, I have simply plugged and chugged homework assignments into ChatGPT and submitted it. Those assignments feedback from teachers was positive and I was getting good grades, but I definitely felt that my own learning outcomes to be significantly worse.” Other students viewed AI use as fundamentally incompatible with their educational goals. As one humanities student explained: “my task is as a humanities student is to think, not calculate; why should I let AI do the thinking for me? It would defeat the purpose of pursuing my education.”\r\n\r\nMany students described ethical concerns about AI use. Responses suggested uncertainty about appropriate boundaries. One student noted: “I never use it to explicitly write something because that feels like overt cheating, but sometimes I hesitate when it completely solves Econ problems. I understand how it does it, and it helps me to learn, but it still sometimes feels a little morally gray.” Another expressed similar ambivalence: “I tend to only use it when [it] will save me time in a moral way.”\r\n\r\nStudents also emphasized the importance of maintaining ownership of their work. Many expressed pride in producing original work and hesitation about diluting that ownership through AI use. As one student explained: “I don’t have interest in using generative AI for my academic work because I want my work to reflect my own ideas.” Another noted: “It usually would not even occur to me to turn to AI to substitute writing because I want to take credit for my work, and using AI seems to diminish that.”\r\n\r\nTechnical limitations deterred some students from using AI tools. Students reported concerns about inaccurate outputs (“hallucinations”) and poor output quality, particularly for creative writing or complex analytical tasks. For example, one student noted that “In my poetry class we were instructed to use it to come up with poems and they were awful, so that kinda turned me away from using it to do my work for me.”\r\n\r\nC.5Discussion\r\nWe conclude with two overarching themes that emerged from the responses.\r\n\r\nFirst, students vary substantially in how they incorporate AI into their academic lives. Crucially, this heterogeneity largely depends on what students perceive as “appropriate” uses of AI. For activities that they perceive as core to their academic journey—like writing essays or solving problems—many students hesitate to use AI. A student articulated this clearly: “Most of my work is writing or reading. If I’m not doing the writing, what is the purpose of me taking the class?” Yet, students draw different boundaries between central tasks and grunt work. Some use AI extensively, viewing their role as akin to a manager that provides high-level direction while AI handles implementation. Others restrict AI use to specific tasks like brainstorming, editing, or drafting emails. Still others avoid AI entirely for academic work, often for ethical reasons. Even among AI users, adoption patterns reflect individual trade-offs between time savings, learning goals, and academic integrity concerns.\r\n\r\nSecond, there appears to be a fundamental tension between efficiency and learning. The time-saving benefits are easy to observe, quantifiable, and tangible. But these time savings are unlikely to be a free lunch. Some benefits may come at the cost of spending less time with material that requires deeper engagement to digest. As one student noted: “There may be a negative effect in that it eliminates much of the ’struggle’ in learning.” Yet, having an on-demand tutor that explains concepts in relatable ways can also improve learning. As one student explained: “It can explain concepts to me in a way that is tailored to my learning style.” This suggests that the impact of AI use on learning outcomes depends not on whether students use AI—almost all do, to some extent—but rather on how they employ these tools.\r\n\r\nFigure C1:Word Cloud of Student Motivations for Generative AI Use\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 147 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?’\r\n\r\nFigure C2:Relationship Between AI Sentiment and AI Adoption\r\nPanel A. Ever Used Generative AI\r\nRefer to caption\r\nPanel B. Uses AI for Academic Purposes\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents the relationship between AI sentiment and AI adoption. Panel A shows the proportion of respondents who have ever used generative AI, while Panel B shows the proportion who use AI for academic purposes. Each point represents the mean adoption rate for respondents within sentiment score bins of width 0.2. Sentiment scores are standardized compound scores computed using Hutto and Gilbert, (2014)’s VADER algorithm applied to responses to an open-ended question about generative AI. Positive values indicate positive sentiment and negative values indicate negative sentiment. The dashed lines show OLS best-fit lines estimated on the microdata, with coefficients and standard errors (in parentheses) displayed in the top-left corner of each panel.\r\n\r\nFigure C3:Frequency of Keywords in Student Motivations for AI Use\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to AI use. The responses come from the question “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” Color coding indicates the category of each theme. Usage type refers to how students use AI tools. Motivation captures what drove students to try AI. Concerns include mentions of course policies and academic integrity, individual reservations about AI use, worries about AI’s impact on education, and AI’s technical limitations.\r\n\r\nAppendix DQualitative Evidence on Student Views of AI Policies\r\nIn this section, we analyze student responses to an open-ended question about Middlebury’s AI policies. The survey asked: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Appendix Figure D1 presents a word cloud of the most frequent words in student responses. To analyze these responses systematically, we classified each response using keywords based on their content. Appendix Figure D2 shows the frequency of keywords in our classification.\r\n\r\nD.1Polarized Views on Generative AI Policy Approaches\r\nStudents expressed markedly different views about appropriate AI policies, revealing fundamental disagreement about the path forward. Some strongly advocated for embracing AI technology. As one student argued, “The tool is there, there is supply and there is demand. Don’t fight another war on drugs. Don’t live in a fake reality”. Others called for significant restrictions, arguing that “the use of generative AI is dishonest and corrosive” and that it “prohibits these organic processes and divorces students from true learning”.\r\n\r\nHowever, the most common position advocated for a balanced approach that would allow beneficial uses while restricting harmful ones. Students distinguished between uses that enhance learning (like concept explanation) and those that substitute for learning (like generating entire essays). One student articulated this nuanced view particularly well: “AI also can really be helpful at explaining a textbook problem that doesn’t make sense, or guiding slightly with homework, or creating study materials, or editing/tightening up your prose. All of those things are good, and universities should figure out how to maximize AI use for those reasons and to minimize students just feeding their problem sets into ChatGPT”\r\n\r\nA recurring theme was the futility of blanket bans. Many students emphasized that prohibition would be ineffective, with one noting “I don’t think anyone really cares what the policy of any given class is. If professors want people to not use it, they need to structure assessments in a way that will discourage use.” Another compared AI bans to restricting internet use, arguing “AI policies seem to be totally irrelevant. It’s like telling people they can’t use the internet as a resource for the class.” This ineffectiveness of bans creates fairness concerns. As one student explained: “I think if it is banned in a class, that should be enforced (and right now it absolutely is not)… As with any form of cheating, those who don’t cheat are put at a disadvantage.” Another student expressed similar frustration: “I find it discouraging when I hear classmates saying they use AI for things such as essays when they use it in dishonest ways.”\r\n\r\nD.2Need for Clear Guidelines\r\nThe most frequently expressed concern was the need for clear guidelines about generative AI use. Several students reported confusion about what constitutes acceptable use, with one noting “I think it should be more clear whether we can use it and how and how to cite it since most professors rarely mention it at all”. Other students emphasized the importance of professors explaining their policies upfront and their rationale, with one stating “I think that Professor’s should be very specific about what is allowed and their reasoning behind their policy”.\r\n\r\nMany students advocated for standardization across classes, observing that “Sometimes its confusing when one class allows it and another doesn’t and the other encourages it and so on so if there was a school wide or department wide policy that could help”. Yet there is also disagreement regarding standardization. Some students preferred leaving the decision to individual professors, arguing that “GenAI is more effective in some classes/majors than others. Making sure professors understand how students use GenAI and how useful GenAI is in their class (given the course structure, nature of assignments/material, etc.) is very important for the class policy.”\r\n\r\nD.3Training and Support Services\r\nStudents strongly emphasized the need for training in appropriate generative AI use. Many suggested that the college should provide guidance on using AI tools effectively while maintaining academic integrity. One student proposed “a workshop that teaches you to effectively use GenAI without violating the honor code”. Students also expressed interest in learning how to leverage AI to enhance their learning experience rather than circumvent it. As one student explained: “I think it could be useful to develop some sort of training. How do we use AI in a way that actually benefits our learning? I tried out some things on my own but I feel that I need more guidance.”\r\n\r\nThis desire for training was often linked to workplace preparedness. Students recognized that AI proficiency would be valuable in their careers, with one noting “As the world uses more and more AI, I think it is an important tool that students should know how to leverage”. Another emphasized: “The moment us students leave campus, we will be using it in the professional world, and when used in combination with one’s own skills, it is merely a tool to maximize efficiency”.\r\n\r\nD.4Discussion\r\nThe responses reveal several key tensions in AI policy implementation. First, students express a desire to use AI in ways that enhance rather than substitute for learning, yet they recognize that blanket bans are ineffective and worry about being disadvantaged if they follow restrictions while others do not.\r\n\r\nSecond, while students desire clear guidelines, they also want flexibility to accommodate legitimate uses that vary by discipline and assignment type. Different courses and majors may find different AI uses appropriate based on their learning objectives and assessment types.\r\n\r\nThird, there is tension between faculty autonomy in setting course policies and students’ desire for consistent institutional standards. While some support letting professors determine appropriate AI use for their specific courses, others argue that varying policies across classes create confusion and enforcement challenges.\r\n\r\nFinally, students report significant variation in faculty attitudes toward AI. Some students perceived faculty fear or misunderstanding of AI tools, noting that categorical bans often reflect a lack of understanding about AI’s capabilities and limitations rather than pedagogical considerations.\r\n\r\nFigure D1:Word Cloud of Student Feedback on Generative AI Policies\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 133 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?”\r\n\r\nFigure D2:Frequency of Keywords in Student Feedback on AI Policies\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to Middlebury’s AI policies. The responses come from the question “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Color coding indicates the category of each theme. Policy Approach captures suggestions about how AI should be regulated at the college. Adoption Views reflect positions on whether and how AI should be integrated into academic work. Training Needs indicates requests for guidance and support. Other Concerns include issues of workplace relevance and fairness.\r\n\r\nSee pages 1 of results/ai-survey-midd.pdf See pages 2- of results//ai-survey-midd.pdf",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== story.txt ===\nI remember the moment that sparked this project. It was during a walk across Middlebury’s campus late one fall afternoon—just after office hours. A student had come in, clearly bright but struggling, and told me they’d been using ChatGPT late at night to help with assignments because they were too anxious to speak up in class. That confession stuck with me.\r\n\r\nLater, over coffee with Germán, we started wondering—how many students were turning to generative AI as a quiet lifeline? Not to cheat, but to learn? That simple question became the seed for this entire study. What we found wasn’t just statistics. It was a story of adaptation, of students reshaping how they learn, and of institutions racing to catch up.\"\n\n==================================================\n\n=== Generative AI in Higher Education E.txt ===\nGenerative AI in Higher Education: Evidence from an Elite College†\r\nZara Contractor\r\nGermán Reyes\r\nAbstract\r\nGenerative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT’s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI’s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI’s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\r\n\r\n1Introduction\r\nThe rapid advancement of generative artificial intelligence (AI) is transforming higher education at an unprecedented pace. The launch of ChatGPT and similar tools has introduced technologies capable of performing tasks central to academic assessment and learning—writing essays, solving complex problems, and explaining intricate concepts—instantly and at near-zero marginal cost. Yet systematic evidence on the nature and implications of this new technology remains scarce. How widespread is generative AI adoption among students, and what factors drive it? Do students primarily use AI to augment their learning or to automate coursework, potentially harming human capital development? Could disparities in access to premium AI resources amplify existing educational inequalities?\r\n\r\nThese questions are at the heart of ongoing debates about the role of AI in education, which have relied heavily on anecdotal evidence and speculation rather than rigorous evidence. As a result, universities have implemented policies that vary dramatically, from outright bans to permissive adoption, often without clear evidence on their effectiveness or unintended consequences (Nolan, , 2023; Xiao et al., , 2023; McDonald et al., , 2025).\r\n\r\nThis paper addresses this gap by systematically examining generative AI adoption at Middlebury College, a highly selective liberal arts college in Vermont. With approximately 2,800 undergraduate students, Middlebury offers 49 majors across the arts, humanities, languages, social sciences, and natural sciences. Over the past decade, Middlebury has consistently ranked among the top 10 liberal arts colleges in U.S. News & World Report, with an average ranking of 8.5 between 2015 and 2025. The college’s selectivity is comparable to that of many elite research universities—Middlebury’s acceptance rate of approximately 10 percent is similar to that of Boston University (11 percent), Georgetown University (13 percent), and the University of Virginia (16 percent).\r\n\r\nOur analysis draws on survey data collected from the student population between December 2024 and February 2025. The survey collected detailed information about AI usage, including the frequency and purpose of use, perceived impacts, and responses to institutional policies. To minimize selection bias, we framed the survey broadly as examining technology use and provided incentives for participation, achieving a 22.9 percent response rate with 634 responses representing a broad cross-section of the student population.\r\n\r\nWe document five main findings. First, generative AI is approaching near-universal adoption at an unprecedented speed. Over 80 percent of students use AI for academic purposes, up from less than 10 percent before Spring 2023. This represents one of the fastest technology adoption episodes ever documented, dramatically exceeding the 40 percent adoption rate among U.S. workers (Bick et al., , 2025) and the 23 percent among all U.S. adults (McClain, , 2024). These levels are consistent with international findings of 50–70 percent adoption in university contexts (Nam, , 2023; Stöhr et al., , 2024; Ravšelj et al., , 2025).\r\n\r\nSecond, AI adoption is markedly unequal across academic disciplines and demographic groups. Field of study is the strongest predictor of adoption, likely reflecting how well AI capabilities align with the academic tasks required across fields. Adoption ranges from 91.1 percent in Natural Sciences majors (including mathematics and computer science) to significantly lower levels in Literature (48.6 percent) and Languages (57.4 percent). Additionally, demographic disparities exist. For example, males adopt AI at higher rates than females (88.7 versus 78.4 percent), a pattern that is consistent with documented AI gender gaps in other contexts (Otis et al., , 2024). Most notably, lower-achieving students have a higher adoption rate than their higher-achieving peers (87.1 versus 80.3 percent). As a result, AI could serve as an equalizing force if it enhances student learning—helping struggling students catch up to their peers—but could also widen achievement gaps if AI undermines skill development.\r\n\r\nThird, generative AI transforms the students’ learning production function by both augmenting student capabilities and automating academic tasks. We classify academic tasks as augmentation when AI enhances human capabilities while maintaining student engagement (e.g., explaining concepts, proofreading) versus automation when AI directly produces outputs with minimal cognitive involvement (e.g., writing essays, creating images), and average usage rates across tasks within each category. We find that 61.2 percent of AI users employ these tool for augmentation purposes, while 41.9 percent use it for automation. Qualitative evidence reinforces these patterns: students describe AI as an “on-demand tutor” for augmentation purposes, particularly valuable when traditional resources like office hours are unavailable, while automation use centers on time savings during periods of overwhelming workload. These self-reported patterns align closely with actual usage data from Claude conversation logs (Handa et al., 2025a,).\r\n\r\nFourth, institutional policies can significantly influence AI adoption, though their effectiveness can be undermined by informational frictions. We find that explicit prohibitions dramatically reduce self-reported intended use 39 percentage points (pp). Importantly, the effects of this policy vary substantially across student groups, with females showing larger reductions in usage under prohibition than males, suggesting that one-size-fits-all policies may inadvertently create disparate impacts across groups. Moreover, we also document substantial informational gaps about institutional AI policies. Many students (19.2 percent) do not understand AI policy rules, few know about institution-provided premium AI resources (10.1 percent) or proper AI citation practices (32.6 percent)—a skill that is necessary for academic integrity.\r\n\r\nFifth, students believe that AI has a positive learning impact and these beliefs strongly predict their own AI usage. Most students believe that AI improves their understanding of course materials (70.2 percent) and learning ability (60.1 percent), though fewer believe it improves grades (41.1 percent). Perhaps unsurprisingly, students who believe that AI improves their academic outcomes are significantly more likely to use these tools. For example, a ten-percentage-point increase in the belief that AI improves learning corresponds to a 4.9 percentage-point increase in adoption, highlighting how perceptions can affect technology diffusion in educational settings.\r\n\r\nTaken together, our findings reveal that generative AI has already reshaped students’ college experience. The substantial information gaps that we document—regarding both permitted uses and available resources—suggest straightforward opportunities for policy intervention. Yet the observed heterogeneity across disciplines and student groups indicates that uniform policies risk unintended consequences: blanket prohibitions may disproportionately disadvantage students who benefit most from AI augmentation, while unrestricted use may encourage automation practices detrimental to skill development. Thus, effective policy requires distinguishing clearly between AI uses that enhance learning and those that undermine it.\r\n\r\nWhile our findings are specific to the population we study—students at an elite liberal arts college—this setting offers valuable into AI’s broader societal impact. The rapid uptake of generative AI in higher education may reflect a generalizable principle regarding the diffusion of generative AI: adoption may be fastest in contexts where AI consolidates fragmented tools into a unified platform. Students have long had access to various tools for the academic tasks for which they now use generative AI.1 What distinguishes AI is its ability to provide all these services through a single, instantly accessible interface at virtually zero marginal cost. This consolidation likely drives students’ perception of enhanced learning efficiency and contributes to the fast adoption. This principle may extend well beyond education: industries with fragmented, specialized tools could experience similar trajectories of rapid AI adoption, even if AI itself does not introduce fundamentally new capabilities.\r\n\r\nOur findings contribute to a rapidly growing literature examining the adoption and impacts of generative AI. Recent work has focused on AI’s effects on worker productivity (Noy and Zhang, , 2023; Dell’Acqua et al., , 2023; Peng et al., , 2023; Cui et al., , 2024; Brynjolfsson et al., , 2025) and its potential to transform occupations (Felten et al., , 2021, 2023; Eloundou et al., , 2024). While several papers document AI adoption in workplace settings (Bick et al., , 2025; Hartley et al., , 2025; Humlum and Vestergaard, , 2025) and at the firm level (McElheran et al., , 2023; Bonney et al., , 2024; Kharazian, , 2025), we examine adoption in higher education—a critical setting where future high-skilled workers develop human capital.\r\n\r\nWe add to an emerging literature examining generative AI in education. Some studies focus on the learning impacts of generative AI, finding mixed impacts depending on the setting (e.g., Lehmann et al., , 2024; Bastani et al., , 2025). A complementary strand of the literature documents adoption patterns across diverse educational contexts, including universities in Australia (Kelly et al., , 2023), Ghana (Bonsu and Baffour-Koduah, , 2023), Norway (Carvajal et al., , 2024), Sweden (Stöhr et al., , 2024), and multi-country studies (Ravšelj et al., , 2025). To the best of our knowledge, this paper provides the first systematic evidence on generative AI adoption in U.S. higher education. Importantly, we focus specifically on how students use AI for academic purposes—unlike studies that measure more general usage including personal lives (e.g., Ravšelj et al., , 2025)—and go beyond documenting adoption rates to examine how students integrate AI into their academic workflows, distinguishing between augmentation and automation, the role of student beliefs about AI’s educational impacts, and how institutional policies shape usage patterns.\r\n\r\nOur findings also contribute to the literature on technology diffusion. Unlike the S-shaped adoption curves documented for most technologies (Griliches, , 1957; Rogers, , 1962), we observe extremely rapid adoption of generative AI, with 80 percent uptake within two years of ChatGPT’s release. This pattern contrasts sharply with the adoption trajectories of historical General Purpose Technologies (GPTs) like electricity, which required decades to achieve widespread use (David, , 1990; Bresnahan and Trajtenberg, , 1995). The rapid diffusion we document aligns with evidence that newer technologies diffuse faster than older ones (Comin and Hobijn, 2010a,; Comin and Hobijn, 2010b,). Two factors likely explain AI’s exceptional adoption speed compared to other GPTs. First, unlike electricity or steam power, generative AI requires minimal physical infrastructure—students access it through existing devices—and is available at no cost to consumers, eliminating the financial barriers that typically slow technology adoption. Second, as noted above, AI consolidates multiple specialized tools into a single platform, making its benefits immediately apparent without requiring specialized training or organizational restructuring, unlike previous GPTs (Brynjolfsson and Hitt, , 2000).\r\n\r\n2Data: Novel Student Survey\r\n2.1Recruitment and Structure\r\nWe conducted the survey from December 2024 to February 2025. All Middlebury College students were contacted via email and received a reminder a few weeks and two months after the initial invitation. To minimize selection bias, the recruitment materials described the survey broadly as a study on students’ use of technology in their academic and personal lives. To incentivize participation, students who completed the survey qualified for entry into a lottery for Amazon gift cards ranging in value from $50 to $500. The full survey instrument is provided in Appendix LABEL:app:survey.\r\n\r\nThe survey contains three main sections (see Appendix Figure A1 for the survey flow). First, we gather demographic and academic information, including gender, race/ethnicity, type of high school attended (private or public), current academic year, and declared or intended major.2 We also collect data on academic inputs and performance through students’ self-reported typical weekly hours spent studying and their first-year GPA.3\r\n\r\nSecond, we measure students’ experience with generative AI tools. We begin by asking whether students have ever used generative AI tools like ChatGPT or Claude. For those who have, we collect information about their usage patterns, including frequency of use during the academic semester, which specific AI models they use, and whether they pay for AI tools. We also gather data on how students use AI for different academic tasks, including writing assistance, learning support, and coding.\r\n\r\nThe final part of the survey elicits students’ beliefs about generative AI’s adoption and impacts. We ask students about their perceptions of AI’s effects on their academic experience across multiple dimensions, including learning, grades, time management, and understanding of course material. We then collect information on the role of institutional policies, asking students how different policy environments influence their likelihood of using AI. We also elicit students’ beliefs about AI usage among their peers, including their estimates of the fraction of Middlebury students who use AI for schoolwork and leisure. The survey concludes with two open-ended questions that allow students to share their experiences with AI in academic settings and provide feedback on the college’s AI policies and support services.\r\n\r\n2.2Sample and Summary Statistics\r\nOut of Middlebury’s 2,760 enrolled students, 739 began the survey. We exclude 105 respondents who left the survey before reaching the generative AI usage module, leaving us with an analysis sample of 634 students (22.9 percent of the student body). This response rate is comparable to that of similar surveys (Wu et al., , 2022). To make our sample more representative of Middlebury’s student body, we construct poststratification weights based on the distribution of declared majors from administrative records.4 Specifically, we weight observations by the ratio of each major’s share in the student population to its share in our survey responses.5\r\n\r\nTable Figures and Tables presents summary statistics for our sample. Column 1 reports unweighted survey averages, column 2 shows averages after applying poststratification weights, and column 3 provides population benchmarks from administrative records where available. Our unweighted sample consists of 44.6 percent male and 50.8 percent females. The racial/ethnic composition includes 61.8 percent white, 15.5 percent Asian, 9.9 percent Hispanic, and 3.6 percent Black students. The majority of students (54.3 percent) attended a public school, while 42.0 percent attended a private high school. Our sample represents 43 different majors across seven fields of study, with 31.1 percent of respondents not yet having declared a major. In our analysis, we group these undeclared students by their intended field of study as reported in the survey.\r\n\r\nComparing our unweighted sample to administrative records reveals notable differences. Our sample overrepresents white students (61.8 versus 53.8 percent) and Asian students (15.5 versus 7.3 percent), while underrepresenting Black students (3.6 versus 5.2 percent) and Hispanic students (9.9 versus 12.4 percent). First-year students are overrepresented (31.1 versus 25.5 percent), while senior students are underrepresented (21.3 versus 28.7 percent). Our weighting procedure partially addresses these discrepancies—particularly in the distribution of academic fields, where the weighted figures closely approximate administrative records—but some demographic differences persist. Despite these differences, our weighted sample provides reasonably close approximations to the college population on most dimensions, particularly for academic characteristics and field distributions.\r\n\r\n3Generative AI Usage Patterns Among Students\r\n3.1Adoption of Generative AI\r\nGenerative AI has achieved a remarkably high adoption rate among Middlebury College students. Figure 1 presents the distribution of AI usage frequency during the academic semester, categorized into four levels: “Rarely” (a few times per semester), “Occasionally” (a few times per month), “Frequently” (a few times per week), and “Very Frequently” (daily or almost daily). Overall, 82.5 percent of students report using generative AI for academic purposes, with substantial variation in usage intensity: 23.5 percent use it rarely, 22.2 percent occasionally, 26.2 percent frequently, and 10.6 percent very frequently.\r\n\r\nThe adoption rate at Middlebury aligns closely with patterns observed across other higher education institutions. A BestColleges survey of U.S. undergraduate and graduate students in fall 2023 found that 56 percent had used AI on assignments or exams (Nam, , 2023). Ravšelj et al., (2025) surveyed higher education students globally between late 2023 and early 2024, finding that 71.4 percent had ever used ChatGPT—though this figure encompasses all usage rather than academic use specifically. Similarly, Stöhr et al., (2024) document that 63 percent of Swedish university students had used ChatGPT by spring 2023, with 35.4 percent reporting regular use and 27.6 percent rare use. Among Norwegian university students, Carvajal et al., (2024) find that 68.9 percent use AI tools occasionally or more frequently.\r\n\r\nThese adoption rates in higher education far exceed those in the general population and workforce. Pew Research finds that only 23 percent of U.S. adults have ever used ChatGPT (McClain, , 2024), while Gallup reports that just one-third of U.S. workers have used AI for work (Gallup, , 2024). Bick et al., (2025) and Hartley et al., (2025) estimate that about 40 percent of the U.S. working-age population used generative AI for work as of late 2024 and early 2025. Both studies document substantial heterogeneity across industries, with information services showing the highest adoption rates at 56–62 percent—still considerably below the 80 percent adoption rate at Middlebury. Even among workers in AI-exposed occupations, Humlum and Vestergaard, (2025) find adoption rates of only 41 percent.6\r\n\r\n3.2Adoption by Student Characteristics and Field of Study\r\nAI adoption varies considerably across demographic groups and academic disciplines (Figure 1 and Appendix Table LABEL:tab:ai_freq_het). Males report higher usage rates than females (88.7 versus 78.4 percent). Usage patterns differ markedly by race/ethnicity: Black students (92.3 percent) and Asian students (91.3 percent) exhibit the highest adoption rates, while white students (80.2 percent) and Hispanic students (77.9 percent) report lower usage. Students from private high schools use AI more frequently than those from public schools (84.1 versus 80.4 percent). Notably, students with below-median GPAs report higher usage rates than their higher-achieving peers (87.1 versus 80.3 percent). Adoption varies widely by field of study: Natural Sciences leads with 91.1 percent usage, followed by Social Sciences at 84.6 percent, while Languages (57.4 percent) and Literature (48.6 percent) show substantially lower adoption rates.\r\n\r\nTo examine how student characteristics jointly relate to AI adoption, we estimate multivariate OLS regressions that include all observed characteristics simultaneously.7 Table LABEL:tab:ai_usage_correlates presents regression estimates using four usage thresholds as outcomes. Each column represents the probability of meeting progressively higher frequency thresholds: any AI use (column 1), at least monthly use (column 2), at least weekly use (column 3), and daily use (column 4).\r\n\r\nThe regression results confirm the patterns observed in the descriptive statistics. Holding other characteristics constant, males are 10.3 pp more likely than females to use AI (column 1, \r\np\r\n<\r\n0.05\r\n), with this gender gap typically widening at higher usage frequencies (columns 2–4, all \r\np\r\n<\r\n0.01\r\n). Black and Asian students show substantially higher adoption rates than white students, at 11.8 and 10.7 pp respectively (column 1, both \r\np\r\n<\r\n0.01\r\n). Students from public high schools are 3.0 pp less likely to use AI than those from private schools, but this difference is not statistically significant. Field of study emerges as the strongest predictor of adoption. Compared to Natural Sciences majors, students in Literature, Languages, Arts, and Humanities all show lower usage rates, with the differences being statistically significant at high usage frequencies (columns 3 and 4). Social Sciences majors exhibit adoption rates similar in magnitude to Natural Sciences majors across all usage frequency thresholds.\r\n\r\nOur findings on heterogeneity in AI adoption align with patterns documented in other settings. The gender gap in AI adoption at Middlebury—10.3 pp higher for males—is consistent with evidence across multiple studies. A meta-analysis of 18 studies by Otis et al., (2024) finds that males are 10–20 percentage points more likely to use generative AI than females. This gender gap in AI adoption appears in all educational studies (Nam, , 2023; Carvajal et al., , 2024; Stöhr et al., , 2024; Ravšelj et al., , 2025). Our finding that students with below-median GPAs are more likely to use AI aligns with Carvajal et al., (2024), who document higher adoption rates among students with lower admission grades. This pattern of greater adoption among lower-achieving students suggests that AI could narrow achievement gaps if it enhances learning and skill development, but could widen these gaps if it undermines the acquisition of fundamental skills.\r\n\r\nThe differences in adoption rates across academic fields at Middlebury mirror patterns documented in other educational settings. Stöhr et al., (2024) find that technology and engineering students exhibit significantly higher ChatGPT usage compared to students in humanities. Similarly, Nam, (2023) report that 62 percent of business majors and 59 percent of STEM majors have used AI tools for coursework, compared to 52 percent of humanities majors. Ravšelj et al., (2025) document comparable disciplinary differences, with applied sciences students showing substantially higher usage rates than arts and humanities students.\r\n\r\nThese academic differences persist into the workforce. Bick et al., (2025) find stark variation by college major: STEM graduates have the highest AI adoption rates (46.0 percent), followed by Business/Economics graduates (40.0 percent), while Liberal Arts graduates show substantially lower rates (22.4 percent). Humlum and Vestergaard, (2025) document similar patterns by occupation, finding that roles requiring strong writing and technical skills—such as marketing specialists and journalists—have the highest adoption rates. These consistent patterns across educational and professional contexts suggest that field-specific factors, particularly the applicability of AI tools to different types of tasks, shape adoption in systematic ways.\r\n\r\n3.3Timing of Generative AI Adoption\r\nThe speed of technology diffusion is a critical determinant of its economic and social impact (David, , 1990; Hall and Khan, , 2003; Stokey, , 2021). In educational contexts, rapid technology adoption can create or exacerbate inequalities between early and late adopters, particularly if the technology confers significant learning advantages (World Bank, , 2016). To track the timing of AI adoption among Middlebury students, we asked them when they first began using generative AI for academic purposes, with options ranging from “This semester (Fall 2024)” to “Before Spring 2023” (as a reference, ChatGPT’s public launch was in November 2022).\r\n\r\nStudents adopted generative AI at an extraordinary pace. Figure 2 shows that the cumulative adoption rate grew dramatically from less than 10 percent before Spring 2023 to slightly above 80 percent by Fall 2024. The pace of adoption has accelerated over time, likely reflecting improvements in AI capabilities. Among current users, 25.7 percent adopted AI in Fall 2024 alone, compared to 19.9 percent in Spring 2024 and 16.3 percent in Fall 2023.\r\n\r\nThe adoption rate among Middlebury students far exceeds that observed in other populations and represents one of the fastest technology adoption episodes ever documented. For comparison, Bick et al., (2025) show that it took over 20 years for computers to reach an 80 percent adoption rate in the U.S. working-age population, and about 15 years for internet adoption to reach similar levels. Even generative AI adoption in the broader population has been markedly slower: Pew Research found that just 23 percent of U.S. adults had ever used ChatGPT as of February 2024, up from 18 percent in July 2023 (McClain, , 2024). The dramatically faster adoption rate we document—reaching over 80 percent in less than two years—suggests academic settings may uniquely accelerate AI diffusion.\r\n\r\nTo identify early versus late adopters of generative AI, in Table LABEL:tab:ai_adopt_correlates and Appendix Figure A2, we analyze how adoption timing varies across student characteristics. The rate of adoption varies substantially across student characteristics. Male students led adoption, with a 8.9 pp higher probability of using AI before Spring 2023 compared to females (column 1, \r\np\r\n<\r\n0.01\r\n)—a gender gap that persists across all periods (columns 2–5). Black and Asian students also adopted earlier than white students, though these differences reach statistical significance only in later periods (columns 4–5). Field of study is a strong predictor of adoption timing. For example, students in Languages consistently lagged behind Natural and Social Sciences majors, with significantly lower adoption rates across nearly all time periods.\r\n\r\n3.4Choice of Generative AI Models\r\nMajor AI companies operate on a freemium model, where free versions coexist with premium subscriptions that offer higher usage limits and access to more advanced models. This tiered structure creates potential for a new form of educational disparities: if paid versions confer substantial academic advantages, students who cannot afford subscriptions may be systematically disadvantaged. To investigate these potential disparities, we presented respondents with a list of options including both free and paid versions of popular models. We also collected information on monthly subscription expenditures, with response options ranging from no active subscription to spending more than $40 monthly (standard subscriptions to major models like ChatGPT cost $20 per month at the time).\r\n\r\nOpenAI’s ChatGPT dominates AI usage among Middlebury AI users, with the free version capturing the largest market share. Figure 3 shows that 89.3 percent of AI users rely on the free version of ChatGPT, making it dramatically more popular than any alternative. Google Gemini (13.5 percent) and Microsoft Copilot (7.7 percent) are distant competitors, while other platforms each capture less than 5 percent of users. This dominance mirrors patterns in other academic settings, where Stöhr et al., (2024) find substantially higher familiarity with ChatGPT compared to alternative platforms, and aligns with broader workforce trends documented by Bick et al., (2025).8\r\n\r\nDespite near-universal AI adoption, only 11.3 percent of AI users pay for any AI service.9 This figure is remarkably similar to the 8 percent found in Ravšelj et al., (2025)’s multi-country survey. This low payment rate suggests that for most students, the premium features of paid versions—primarily higher usage limits and access to more advanced models—do not justify the subscription cost. However, payment patterns reveal significant disparities: males and Asian students are substantially more likely to purchase AI subscriptions (Appendix Table LABEL:tab:ai_model_correlates and Appendix Figure A3), potentially reflecting differences in usage intensity across demographic groups.\r\n\r\n4Generative AI and the Production of Learning\r\n4.1The Use of Generative AI across Academic Tasks\r\nHow is generative AI transforming the traditional inputs to student learning? The educational production function includes inputs like time spent studying (e.g., Stinebrickner and Stinebrickner, , 2008), faculty instruction (e.g., Fairlie et al., , 2014), peer interactions (e.g., Sacerdote, , 2001), and academic support services (e.g., Angrist et al., , 2009). AI tools have the potential to complement or substitute for these traditional inputs. For example, using AI to explain concepts might substitute for faculty office hours, while using it for proofreading might reduce time needed for academic support services.\r\n\r\nTo understand the role of AI in students’ learning production function, we collected information about students’ AI usage across ten common academic tasks: proofreading, generating ideas, writing essays, editing essay drafts, coding assistance, creating images, explaining concepts, composing emails, summarizing materials, and finding information. For each task, students indicated their frequency of AI use on a five-point scale ranging from never to daily. We supplemented this quantitative data with open-ended responses about how AI influences their academic work process.\r\n\r\nStudents use generative AI for a wide range of academic tasks, with the highest adoption rates for learning support and text-processing activities. Figure 4, Panel A shows that explaining concepts is the most common use case, with 80.3 percent of AI users using it for this task. Summarizing texts follows as the second most common task (74.0 percent), followed by finding information and generating ideas (63.1 and 61.9 percent). Writing assistance tasks like proofreading and editing essays are also common, used by 54.1 and 47.3 percent of AI users, respectively. Technical applications like coding help are significant (34.4 percent), considering that many academic degrees involve no programming. Notably, while 23.5 percent of AI users report using it for writing essays, this represents a relatively low adoption rate compared to other academic uses, suggesting students may be more hesitant to use AI for primary content creation. The lowest adoption rate is for creating images at 20.4 percent, likely reflecting fewer academic use cases for this capability.\r\n\r\nAn important limitation of our survey is that it relies on self-reported usage, which may introduce measurement error. For example, students might underreport uses they perceive as academically inappropriate—such as essay writing—while overreporting those viewed as legitimate learning tools (Ling and Imas, , 2025). To assess the validity of our self-reported measures, we compare our findings with Handa et al., 2025a, who analyze actual Claude usage patterns among users with university email addresses. Several caveats apply: their data captures conversation-level interactions rather than student-level usage, and most students in our sample use ChatGPT rather than Claude (Section 3.4). Despite these limitations, the comparison provides a useful benchmark for evaluating our survey responses.\r\n\r\nReassuringly, our results are consistent with Anthropic’s data. Both studies identify explaining concepts and technical problem-solving as primary use cases. Handa et al., 2025a report that the second largest use case (33.5 percent of conversations) involves “technical explanations or solutions for academic assignments,” while we find that 80.3 percent of AI users use it for explaining concepts—a difference likely attributable to our student-level versus their conversation-level measurement. Similarly, the most common usage category in Claude involves “designing practice questions, editing essays, or summarizing academic material” (39.3 percent of conversations), aligning with our findings that 74.0 percent of AI users use it for summarizing texts and 47.3 percent for editing essays. The disciplinary patterns also converge: Handa et al., 2025a finds that computer science, natural sciences, and mathematics conversations are overrepresented, which mirrors our finding that Natural Science majors show significantly higher AI adoption rates (Figure 1). Overall, these convergent findings from self-reported survey data and actual usage logs suggest that our results capture genuine patterns of student AI engagement rather than merely reflecting social desirability in responses.\r\n\r\n4.2Automation versus Augmentation\r\nAre students using generative AI primarily to augment their learning or to automate their coursework? This distinction is crucial for understanding AI’s impacts (Autor and Thompson, , 2025)—augmentation may enhance students’ learning processes while maintaining their active engagement and critical thinking, whereas automation produces fully-formed outputs that could be submitted with minimal student input, potentially harming learning.\r\n\r\nTo examine this empirically, Figure 4, Panel B categorizes the ten measured tasks based on whether they augment or automate academic tasks. We classify tasks as augmentation when they enhance human capabilities (explaining concepts, finding information, proofreading, and editing drafts) and as automation when they directly produce outputs (writing essays, creating images, composing emails, summarizing texts, generating ideas, and coding assistance). We then calculate the percentage of AI users who employ each category at various frequencies.10\r\n\r\nStudents use generative AI for both augmentation and automation, but with markedly different frequency. While 61.2 percent of AI users report using AI for augmentation tasks, 41.9 percent report using it for automation—thus, there is a substantial 19.3 pp difference. This gap is mainly driven by occasional use (19.7 percent for augmentation versus 12.7 percent for automation) and frequent use (16.3 versus 9.0 percent). Most strikingly, students are more than twice as likely to use AI daily for augmentation (5.0 percent) compared to automation (2.5 percent). The higher frequency of augmentation use also suggests that students find augmentation uses more valuable for their day-to-day academic activities. Our findings align closely with patterns observed in actual AI usage data: Handa et al., 2025b analyze real conversation logs from Claude and find that 57 percent of workplace AI interactions involve augmentation while 43 percent involve automation.\r\n\r\nQualitative evidence from open-ended responses provides additional insights into students’ motivations for using AI (see Appendix C for additional results and validation of the open-ended measure). Students’ descriptions align closely with the augmentation-automation framework. For augmentation, many characterize AI as an “on-demand tutor,” particularly valuable when traditional resources like office hours are unavailable. Non-native English speakers frequently mention using AI for proofreading to overcome language barriers, while students from technical majors describe using it to debug code and understand error messages. For automation, time savings emerged as the dominant motivation, with 21.7 percent of open-ended responses explicitly mentioning efficiency benefits (Appendix Figure C3). Students describe turning to AI during periods of overwhelming workload or looming deadlines, using it to generate initial drafts or complete routine assignments. That students automate tasks mostly when time-pressed helps explain why automation is less common than augmentation.11\r\n\r\nOur finding that students favor augmentation over automation extends beyond Middlebury. In Appendix Figure A4, we re-analyze data from Ravšelj et al., (2025)—a multi-country survey of higher education students—to examine how this balance varies across institutional quality.12 We classify universities into quintiles based on their Times Higher Education World University Rankings. Overall, students who use AI worldwide show similar adoption rates for augmentation (64.6 percent) and automation (63.1 percent) tasks—a much smaller gap than at Middlebury (Panel A). However, this aggregate pattern masks heterogeneity by institutional quality. Students at top-quintile universities show a modest preference for augmentation over automation, with this gap narrowing monotonically down the institution quality distribution. By the bottom quintile, augmentation and automation usage are virtually identical (Panel B).\r\n\r\n4.3Heterogeneity in Augmentation versus Automation Usage\r\nUnderstanding whether augmentation and automation patterns vary across student populations may help to design targeted support policies. To assess this, we construct four measures of augmentation and automation usage. First, we create binary indicators for whether students use AI for any augmentation or automation task. Second, we calculate the proportion of tasks in each category for which students employ AI. Third, using the Likert-scale responses, we create intensity measures that capture how frequently students use AI for augmentation and automation purposes. Finally, we compute the difference between augmentation and automation variables, as a measure that directly compare students’ relative preference for augmentation versus automation. Table LABEL:tab:ai_autom_regs presents regression estimates using these measures as outcomes.\r\n\r\nThe balance between augmentation and automation varies substantially by student characteristics. Males show higher adoption of both augmentation and automation tasks, with a similar magnitude in each case—they are 6.0 pp more likely to use augmentation prompts and 5.5 pp more likely to use automation prompts compared to females (columns 1 and 4, \r\np\r\n<\r\n0.05\r\n). Black students have a substantially higher augmentation intensity than white students (0.871 points higher, equivalent to 38.1 percent of the outcome mean, \r\np\r\n<\r\n0.01\r\n), but show no statistically significant differences in automation usage. This results in Black students having the strongest preference for augmentation over automation, with a 19.5 pp higher share of augmentation tasks relative to automation (column 7, \r\np\r\n<\r\n0.01\r\n). Asian students show high usage of both kinds relative to white students. Differences across field of study effects are sizable: humanities, languages, and literature majors tend to show lower usage of both augmentation and automation tools compared to natural science majors.\r\n\r\n5Institutional Policies and AI Adoption\r\n5.1The Role of Institutional Policies in Shaping Student Behavior\r\nInstitutional policies are central to shaping the adoption and diffusion of new technologies (Acemoglu, , 2025). In the context of generative AI in higher education, understanding how policies affect student behavior is essential for guiding evidence-based decision-making. To examine this, we asked students to report their likelihood of using AI under various policy scenarios, ranging from complete prohibition to unrestricted use. This analysis is particularly relevant in light of the ongoing debate about how universities should regulate generative AI use in academic settings (Nolan, , 2023), and the widespread variation in institutional policies across colleges (Xiao et al., , 2023).\r\n\r\nInstitutional policies substantially influence students’ reported likelihood of using generative AI. Figure 5 shows that when generative AI use is unrestricted, 52.4 percent of students report being likely or extremely likely to use it. This likelihood decreases modestly when policies require citation (40.9 percent) or when no explicit policy exists (42.3 percent). However, explicit prohibition creates a dramatic shift: only 13.4 percent of students report they would be likely or extremely likely to use AI when it is banned, while 72.9 percent say they would be unlikely or extremely unlikely to do so (note that these figures include both current AI users and students who do not use generative AI for academic purposes). These results suggest that institutional policies can significantly influence generative AI usage patterns, though a small fraction of students report they would likely use AI even when explicitly prohibited.\r\n\r\nThe magnitude of policy effects in our study aligns closely with findings from other contexts. Carvajal et al., (2024) estimate that banning AI reduces usage by 37.2 pp among females and 20.6 pp among males, for an overall drop of about 28.9 pp. In our survey, a ban leads to a 37.8 pp decline in usage, with a larger decrease among females (49.6 pp) than males (40.1 pp). These parallel results underscore how institutional policies can unintentionally produce disparate effects across gender. Notably, these differential policy effects extend beyond gender. Other demographic and academic characteristics—such as race and field of study—also moderate how students respond to policy restrictions (Appendix Table LABEL:tab:ai_policy_het), underscoring that institutional policies can produce non-neutral impacts along multiple dimensions.\r\n\r\n5.2Understanding of Institutional Policies and Resources\r\nInformation gaps and inattention can significantly affect technology adoption decisions (Duflo et al., , 2011; Hanna et al., , 2014). For example, imperfect information about rules, available resources, or proper usage guidelines could lead to underadoption of beneficial technologies or inadvertent policy violations. To test for the existence of information gaps, we examine three dimensions of policy understanding. First, we asked students whether they find AI policies in their current classes clear. Second, we measured awareness of free access to premium AI tools through the college—a resource that could reduce inequality but only if students know it exists. Third, we assessed whether students know how to properly cite AI when required, a mechanical skill necessary for academic integrity. We supplemented these measures with open-ended feedback about the college’s AI policies and support services.\r\n\r\nStudent understanding of institutional policies is high, though significant gaps remain. Figure 6, Panel A shows that most students (79.1 percent) understand when and where they are allowed to use AI in their classes, but a nontrivial minority (19.2 percent) find AI policies unclear. Moreover, critical knowledge gaps persist elsewhere. Only 10.1 percent know they have free access to Microsoft Copilot through the college (Panel B), and just 32.6 percent understand how to properly cite AI use (Panel C). These gaps vary systematically: females show better policy understanding than males (81.5 versus 75.8 percent), and non-white students demonstrate higher awareness across all three dimensions compared to white students.13\r\n\r\nThe qualitative evidence from open-ended responses reinforces these information frictions and reveals implementation challenges (Appendix D). Students express frustration with vague guidelines, requesting specific examples of acceptable versus unacceptable use cases. Many advocate for formal training, noting that simply knowing policies exist differs from understanding how to effectively integrate AI into their workflow. A particularly striking theme is students’ perception of blanket prohibitions as both ineffective and unfair, as they create a prisoner’s dilemma situation where compliant students are disadvantaged relative to those who secretly violate restrictions. Many responses call for a balanced approach—permitting AI use that supports learning while restricting uses that replace it—though the boundary between the two remains contested.\r\n\r\n6Beliefs About AI’s Educational Impact and Peer Usage\r\n6.1Student Beliefs of AI’s Impact on Educational Outcomes\r\nTechnology adoption decisions are shaped by beliefs about potential returns (Foster and Rosenzweig, , 2010). Students’ perceptions of how AI affects their learning may influence whether they adopt these tools and how they integrate them into their academic workflows. To elicit these beliefs, we asked students to evaluate AI’s impact across four dimensions of their academic performance: understanding of course materials, overall learning ability, time management, and course grades. For each dimension, students rated AI’s effect on a five-point scale ranging from “significantly reduces” to “significantly improves.” These subjective assessments provide insight into the perceived value of AI tools from the student perspective, which may differ from their actual effects on learning outcomes.\r\n\r\nStudents tend to believe that generative AI is beneficial for their academic performance, though the perceived benefits vary across dimensions. Figure 7 shows that the majority of students (70.2 percent) believe that generative AI improves their understanding of course materials, and 60.1 percent report improvement in their ability to grasp concepts, retain information, or learn new skills. Similarly, 59.4 percent report that AI improves their ability to complete assignments on time. Notably, while students believe AI helps their learning and assignment completion, they are less confident about its impact on course grades—41.1 percent believe it improves their grades, while 55.4 percent report no effect and 3.5 percent report negative effects. This pattern suggests that while students perceive generative AI as improving their learning process and workflow—through better understanding, skill development, and timely completion of work—these benefits do not necessarily translate into better course grades.14\r\n\r\nBeliefs about AI’s benefits strongly predict adoption. In Figure 8, we plot the relationship between the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis) for different subgroups of students (e.g., males, white students, public-school students, etc.). Across all four measured academic dimensions, there is a strong and statistically significant relationship between perceiving positive AI effects and AI adoption. The relationship is strongest for beliefs about the ability to improve course grades (Panel D): a 10-pp increase in the belief that AI improves grades is associated with a 5.6 pp increase in AI adoption (\r\np\r\n<\r\n0.01\r\n). Similar positive relationships exist for beliefs about learning ability (4.3 pp, \r\np\r\n<\r\n0.05\r\n), understanding of course materials (4.5 pp, \r\np\r\n<\r\n0.01\r\n), and timely assignment completion (5.3 pp, \r\np\r\n<\r\n0.05\r\n). These findings suggest that student beliefs about AI’s academic benefits—irrespective of the actual benefits—may play a crucial role in shaping adoption decisions.\r\n\r\nThese positive beliefs align with findings from other contexts. Ravšelj et al., (2025) report that the majority of students in their sample believe that ChatGPT improves their general knowledge (68.8 percent) and specific knowledge (62.7 percent)—remarkably similar to our finding that most students believe AI improves understanding of course materials (68.5 percent) and learning ability (57.6 percent). Similarly, 57.4 percent of students in Ravšelj et al., (2025)’s sample believe that ChatGPT helps meet assignment deadlines, while 59.4 percent in our data report AI improves timely assignment completion. Stöhr et al., (2024) provide complementary evidence from Swedish universities, where 47.7 percent of students believe that AI makes them more effective learners, yet only 17.3 percent believe these tools improve their grades—mirroring our finding that perceived learning benefits from AI use exceed perceived effects on course performance.\r\n\r\n6.2Student Beliefs About Peer Use of Generative AI\r\nStudents’ beliefs about their peers’ AI usage may influence their own adoption through multiple channels, including social norms (e.g., Giaccherini et al., , 2019), social learning (e.g., Foster and Rosenzweig, , 1995; Beaman et al., , 2021), peer effects (e.g., Bailey et al., , 2022), and competitive pressure to avoid falling behind (e.g., Goehring et al., , 2024). To measure these beliefs, we asked students to estimate what fraction of their peers use generative AI for different purposes and would use under different policy environments. Figure 9 presents the distribution of these beliefs. Panels A-C show students’ estimates of peer AI usage for schoolwork, leisure, and any purpose respectively. Panels D-F display students’ beliefs about AI usage under three policy environments: when classes have no explicit AI policy, when classes allow AI use, and when classes prohibit AI use.\r\n\r\nStudents systematically underestimate their peers’ AI usage. On average, they believe 65.2 percent of peers regularly use generative AI for schoolwork (Panel A), while our survey reveals an actual usage rate of 82.5 percent—a 17.3 pp gap. This underestimation appears consistent across educational contexts: Stöhr et al., (2024) find that only 38.7 percent of Swedish students believe AI chatbot use is common among peers, despite 63 percent actual usage. When we examine beliefs about policy-contingent behavior, students estimate that 62.7 percent of peers use AI when no explicit policy exists (Panel D), rising to 72.2 percent under permissive policies (Panel E) and falling to 43.5 percent under prohibition (Panel F). These beliefs align directionally with self-reported intentions, though with notable magnitude differences (Appendix Figure A5). For instance, while students believe 43.5 percent of their peers use AI in classes that prohibit it, only 28.4 percent report they themselves would be likely to do so.15\r\n\r\nNotably, students’ perceptions of peer AI adoption are closely linked to their own adoption behavior. Appendix Figure A6 shows a strong positive association between beliefs about peer usage and actual usage rates across student groups. This behavior is consistent with several psychological mechanisms, including the “false consensus effect” (Ross et al., , 1977), selection neglect (Jehiel, , 2018), or interpersonal projection bias (Bushong and Gagnon-Bartsch, , 2024). Still, virtually all groups systematically underestimate peer usage—actual usage rates exceed believed usage for every demographic and academic group we examine. This figure also shows that underestimation varies substantially across student characteristics: males use AI at 88.7 percent but believe only 66.6 percent of peers do so (22.1 pp gap), while females show a smaller gap (77.6 percent actual versus 65.1 percent believed, 13.3 pp gap).\r\n\r\nTaken together, students’ systematic underestimation of peer AI usage coupled with the strong relationship between beliefs and adoption suggests that misperceptions about social norms may shape technology diffusion. If AI enhances learning, then correcting these misperceptions through information provision could accelerate beneficial adoption; conversely, if AI undermines skill development, then students’ underestimation of peer usage may serve as an unintentional safeguard against harmful overadoption.\r\n\r\n7Discussion\r\nThis paper presents the first systematic evidence on generative AI adoption at a highly selective U.S. college. Using novel survey data, we document exceptionally rapid and widespread adoption, substantial shifts in the educational production function through augmentation and automation, and the significant roles of students’ beliefs and institutional policies in shaping AI use.\r\n\r\nOur results offer three implications for institutional policy and ongoing debates about AI in education. First, we identify low-cost opportunities to improve institutional policy effectiveness through targeted information provision. The significant gaps we document in students’ understanding of institutional AI policies, citation practices, and available AI resources suggest that simple interventions—such as clear guidelines, illustrative examples of acceptable uses, and AI literacy programs—can reduce unintentional academic integrity violations and support beneficial AI integration. Qualitative feedback strongly indicates student demand for more explicit guidance on responsible AI use.\r\n\r\nSecond, our evidence challenges alarmist narratives that conflate widespread AI adoption with universal academic dishonesty based on anecdotal accounts.16 Although AI use is indeed near-universal, we find clear evidence that students primarily employ AI as a tool for strategic task management: to enhance learning (augmentation) and selectively automate tasks when facing high time opportunity cost—not solely to circumvent academic effort. This distinction matters: by normalizing academic dishonesty as inevitable and universal, these narratives may shift social norms and encourage students who would otherwise use AI responsibly to engage in prohibited behaviors, believing “everyone else is doing it.”\r\n\r\nThird, our findings caution against policy extremes of either blanket prohibition or unrestricted AI use (Merchant, , 2024; McDonald et al., , 2025). Blanket prohibitions disproportionately harm students who benefit most from AI’s augmentation functions—particularly lower-achieving students—while also creating uneven compliance, placing conscientious students at a disadvantage relative to rule-breakers. Conversely, unrestricted AI use based solely on revealed preference arguments ignores important market failures in educational settings. Students often hold overly optimistic beliefs about AI’s learning benefits despite mixed empirical evidence, potentially leading to unintended negative learning outcomes. Most concerningly, permissive policies risk creating competitive dynamics where students feel compelled to adopt AI not for its learning benefits but simply to avoid falling behind in an educational “arms race” (Goehring et al., , 2024).\r\n\r\nFigures and Tables\r\nFigure 1:The Adoption of Generative AI among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the fraction of students who report using AI during the academic semester, categorized by demographic characteristics, high school type, academic cohort, GPA, and field of study. Usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily).\r\n\r\nThe category “All students” provides the baseline usage rate for the full sample. Gender categories are based on self-identification, with non-binary responses excluded due to a small sample size. “Private HS” refers to students who attended private high schools, while “Public HS” includes public institutions. “Cohort” denotes the student’s academic year, ranging from first-year (“Freshman”) to fourth-year and beyond (“Senior”). GPA categories (“GPA \r\n>\r\n p50” and “GPA \r\n<\r\n p50”) split students into groups above or below the median first-year GPA, as self-reported on a 4.0 scale. See Appendix B.1 for the classification of majors into fields of study.\r\n\r\nFigure 2:The Evolution of Generative AI Adoption among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the cumulative percent of students who reported using generative AI tools for academic purposes over time. The data is based on retrospective self-reports collected in our December 2024 survey, where students were asked “When did you first start using any form of Generative AI for academic purposes?” Response options ranged from “Before Spring 2023” to “This semester (Fall 2024).” The \r\nx\r\n-axis represents academic semesters, while the \r\ny\r\naxis represents the cumulative adoption rate. Vertical lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure 3:Adoption of Generative AI Models Among College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the adoption rates of various AI models as of Fall 2024. The horizontal axis shows the percent of students who reported using each tool, and the vertical axis lists the tools in descending order of adoption rates.\r\n\r\nFigure 4:Academic Uses of Generative AI\r\nPanel A. Across Common Academic Tasks\r\nRefer to caption\r\nPanel B. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of AI students who use generative AI for different academic tasks. For each task, usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). The number at the end of each bar represents the total percent of students who use AI for that purpose at any frequency. Tasks are ordered by total usage, from highest to lowest. Results are based on responses to the question: “For academic purposes, which of the following tasks do you typically use generative AI for?” Sample includes all students who reported using AI during the academic semester.\r\n\r\nFigure 5:Student Reported Likelihood of Using AI under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of students who report different likelihoods of using AI under various policy scenarios. For each policy, responses are categorized on a five-point scale from “Extremely unlikely to use AI” to “Extremely likely to use AI.” The sample includes all survey respondents. The question asked was: “How likely are you to use generative AI in a class with each of the following AI policies?”\r\n\r\nFigure 6:Understanding of Generative AI Policies and Resources\r\nPanel A. Understanding of AI Policies\r\nRefer to caption\r\nPanel B. Awareness of Copilot Access\r\nRefer to caption\r\nPanel C. Knowledge of Citation Requirements\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows students’ understanding of institutional AI policies and resources. Panel A displays the percent of students who report understanding AI policies in their classes, those who report having no explicit policy, and those who find policies unclear, broken down by demographic characteristics. Panel B shows the percent of students who are aware of their free access to Microsoft Copilot through Middlebury College. Panel C presents the percent of students who report knowing how to properly cite AI use in their academic work when required. For Panels B and C, horizontal lines represent 95 percent confidence intervals. Sample includes all survey respondents.\r\n\r\nFigure 7:Student Beliefs about the Impact of AI on their Academic Performance\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of Middlebury students who believe that AI improves, reduces, or has no effect on different aspects of their academic experience. For each outcome, responses are categorized into three groups: “Improves” combines “significantly improves” and “somewhat improves” responses, “Reduces” combines “significantly reduces” and “somewhat reduces” responses, and “No effect” represents neutral responses. Sample includes all students who report using AI during the academic semester. “Don’t know” responses are excluded.\r\n\r\nFigure 8:Relationship Between AI Adoption and Beliefs About AI’s Academic Benefits\r\nPanel A. Learning ability (e.g., ability to grasp concepts, learn new skills, etc.)\r\nRefer to caption\r\nPanel B. Understanding of\r\ncourse materials\r\nRefer to caption\r\nPanel C. Ability to complete\r\nassignments on time\r\nRefer to caption\r\nPanel D. Course grades\r\n \r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the relationship between AI adoption rates and beliefs that AI improves various academic outcomes across different student groups. Each panel plots the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis). Points represent different student groups categorized by demographics (circles), academic characteristics (triangles), and field of study (squares). The dashed line shows the linear fit across all groups. Groups with fewer than 10 students are excluded.\r\n\r\nFigure 9:Student Beliefs about Generative AI Usage at Middlebury College\r\nPanel A. For Schoolwork\r\nRefer to caption\r\nPanel B. For Leisure\r\nRefer to caption\r\nPanel C. For Any Purpose\r\nRefer to caption\r\nPanel D. Classes with No Policy\r\nRefer to caption\r\nPanel E. Classes that Allow AI\r\nRefer to caption\r\nPanel F. Classes that Disallow AI\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the distribution of students’ beliefs about generative AI usage among their peers at Middlebury College. Panels A-C display students’ estimates of the percent of their peers who regularly use AI for schoolwork, leisure activities, and any purpose, respectively. Panels D-F show students’ beliefs about AI usage in classes with different AI policies: those without an explicit policy (Panel D), those that allow AI use (Panel E), and those that prohibit AI use (Panel F). Each panel shows a histogram with bins of width ten percentage points (e.g., responses between 1-10 fall in the 10 bin, 11-20 in the 20 bin, etc.). The red dashed line indicates the mean response. Sample excludes respondents with missing values or who selected the default response for all six categories (which equals zero).\r\n\r\nTable 1:Summary Statistics of Survey Participants\r\nSurvey Sample\tAdmin records\r\nUnweighted\tWeighted\t\r\n(1)\t(2)\t(3)\r\nPanel A. Demographics\r\nMale\t0.446\t0.433\t0.463\r\nFemale\t0.508\t0.516\t0.533\r\nWhite\t0.618\t0.603\t0.538\r\nBlack\t0.036\t0.033\t0.052\r\nHispanic\t0.099\t0.106\t0.124\r\nAsian\t0.155\t0.162\t0.073\r\nPrivate high school\t0.420\t0.399\t–\r\nPublic high school\t0.543\t0.556\t–\r\nPanel B. Academic Characteristics    GPA 3.740 3.736 3.670 Hours spent on academics per week 17.899 17.889 – Freshman 0.311 0.355 0.255 Sophomore 0.273 0.272 0.257 Junior 0.202 0.179 0.201 Senior 0.213 0.194 0.287   Panel C. Field of Study    Arts 0.011 0.021 0.023 Humanities 0.052 0.068 0.073 Languages 0.021 0.020 0.025 Literature 0.035 0.025 0.025 Natural Sciences 0.218 0.249 0.244 Social Sciences 0.353 0.247 0.243 Has not declared major 0.311 0.371 0.364\r\n\r\nN (# degrees) 43 43 49 N (# students) 634 2,760 2,760\r\n\r\n\\justify\r\nNotes: This table presents summary statistics from our survey of college students. Panel A reports demographic characteristics, including the proportion of participants identifying as male, female, white, Black, Hispanic, Asian, or who attended a private or public high school. Panel B provides academic characteristics, such as GPA (only available for non-freshmen), average weekly hours spent on academics, and academic year distribution (Freshman, Sophomore, Junior, and Senior). Note that in column 1–2, GPA refers to self-reported first-year GPA while in column 3 it is the overall GPA during Spring 2024. Panel C summarizes the distribution of participants across different fields of study. Major groups are mutually exclusive.\r\n\r\n0.8250.5890.3680.106 R\r\n−\r\nsquared0.0770.0890.1160.058 N (Students)616616616616 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI usage frequency threshold and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column uses a different threshold for AI usage frequency, categorized as: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). Column 1 defines usage as any nonzero frequency; column 2 includes at least occasional use; column 3 includes frequent or higher use; and column 4 captures only very frequent use.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.0890.1990.3570.5510.801 R\r\n−\r\nsquared0.0450.0610.0880.0800.069 N (Students)633633633633633 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI adoption date and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column presents results for a different threshold of AI adoption. Column 1 shows the probability of adopting AI before Spring 2023; column 2 by Spring 2023; column 3 by Fall 2023; column 4 by Spring 2024; and column 5 by Fall 2024. The dependent variable in each regression is a binary indicator equal to one if the student had adopted AI by the specified time period.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.9120.6122.2850.9130.4191.8010.1930.484 R\r\n−\r\nsquared0.0360.0730.1260.0530.1170.1290.0420.063 N (Students)515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and their use of generative AI for academic tasks. In columns 1 and 4, the outcome is a dummy that equals one if a student reports using AI with any frequency for at least one augmentation or automation task, respectively. In columns 2 and 5, the outcome is the share of tasks within each category for which the student reports any use. In columns 3 and 6, the outcome is a continuous measure capturing average usage frequency for each task category, based on raw Likert-style responses. In columns 7 and 8, the outcome is the difference in average task share and usage intensity between augmentation and automation, respectively. Regressions are weighted and report robust standard errors clustered at the student level. ∗∗∗ \r\np\r\n<\r\n0.01\r\n, ∗∗ \r\np\r\n<\r\n0.05\r\n, ∗ \r\np\r\n<\r\n0.10\r\n.\r\n\r\nReferences\r\nAcemoglu, (2025)\r\nAcemoglu, D. (2025).Nobel lecture: Institutions, technology, and prosperity.American Economic Review, 115(6):1709–1748.\r\nAngrist et al., (2009)\r\nAngrist, J., Lang, D., and Oreopoulos, P. (2009).Incentives and services for college achievement: Evidence from a randomized trial.American Economic Journal: Applied Economics, 1(1):136–163.\r\nAutor and Thompson, (2025)\r\nAutor, D. and Thompson, N. (2025).Expertise.\r\nBailey et al., (2022)\r\nBailey, M., Johnston, D., Kuchler, T., Stroebel, J., and Wong, A. (2022).Peer effects in product adoption.American Economic Journal: Applied Economics, 14(3):488–526.\r\nBastani et al., (2025)\r\nBastani, H., Bastani, O., Sungu, A., Ge, H., Kabakcı, Ö., and Mariman, R. (2025).Generative AI without guardrails can harm learning: Evidence from high school mathematics.Proceedings of the National Academy of Sciences, 122(26):e2422633122.\r\nBeaman et al., (2021)\r\nBeaman, L., BenYishay, A., Magruder, J., and Mobarak, A. M. (2021).Can network theory-based targeting increase technology adoption?American Economic Review, 111(6):1918–1943.\r\nBick et al., (2025)\r\nBick, A., Blandin, A., and Deming, D. J. (2025).The rapid adoption of generative AI.Technical report, National Bureau of Economic Research.\r\nBonney et al., (2024)\r\nBonney, K., Breaux, C., Buffington, C., Dinlersoz, E., Foster, L. S., Goldschlag, N., Haltiwanger, J. C., Kroff, Z., and Savage, K. (2024).Tracking Firm Use of AI in Real Time: A Snapshot from the Business Trends and Outlook Survey.\r\nBonsu and Baffour-Koduah, (2023)\r\nBonsu, E. M. and Baffour-Koduah, D. (2023).From the consumers’ side: Determining students’ perception and intention to use chatgpt in ghanaian higher education.Journal of Education, Society & Multiculturalism, 4(1):1–29.\r\nBresnahan and Trajtenberg, (1995)\r\nBresnahan, T. F. and Trajtenberg, M. (1995).General purpose technologies ‘engines of growth’?Journal of econometrics, 65(1):83–108.\r\nBrynjolfsson and Hitt, (2000)\r\nBrynjolfsson, E. and Hitt, L. M. (2000).Beyond computation: Information technology, organizational transformation and business performance.Journal of Economic Perspectives, 14(4):23–48.\r\nBrynjolfsson et al., (2025)\r\nBrynjolfsson, E., Li, D., and Raymond, L. (2025).Generative AI at Work.The Quarterly Journal of Economics, 140(2):889–942.\r\nBushong and Gagnon-Bartsch, (2024)\r\nBushong, B. and Gagnon-Bartsch, T. (2024).Failures in forecasting: An experiment on interpersonal projection bias.Management Science, 70(12):8735–8752.\r\nCarvajal et al., (2024)\r\nCarvajal, D., Franco, C., and Isaksson, S. (2024).Will artificial intelligence get in the way of achieving gender equality?Discussion Paper 03, NHH Norwegian School of Economics.\r\n(15)\r\nComin, D. and Hobijn, B. (2010a).An exploration of technology diffusion.American Economic Review, 100(5):2031–2059.\r\n(16)\r\nComin, D. and Hobijn, B. (2010b).Technology Diffusion and Postwar Growth, volume 25, pages 209–246.University of Chicago Press.\r\nCui et al., (2024)\r\nCui, Z. K., Demirer, M., Jaffe, S., Musolff, L., Peng, S., and Salz, T. (2024).The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers.\r\nDavid, (1990)\r\nDavid, P. A. (1990).The dynamo and the computer: an historical perspective on the modern productivity paradox.The American economic review, 80(2):355–361.\r\nDell’Acqua et al., (2023)\r\nDell’Acqua, F., McFowland, E., Mollick, E. R., Lifshitz-Assaf, H., Kellogg, K., Rajendran, S., Krayer, L., Candelon, F., and Lakhani, K. R. (2023).Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality.\r\nDuflo et al., (2011)\r\nDuflo, E., Kremer, M., and Robinson, J. (2011).Nudging Farmers to Use Fertilizer: Theory and Experimental Evidence from Kenya.American Economic Review, 101(6):2350–2390.\r\nEloundou et al., (2024)\r\nEloundou, T., Manning, S., Mishkin, P., and Rock, D. (2024).GPTs are GPTs: Labor market impact potential of LLMs.Science, 384(6702):1306–1308.\r\nFairlie et al., (2014)\r\nFairlie, R. W., Hoffmann, F., and Oreopoulos, P. (2014).A community college instructor like me: Race and ethnicity interactions in the classroom.American Economic Review, 104(8):2567–2591.\r\nFelten et al., (2021)\r\nFelten, E. W., Raj, M., and Seamans, R. (2021).Occupational, Industry, and Geographic Exposure to Artificial Intelligence: A Novel Dataset and Its Potential Uses.\r\nFelten et al., (2023)\r\nFelten, E. W., Raj, M., and Seamans, R. (2023).How will Language Modelers like ChatGPT Affect Occupations and Industries?\r\nFoster and Rosenzweig, (1995)\r\nFoster, A. D. and Rosenzweig, M. R. (1995).Learning by doing and learning from others: Human capital and technical change in agriculture.Journal of political Economy, 103(6):1176–1209.\r\nFoster and Rosenzweig, (2010)\r\nFoster, A. D. and Rosenzweig, M. R. (2010).Microeconomics of technology adoption.Annu. Rev. Econ., 2(1):395–424.\r\nGallup, (2024)\r\nGallup (2024).AI in the Workplace: Answering 3 Big Questions.\r\nGiaccherini et al., (2019)\r\nGiaccherini, M., Herberich, D. H., Jimenez-Gomez, D., List, J. A., Ponti, G., and Price, M. K. (2019).The behavioralist goes door-to-door: Understanding household technological diffusion using a theory-driven natural field experiment.Technical report, National Bureau of Economic Research.\r\nGoehring et al., (2024)\r\nGoehring, G., Mezzanotti, F., and Ravid, S. A. A. (2024).Technology adoption and career concerns: Evidence from the adoption of digital technology in motion pictures.Technical report, National Bureau of Economic Research.\r\nGriliches, (1957)\r\nGriliches, Z. (1957).Hybrid corn: An exploration in the economics of technological change.Econometrica, 25(4):501–522.\r\nHall and Khan, (2003)\r\nHall, B. H. and Khan, B. (2003).Adoption of new technology.\r\n(32)\r\nHanda, K., Bent, D., Tamkin, A., McCain, M., Durmus, E., Stern, M., Schiraldi, M., Huang, S., Ritchie, S., Syverud, S., Jagadish, K., Vo, M., Bell, M., and Ganguli, D. (2025a).Anthropic education report: How university students use claude.\r\n(33)\r\nHanda, K., Tamkin, A., McCain, M., Huang, S., Durmus, E., Heck, S., Mueller, J., Hong, J., Ritchie, S., Belonax, T., et al. (2025b).Which economic tasks are performed with ai? evidence from millions of claude conversations.arXiv preprint arXiv:2503.04761.\r\nHanna et al., (2014)\r\nHanna, R., Mullainathan, S., and Schwartzstein, J. (2014).Learning Through Noticing: Theory and Evidence from a Field Experiment.The Quarterly Journal of Economics, 129(3):1311–1353.\r\nHartley et al., (2025)\r\nHartley, J., Jolevski, F., Melo, V., and Moore, B. (2025).The Labor Market Effects of Generative Artificial Intelligence.\r\nHumlum and Vestergaard, (2025)\r\nHumlum, A. and Vestergaard, E. (2025).Large Language Models, Small Labor Market Effects.\r\nHutto and Gilbert, (2014)\r\nHutto, C. and Gilbert, E. (2014).Vader: A parsimonious rule-based model for sentiment analysis of social media text.In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216–225.\r\nJehiel, (2018)\r\nJehiel, P. (2018).Investment strategy and selection bias: An equilibrium perspective on overoptimism.American Economic Review, 108(6):1582–1597.\r\nKelly et al., (2023)\r\nKelly, A., Sullivan, M., and Strampel, K. (2023).Generative artificial intelligence: University student awareness, experience, and confidence in use across disciplines.Journal of University Teaching and Learning Practice, 20(6):1–16.\r\nKharazian, (2025)\r\nKharazian, A. (2025).Ramp ai index: Monthly measurement of ai adoption by american businesses.Technical report, Ramp Economics Lab.\r\nLehmann et al., (2024)\r\nLehmann, M., Cornelius, P. B., and Sting, F. J. (2024).AI Meets the Classroom: When Does ChatGPT Harm Learning?\r\nLing and Imas, (2025)\r\nLing, Y. and Imas, A. (2025).Underreporting of AI use: The role of social desirability bias.Available at SSRN.\r\nLiu and Wang, (2024)\r\nLiu, Y. and Wang, H. (2024).Who on earth is using generative ai?Policy Research Working Paper WPS10870, World Bank Group, Washington, D.C.\r\nMcClain, (2024)\r\nMcClain, C. (2024).Americans’ use of ChatGPT is ticking up, but few trust its election information.\r\nMcDonald et al., (2025)\r\nMcDonald, N., Johri, A., Ali, A., and Collier, A. H. (2025).Generative artificial intelligence in higher education: Evidence from an analysis of institutional policies and guidelines.Computers in Human Behavior: Artificial Humans, 3:100121.\r\nMcElheran et al., (2023)\r\nMcElheran, K., Li, J. F., Brynjolfsson, E., Kroff, Z., Dinlersoz, E., Foster, L. S., and Zolas, N. (2023).AI Adoption in America: Who, What, and Where.\r\nMerchant, (2024)\r\nMerchant, B. (2024).The new luddites aren’t backing down.The Atlantic.Accessed: 2025-06-16.\r\nNam, (2023)\r\nNam, J. (2023).56% of college students have used ai on assignments or exams.Technical report.Edited by Lyss Welding. Accessed June 11, 2025.\r\nNolan, (2023)\r\nNolan, B. (2023).Here are the schools and colleges that have banned chatgpt.Business Insider.Accessed: 2025-01-27.\r\nNoy and Zhang, (2023)\r\nNoy, S. and Zhang, W. (2023).Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence.\r\nOtis et al., (2024)\r\nOtis, N. G., Delecourt, S., Cranney, K., and Koning, R. (2024).Global Evidence on Gender Gaps and Generative AI.Harvard Business School.\r\nPeng et al., (2023)\r\nPeng, S., Kalliamvakou, E., Cihon, P., and Demirer, M. (2023).The Impact of AI on Developer Productivity: Evidence from GitHub Copilot.\r\nRavšelj et al., (2025)\r\nRavšelj, D., Keržič, D., Tomaževič, N., Umek, L., Brezovar, N., A. Iahad, N., Abdulla, A. A., Akopyan, A., Aldana Segura, M. W., AlHumaid, J., Allam, M. F., Alló, M., Andoh, R. P. K., Andronic, O., Arthur, Y. D., Aydın, F., Badran, A., Balbontín-Alvarado, R., Ben Saad, H., Bencsik, A., Benning, I., Besimi, A., Bezerra, D. D. S., Buizza, C., Burro, R., Bwalya, A., Cachero, C., Castillo-Briceno, P., Castro, H., Chai, C. S., Charalambous, C., Chiu, T. K. F., Clipa, O., Colombari, R., Corral Escobedo, L. J. H., Costa, E., Cre\\textcommabelowtulescu, R. G., Crispino, M., Cucari, N., Dalton, F., Demir Kaya, M., Dumić-Čule, I., Dwidienawati, D., Ebardo, R., Egbenya, D. L., Faris, M. E., Fečko, M., Ferrinho, P., Florea, A., Fong, C. Y., Francis, Z., Ghilardi, A., González-Fernández, B., Hau, D., Hossain, M. S., Hug, T., Inasius, F., Ismail, M. J., Jahić, H., Jessa, M. O., Kapanadze, M., Kar, S. K., Kateeb, E. T., Kaya, F., Khadri, H. O., Kikuchi, M., Kobets, V. M., Kostova, K. M., Krasmane, E., Lau, J., Law, W. H. C., Lazăr, F., Lazović-Pita, L., Lee, V. W. Y., Li, J., López-Aguilar, D. V., Luca, A., Luciano, R. G., Machin-Mastromatteo, J. D., Madi, M., Manguele, A. L., Manrique, R. F., Mapulanga, T., Marimon, F., Marinova, G. I., Mas-Machuca, M., Mejía-Rodríguez, O., Meletiou-Mavrotheris, M., Méndez-Prado, S. M., Meza-Cano, J. M., Mirķe, E., Mishra, A., Mital, O., Mollica, C., Morariu, D. I., Mospan, N., Mukuka, A., Navarro Jiménez, S. G., Nikaj, I., Nisheva, M. M., Nisiforou, E., Njiku, J., Nomnian, S., Nuredini-Mehmedi, L., Nyamekye, E., Obadić, A., Okela, A. H., Olenik-Shemesh, D., Ostoj, I., Peralta-Rizzo, K. J., Peštek, A., Pilav-Velić, A., Pires, D. R. M., Rabin, E., Raccanello, D., Ramie, A., Rashid, M. M. U., Reuter, R. A. P., Reyes, V., Rodrigues, A. S., Rodway, P., Ručinská, S., Sadzaglishvili, S., Salem, A. A. M. S., Savić, G., Schepman, A., Shahpo, S. M., Snouber, A., Soler, E., Sonyel, B., Stefanova, E., Stone, A., Strzelecki, A., Tanaka, T., Tapia Cortes, C., Teira-Fachado, A., Tilga, H., Titko, J., Tolmach, M., Turmudi, D., Varela-Candamio, L., Vekiri, I., Vicentini, G., Woyo, E., Yorulmaz, Ö., Yunus, S. A. S., Zamfir, A.-M., Zhou, M., and Aristovnik, A. (2025).Higher education students’ perceptions of ChatGPT: A global study of early reactions.PLOS ONE, 20(2):e0315011.\r\nRogers, (1962)\r\nRogers, E. M. (1962).Diffusion of Innovations.Free Press, New York.\r\nRoss et al., (1977)\r\nRoss, L., Greene, D., and House, P. (1977).The “false consensus effect”: An egocentric bias in social perception and attribution processes.Journal of experimental social psychology, 13(3):279–301.\r\nSacerdote, (2001)\r\nSacerdote, B. (2001).Peer Effects with Random Assignment: Results for Dartmouth Roommates.The Quarterly Journal of Economics, 116(2):681–704.\r\nStinebrickner and Stinebrickner, (2008)\r\nStinebrickner, R. and Stinebrickner, T. R. (2008).The causal effect of studying on academic performance.The BE Journal of Economic Analysis & Policy, 8(1).\r\nStöhr et al., (2024)\r\nStöhr, C., Ou, A. W., and Malmström, H. (2024).Perceptions and usage of AI chatbots among students in higher education across genders, academic levels and fields of study.Computers and Education: Artificial Intelligence, 7:100259.\r\nStokey, (2021)\r\nStokey, N. L. (2021).Technology diffusion.Review of Economic Dynamics, 42:15–36.\r\nWorld Bank, (2016)\r\nWorld Bank (2016).World development report 2016: Digital dividends.World Bank Publications.\r\nWu et al., (2022)\r\nWu, M.-J., Zhao, K., and Fils-Aime, F. (2022).Response rates of online surveys in published research: A meta-analysis.Computers in Human Behavior Reports, 7.\r\nXiao et al., (2023)\r\nXiao, P., Chen, Y., and Bao, W. (2023).Waiting, banning, and embracing: An empirical analysis of adapting policies for generative ai in higher education.arXiv preprint arXiv:2305.18617.\r\nAppendix\r\n\r\nAppendix AAppendix Figures and Tables\r\nFigure A1:Generative AI Usage Survey Design Overview\r\nKey\t\r\nSurvey Structure\r\nSurvey Sections\r\nUsage Data Collection\r\nBeliefs & Perceptions\r\nAI Usage Survey at Middlebury College\r\nDecember 2024\r\nParticipant Recruitment\r\nCampus-wide email invitations, framed as general technology use survey\r\nParticipation Incentives\r\nAmazon gift card lottery ranging from $50-$500\r\nSection 1: Demographics & Academic Information\r\nStudent Characteristics\r\nGender, race/ethnicity, high school type\r\nAcademic Profile\r\nYear, major, self-reported GPA, study hours\r\nSection 2: generative AI Usage Patterns\r\nAdoption Metrics\r\nFrequency, timing of first use, specific AI models used\r\nAcademic Applications\r\nTasks performed with AI, payment for premium services\r\nSection 3: Perceptions & Institutional Policies\r\nImpact Perceptions\r\nEffects on learning, grades, time management\r\nPolicy Responses\r\nUsage likelihood under different policies\r\nPeer Usage Beliefs\r\nEstimates of AI adoption among peers for schoolwork and leisure\r\nOpen-Ended Response Collection\r\nMotivations for AI use, policy feedback, suggestions for improvement\r\nNote: This figure illustrates the structure of the AI usage survey conducted at Middlebury College in December 2024. The survey collected information across three main sections: (1) demographic and academic background, (2) patterns of generative AI usage including adoption timing, frequency, and specific applications, and (3) perceptions of AI’s impact on learning and responses to institutional policies.\r\n\r\nFigure A2:Cumulative Generative AI Use by Student Characteristic\r\nPanel A. By Gender\r\nRefer to caption\r\nPanel B. By Race\r\nRefer to caption\r\nPanel C. By High School Type\r\nRefer to caption\r\nPanel D. By Freshman Year GPA\r\nRefer to caption\r\nPanel E. By Cohort\r\nRefer to caption\r\nPanel F. By Field of Study\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents cumulative AI use based on different student characteristics. Each panel displays the cumulative distribution of AI use based on a specific characteristic: gender, race, school type, first-year GPA, cohort, or field of study. The cumulative percent of students is plotted against usage categories. The legends and colors correspond to subgroups within each demographic variable.\r\n\r\nFigure A3:Percent of Students Who Pay for Generative AI Tools\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of AI users who pay for AI tools (through any platform) across different demographic groups. Horizontal lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure A4:Academic Uses of ChatGPT: Evidence from Global Survey\r\nPanel A. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\nPanel B. Across the College Quality Distribution\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of students who use ChatGPT for different academic tasks based on data from Ravšelj et al., (2025). Panel A displays usage patterns across tasks categorized as augmenting (proofreading, translating, study assistance, research assistance) versus automating (academic writing, professional writing, creative writing, brainstorming, summarizing, calculating, coding assistance, personal assistance) student effort. Panel B shows usage patterns across university quality quintiles based on World University Rankings, with universities ranked in the top 20% (top quintile) showing slightly higher rates of augmentation relative to automation compared to bottom quintile institutions. The analysis includes universities with at least 30 student responses and excludes observations with missing usage data.\r\n\r\nFigure A5:Student Beliefs about AI Usage versus Actual Usage under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure compares students’ beliefs about AI usage with actual usage rates under different AI policies. For each policy type, blue bars show the percent of students who report being “Neutral,” “Likely,” or “Very Likely” to use AI, while red bars show students’ mean beliefs about what percent of their peers use AI in classes with that policy. The sample includes all survey respondents for both actual usage and beliefs measures. “AI Use is Unrestricted” refers to classes with no restrictions on AI use, “No Explicit AI Policy” refers to classes without a stated policy, and “AI Use is Prohibited” refers to classes that ban AI use entirely.\r\n\r\nFigure A6:Relationship Between Beliefs About AI Usage and Actual AI Usage\r\nRefer to caption\\justify\r\nNotes: This figure shows the relationship between students’ beliefs about AI usage among their peers and actual AI usage rates across different demographic groups. Each point represents a different group of students (by demographics, academic characteristics, or field of study). The \r\ny\r\n-axis shows the percent of students in each group who report using AI for academic purposes. The \r\nx\r\n-axis shows the mean belief within each group about what percent of Middlebury students use AI. The dashed line shows the linear fit. Sample includes all survey respondents with at least ten observations per group. Students’ beliefs about AI usage are positively correlated with actual usage patterns, suggesting that students have relatively accurate perceptions of AI adoption among their peers.\r\n\r\nGPA > p50 80.3 26.3 17.4 23.2 13.4 GPA < p50 87.1 18.7 26.3 32.2 9.9 Freshman 81.3 24.3 24.1 24.7 8.1 Sophomore 84.4 25.6 22.6 23.7 12.4 Junior 86.2 21.6 23.1 28.6 12.8 Senior 78.7 21.0 17.4 29.9 10.4\r\n\r\nPanel C. Field of Study    Arts 73.3 21.0 0.0 52.3 0.0 Humanities 74.8 39.3 21.8 12.1 1.7 Languages 57.4 27.0 30.3 0.0 0.0 Literature 48.6 10.0 33.7 5.0 0.0 Natural Sciences 91.1 22.0 21.9 33.8 13.3 Social Sciences 84.6 17.9 20.4 29.1 17.2\r\n\r\n\\justify\r\nNotes: This table presents the percent of students in each demographic group who report using AI at different frequencies during the academic semester. Each cell shows the percent of students within that group. Column 1 reports the total percent who use AI at any frequency. Columns 2 to 5 represent increasing usage frequencies: rarely (1–2 times per semester), occasionally (monthly), frequently (weekly), and very frequently (multiple times per week). Panel A reports percentages by demographic characteristics. Panel B shows percentages by academic characteristics. Panel C presents percentages by field of study.\r\n\r\n0.9730.1350.0770.1100.114 R\r\n−\r\nsquared0.0120.0510.0270.0240.138 N (Students)516516516516516 \\justifyNotes: This table assesses the relationship between AI model adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI model usage (columns 1-4) or payment for AI services (column 5), and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division.\r\n\r\nEach column presents results for a different model or payment outcome. Column 1 shows usage of OpenAI’s ChatGPT, column 2 Google Gemini, column 3 Microsoft Copilot, column 4 any other AI model, and column 5 whether the student pays for any generative AI service.\r\n\r\nThe omitted categories are Natural Sciences for academic division, white students for race/ethnicity, and freshmen for cohort. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.7510.6760.6610.2710.479 R\r\n−\r\nsquared0.0870.1130.0760.0680.041 N (Students)599599599599599 \\justifyNotes: This table examines how student characteristics relate to self-reported likelihood of using generative AI under different policy scenarios. Each column presents results for different policy scenarios. In columns 1-4, the dependent variable equals one if the student reports being“neutral,” “likely” or “extremely likely” to use AI under the specified policy, and zero if they report being “unlikely,” or “extremely unlikely.” Column 5 represents the impact of moving from unrestricted use to complete prohibition.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.5760.6850.3760.4890.564 R\r\n−\r\nsquared0.1020.1190.0840.0370.087 N (Students)561565543560560 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. Each column presents results for beliefs about different academic outcomes: learning ability (e.g., ability to grasp concepts, retain information, or learn new skills) in column 1, understanding of course materials in column 2, course grades in column 3, ability to complete assignments on time in column 4, and time spent on academics in column 5. The dependent variable in each regression equals one if the student believes AI somewhat improves” or significantly improves” the outcome, and zero if they believe it has no effect, reduces, or significantly reduces the outcome. “Don’t know” responses are excluded.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\nAppendix BEmpirical Appendix\r\nB.1Field of Study Classifications\r\nThis appendix details the classification of majors into broad fields of study used in Figure 1:\r\n\r\n• Natural Sciences: Includes Biology, Biochemistry, Chemistry, Computer Science, Earth and Climate Sciences/Geology, Environmental Studies, Mathematics, Molecular Biology & Biochemistry, Neuroscience, Physics, and Statistics.\r\n• Social Sciences: Includes Anthropology, Economics, Education, Geography, International & Global Studies, International Politics & Economics, Political Science, Psychology, and Sociology.\r\n• Humanities: Includes American Studies, Black Studies, Classics, History, Architectural Studies, Art History & Museum Studies, History of Art & Architecture, Philosophy, Religion, and Classical Studies.\r\n• Literature: Includes Comparative Literature, English/English & American Literatures, and Literary Studies.\r\n• Languages: Includes Arabic, Chinese, French & Francophone Studies, German, Japanese Studies, Russian, and Spanish.\r\n• Arts: Includes Film & Media Culture, Music, Studio Art, and Theatre.\r\nB.2Task-Specific Use of Generative AI\r\nThe aggregate patterns documented in Section 4 may mask important heterogeneity across student groups. Different students may adopt AI tools at varying rates depending on their academic needs and field-specific norms. To investigate this heterogeneity systematically, Appendix Table LABEL:tab:ai_purpose_correlates presents regression estimates examining how student characteristics predict AI usage for each of our ten measured academic tasks.\r\n\r\nUsage patterns differ markedly by student characteristics. Male students consistently exhibit higher adoption rates across most applications, with particularly pronounced differences for finding information (16.4 pp; \r\np\r\n<\r\n0.01\r\n), summarizing texts (12.5 pp; \r\np\r\n<\r\n0.01\r\n), and creating images (16.9 pp; \r\np\r\n<\r\n0.01\r\n). Black students show substantially higher usage for information-gathering tasks, being 23.8 pp more likely to use AI for finding information compared to white students (\r\np\r\n<\r\n0.05\r\n). They also exhibit dramatically higher adoption of writing assistance tools, with 27.6 pp higher usage for editing text (\r\np\r\n<\r\n0.05\r\n) and 23.6 pp for writing emails (\r\np\r\n<\r\n0.10\r\n). Latino students demonstrate significantly higher usage for generating ideas (24.0 pp; \r\np\r\n<\r\n0.01\r\n) and writing emails (20.7 pp; \r\np\r\n<\r\n0.01\r\n), while Asian students show markedly higher adoption for writing emails (24.5 pp; \r\np\r\n<\r\n0.01\r\n) and explaining concepts (10.9 pp, \r\np\r\n<\r\n0.05\r\n). Students from public high schools report lower usage rates for several applications, particularly for concept explanation (7.3 pp lower; \r\np\r\n<\r\n0.10\r\n) and writing emails (7.4 pp lower; \r\np\r\n<\r\n0.10\r\n), but higher usage for proofreading (8.7 pp higher; \r\np\r\n<\r\n0.10\r\n).\r\n\r\nField of study emerges as a particularly strong predictor of usage patterns. Arts majors show significantly lower adoption across multiple tasks compared to Natural Science students, with gaps of 49.7 pp for finding information (\r\np\r\n<\r\n0.01\r\n), 44.0 pp for generating ideas (\r\np\r\n<\r\n0.05\r\n), and 62.1 pp for coding assistance (\r\np\r\n<\r\n0.01\r\n). Similarly, humanities majors are 36.8 pp less likely to use AI for generating ideas (\r\np\r\n<\r\n0.01\r\n) and 30.8 pp less likely for coding assistance (\r\np\r\n<\r\n0.01\r\n). Languages majors exhibit the most pronounced differences, being 52.4 pp less likely to use AI for summarizing texts (\r\np\r\n<\r\n0.01\r\n) and 46.9 pp less likely for generating ideas (\r\np\r\n<\r\n0.01\r\n). By contrast, social science students show higher usage for explaining concepts (8.0 pp; \r\np\r\n<\r\n0.10\r\n) and summarizing texts (14.7 pp; \r\np\r\n<\r\n0.01\r\n).\r\n\r\n0.8030.7400.6310.6190.5410.4730.3710.3440.2350.204 R\r\n−\r\nsquared0.0780.0870.0770.1160.0280.0540.0830.1350.0400.094 N (Students)515515515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and use of AI for specific academic tasks. Each column shows the result for a different academic task. The omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Regressions are weighted and use heteroskedasticity-robust standard errors clustered at the student level. ∗∗∗, ∗∗, and ∗ indicate significance at the 1%, 5%, and 10% levels, respectively.\r\n\r\nAppendix CQualitative Evidence on Student Perspectives on AI Use\r\nIn this section, we analyze student responses to an open-ended question about their use of generative AI. The survey question asked: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” This was an optional question that 48.3 percent of survey respondents answered. Appendix Figure C1 presents a word cloud of the most frequent words in student responses.\r\n\r\nC.1Validating the Open-Ended Response Measure\r\nWe begin by validating our open-ended response measure to show that it contains meaningful signal. To validate this measure, we employ sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner), a widely-used lexicon and rule-based sentiment analysis tool (Hutto and Gilbert, , 2014). We calculate sentiment scores for each response and examine whether these scores correlate with students’ actual AI adoption behaviors. Intuitively, students who express more positive sentiment toward AI in their open-ended responses should be more likely to have adopted AI tools in practice. We test this hypothesis by examining two measures of AI adoption: whether students have ever used generative AI and whether they currently use AI for academic purposes. Appendix Figure C2 presents binned scatterplots showing the relationship between AI adoption rates and standardized sentiment scores.\r\n\r\nSentiment towards generative AI strongly predicts AI adoption. For both outcomes, students with negative sentiment scores (below zero) show substantially lower adoption rates—around 70-85 percent for ever using AI and 70-75 percent for academic use. In contrast, students with positive sentiment scores display markedly higher adoption rates, reaching nearly 100 percent for general use and 95-100 percent for academic purposes among those with the most positive sentiment. The relationship is particularly pronounced for academic AI use (Panel B), where the coefficient of \r\nβ\r\n^\r\n=\r\n0.168\r\n is twice as large as for general adoption.17 These systematic patterns confirm that our open-ended responses capture meaningful variation in student attitudes that corresponds to real behavioral differences.\r\n\r\nC.2How Students use Generative AI\r\nTo analyze these responses systematically, we classified each response using keywords based on their content. For example, if a student mentioned using AI to save time, we tagged the response with the keyword “time-saver.” If a student expressed concerns about AI’s impact on learning, we tagged it with “negative learning.” Responses could receive multiple keywords if they discussed several themes. Appendix Figure C3 shows the frequency of keywords in our classification. The responses reveal how students use AI tools, what motivates this use, and what causes some to avoid or limit their use.\r\n\r\nThe most common use of AI is as an explanatory tool. Nearly 30 percent of responses mentioned using AI to understand course material. Students frequently ask AI to break down complex concepts from readings and lectures, particularly when they find the material difficult to understand. For example, one student reported: “I can ask AI to explain concepts to me that I have a hard time grasping. […] I can keep asking ’simplify’ or ’break down even more.”’ Students also use AI to summarize dense academic readings, which they argue helps them manage heavy reading loads.\r\n\r\nStudents employ AI throughout different stages of the writing process. Some use AI to generate initial drafts that serve as starting points. One student explained: “Helps me get started with a base for most of my essays. It feels easier to edit something already written and make it my own than to write from scratch.” Others use AI more narrowly for brainstorming when stuck on specific problems. As one student noted: “I use it if I am feeling stuck to push me to the right direction (whether a mathematical problem or an essay idea).” Many also report using AI as an editing tool to improve grammar, sentence structure, and overall writing flow. This is especially the case for non-native English speakers. As one student explained: “English is not my first language and it frustrates me sometimes that I cannot find the best way to phrase a certain idea and AI is a useful tool to have to find alternate expressions.”\r\n\r\nStudents frequently mentioned using AI for specific academic tasks. In courses that require coding, students often use AI for debugging code and understanding programming concepts. Students also employ AI for administrative tasks like formatting citations and drafting routine emails. Finally, many students use AI as an enhanced search engine. One student reported: “It has significantly reduced the time it takes to conduct research on new topics and ideas, and helps me by giving me a thorough selection of sources to use for projects of any kind.”\r\n\r\nC.3Why Students Adopt AI\r\nTime savings was the most commonly cited reason for using AI. Nearly 30 percent of responses mentioned using AI to complete work more efficiently. Students often viewed AI as a way to manage demanding course loads. Many students particularly embrace AI assistance for tasks viewed as mechanical or administrative. A student noted they use AI for “Writing emails quick and creating resume/ cover letter templates.” But AI assistance goes beyond grunt work. Some students use it to “spend less time doing assignments and homework.” This is particularly true if students don’t view the work as central to their academic experience. As one student explained: “when I come across work I deem as ineffective, I want to spend as little time as possible doing it.”\r\n\r\nHaving an on-demand tutor for academic support was another key motivation. One-quarter of responses described using AI as an “explainer” when other resources were unavailable or inconvenient. As one student noted, “I use it as a last resort (if there are no office hours, after looking up videos, etc.) if I need extra help. I’d like to think that the way I use it is similar to going to office hours or TA hours.”\r\n\r\nPeer influence also drove AI adoption. Some students reported feeling pressure to use AI to remain competitive with their classmates. One student explained: “I noticed others use it, are getting better grades than me, and they say they learn better with the help of AI, so I gave it a try.” Others worried about being at a competitive disadvantage: “Other people were using it and told me about it. I felt like I would be at a disadvantage if I wasn’t also using it.”\r\n\r\nC.4Concerns and Limitations\r\nStudents expressed several concerns about AI use in academic work. The most frequent worry was about negative impacts on learning. One student who initially used AI extensively reported: “In the past, I have simply plugged and chugged homework assignments into ChatGPT and submitted it. Those assignments feedback from teachers was positive and I was getting good grades, but I definitely felt that my own learning outcomes to be significantly worse.” Other students viewed AI use as fundamentally incompatible with their educational goals. As one humanities student explained: “my task is as a humanities student is to think, not calculate; why should I let AI do the thinking for me? It would defeat the purpose of pursuing my education.”\r\n\r\nMany students described ethical concerns about AI use. Responses suggested uncertainty about appropriate boundaries. One student noted: “I never use it to explicitly write something because that feels like overt cheating, but sometimes I hesitate when it completely solves Econ problems. I understand how it does it, and it helps me to learn, but it still sometimes feels a little morally gray.” Another expressed similar ambivalence: “I tend to only use it when [it] will save me time in a moral way.”\r\n\r\nStudents also emphasized the importance of maintaining ownership of their work. Many expressed pride in producing original work and hesitation about diluting that ownership through AI use. As one student explained: “I don’t have interest in using generative AI for my academic work because I want my work to reflect my own ideas.” Another noted: “It usually would not even occur to me to turn to AI to substitute writing because I want to take credit for my work, and using AI seems to diminish that.”\r\n\r\nTechnical limitations deterred some students from using AI tools. Students reported concerns about inaccurate outputs (“hallucinations”) and poor output quality, particularly for creative writing or complex analytical tasks. For example, one student noted that “In my poetry class we were instructed to use it to come up with poems and they were awful, so that kinda turned me away from using it to do my work for me.”\r\n\r\nC.5Discussion\r\nWe conclude with two overarching themes that emerged from the responses.\r\n\r\nFirst, students vary substantially in how they incorporate AI into their academic lives. Crucially, this heterogeneity largely depends on what students perceive as “appropriate” uses of AI. For activities that they perceive as core to their academic journey—like writing essays or solving problems—many students hesitate to use AI. A student articulated this clearly: “Most of my work is writing or reading. If I’m not doing the writing, what is the purpose of me taking the class?” Yet, students draw different boundaries between central tasks and grunt work. Some use AI extensively, viewing their role as akin to a manager that provides high-level direction while AI handles implementation. Others restrict AI use to specific tasks like brainstorming, editing, or drafting emails. Still others avoid AI entirely for academic work, often for ethical reasons. Even among AI users, adoption patterns reflect individual trade-offs between time savings, learning goals, and academic integrity concerns.\r\n\r\nSecond, there appears to be a fundamental tension between efficiency and learning. The time-saving benefits are easy to observe, quantifiable, and tangible. But these time savings are unlikely to be a free lunch. Some benefits may come at the cost of spending less time with material that requires deeper engagement to digest. As one student noted: “There may be a negative effect in that it eliminates much of the ’struggle’ in learning.” Yet, having an on-demand tutor that explains concepts in relatable ways can also improve learning. As one student explained: “It can explain concepts to me in a way that is tailored to my learning style.” This suggests that the impact of AI use on learning outcomes depends not on whether students use AI—almost all do, to some extent—but rather on how they employ these tools.\r\n\r\nFigure C1:Word Cloud of Student Motivations for Generative AI Use\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 147 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?’\r\n\r\nFigure C2:Relationship Between AI Sentiment and AI Adoption\r\nPanel A. Ever Used Generative AI\r\nRefer to caption\r\nPanel B. Uses AI for Academic Purposes\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents the relationship between AI sentiment and AI adoption. Panel A shows the proportion of respondents who have ever used generative AI, while Panel B shows the proportion who use AI for academic purposes. Each point represents the mean adoption rate for respondents within sentiment score bins of width 0.2. Sentiment scores are standardized compound scores computed using Hutto and Gilbert, (2014)’s VADER algorithm applied to responses to an open-ended question about generative AI. Positive values indicate positive sentiment and negative values indicate negative sentiment. The dashed lines show OLS best-fit lines estimated on the microdata, with coefficients and standard errors (in parentheses) displayed in the top-left corner of each panel.\r\n\r\nFigure C3:Frequency of Keywords in Student Motivations for AI Use\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to AI use. The responses come from the question “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” Color coding indicates the category of each theme. Usage type refers to how students use AI tools. Motivation captures what drove students to try AI. Concerns include mentions of course policies and academic integrity, individual reservations about AI use, worries about AI’s impact on education, and AI’s technical limitations.\r\n\r\nAppendix DQualitative Evidence on Student Views of AI Policies\r\nIn this section, we analyze student responses to an open-ended question about Middlebury’s AI policies. The survey asked: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Appendix Figure D1 presents a word cloud of the most frequent words in student responses. To analyze these responses systematically, we classified each response using keywords based on their content. Appendix Figure D2 shows the frequency of keywords in our classification.\r\n\r\nD.1Polarized Views on Generative AI Policy Approaches\r\nStudents expressed markedly different views about appropriate AI policies, revealing fundamental disagreement about the path forward. Some strongly advocated for embracing AI technology. As one student argued, “The tool is there, there is supply and there is demand. Don’t fight another war on drugs. Don’t live in a fake reality”. Others called for significant restrictions, arguing that “the use of generative AI is dishonest and corrosive” and that it “prohibits these organic processes and divorces students from true learning”.\r\n\r\nHowever, the most common position advocated for a balanced approach that would allow beneficial uses while restricting harmful ones. Students distinguished between uses that enhance learning (like concept explanation) and those that substitute for learning (like generating entire essays). One student articulated this nuanced view particularly well: “AI also can really be helpful at explaining a textbook problem that doesn’t make sense, or guiding slightly with homework, or creating study materials, or editing/tightening up your prose. All of those things are good, and universities should figure out how to maximize AI use for those reasons and to minimize students just feeding their problem sets into ChatGPT”\r\n\r\nA recurring theme was the futility of blanket bans. Many students emphasized that prohibition would be ineffective, with one noting “I don’t think anyone really cares what the policy of any given class is. If professors want people to not use it, they need to structure assessments in a way that will discourage use.” Another compared AI bans to restricting internet use, arguing “AI policies seem to be totally irrelevant. It’s like telling people they can’t use the internet as a resource for the class.” This ineffectiveness of bans creates fairness concerns. As one student explained: “I think if it is banned in a class, that should be enforced (and right now it absolutely is not)… As with any form of cheating, those who don’t cheat are put at a disadvantage.” Another student expressed similar frustration: “I find it discouraging when I hear classmates saying they use AI for things such as essays when they use it in dishonest ways.”\r\n\r\nD.2Need for Clear Guidelines\r\nThe most frequently expressed concern was the need for clear guidelines about generative AI use. Several students reported confusion about what constitutes acceptable use, with one noting “I think it should be more clear whether we can use it and how and how to cite it since most professors rarely mention it at all”. Other students emphasized the importance of professors explaining their policies upfront and their rationale, with one stating “I think that Professor’s should be very specific about what is allowed and their reasoning behind their policy”.\r\n\r\nMany students advocated for standardization across classes, observing that “Sometimes its confusing when one class allows it and another doesn’t and the other encourages it and so on so if there was a school wide or department wide policy that could help”. Yet there is also disagreement regarding standardization. Some students preferred leaving the decision to individual professors, arguing that “GenAI is more effective in some classes/majors than others. Making sure professors understand how students use GenAI and how useful GenAI is in their class (given the course structure, nature of assignments/material, etc.) is very important for the class policy.”\r\n\r\nD.3Training and Support Services\r\nStudents strongly emphasized the need for training in appropriate generative AI use. Many suggested that the college should provide guidance on using AI tools effectively while maintaining academic integrity. One student proposed “a workshop that teaches you to effectively use GenAI without violating the honor code”. Students also expressed interest in learning how to leverage AI to enhance their learning experience rather than circumvent it. As one student explained: “I think it could be useful to develop some sort of training. How do we use AI in a way that actually benefits our learning? I tried out some things on my own but I feel that I need more guidance.”\r\n\r\nThis desire for training was often linked to workplace preparedness. Students recognized that AI proficiency would be valuable in their careers, with one noting “As the world uses more and more AI, I think it is an important tool that students should know how to leverage”. Another emphasized: “The moment us students leave campus, we will be using it in the professional world, and when used in combination with one’s own skills, it is merely a tool to maximize efficiency”.\r\n\r\nD.4Discussion\r\nThe responses reveal several key tensions in AI policy implementation. First, students express a desire to use AI in ways that enhance rather than substitute for learning, yet they recognize that blanket bans are ineffective and worry about being disadvantaged if they follow restrictions while others do not.\r\n\r\nSecond, while students desire clear guidelines, they also want flexibility to accommodate legitimate uses that vary by discipline and assignment type. Different courses and majors may find different AI uses appropriate based on their learning objectives and assessment types.\r\n\r\nThird, there is tension between faculty autonomy in setting course policies and students’ desire for consistent institutional standards. While some support letting professors determine appropriate AI use for their specific courses, others argue that varying policies across classes create confusion and enforcement challenges.\r\n\r\nFinally, students report significant variation in faculty attitudes toward AI. Some students perceived faculty fear or misunderstanding of AI tools, noting that categorical bans often reflect a lack of understanding about AI’s capabilities and limitations rather than pedagogical considerations.\r\n\r\nFigure D1:Word Cloud of Student Feedback on Generative AI Policies\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 133 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?”\r\n\r\nFigure D2:Frequency of Keywords in Student Feedback on AI Policies\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to Middlebury’s AI policies. The responses come from the question “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Color coding indicates the category of each theme. Policy Approach captures suggestions about how AI should be regulated at the college. Adoption Views reflect positions on whether and how AI should be integrated into academic work. Training Needs indicates requests for guidance and support. Other Concerns include issues of workplace relevance and fairness.\r\n\r\nSee pages 1 of results/ai-survey-midd.pdf See pages 2- of results//ai-survey-midd.pdf\n",
    "totalFiles": 2
  },
  {
    "id": "extraction_1754349453440_05vxf114f",
    "timestamp": "2025-08-04T23:17:33.440Z",
    "files": [
      {
        "name": "story.txt",
        "size": 730,
        "type": "text/plain",
        "extractedText": "I remember the moment that sparked this project. It was during a walk across Middlebury’s campus late one fall afternoon—just after office hours. A student had come in, clearly bright but struggling, and told me they’d been using ChatGPT late at night to help with assignments because they were too anxious to speak up in class. That confession stuck with me.\r\n\r\nLater, over coffee with Germán, we started wondering—how many students were turning to generative AI as a quiet lifeline? Not to cheat, but to learn? That simple question became the seed for this entire study. What we found wasn’t just statistics. It was a story of adaptation, of students reshaping how they learn, and of institutions racing to catch up.\"",
        "extractionStatus": "success"
      },
      {
        "name": "Generative AI in Higher Education E.txt",
        "size": 117470,
        "type": "text/plain",
        "extractedText": "Generative AI in Higher Education: Evidence from an Elite College†\r\nZara Contractor\r\nGermán Reyes\r\nAbstract\r\nGenerative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT’s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI’s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI’s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\r\n\r\n1Introduction\r\nThe rapid advancement of generative artificial intelligence (AI) is transforming higher education at an unprecedented pace. The launch of ChatGPT and similar tools has introduced technologies capable of performing tasks central to academic assessment and learning—writing essays, solving complex problems, and explaining intricate concepts—instantly and at near-zero marginal cost. Yet systematic evidence on the nature and implications of this new technology remains scarce. How widespread is generative AI adoption among students, and what factors drive it? Do students primarily use AI to augment their learning or to automate coursework, potentially harming human capital development? Could disparities in access to premium AI resources amplify existing educational inequalities?\r\n\r\nThese questions are at the heart of ongoing debates about the role of AI in education, which have relied heavily on anecdotal evidence and speculation rather than rigorous evidence. As a result, universities have implemented policies that vary dramatically, from outright bans to permissive adoption, often without clear evidence on their effectiveness or unintended consequences (Nolan, , 2023; Xiao et al., , 2023; McDonald et al., , 2025).\r\n\r\nThis paper addresses this gap by systematically examining generative AI adoption at Middlebury College, a highly selective liberal arts college in Vermont. With approximately 2,800 undergraduate students, Middlebury offers 49 majors across the arts, humanities, languages, social sciences, and natural sciences. Over the past decade, Middlebury has consistently ranked among the top 10 liberal arts colleges in U.S. News & World Report, with an average ranking of 8.5 between 2015 and 2025. The college’s selectivity is comparable to that of many elite research universities—Middlebury’s acceptance rate of approximately 10 percent is similar to that of Boston University (11 percent), Georgetown University (13 percent), and the University of Virginia (16 percent).\r\n\r\nOur analysis draws on survey data collected from the student population between December 2024 and February 2025. The survey collected detailed information about AI usage, including the frequency and purpose of use, perceived impacts, and responses to institutional policies. To minimize selection bias, we framed the survey broadly as examining technology use and provided incentives for participation, achieving a 22.9 percent response rate with 634 responses representing a broad cross-section of the student population.\r\n\r\nWe document five main findings. First, generative AI is approaching near-universal adoption at an unprecedented speed. Over 80 percent of students use AI for academic purposes, up from less than 10 percent before Spring 2023. This represents one of the fastest technology adoption episodes ever documented, dramatically exceeding the 40 percent adoption rate among U.S. workers (Bick et al., , 2025) and the 23 percent among all U.S. adults (McClain, , 2024). These levels are consistent with international findings of 50–70 percent adoption in university contexts (Nam, , 2023; Stöhr et al., , 2024; Ravšelj et al., , 2025).\r\n\r\nSecond, AI adoption is markedly unequal across academic disciplines and demographic groups. Field of study is the strongest predictor of adoption, likely reflecting how well AI capabilities align with the academic tasks required across fields. Adoption ranges from 91.1 percent in Natural Sciences majors (including mathematics and computer science) to significantly lower levels in Literature (48.6 percent) and Languages (57.4 percent). Additionally, demographic disparities exist. For example, males adopt AI at higher rates than females (88.7 versus 78.4 percent), a pattern that is consistent with documented AI gender gaps in other contexts (Otis et al., , 2024). Most notably, lower-achieving students have a higher adoption rate than their higher-achieving peers (87.1 versus 80.3 percent). As a result, AI could serve as an equalizing force if it enhances student learning—helping struggling students catch up to their peers—but could also widen achievement gaps if AI undermines skill development.\r\n\r\nThird, generative AI transforms the students’ learning production function by both augmenting student capabilities and automating academic tasks. We classify academic tasks as augmentation when AI enhances human capabilities while maintaining student engagement (e.g., explaining concepts, proofreading) versus automation when AI directly produces outputs with minimal cognitive involvement (e.g., writing essays, creating images), and average usage rates across tasks within each category. We find that 61.2 percent of AI users employ these tool for augmentation purposes, while 41.9 percent use it for automation. Qualitative evidence reinforces these patterns: students describe AI as an “on-demand tutor” for augmentation purposes, particularly valuable when traditional resources like office hours are unavailable, while automation use centers on time savings during periods of overwhelming workload. These self-reported patterns align closely with actual usage data from Claude conversation logs (Handa et al., 2025a,).\r\n\r\nFourth, institutional policies can significantly influence AI adoption, though their effectiveness can be undermined by informational frictions. We find that explicit prohibitions dramatically reduce self-reported intended use 39 percentage points (pp). Importantly, the effects of this policy vary substantially across student groups, with females showing larger reductions in usage under prohibition than males, suggesting that one-size-fits-all policies may inadvertently create disparate impacts across groups. Moreover, we also document substantial informational gaps about institutional AI policies. Many students (19.2 percent) do not understand AI policy rules, few know about institution-provided premium AI resources (10.1 percent) or proper AI citation practices (32.6 percent)—a skill that is necessary for academic integrity.\r\n\r\nFifth, students believe that AI has a positive learning impact and these beliefs strongly predict their own AI usage. Most students believe that AI improves their understanding of course materials (70.2 percent) and learning ability (60.1 percent), though fewer believe it improves grades (41.1 percent). Perhaps unsurprisingly, students who believe that AI improves their academic outcomes are significantly more likely to use these tools. For example, a ten-percentage-point increase in the belief that AI improves learning corresponds to a 4.9 percentage-point increase in adoption, highlighting how perceptions can affect technology diffusion in educational settings.\r\n\r\nTaken together, our findings reveal that generative AI has already reshaped students’ college experience. The substantial information gaps that we document—regarding both permitted uses and available resources—suggest straightforward opportunities for policy intervention. Yet the observed heterogeneity across disciplines and student groups indicates that uniform policies risk unintended consequences: blanket prohibitions may disproportionately disadvantage students who benefit most from AI augmentation, while unrestricted use may encourage automation practices detrimental to skill development. Thus, effective policy requires distinguishing clearly between AI uses that enhance learning and those that undermine it.\r\n\r\nWhile our findings are specific to the population we study—students at an elite liberal arts college—this setting offers valuable into AI’s broader societal impact. The rapid uptake of generative AI in higher education may reflect a generalizable principle regarding the diffusion of generative AI: adoption may be fastest in contexts where AI consolidates fragmented tools into a unified platform. Students have long had access to various tools for the academic tasks for which they now use generative AI.1 What distinguishes AI is its ability to provide all these services through a single, instantly accessible interface at virtually zero marginal cost. This consolidation likely drives students’ perception of enhanced learning efficiency and contributes to the fast adoption. This principle may extend well beyond education: industries with fragmented, specialized tools could experience similar trajectories of rapid AI adoption, even if AI itself does not introduce fundamentally new capabilities.\r\n\r\nOur findings contribute to a rapidly growing literature examining the adoption and impacts of generative AI. Recent work has focused on AI’s effects on worker productivity (Noy and Zhang, , 2023; Dell’Acqua et al., , 2023; Peng et al., , 2023; Cui et al., , 2024; Brynjolfsson et al., , 2025) and its potential to transform occupations (Felten et al., , 2021, 2023; Eloundou et al., , 2024). While several papers document AI adoption in workplace settings (Bick et al., , 2025; Hartley et al., , 2025; Humlum and Vestergaard, , 2025) and at the firm level (McElheran et al., , 2023; Bonney et al., , 2024; Kharazian, , 2025), we examine adoption in higher education—a critical setting where future high-skilled workers develop human capital.\r\n\r\nWe add to an emerging literature examining generative AI in education. Some studies focus on the learning impacts of generative AI, finding mixed impacts depending on the setting (e.g., Lehmann et al., , 2024; Bastani et al., , 2025). A complementary strand of the literature documents adoption patterns across diverse educational contexts, including universities in Australia (Kelly et al., , 2023), Ghana (Bonsu and Baffour-Koduah, , 2023), Norway (Carvajal et al., , 2024), Sweden (Stöhr et al., , 2024), and multi-country studies (Ravšelj et al., , 2025). To the best of our knowledge, this paper provides the first systematic evidence on generative AI adoption in U.S. higher education. Importantly, we focus specifically on how students use AI for academic purposes—unlike studies that measure more general usage including personal lives (e.g., Ravšelj et al., , 2025)—and go beyond documenting adoption rates to examine how students integrate AI into their academic workflows, distinguishing between augmentation and automation, the role of student beliefs about AI’s educational impacts, and how institutional policies shape usage patterns.\r\n\r\nOur findings also contribute to the literature on technology diffusion. Unlike the S-shaped adoption curves documented for most technologies (Griliches, , 1957; Rogers, , 1962), we observe extremely rapid adoption of generative AI, with 80 percent uptake within two years of ChatGPT’s release. This pattern contrasts sharply with the adoption trajectories of historical General Purpose Technologies (GPTs) like electricity, which required decades to achieve widespread use (David, , 1990; Bresnahan and Trajtenberg, , 1995). The rapid diffusion we document aligns with evidence that newer technologies diffuse faster than older ones (Comin and Hobijn, 2010a,; Comin and Hobijn, 2010b,). Two factors likely explain AI’s exceptional adoption speed compared to other GPTs. First, unlike electricity or steam power, generative AI requires minimal physical infrastructure—students access it through existing devices—and is available at no cost to consumers, eliminating the financial barriers that typically slow technology adoption. Second, as noted above, AI consolidates multiple specialized tools into a single platform, making its benefits immediately apparent without requiring specialized training or organizational restructuring, unlike previous GPTs (Brynjolfsson and Hitt, , 2000).\r\n\r\n2Data: Novel Student Survey\r\n2.1Recruitment and Structure\r\nWe conducted the survey from December 2024 to February 2025. All Middlebury College students were contacted via email and received a reminder a few weeks and two months after the initial invitation. To minimize selection bias, the recruitment materials described the survey broadly as a study on students’ use of technology in their academic and personal lives. To incentivize participation, students who completed the survey qualified for entry into a lottery for Amazon gift cards ranging in value from $50 to $500. The full survey instrument is provided in Appendix LABEL:app:survey.\r\n\r\nThe survey contains three main sections (see Appendix Figure A1 for the survey flow). First, we gather demographic and academic information, including gender, race/ethnicity, type of high school attended (private or public), current academic year, and declared or intended major.2 We also collect data on academic inputs and performance through students’ self-reported typical weekly hours spent studying and their first-year GPA.3\r\n\r\nSecond, we measure students’ experience with generative AI tools. We begin by asking whether students have ever used generative AI tools like ChatGPT or Claude. For those who have, we collect information about their usage patterns, including frequency of use during the academic semester, which specific AI models they use, and whether they pay for AI tools. We also gather data on how students use AI for different academic tasks, including writing assistance, learning support, and coding.\r\n\r\nThe final part of the survey elicits students’ beliefs about generative AI’s adoption and impacts. We ask students about their perceptions of AI’s effects on their academic experience across multiple dimensions, including learning, grades, time management, and understanding of course material. We then collect information on the role of institutional policies, asking students how different policy environments influence their likelihood of using AI. We also elicit students’ beliefs about AI usage among their peers, including their estimates of the fraction of Middlebury students who use AI for schoolwork and leisure. The survey concludes with two open-ended questions that allow students to share their experiences with AI in academic settings and provide feedback on the college’s AI policies and support services.\r\n\r\n2.2Sample and Summary Statistics\r\nOut of Middlebury’s 2,760 enrolled students, 739 began the survey. We exclude 105 respondents who left the survey before reaching the generative AI usage module, leaving us with an analysis sample of 634 students (22.9 percent of the student body). This response rate is comparable to that of similar surveys (Wu et al., , 2022). To make our sample more representative of Middlebury’s student body, we construct poststratification weights based on the distribution of declared majors from administrative records.4 Specifically, we weight observations by the ratio of each major’s share in the student population to its share in our survey responses.5\r\n\r\nTable Figures and Tables presents summary statistics for our sample. Column 1 reports unweighted survey averages, column 2 shows averages after applying poststratification weights, and column 3 provides population benchmarks from administrative records where available. Our unweighted sample consists of 44.6 percent male and 50.8 percent females. The racial/ethnic composition includes 61.8 percent white, 15.5 percent Asian, 9.9 percent Hispanic, and 3.6 percent Black students. The majority of students (54.3 percent) attended a public school, while 42.0 percent attended a private high school. Our sample represents 43 different majors across seven fields of study, with 31.1 percent of respondents not yet having declared a major. In our analysis, we group these undeclared students by their intended field of study as reported in the survey.\r\n\r\nComparing our unweighted sample to administrative records reveals notable differences. Our sample overrepresents white students (61.8 versus 53.8 percent) and Asian students (15.5 versus 7.3 percent), while underrepresenting Black students (3.6 versus 5.2 percent) and Hispanic students (9.9 versus 12.4 percent). First-year students are overrepresented (31.1 versus 25.5 percent), while senior students are underrepresented (21.3 versus 28.7 percent). Our weighting procedure partially addresses these discrepancies—particularly in the distribution of academic fields, where the weighted figures closely approximate administrative records—but some demographic differences persist. Despite these differences, our weighted sample provides reasonably close approximations to the college population on most dimensions, particularly for academic characteristics and field distributions.\r\n\r\n3Generative AI Usage Patterns Among Students\r\n3.1Adoption of Generative AI\r\nGenerative AI has achieved a remarkably high adoption rate among Middlebury College students. Figure 1 presents the distribution of AI usage frequency during the academic semester, categorized into four levels: “Rarely” (a few times per semester), “Occasionally” (a few times per month), “Frequently” (a few times per week), and “Very Frequently” (daily or almost daily). Overall, 82.5 percent of students report using generative AI for academic purposes, with substantial variation in usage intensity: 23.5 percent use it rarely, 22.2 percent occasionally, 26.2 percent frequently, and 10.6 percent very frequently.\r\n\r\nThe adoption rate at Middlebury aligns closely with patterns observed across other higher education institutions. A BestColleges survey of U.S. undergraduate and graduate students in fall 2023 found that 56 percent had used AI on assignments or exams (Nam, , 2023). Ravšelj et al., (2025) surveyed higher education students globally between late 2023 and early 2024, finding that 71.4 percent had ever used ChatGPT—though this figure encompasses all usage rather than academic use specifically. Similarly, Stöhr et al., (2024) document that 63 percent of Swedish university students had used ChatGPT by spring 2023, with 35.4 percent reporting regular use and 27.6 percent rare use. Among Norwegian university students, Carvajal et al., (2024) find that 68.9 percent use AI tools occasionally or more frequently.\r\n\r\nThese adoption rates in higher education far exceed those in the general population and workforce. Pew Research finds that only 23 percent of U.S. adults have ever used ChatGPT (McClain, , 2024), while Gallup reports that just one-third of U.S. workers have used AI for work (Gallup, , 2024). Bick et al., (2025) and Hartley et al., (2025) estimate that about 40 percent of the U.S. working-age population used generative AI for work as of late 2024 and early 2025. Both studies document substantial heterogeneity across industries, with information services showing the highest adoption rates at 56–62 percent—still considerably below the 80 percent adoption rate at Middlebury. Even among workers in AI-exposed occupations, Humlum and Vestergaard, (2025) find adoption rates of only 41 percent.6\r\n\r\n3.2Adoption by Student Characteristics and Field of Study\r\nAI adoption varies considerably across demographic groups and academic disciplines (Figure 1 and Appendix Table LABEL:tab:ai_freq_het). Males report higher usage rates than females (88.7 versus 78.4 percent). Usage patterns differ markedly by race/ethnicity: Black students (92.3 percent) and Asian students (91.3 percent) exhibit the highest adoption rates, while white students (80.2 percent) and Hispanic students (77.9 percent) report lower usage. Students from private high schools use AI more frequently than those from public schools (84.1 versus 80.4 percent). Notably, students with below-median GPAs report higher usage rates than their higher-achieving peers (87.1 versus 80.3 percent). Adoption varies widely by field of study: Natural Sciences leads with 91.1 percent usage, followed by Social Sciences at 84.6 percent, while Languages (57.4 percent) and Literature (48.6 percent) show substantially lower adoption rates.\r\n\r\nTo examine how student characteristics jointly relate to AI adoption, we estimate multivariate OLS regressions that include all observed characteristics simultaneously.7 Table LABEL:tab:ai_usage_correlates presents regression estimates using four usage thresholds as outcomes. Each column represents the probability of meeting progressively higher frequency thresholds: any AI use (column 1), at least monthly use (column 2), at least weekly use (column 3), and daily use (column 4).\r\n\r\nThe regression results confirm the patterns observed in the descriptive statistics. Holding other characteristics constant, males are 10.3 pp more likely than females to use AI (column 1, \r\np\r\n<\r\n0.05\r\n), with this gender gap typically widening at higher usage frequencies (columns 2–4, all \r\np\r\n<\r\n0.01\r\n). Black and Asian students show substantially higher adoption rates than white students, at 11.8 and 10.7 pp respectively (column 1, both \r\np\r\n<\r\n0.01\r\n). Students from public high schools are 3.0 pp less likely to use AI than those from private schools, but this difference is not statistically significant. Field of study emerges as the strongest predictor of adoption. Compared to Natural Sciences majors, students in Literature, Languages, Arts, and Humanities all show lower usage rates, with the differences being statistically significant at high usage frequencies (columns 3 and 4). Social Sciences majors exhibit adoption rates similar in magnitude to Natural Sciences majors across all usage frequency thresholds.\r\n\r\nOur findings on heterogeneity in AI adoption align with patterns documented in other settings. The gender gap in AI adoption at Middlebury—10.3 pp higher for males—is consistent with evidence across multiple studies. A meta-analysis of 18 studies by Otis et al., (2024) finds that males are 10–20 percentage points more likely to use generative AI than females. This gender gap in AI adoption appears in all educational studies (Nam, , 2023; Carvajal et al., , 2024; Stöhr et al., , 2024; Ravšelj et al., , 2025). Our finding that students with below-median GPAs are more likely to use AI aligns with Carvajal et al., (2024), who document higher adoption rates among students with lower admission grades. This pattern of greater adoption among lower-achieving students suggests that AI could narrow achievement gaps if it enhances learning and skill development, but could widen these gaps if it undermines the acquisition of fundamental skills.\r\n\r\nThe differences in adoption rates across academic fields at Middlebury mirror patterns documented in other educational settings. Stöhr et al., (2024) find that technology and engineering students exhibit significantly higher ChatGPT usage compared to students in humanities. Similarly, Nam, (2023) report that 62 percent of business majors and 59 percent of STEM majors have used AI tools for coursework, compared to 52 percent of humanities majors. Ravšelj et al., (2025) document comparable disciplinary differences, with applied sciences students showing substantially higher usage rates than arts and humanities students.\r\n\r\nThese academic differences persist into the workforce. Bick et al., (2025) find stark variation by college major: STEM graduates have the highest AI adoption rates (46.0 percent), followed by Business/Economics graduates (40.0 percent), while Liberal Arts graduates show substantially lower rates (22.4 percent). Humlum and Vestergaard, (2025) document similar patterns by occupation, finding that roles requiring strong writing and technical skills—such as marketing specialists and journalists—have the highest adoption rates. These consistent patterns across educational and professional contexts suggest that field-specific factors, particularly the applicability of AI tools to different types of tasks, shape adoption in systematic ways.\r\n\r\n3.3Timing of Generative AI Adoption\r\nThe speed of technology diffusion is a critical determinant of its economic and social impact (David, , 1990; Hall and Khan, , 2003; Stokey, , 2021). In educational contexts, rapid technology adoption can create or exacerbate inequalities between early and late adopters, particularly if the technology confers significant learning advantages (World Bank, , 2016). To track the timing of AI adoption among Middlebury students, we asked them when they first began using generative AI for academic purposes, with options ranging from “This semester (Fall 2024)” to “Before Spring 2023” (as a reference, ChatGPT’s public launch was in November 2022).\r\n\r\nStudents adopted generative AI at an extraordinary pace. Figure 2 shows that the cumulative adoption rate grew dramatically from less than 10 percent before Spring 2023 to slightly above 80 percent by Fall 2024. The pace of adoption has accelerated over time, likely reflecting improvements in AI capabilities. Among current users, 25.7 percent adopted AI in Fall 2024 alone, compared to 19.9 percent in Spring 2024 and 16.3 percent in Fall 2023.\r\n\r\nThe adoption rate among Middlebury students far exceeds that observed in other populations and represents one of the fastest technology adoption episodes ever documented. For comparison, Bick et al., (2025) show that it took over 20 years for computers to reach an 80 percent adoption rate in the U.S. working-age population, and about 15 years for internet adoption to reach similar levels. Even generative AI adoption in the broader population has been markedly slower: Pew Research found that just 23 percent of U.S. adults had ever used ChatGPT as of February 2024, up from 18 percent in July 2023 (McClain, , 2024). The dramatically faster adoption rate we document—reaching over 80 percent in less than two years—suggests academic settings may uniquely accelerate AI diffusion.\r\n\r\nTo identify early versus late adopters of generative AI, in Table LABEL:tab:ai_adopt_correlates and Appendix Figure A2, we analyze how adoption timing varies across student characteristics. The rate of adoption varies substantially across student characteristics. Male students led adoption, with a 8.9 pp higher probability of using AI before Spring 2023 compared to females (column 1, \r\np\r\n<\r\n0.01\r\n)—a gender gap that persists across all periods (columns 2–5). Black and Asian students also adopted earlier than white students, though these differences reach statistical significance only in later periods (columns 4–5). Field of study is a strong predictor of adoption timing. For example, students in Languages consistently lagged behind Natural and Social Sciences majors, with significantly lower adoption rates across nearly all time periods.\r\n\r\n3.4Choice of Generative AI Models\r\nMajor AI companies operate on a freemium model, where free versions coexist with premium subscriptions that offer higher usage limits and access to more advanced models. This tiered structure creates potential for a new form of educational disparities: if paid versions confer substantial academic advantages, students who cannot afford subscriptions may be systematically disadvantaged. To investigate these potential disparities, we presented respondents with a list of options including both free and paid versions of popular models. We also collected information on monthly subscription expenditures, with response options ranging from no active subscription to spending more than $40 monthly (standard subscriptions to major models like ChatGPT cost $20 per month at the time).\r\n\r\nOpenAI’s ChatGPT dominates AI usage among Middlebury AI users, with the free version capturing the largest market share. Figure 3 shows that 89.3 percent of AI users rely on the free version of ChatGPT, making it dramatically more popular than any alternative. Google Gemini (13.5 percent) and Microsoft Copilot (7.7 percent) are distant competitors, while other platforms each capture less than 5 percent of users. This dominance mirrors patterns in other academic settings, where Stöhr et al., (2024) find substantially higher familiarity with ChatGPT compared to alternative platforms, and aligns with broader workforce trends documented by Bick et al., (2025).8\r\n\r\nDespite near-universal AI adoption, only 11.3 percent of AI users pay for any AI service.9 This figure is remarkably similar to the 8 percent found in Ravšelj et al., (2025)’s multi-country survey. This low payment rate suggests that for most students, the premium features of paid versions—primarily higher usage limits and access to more advanced models—do not justify the subscription cost. However, payment patterns reveal significant disparities: males and Asian students are substantially more likely to purchase AI subscriptions (Appendix Table LABEL:tab:ai_model_correlates and Appendix Figure A3), potentially reflecting differences in usage intensity across demographic groups.\r\n\r\n4Generative AI and the Production of Learning\r\n4.1The Use of Generative AI across Academic Tasks\r\nHow is generative AI transforming the traditional inputs to student learning? The educational production function includes inputs like time spent studying (e.g., Stinebrickner and Stinebrickner, , 2008), faculty instruction (e.g., Fairlie et al., , 2014), peer interactions (e.g., Sacerdote, , 2001), and academic support services (e.g., Angrist et al., , 2009). AI tools have the potential to complement or substitute for these traditional inputs. For example, using AI to explain concepts might substitute for faculty office hours, while using it for proofreading might reduce time needed for academic support services.\r\n\r\nTo understand the role of AI in students’ learning production function, we collected information about students’ AI usage across ten common academic tasks: proofreading, generating ideas, writing essays, editing essay drafts, coding assistance, creating images, explaining concepts, composing emails, summarizing materials, and finding information. For each task, students indicated their frequency of AI use on a five-point scale ranging from never to daily. We supplemented this quantitative data with open-ended responses about how AI influences their academic work process.\r\n\r\nStudents use generative AI for a wide range of academic tasks, with the highest adoption rates for learning support and text-processing activities. Figure 4, Panel A shows that explaining concepts is the most common use case, with 80.3 percent of AI users using it for this task. Summarizing texts follows as the second most common task (74.0 percent), followed by finding information and generating ideas (63.1 and 61.9 percent). Writing assistance tasks like proofreading and editing essays are also common, used by 54.1 and 47.3 percent of AI users, respectively. Technical applications like coding help are significant (34.4 percent), considering that many academic degrees involve no programming. Notably, while 23.5 percent of AI users report using it for writing essays, this represents a relatively low adoption rate compared to other academic uses, suggesting students may be more hesitant to use AI for primary content creation. The lowest adoption rate is for creating images at 20.4 percent, likely reflecting fewer academic use cases for this capability.\r\n\r\nAn important limitation of our survey is that it relies on self-reported usage, which may introduce measurement error. For example, students might underreport uses they perceive as academically inappropriate—such as essay writing—while overreporting those viewed as legitimate learning tools (Ling and Imas, , 2025). To assess the validity of our self-reported measures, we compare our findings with Handa et al., 2025a, who analyze actual Claude usage patterns among users with university email addresses. Several caveats apply: their data captures conversation-level interactions rather than student-level usage, and most students in our sample use ChatGPT rather than Claude (Section 3.4). Despite these limitations, the comparison provides a useful benchmark for evaluating our survey responses.\r\n\r\nReassuringly, our results are consistent with Anthropic’s data. Both studies identify explaining concepts and technical problem-solving as primary use cases. Handa et al., 2025a report that the second largest use case (33.5 percent of conversations) involves “technical explanations or solutions for academic assignments,” while we find that 80.3 percent of AI users use it for explaining concepts—a difference likely attributable to our student-level versus their conversation-level measurement. Similarly, the most common usage category in Claude involves “designing practice questions, editing essays, or summarizing academic material” (39.3 percent of conversations), aligning with our findings that 74.0 percent of AI users use it for summarizing texts and 47.3 percent for editing essays. The disciplinary patterns also converge: Handa et al., 2025a finds that computer science, natural sciences, and mathematics conversations are overrepresented, which mirrors our finding that Natural Science majors show significantly higher AI adoption rates (Figure 1). Overall, these convergent findings from self-reported survey data and actual usage logs suggest that our results capture genuine patterns of student AI engagement rather than merely reflecting social desirability in responses.\r\n\r\n4.2Automation versus Augmentation\r\nAre students using generative AI primarily to augment their learning or to automate their coursework? This distinction is crucial for understanding AI’s impacts (Autor and Thompson, , 2025)—augmentation may enhance students’ learning processes while maintaining their active engagement and critical thinking, whereas automation produces fully-formed outputs that could be submitted with minimal student input, potentially harming learning.\r\n\r\nTo examine this empirically, Figure 4, Panel B categorizes the ten measured tasks based on whether they augment or automate academic tasks. We classify tasks as augmentation when they enhance human capabilities (explaining concepts, finding information, proofreading, and editing drafts) and as automation when they directly produce outputs (writing essays, creating images, composing emails, summarizing texts, generating ideas, and coding assistance). We then calculate the percentage of AI users who employ each category at various frequencies.10\r\n\r\nStudents use generative AI for both augmentation and automation, but with markedly different frequency. While 61.2 percent of AI users report using AI for augmentation tasks, 41.9 percent report using it for automation—thus, there is a substantial 19.3 pp difference. This gap is mainly driven by occasional use (19.7 percent for augmentation versus 12.7 percent for automation) and frequent use (16.3 versus 9.0 percent). Most strikingly, students are more than twice as likely to use AI daily for augmentation (5.0 percent) compared to automation (2.5 percent). The higher frequency of augmentation use also suggests that students find augmentation uses more valuable for their day-to-day academic activities. Our findings align closely with patterns observed in actual AI usage data: Handa et al., 2025b analyze real conversation logs from Claude and find that 57 percent of workplace AI interactions involve augmentation while 43 percent involve automation.\r\n\r\nQualitative evidence from open-ended responses provides additional insights into students’ motivations for using AI (see Appendix C for additional results and validation of the open-ended measure). Students’ descriptions align closely with the augmentation-automation framework. For augmentation, many characterize AI as an “on-demand tutor,” particularly valuable when traditional resources like office hours are unavailable. Non-native English speakers frequently mention using AI for proofreading to overcome language barriers, while students from technical majors describe using it to debug code and understand error messages. For automation, time savings emerged as the dominant motivation, with 21.7 percent of open-ended responses explicitly mentioning efficiency benefits (Appendix Figure C3). Students describe turning to AI during periods of overwhelming workload or looming deadlines, using it to generate initial drafts or complete routine assignments. That students automate tasks mostly when time-pressed helps explain why automation is less common than augmentation.11\r\n\r\nOur finding that students favor augmentation over automation extends beyond Middlebury. In Appendix Figure A4, we re-analyze data from Ravšelj et al., (2025)—a multi-country survey of higher education students—to examine how this balance varies across institutional quality.12 We classify universities into quintiles based on their Times Higher Education World University Rankings. Overall, students who use AI worldwide show similar adoption rates for augmentation (64.6 percent) and automation (63.1 percent) tasks—a much smaller gap than at Middlebury (Panel A). However, this aggregate pattern masks heterogeneity by institutional quality. Students at top-quintile universities show a modest preference for augmentation over automation, with this gap narrowing monotonically down the institution quality distribution. By the bottom quintile, augmentation and automation usage are virtually identical (Panel B).\r\n\r\n4.3Heterogeneity in Augmentation versus Automation Usage\r\nUnderstanding whether augmentation and automation patterns vary across student populations may help to design targeted support policies. To assess this, we construct four measures of augmentation and automation usage. First, we create binary indicators for whether students use AI for any augmentation or automation task. Second, we calculate the proportion of tasks in each category for which students employ AI. Third, using the Likert-scale responses, we create intensity measures that capture how frequently students use AI for augmentation and automation purposes. Finally, we compute the difference between augmentation and automation variables, as a measure that directly compare students’ relative preference for augmentation versus automation. Table LABEL:tab:ai_autom_regs presents regression estimates using these measures as outcomes.\r\n\r\nThe balance between augmentation and automation varies substantially by student characteristics. Males show higher adoption of both augmentation and automation tasks, with a similar magnitude in each case—they are 6.0 pp more likely to use augmentation prompts and 5.5 pp more likely to use automation prompts compared to females (columns 1 and 4, \r\np\r\n<\r\n0.05\r\n). Black students have a substantially higher augmentation intensity than white students (0.871 points higher, equivalent to 38.1 percent of the outcome mean, \r\np\r\n<\r\n0.01\r\n), but show no statistically significant differences in automation usage. This results in Black students having the strongest preference for augmentation over automation, with a 19.5 pp higher share of augmentation tasks relative to automation (column 7, \r\np\r\n<\r\n0.01\r\n). Asian students show high usage of both kinds relative to white students. Differences across field of study effects are sizable: humanities, languages, and literature majors tend to show lower usage of both augmentation and automation tools compared to natural science majors.\r\n\r\n5Institutional Policies and AI Adoption\r\n5.1The Role of Institutional Policies in Shaping Student Behavior\r\nInstitutional policies are central to shaping the adoption and diffusion of new technologies (Acemoglu, , 2025). In the context of generative AI in higher education, understanding how policies affect student behavior is essential for guiding evidence-based decision-making. To examine this, we asked students to report their likelihood of using AI under various policy scenarios, ranging from complete prohibition to unrestricted use. This analysis is particularly relevant in light of the ongoing debate about how universities should regulate generative AI use in academic settings (Nolan, , 2023), and the widespread variation in institutional policies across colleges (Xiao et al., , 2023).\r\n\r\nInstitutional policies substantially influence students’ reported likelihood of using generative AI. Figure 5 shows that when generative AI use is unrestricted, 52.4 percent of students report being likely or extremely likely to use it. This likelihood decreases modestly when policies require citation (40.9 percent) or when no explicit policy exists (42.3 percent). However, explicit prohibition creates a dramatic shift: only 13.4 percent of students report they would be likely or extremely likely to use AI when it is banned, while 72.9 percent say they would be unlikely or extremely unlikely to do so (note that these figures include both current AI users and students who do not use generative AI for academic purposes). These results suggest that institutional policies can significantly influence generative AI usage patterns, though a small fraction of students report they would likely use AI even when explicitly prohibited.\r\n\r\nThe magnitude of policy effects in our study aligns closely with findings from other contexts. Carvajal et al., (2024) estimate that banning AI reduces usage by 37.2 pp among females and 20.6 pp among males, for an overall drop of about 28.9 pp. In our survey, a ban leads to a 37.8 pp decline in usage, with a larger decrease among females (49.6 pp) than males (40.1 pp). These parallel results underscore how institutional policies can unintentionally produce disparate effects across gender. Notably, these differential policy effects extend beyond gender. Other demographic and academic characteristics—such as race and field of study—also moderate how students respond to policy restrictions (Appendix Table LABEL:tab:ai_policy_het), underscoring that institutional policies can produce non-neutral impacts along multiple dimensions.\r\n\r\n5.2Understanding of Institutional Policies and Resources\r\nInformation gaps and inattention can significantly affect technology adoption decisions (Duflo et al., , 2011; Hanna et al., , 2014). For example, imperfect information about rules, available resources, or proper usage guidelines could lead to underadoption of beneficial technologies or inadvertent policy violations. To test for the existence of information gaps, we examine three dimensions of policy understanding. First, we asked students whether they find AI policies in their current classes clear. Second, we measured awareness of free access to premium AI tools through the college—a resource that could reduce inequality but only if students know it exists. Third, we assessed whether students know how to properly cite AI when required, a mechanical skill necessary for academic integrity. We supplemented these measures with open-ended feedback about the college’s AI policies and support services.\r\n\r\nStudent understanding of institutional policies is high, though significant gaps remain. Figure 6, Panel A shows that most students (79.1 percent) understand when and where they are allowed to use AI in their classes, but a nontrivial minority (19.2 percent) find AI policies unclear. Moreover, critical knowledge gaps persist elsewhere. Only 10.1 percent know they have free access to Microsoft Copilot through the college (Panel B), and just 32.6 percent understand how to properly cite AI use (Panel C). These gaps vary systematically: females show better policy understanding than males (81.5 versus 75.8 percent), and non-white students demonstrate higher awareness across all three dimensions compared to white students.13\r\n\r\nThe qualitative evidence from open-ended responses reinforces these information frictions and reveals implementation challenges (Appendix D). Students express frustration with vague guidelines, requesting specific examples of acceptable versus unacceptable use cases. Many advocate for formal training, noting that simply knowing policies exist differs from understanding how to effectively integrate AI into their workflow. A particularly striking theme is students’ perception of blanket prohibitions as both ineffective and unfair, as they create a prisoner’s dilemma situation where compliant students are disadvantaged relative to those who secretly violate restrictions. Many responses call for a balanced approach—permitting AI use that supports learning while restricting uses that replace it—though the boundary between the two remains contested.\r\n\r\n6Beliefs About AI’s Educational Impact and Peer Usage\r\n6.1Student Beliefs of AI’s Impact on Educational Outcomes\r\nTechnology adoption decisions are shaped by beliefs about potential returns (Foster and Rosenzweig, , 2010). Students’ perceptions of how AI affects their learning may influence whether they adopt these tools and how they integrate them into their academic workflows. To elicit these beliefs, we asked students to evaluate AI’s impact across four dimensions of their academic performance: understanding of course materials, overall learning ability, time management, and course grades. For each dimension, students rated AI’s effect on a five-point scale ranging from “significantly reduces” to “significantly improves.” These subjective assessments provide insight into the perceived value of AI tools from the student perspective, which may differ from their actual effects on learning outcomes.\r\n\r\nStudents tend to believe that generative AI is beneficial for their academic performance, though the perceived benefits vary across dimensions. Figure 7 shows that the majority of students (70.2 percent) believe that generative AI improves their understanding of course materials, and 60.1 percent report improvement in their ability to grasp concepts, retain information, or learn new skills. Similarly, 59.4 percent report that AI improves their ability to complete assignments on time. Notably, while students believe AI helps their learning and assignment completion, they are less confident about its impact on course grades—41.1 percent believe it improves their grades, while 55.4 percent report no effect and 3.5 percent report negative effects. This pattern suggests that while students perceive generative AI as improving their learning process and workflow—through better understanding, skill development, and timely completion of work—these benefits do not necessarily translate into better course grades.14\r\n\r\nBeliefs about AI’s benefits strongly predict adoption. In Figure 8, we plot the relationship between the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis) for different subgroups of students (e.g., males, white students, public-school students, etc.). Across all four measured academic dimensions, there is a strong and statistically significant relationship between perceiving positive AI effects and AI adoption. The relationship is strongest for beliefs about the ability to improve course grades (Panel D): a 10-pp increase in the belief that AI improves grades is associated with a 5.6 pp increase in AI adoption (\r\np\r\n<\r\n0.01\r\n). Similar positive relationships exist for beliefs about learning ability (4.3 pp, \r\np\r\n<\r\n0.05\r\n), understanding of course materials (4.5 pp, \r\np\r\n<\r\n0.01\r\n), and timely assignment completion (5.3 pp, \r\np\r\n<\r\n0.05\r\n). These findings suggest that student beliefs about AI’s academic benefits—irrespective of the actual benefits—may play a crucial role in shaping adoption decisions.\r\n\r\nThese positive beliefs align with findings from other contexts. Ravšelj et al., (2025) report that the majority of students in their sample believe that ChatGPT improves their general knowledge (68.8 percent) and specific knowledge (62.7 percent)—remarkably similar to our finding that most students believe AI improves understanding of course materials (68.5 percent) and learning ability (57.6 percent). Similarly, 57.4 percent of students in Ravšelj et al., (2025)’s sample believe that ChatGPT helps meet assignment deadlines, while 59.4 percent in our data report AI improves timely assignment completion. Stöhr et al., (2024) provide complementary evidence from Swedish universities, where 47.7 percent of students believe that AI makes them more effective learners, yet only 17.3 percent believe these tools improve their grades—mirroring our finding that perceived learning benefits from AI use exceed perceived effects on course performance.\r\n\r\n6.2Student Beliefs About Peer Use of Generative AI\r\nStudents’ beliefs about their peers’ AI usage may influence their own adoption through multiple channels, including social norms (e.g., Giaccherini et al., , 2019), social learning (e.g., Foster and Rosenzweig, , 1995; Beaman et al., , 2021), peer effects (e.g., Bailey et al., , 2022), and competitive pressure to avoid falling behind (e.g., Goehring et al., , 2024). To measure these beliefs, we asked students to estimate what fraction of their peers use generative AI for different purposes and would use under different policy environments. Figure 9 presents the distribution of these beliefs. Panels A-C show students’ estimates of peer AI usage for schoolwork, leisure, and any purpose respectively. Panels D-F display students’ beliefs about AI usage under three policy environments: when classes have no explicit AI policy, when classes allow AI use, and when classes prohibit AI use.\r\n\r\nStudents systematically underestimate their peers’ AI usage. On average, they believe 65.2 percent of peers regularly use generative AI for schoolwork (Panel A), while our survey reveals an actual usage rate of 82.5 percent—a 17.3 pp gap. This underestimation appears consistent across educational contexts: Stöhr et al., (2024) find that only 38.7 percent of Swedish students believe AI chatbot use is common among peers, despite 63 percent actual usage. When we examine beliefs about policy-contingent behavior, students estimate that 62.7 percent of peers use AI when no explicit policy exists (Panel D), rising to 72.2 percent under permissive policies (Panel E) and falling to 43.5 percent under prohibition (Panel F). These beliefs align directionally with self-reported intentions, though with notable magnitude differences (Appendix Figure A5). For instance, while students believe 43.5 percent of their peers use AI in classes that prohibit it, only 28.4 percent report they themselves would be likely to do so.15\r\n\r\nNotably, students’ perceptions of peer AI adoption are closely linked to their own adoption behavior. Appendix Figure A6 shows a strong positive association between beliefs about peer usage and actual usage rates across student groups. This behavior is consistent with several psychological mechanisms, including the “false consensus effect” (Ross et al., , 1977), selection neglect (Jehiel, , 2018), or interpersonal projection bias (Bushong and Gagnon-Bartsch, , 2024). Still, virtually all groups systematically underestimate peer usage—actual usage rates exceed believed usage for every demographic and academic group we examine. This figure also shows that underestimation varies substantially across student characteristics: males use AI at 88.7 percent but believe only 66.6 percent of peers do so (22.1 pp gap), while females show a smaller gap (77.6 percent actual versus 65.1 percent believed, 13.3 pp gap).\r\n\r\nTaken together, students’ systematic underestimation of peer AI usage coupled with the strong relationship between beliefs and adoption suggests that misperceptions about social norms may shape technology diffusion. If AI enhances learning, then correcting these misperceptions through information provision could accelerate beneficial adoption; conversely, if AI undermines skill development, then students’ underestimation of peer usage may serve as an unintentional safeguard against harmful overadoption.\r\n\r\n7Discussion\r\nThis paper presents the first systematic evidence on generative AI adoption at a highly selective U.S. college. Using novel survey data, we document exceptionally rapid and widespread adoption, substantial shifts in the educational production function through augmentation and automation, and the significant roles of students’ beliefs and institutional policies in shaping AI use.\r\n\r\nOur results offer three implications for institutional policy and ongoing debates about AI in education. First, we identify low-cost opportunities to improve institutional policy effectiveness through targeted information provision. The significant gaps we document in students’ understanding of institutional AI policies, citation practices, and available AI resources suggest that simple interventions—such as clear guidelines, illustrative examples of acceptable uses, and AI literacy programs—can reduce unintentional academic integrity violations and support beneficial AI integration. Qualitative feedback strongly indicates student demand for more explicit guidance on responsible AI use.\r\n\r\nSecond, our evidence challenges alarmist narratives that conflate widespread AI adoption with universal academic dishonesty based on anecdotal accounts.16 Although AI use is indeed near-universal, we find clear evidence that students primarily employ AI as a tool for strategic task management: to enhance learning (augmentation) and selectively automate tasks when facing high time opportunity cost—not solely to circumvent academic effort. This distinction matters: by normalizing academic dishonesty as inevitable and universal, these narratives may shift social norms and encourage students who would otherwise use AI responsibly to engage in prohibited behaviors, believing “everyone else is doing it.”\r\n\r\nThird, our findings caution against policy extremes of either blanket prohibition or unrestricted AI use (Merchant, , 2024; McDonald et al., , 2025). Blanket prohibitions disproportionately harm students who benefit most from AI’s augmentation functions—particularly lower-achieving students—while also creating uneven compliance, placing conscientious students at a disadvantage relative to rule-breakers. Conversely, unrestricted AI use based solely on revealed preference arguments ignores important market failures in educational settings. Students often hold overly optimistic beliefs about AI’s learning benefits despite mixed empirical evidence, potentially leading to unintended negative learning outcomes. Most concerningly, permissive policies risk creating competitive dynamics where students feel compelled to adopt AI not for its learning benefits but simply to avoid falling behind in an educational “arms race” (Goehring et al., , 2024).\r\n\r\nFigures and Tables\r\nFigure 1:The Adoption of Generative AI among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the fraction of students who report using AI during the academic semester, categorized by demographic characteristics, high school type, academic cohort, GPA, and field of study. Usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily).\r\n\r\nThe category “All students” provides the baseline usage rate for the full sample. Gender categories are based on self-identification, with non-binary responses excluded due to a small sample size. “Private HS” refers to students who attended private high schools, while “Public HS” includes public institutions. “Cohort” denotes the student’s academic year, ranging from first-year (“Freshman”) to fourth-year and beyond (“Senior”). GPA categories (“GPA \r\n>\r\n p50” and “GPA \r\n<\r\n p50”) split students into groups above or below the median first-year GPA, as self-reported on a 4.0 scale. See Appendix B.1 for the classification of majors into fields of study.\r\n\r\nFigure 2:The Evolution of Generative AI Adoption among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the cumulative percent of students who reported using generative AI tools for academic purposes over time. The data is based on retrospective self-reports collected in our December 2024 survey, where students were asked “When did you first start using any form of Generative AI for academic purposes?” Response options ranged from “Before Spring 2023” to “This semester (Fall 2024).” The \r\nx\r\n-axis represents academic semesters, while the \r\ny\r\naxis represents the cumulative adoption rate. Vertical lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure 3:Adoption of Generative AI Models Among College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the adoption rates of various AI models as of Fall 2024. The horizontal axis shows the percent of students who reported using each tool, and the vertical axis lists the tools in descending order of adoption rates.\r\n\r\nFigure 4:Academic Uses of Generative AI\r\nPanel A. Across Common Academic Tasks\r\nRefer to caption\r\nPanel B. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of AI students who use generative AI for different academic tasks. For each task, usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). The number at the end of each bar represents the total percent of students who use AI for that purpose at any frequency. Tasks are ordered by total usage, from highest to lowest. Results are based on responses to the question: “For academic purposes, which of the following tasks do you typically use generative AI for?” Sample includes all students who reported using AI during the academic semester.\r\n\r\nFigure 5:Student Reported Likelihood of Using AI under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of students who report different likelihoods of using AI under various policy scenarios. For each policy, responses are categorized on a five-point scale from “Extremely unlikely to use AI” to “Extremely likely to use AI.” The sample includes all survey respondents. The question asked was: “How likely are you to use generative AI in a class with each of the following AI policies?”\r\n\r\nFigure 6:Understanding of Generative AI Policies and Resources\r\nPanel A. Understanding of AI Policies\r\nRefer to caption\r\nPanel B. Awareness of Copilot Access\r\nRefer to caption\r\nPanel C. Knowledge of Citation Requirements\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows students’ understanding of institutional AI policies and resources. Panel A displays the percent of students who report understanding AI policies in their classes, those who report having no explicit policy, and those who find policies unclear, broken down by demographic characteristics. Panel B shows the percent of students who are aware of their free access to Microsoft Copilot through Middlebury College. Panel C presents the percent of students who report knowing how to properly cite AI use in their academic work when required. For Panels B and C, horizontal lines represent 95 percent confidence intervals. Sample includes all survey respondents.\r\n\r\nFigure 7:Student Beliefs about the Impact of AI on their Academic Performance\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of Middlebury students who believe that AI improves, reduces, or has no effect on different aspects of their academic experience. For each outcome, responses are categorized into three groups: “Improves” combines “significantly improves” and “somewhat improves” responses, “Reduces” combines “significantly reduces” and “somewhat reduces” responses, and “No effect” represents neutral responses. Sample includes all students who report using AI during the academic semester. “Don’t know” responses are excluded.\r\n\r\nFigure 8:Relationship Between AI Adoption and Beliefs About AI’s Academic Benefits\r\nPanel A. Learning ability (e.g., ability to grasp concepts, learn new skills, etc.)\r\nRefer to caption\r\nPanel B. Understanding of\r\ncourse materials\r\nRefer to caption\r\nPanel C. Ability to complete\r\nassignments on time\r\nRefer to caption\r\nPanel D. Course grades\r\n \r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the relationship between AI adoption rates and beliefs that AI improves various academic outcomes across different student groups. Each panel plots the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis). Points represent different student groups categorized by demographics (circles), academic characteristics (triangles), and field of study (squares). The dashed line shows the linear fit across all groups. Groups with fewer than 10 students are excluded.\r\n\r\nFigure 9:Student Beliefs about Generative AI Usage at Middlebury College\r\nPanel A. For Schoolwork\r\nRefer to caption\r\nPanel B. For Leisure\r\nRefer to caption\r\nPanel C. For Any Purpose\r\nRefer to caption\r\nPanel D. Classes with No Policy\r\nRefer to caption\r\nPanel E. Classes that Allow AI\r\nRefer to caption\r\nPanel F. Classes that Disallow AI\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the distribution of students’ beliefs about generative AI usage among their peers at Middlebury College. Panels A-C display students’ estimates of the percent of their peers who regularly use AI for schoolwork, leisure activities, and any purpose, respectively. Panels D-F show students’ beliefs about AI usage in classes with different AI policies: those without an explicit policy (Panel D), those that allow AI use (Panel E), and those that prohibit AI use (Panel F). Each panel shows a histogram with bins of width ten percentage points (e.g., responses between 1-10 fall in the 10 bin, 11-20 in the 20 bin, etc.). The red dashed line indicates the mean response. Sample excludes respondents with missing values or who selected the default response for all six categories (which equals zero).\r\n\r\nTable 1:Summary Statistics of Survey Participants\r\nSurvey Sample\tAdmin records\r\nUnweighted\tWeighted\t\r\n(1)\t(2)\t(3)\r\nPanel A. Demographics\r\nMale\t0.446\t0.433\t0.463\r\nFemale\t0.508\t0.516\t0.533\r\nWhite\t0.618\t0.603\t0.538\r\nBlack\t0.036\t0.033\t0.052\r\nHispanic\t0.099\t0.106\t0.124\r\nAsian\t0.155\t0.162\t0.073\r\nPrivate high school\t0.420\t0.399\t–\r\nPublic high school\t0.543\t0.556\t–\r\nPanel B. Academic Characteristics    GPA 3.740 3.736 3.670 Hours spent on academics per week 17.899 17.889 – Freshman 0.311 0.355 0.255 Sophomore 0.273 0.272 0.257 Junior 0.202 0.179 0.201 Senior 0.213 0.194 0.287   Panel C. Field of Study    Arts 0.011 0.021 0.023 Humanities 0.052 0.068 0.073 Languages 0.021 0.020 0.025 Literature 0.035 0.025 0.025 Natural Sciences 0.218 0.249 0.244 Social Sciences 0.353 0.247 0.243 Has not declared major 0.311 0.371 0.364\r\n\r\nN (# degrees) 43 43 49 N (# students) 634 2,760 2,760\r\n\r\n\\justify\r\nNotes: This table presents summary statistics from our survey of college students. Panel A reports demographic characteristics, including the proportion of participants identifying as male, female, white, Black, Hispanic, Asian, or who attended a private or public high school. Panel B provides academic characteristics, such as GPA (only available for non-freshmen), average weekly hours spent on academics, and academic year distribution (Freshman, Sophomore, Junior, and Senior). Note that in column 1–2, GPA refers to self-reported first-year GPA while in column 3 it is the overall GPA during Spring 2024. Panel C summarizes the distribution of participants across different fields of study. Major groups are mutually exclusive.\r\n\r\n0.8250.5890.3680.106 R\r\n−\r\nsquared0.0770.0890.1160.058 N (Students)616616616616 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI usage frequency threshold and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column uses a different threshold for AI usage frequency, categorized as: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). Column 1 defines usage as any nonzero frequency; column 2 includes at least occasional use; column 3 includes frequent or higher use; and column 4 captures only very frequent use.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.0890.1990.3570.5510.801 R\r\n−\r\nsquared0.0450.0610.0880.0800.069 N (Students)633633633633633 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI adoption date and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column presents results for a different threshold of AI adoption. Column 1 shows the probability of adopting AI before Spring 2023; column 2 by Spring 2023; column 3 by Fall 2023; column 4 by Spring 2024; and column 5 by Fall 2024. The dependent variable in each regression is a binary indicator equal to one if the student had adopted AI by the specified time period.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.9120.6122.2850.9130.4191.8010.1930.484 R\r\n−\r\nsquared0.0360.0730.1260.0530.1170.1290.0420.063 N (Students)515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and their use of generative AI for academic tasks. In columns 1 and 4, the outcome is a dummy that equals one if a student reports using AI with any frequency for at least one augmentation or automation task, respectively. In columns 2 and 5, the outcome is the share of tasks within each category for which the student reports any use. In columns 3 and 6, the outcome is a continuous measure capturing average usage frequency for each task category, based on raw Likert-style responses. In columns 7 and 8, the outcome is the difference in average task share and usage intensity between augmentation and automation, respectively. Regressions are weighted and report robust standard errors clustered at the student level. ∗∗∗ \r\np\r\n<\r\n0.01\r\n, ∗∗ \r\np\r\n<\r\n0.05\r\n, ∗ \r\np\r\n<\r\n0.10\r\n.\r\n\r\nReferences\r\nAcemoglu, (2025)\r\nAcemoglu, D. (2025).Nobel lecture: Institutions, technology, and prosperity.American Economic Review, 115(6):1709–1748.\r\nAngrist et al., (2009)\r\nAngrist, J., Lang, D., and Oreopoulos, P. (2009).Incentives and services for college achievement: Evidence from a randomized trial.American Economic Journal: Applied Economics, 1(1):136–163.\r\nAutor and Thompson, (2025)\r\nAutor, D. and Thompson, N. (2025).Expertise.\r\nBailey et al., (2022)\r\nBailey, M., Johnston, D., Kuchler, T., Stroebel, J., and Wong, A. (2022).Peer effects in product adoption.American Economic Journal: Applied Economics, 14(3):488–526.\r\nBastani et al., (2025)\r\nBastani, H., Bastani, O., Sungu, A., Ge, H., Kabakcı, Ö., and Mariman, R. (2025).Generative AI without guardrails can harm learning: Evidence from high school mathematics.Proceedings of the National Academy of Sciences, 122(26):e2422633122.\r\nBeaman et al., (2021)\r\nBeaman, L., BenYishay, A., Magruder, J., and Mobarak, A. M. (2021).Can network theory-based targeting increase technology adoption?American Economic Review, 111(6):1918–1943.\r\nBick et al., (2025)\r\nBick, A., Blandin, A., and Deming, D. J. (2025).The rapid adoption of generative AI.Technical report, National Bureau of Economic Research.\r\nBonney et al., (2024)\r\nBonney, K., Breaux, C., Buffington, C., Dinlersoz, E., Foster, L. S., Goldschlag, N., Haltiwanger, J. C., Kroff, Z., and Savage, K. (2024).Tracking Firm Use of AI in Real Time: A Snapshot from the Business Trends and Outlook Survey.\r\nBonsu and Baffour-Koduah, (2023)\r\nBonsu, E. M. and Baffour-Koduah, D. (2023).From the consumers’ side: Determining students’ perception and intention to use chatgpt in ghanaian higher education.Journal of Education, Society & Multiculturalism, 4(1):1–29.\r\nBresnahan and Trajtenberg, (1995)\r\nBresnahan, T. F. and Trajtenberg, M. (1995).General purpose technologies ‘engines of growth’?Journal of econometrics, 65(1):83–108.\r\nBrynjolfsson and Hitt, (2000)\r\nBrynjolfsson, E. and Hitt, L. M. (2000).Beyond computation: Information technology, organizational transformation and business performance.Journal of Economic Perspectives, 14(4):23–48.\r\nBrynjolfsson et al., (2025)\r\nBrynjolfsson, E., Li, D., and Raymond, L. (2025).Generative AI at Work.The Quarterly Journal of Economics, 140(2):889–942.\r\nBushong and Gagnon-Bartsch, (2024)\r\nBushong, B. and Gagnon-Bartsch, T. (2024).Failures in forecasting: An experiment on interpersonal projection bias.Management Science, 70(12):8735–8752.\r\nCarvajal et al., (2024)\r\nCarvajal, D., Franco, C., and Isaksson, S. (2024).Will artificial intelligence get in the way of achieving gender equality?Discussion Paper 03, NHH Norwegian School of Economics.\r\n(15)\r\nComin, D. and Hobijn, B. (2010a).An exploration of technology diffusion.American Economic Review, 100(5):2031–2059.\r\n(16)\r\nComin, D. and Hobijn, B. (2010b).Technology Diffusion and Postwar Growth, volume 25, pages 209–246.University of Chicago Press.\r\nCui et al., (2024)\r\nCui, Z. K., Demirer, M., Jaffe, S., Musolff, L., Peng, S., and Salz, T. (2024).The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers.\r\nDavid, (1990)\r\nDavid, P. A. (1990).The dynamo and the computer: an historical perspective on the modern productivity paradox.The American economic review, 80(2):355–361.\r\nDell’Acqua et al., (2023)\r\nDell’Acqua, F., McFowland, E., Mollick, E. R., Lifshitz-Assaf, H., Kellogg, K., Rajendran, S., Krayer, L., Candelon, F., and Lakhani, K. R. (2023).Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality.\r\nDuflo et al., (2011)\r\nDuflo, E., Kremer, M., and Robinson, J. (2011).Nudging Farmers to Use Fertilizer: Theory and Experimental Evidence from Kenya.American Economic Review, 101(6):2350–2390.\r\nEloundou et al., (2024)\r\nEloundou, T., Manning, S., Mishkin, P., and Rock, D. (2024).GPTs are GPTs: Labor market impact potential of LLMs.Science, 384(6702):1306–1308.\r\nFairlie et al., (2014)\r\nFairlie, R. W., Hoffmann, F., and Oreopoulos, P. (2014).A community college instructor like me: Race and ethnicity interactions in the classroom.American Economic Review, 104(8):2567–2591.\r\nFelten et al., (2021)\r\nFelten, E. W., Raj, M., and Seamans, R. (2021).Occupational, Industry, and Geographic Exposure to Artificial Intelligence: A Novel Dataset and Its Potential Uses.\r\nFelten et al., (2023)\r\nFelten, E. W., Raj, M., and Seamans, R. (2023).How will Language Modelers like ChatGPT Affect Occupations and Industries?\r\nFoster and Rosenzweig, (1995)\r\nFoster, A. D. and Rosenzweig, M. R. (1995).Learning by doing and learning from others: Human capital and technical change in agriculture.Journal of political Economy, 103(6):1176–1209.\r\nFoster and Rosenzweig, (2010)\r\nFoster, A. D. and Rosenzweig, M. R. (2010).Microeconomics of technology adoption.Annu. Rev. Econ., 2(1):395–424.\r\nGallup, (2024)\r\nGallup (2024).AI in the Workplace: Answering 3 Big Questions.\r\nGiaccherini et al., (2019)\r\nGiaccherini, M., Herberich, D. H., Jimenez-Gomez, D., List, J. A., Ponti, G., and Price, M. K. (2019).The behavioralist goes door-to-door: Understanding household technological diffusion using a theory-driven natural field experiment.Technical report, National Bureau of Economic Research.\r\nGoehring et al., (2024)\r\nGoehring, G., Mezzanotti, F., and Ravid, S. A. A. (2024).Technology adoption and career concerns: Evidence from the adoption of digital technology in motion pictures.Technical report, National Bureau of Economic Research.\r\nGriliches, (1957)\r\nGriliches, Z. (1957).Hybrid corn: An exploration in the economics of technological change.Econometrica, 25(4):501–522.\r\nHall and Khan, (2003)\r\nHall, B. H. and Khan, B. (2003).Adoption of new technology.\r\n(32)\r\nHanda, K., Bent, D., Tamkin, A., McCain, M., Durmus, E., Stern, M., Schiraldi, M., Huang, S., Ritchie, S., Syverud, S., Jagadish, K., Vo, M., Bell, M., and Ganguli, D. (2025a).Anthropic education report: How university students use claude.\r\n(33)\r\nHanda, K., Tamkin, A., McCain, M., Huang, S., Durmus, E., Heck, S., Mueller, J., Hong, J., Ritchie, S., Belonax, T., et al. (2025b).Which economic tasks are performed with ai? evidence from millions of claude conversations.arXiv preprint arXiv:2503.04761.\r\nHanna et al., (2014)\r\nHanna, R., Mullainathan, S., and Schwartzstein, J. (2014).Learning Through Noticing: Theory and Evidence from a Field Experiment.The Quarterly Journal of Economics, 129(3):1311–1353.\r\nHartley et al., (2025)\r\nHartley, J., Jolevski, F., Melo, V., and Moore, B. (2025).The Labor Market Effects of Generative Artificial Intelligence.\r\nHumlum and Vestergaard, (2025)\r\nHumlum, A. and Vestergaard, E. (2025).Large Language Models, Small Labor Market Effects.\r\nHutto and Gilbert, (2014)\r\nHutto, C. and Gilbert, E. (2014).Vader: A parsimonious rule-based model for sentiment analysis of social media text.In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216–225.\r\nJehiel, (2018)\r\nJehiel, P. (2018).Investment strategy and selection bias: An equilibrium perspective on overoptimism.American Economic Review, 108(6):1582–1597.\r\nKelly et al., (2023)\r\nKelly, A., Sullivan, M., and Strampel, K. (2023).Generative artificial intelligence: University student awareness, experience, and confidence in use across disciplines.Journal of University Teaching and Learning Practice, 20(6):1–16.\r\nKharazian, (2025)\r\nKharazian, A. (2025).Ramp ai index: Monthly measurement of ai adoption by american businesses.Technical report, Ramp Economics Lab.\r\nLehmann et al., (2024)\r\nLehmann, M., Cornelius, P. B., and Sting, F. J. (2024).AI Meets the Classroom: When Does ChatGPT Harm Learning?\r\nLing and Imas, (2025)\r\nLing, Y. and Imas, A. (2025).Underreporting of AI use: The role of social desirability bias.Available at SSRN.\r\nLiu and Wang, (2024)\r\nLiu, Y. and Wang, H. (2024).Who on earth is using generative ai?Policy Research Working Paper WPS10870, World Bank Group, Washington, D.C.\r\nMcClain, (2024)\r\nMcClain, C. (2024).Americans’ use of ChatGPT is ticking up, but few trust its election information.\r\nMcDonald et al., (2025)\r\nMcDonald, N., Johri, A., Ali, A., and Collier, A. H. (2025).Generative artificial intelligence in higher education: Evidence from an analysis of institutional policies and guidelines.Computers in Human Behavior: Artificial Humans, 3:100121.\r\nMcElheran et al., (2023)\r\nMcElheran, K., Li, J. F., Brynjolfsson, E., Kroff, Z., Dinlersoz, E., Foster, L. S., and Zolas, N. (2023).AI Adoption in America: Who, What, and Where.\r\nMerchant, (2024)\r\nMerchant, B. (2024).The new luddites aren’t backing down.The Atlantic.Accessed: 2025-06-16.\r\nNam, (2023)\r\nNam, J. (2023).56% of college students have used ai on assignments or exams.Technical report.Edited by Lyss Welding. Accessed June 11, 2025.\r\nNolan, (2023)\r\nNolan, B. (2023).Here are the schools and colleges that have banned chatgpt.Business Insider.Accessed: 2025-01-27.\r\nNoy and Zhang, (2023)\r\nNoy, S. and Zhang, W. (2023).Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence.\r\nOtis et al., (2024)\r\nOtis, N. G., Delecourt, S., Cranney, K., and Koning, R. (2024).Global Evidence on Gender Gaps and Generative AI.Harvard Business School.\r\nPeng et al., (2023)\r\nPeng, S., Kalliamvakou, E., Cihon, P., and Demirer, M. (2023).The Impact of AI on Developer Productivity: Evidence from GitHub Copilot.\r\nRavšelj et al., (2025)\r\nRavšelj, D., Keržič, D., Tomaževič, N., Umek, L., Brezovar, N., A. Iahad, N., Abdulla, A. A., Akopyan, A., Aldana Segura, M. W., AlHumaid, J., Allam, M. F., Alló, M., Andoh, R. P. K., Andronic, O., Arthur, Y. D., Aydın, F., Badran, A., Balbontín-Alvarado, R., Ben Saad, H., Bencsik, A., Benning, I., Besimi, A., Bezerra, D. D. S., Buizza, C., Burro, R., Bwalya, A., Cachero, C., Castillo-Briceno, P., Castro, H., Chai, C. S., Charalambous, C., Chiu, T. K. F., Clipa, O., Colombari, R., Corral Escobedo, L. J. H., Costa, E., Cre\\textcommabelowtulescu, R. G., Crispino, M., Cucari, N., Dalton, F., Demir Kaya, M., Dumić-Čule, I., Dwidienawati, D., Ebardo, R., Egbenya, D. L., Faris, M. E., Fečko, M., Ferrinho, P., Florea, A., Fong, C. Y., Francis, Z., Ghilardi, A., González-Fernández, B., Hau, D., Hossain, M. S., Hug, T., Inasius, F., Ismail, M. J., Jahić, H., Jessa, M. O., Kapanadze, M., Kar, S. K., Kateeb, E. T., Kaya, F., Khadri, H. O., Kikuchi, M., Kobets, V. M., Kostova, K. M., Krasmane, E., Lau, J., Law, W. H. C., Lazăr, F., Lazović-Pita, L., Lee, V. W. Y., Li, J., López-Aguilar, D. V., Luca, A., Luciano, R. G., Machin-Mastromatteo, J. D., Madi, M., Manguele, A. L., Manrique, R. F., Mapulanga, T., Marimon, F., Marinova, G. I., Mas-Machuca, M., Mejía-Rodríguez, O., Meletiou-Mavrotheris, M., Méndez-Prado, S. M., Meza-Cano, J. M., Mirķe, E., Mishra, A., Mital, O., Mollica, C., Morariu, D. I., Mospan, N., Mukuka, A., Navarro Jiménez, S. G., Nikaj, I., Nisheva, M. M., Nisiforou, E., Njiku, J., Nomnian, S., Nuredini-Mehmedi, L., Nyamekye, E., Obadić, A., Okela, A. H., Olenik-Shemesh, D., Ostoj, I., Peralta-Rizzo, K. J., Peštek, A., Pilav-Velić, A., Pires, D. R. M., Rabin, E., Raccanello, D., Ramie, A., Rashid, M. M. U., Reuter, R. A. P., Reyes, V., Rodrigues, A. S., Rodway, P., Ručinská, S., Sadzaglishvili, S., Salem, A. A. M. S., Savić, G., Schepman, A., Shahpo, S. M., Snouber, A., Soler, E., Sonyel, B., Stefanova, E., Stone, A., Strzelecki, A., Tanaka, T., Tapia Cortes, C., Teira-Fachado, A., Tilga, H., Titko, J., Tolmach, M., Turmudi, D., Varela-Candamio, L., Vekiri, I., Vicentini, G., Woyo, E., Yorulmaz, Ö., Yunus, S. A. S., Zamfir, A.-M., Zhou, M., and Aristovnik, A. (2025).Higher education students’ perceptions of ChatGPT: A global study of early reactions.PLOS ONE, 20(2):e0315011.\r\nRogers, (1962)\r\nRogers, E. M. (1962).Diffusion of Innovations.Free Press, New York.\r\nRoss et al., (1977)\r\nRoss, L., Greene, D., and House, P. (1977).The “false consensus effect”: An egocentric bias in social perception and attribution processes.Journal of experimental social psychology, 13(3):279–301.\r\nSacerdote, (2001)\r\nSacerdote, B. (2001).Peer Effects with Random Assignment: Results for Dartmouth Roommates.The Quarterly Journal of Economics, 116(2):681–704.\r\nStinebrickner and Stinebrickner, (2008)\r\nStinebrickner, R. and Stinebrickner, T. R. (2008).The causal effect of studying on academic performance.The BE Journal of Economic Analysis & Policy, 8(1).\r\nStöhr et al., (2024)\r\nStöhr, C., Ou, A. W., and Malmström, H. (2024).Perceptions and usage of AI chatbots among students in higher education across genders, academic levels and fields of study.Computers and Education: Artificial Intelligence, 7:100259.\r\nStokey, (2021)\r\nStokey, N. L. (2021).Technology diffusion.Review of Economic Dynamics, 42:15–36.\r\nWorld Bank, (2016)\r\nWorld Bank (2016).World development report 2016: Digital dividends.World Bank Publications.\r\nWu et al., (2022)\r\nWu, M.-J., Zhao, K., and Fils-Aime, F. (2022).Response rates of online surveys in published research: A meta-analysis.Computers in Human Behavior Reports, 7.\r\nXiao et al., (2023)\r\nXiao, P., Chen, Y., and Bao, W. (2023).Waiting, banning, and embracing: An empirical analysis of adapting policies for generative ai in higher education.arXiv preprint arXiv:2305.18617.\r\nAppendix\r\n\r\nAppendix AAppendix Figures and Tables\r\nFigure A1:Generative AI Usage Survey Design Overview\r\nKey\t\r\nSurvey Structure\r\nSurvey Sections\r\nUsage Data Collection\r\nBeliefs & Perceptions\r\nAI Usage Survey at Middlebury College\r\nDecember 2024\r\nParticipant Recruitment\r\nCampus-wide email invitations, framed as general technology use survey\r\nParticipation Incentives\r\nAmazon gift card lottery ranging from $50-$500\r\nSection 1: Demographics & Academic Information\r\nStudent Characteristics\r\nGender, race/ethnicity, high school type\r\nAcademic Profile\r\nYear, major, self-reported GPA, study hours\r\nSection 2: generative AI Usage Patterns\r\nAdoption Metrics\r\nFrequency, timing of first use, specific AI models used\r\nAcademic Applications\r\nTasks performed with AI, payment for premium services\r\nSection 3: Perceptions & Institutional Policies\r\nImpact Perceptions\r\nEffects on learning, grades, time management\r\nPolicy Responses\r\nUsage likelihood under different policies\r\nPeer Usage Beliefs\r\nEstimates of AI adoption among peers for schoolwork and leisure\r\nOpen-Ended Response Collection\r\nMotivations for AI use, policy feedback, suggestions for improvement\r\nNote: This figure illustrates the structure of the AI usage survey conducted at Middlebury College in December 2024. The survey collected information across three main sections: (1) demographic and academic background, (2) patterns of generative AI usage including adoption timing, frequency, and specific applications, and (3) perceptions of AI’s impact on learning and responses to institutional policies.\r\n\r\nFigure A2:Cumulative Generative AI Use by Student Characteristic\r\nPanel A. By Gender\r\nRefer to caption\r\nPanel B. By Race\r\nRefer to caption\r\nPanel C. By High School Type\r\nRefer to caption\r\nPanel D. By Freshman Year GPA\r\nRefer to caption\r\nPanel E. By Cohort\r\nRefer to caption\r\nPanel F. By Field of Study\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents cumulative AI use based on different student characteristics. Each panel displays the cumulative distribution of AI use based on a specific characteristic: gender, race, school type, first-year GPA, cohort, or field of study. The cumulative percent of students is plotted against usage categories. The legends and colors correspond to subgroups within each demographic variable.\r\n\r\nFigure A3:Percent of Students Who Pay for Generative AI Tools\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of AI users who pay for AI tools (through any platform) across different demographic groups. Horizontal lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure A4:Academic Uses of ChatGPT: Evidence from Global Survey\r\nPanel A. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\nPanel B. Across the College Quality Distribution\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of students who use ChatGPT for different academic tasks based on data from Ravšelj et al., (2025). Panel A displays usage patterns across tasks categorized as augmenting (proofreading, translating, study assistance, research assistance) versus automating (academic writing, professional writing, creative writing, brainstorming, summarizing, calculating, coding assistance, personal assistance) student effort. Panel B shows usage patterns across university quality quintiles based on World University Rankings, with universities ranked in the top 20% (top quintile) showing slightly higher rates of augmentation relative to automation compared to bottom quintile institutions. The analysis includes universities with at least 30 student responses and excludes observations with missing usage data.\r\n\r\nFigure A5:Student Beliefs about AI Usage versus Actual Usage under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure compares students’ beliefs about AI usage with actual usage rates under different AI policies. For each policy type, blue bars show the percent of students who report being “Neutral,” “Likely,” or “Very Likely” to use AI, while red bars show students’ mean beliefs about what percent of their peers use AI in classes with that policy. The sample includes all survey respondents for both actual usage and beliefs measures. “AI Use is Unrestricted” refers to classes with no restrictions on AI use, “No Explicit AI Policy” refers to classes without a stated policy, and “AI Use is Prohibited” refers to classes that ban AI use entirely.\r\n\r\nFigure A6:Relationship Between Beliefs About AI Usage and Actual AI Usage\r\nRefer to caption\\justify\r\nNotes: This figure shows the relationship between students’ beliefs about AI usage among their peers and actual AI usage rates across different demographic groups. Each point represents a different group of students (by demographics, academic characteristics, or field of study). The \r\ny\r\n-axis shows the percent of students in each group who report using AI for academic purposes. The \r\nx\r\n-axis shows the mean belief within each group about what percent of Middlebury students use AI. The dashed line shows the linear fit. Sample includes all survey respondents with at least ten observations per group. Students’ beliefs about AI usage are positively correlated with actual usage patterns, suggesting that students have relatively accurate perceptions of AI adoption among their peers.\r\n\r\nGPA > p50 80.3 26.3 17.4 23.2 13.4 GPA < p50 87.1 18.7 26.3 32.2 9.9 Freshman 81.3 24.3 24.1 24.7 8.1 Sophomore 84.4 25.6 22.6 23.7 12.4 Junior 86.2 21.6 23.1 28.6 12.8 Senior 78.7 21.0 17.4 29.9 10.4\r\n\r\nPanel C. Field of Study    Arts 73.3 21.0 0.0 52.3 0.0 Humanities 74.8 39.3 21.8 12.1 1.7 Languages 57.4 27.0 30.3 0.0 0.0 Literature 48.6 10.0 33.7 5.0 0.0 Natural Sciences 91.1 22.0 21.9 33.8 13.3 Social Sciences 84.6 17.9 20.4 29.1 17.2\r\n\r\n\\justify\r\nNotes: This table presents the percent of students in each demographic group who report using AI at different frequencies during the academic semester. Each cell shows the percent of students within that group. Column 1 reports the total percent who use AI at any frequency. Columns 2 to 5 represent increasing usage frequencies: rarely (1–2 times per semester), occasionally (monthly), frequently (weekly), and very frequently (multiple times per week). Panel A reports percentages by demographic characteristics. Panel B shows percentages by academic characteristics. Panel C presents percentages by field of study.\r\n\r\n0.9730.1350.0770.1100.114 R\r\n−\r\nsquared0.0120.0510.0270.0240.138 N (Students)516516516516516 \\justifyNotes: This table assesses the relationship between AI model adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI model usage (columns 1-4) or payment for AI services (column 5), and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division.\r\n\r\nEach column presents results for a different model or payment outcome. Column 1 shows usage of OpenAI’s ChatGPT, column 2 Google Gemini, column 3 Microsoft Copilot, column 4 any other AI model, and column 5 whether the student pays for any generative AI service.\r\n\r\nThe omitted categories are Natural Sciences for academic division, white students for race/ethnicity, and freshmen for cohort. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.7510.6760.6610.2710.479 R\r\n−\r\nsquared0.0870.1130.0760.0680.041 N (Students)599599599599599 \\justifyNotes: This table examines how student characteristics relate to self-reported likelihood of using generative AI under different policy scenarios. Each column presents results for different policy scenarios. In columns 1-4, the dependent variable equals one if the student reports being“neutral,” “likely” or “extremely likely” to use AI under the specified policy, and zero if they report being “unlikely,” or “extremely unlikely.” Column 5 represents the impact of moving from unrestricted use to complete prohibition.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.5760.6850.3760.4890.564 R\r\n−\r\nsquared0.1020.1190.0840.0370.087 N (Students)561565543560560 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. Each column presents results for beliefs about different academic outcomes: learning ability (e.g., ability to grasp concepts, retain information, or learn new skills) in column 1, understanding of course materials in column 2, course grades in column 3, ability to complete assignments on time in column 4, and time spent on academics in column 5. The dependent variable in each regression equals one if the student believes AI somewhat improves” or significantly improves” the outcome, and zero if they believe it has no effect, reduces, or significantly reduces the outcome. “Don’t know” responses are excluded.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\nAppendix BEmpirical Appendix\r\nB.1Field of Study Classifications\r\nThis appendix details the classification of majors into broad fields of study used in Figure 1:\r\n\r\n• Natural Sciences: Includes Biology, Biochemistry, Chemistry, Computer Science, Earth and Climate Sciences/Geology, Environmental Studies, Mathematics, Molecular Biology & Biochemistry, Neuroscience, Physics, and Statistics.\r\n• Social Sciences: Includes Anthropology, Economics, Education, Geography, International & Global Studies, International Politics & Economics, Political Science, Psychology, and Sociology.\r\n• Humanities: Includes American Studies, Black Studies, Classics, History, Architectural Studies, Art History & Museum Studies, History of Art & Architecture, Philosophy, Religion, and Classical Studies.\r\n• Literature: Includes Comparative Literature, English/English & American Literatures, and Literary Studies.\r\n• Languages: Includes Arabic, Chinese, French & Francophone Studies, German, Japanese Studies, Russian, and Spanish.\r\n• Arts: Includes Film & Media Culture, Music, Studio Art, and Theatre.\r\nB.2Task-Specific Use of Generative AI\r\nThe aggregate patterns documented in Section 4 may mask important heterogeneity across student groups. Different students may adopt AI tools at varying rates depending on their academic needs and field-specific norms. To investigate this heterogeneity systematically, Appendix Table LABEL:tab:ai_purpose_correlates presents regression estimates examining how student characteristics predict AI usage for each of our ten measured academic tasks.\r\n\r\nUsage patterns differ markedly by student characteristics. Male students consistently exhibit higher adoption rates across most applications, with particularly pronounced differences for finding information (16.4 pp; \r\np\r\n<\r\n0.01\r\n), summarizing texts (12.5 pp; \r\np\r\n<\r\n0.01\r\n), and creating images (16.9 pp; \r\np\r\n<\r\n0.01\r\n). Black students show substantially higher usage for information-gathering tasks, being 23.8 pp more likely to use AI for finding information compared to white students (\r\np\r\n<\r\n0.05\r\n). They also exhibit dramatically higher adoption of writing assistance tools, with 27.6 pp higher usage for editing text (\r\np\r\n<\r\n0.05\r\n) and 23.6 pp for writing emails (\r\np\r\n<\r\n0.10\r\n). Latino students demonstrate significantly higher usage for generating ideas (24.0 pp; \r\np\r\n<\r\n0.01\r\n) and writing emails (20.7 pp; \r\np\r\n<\r\n0.01\r\n), while Asian students show markedly higher adoption for writing emails (24.5 pp; \r\np\r\n<\r\n0.01\r\n) and explaining concepts (10.9 pp, \r\np\r\n<\r\n0.05\r\n). Students from public high schools report lower usage rates for several applications, particularly for concept explanation (7.3 pp lower; \r\np\r\n<\r\n0.10\r\n) and writing emails (7.4 pp lower; \r\np\r\n<\r\n0.10\r\n), but higher usage for proofreading (8.7 pp higher; \r\np\r\n<\r\n0.10\r\n).\r\n\r\nField of study emerges as a particularly strong predictor of usage patterns. Arts majors show significantly lower adoption across multiple tasks compared to Natural Science students, with gaps of 49.7 pp for finding information (\r\np\r\n<\r\n0.01\r\n), 44.0 pp for generating ideas (\r\np\r\n<\r\n0.05\r\n), and 62.1 pp for coding assistance (\r\np\r\n<\r\n0.01\r\n). Similarly, humanities majors are 36.8 pp less likely to use AI for generating ideas (\r\np\r\n<\r\n0.01\r\n) and 30.8 pp less likely for coding assistance (\r\np\r\n<\r\n0.01\r\n). Languages majors exhibit the most pronounced differences, being 52.4 pp less likely to use AI for summarizing texts (\r\np\r\n<\r\n0.01\r\n) and 46.9 pp less likely for generating ideas (\r\np\r\n<\r\n0.01\r\n). By contrast, social science students show higher usage for explaining concepts (8.0 pp; \r\np\r\n<\r\n0.10\r\n) and summarizing texts (14.7 pp; \r\np\r\n<\r\n0.01\r\n).\r\n\r\n0.8030.7400.6310.6190.5410.4730.3710.3440.2350.204 R\r\n−\r\nsquared0.0780.0870.0770.1160.0280.0540.0830.1350.0400.094 N (Students)515515515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and use of AI for specific academic tasks. Each column shows the result for a different academic task. The omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Regressions are weighted and use heteroskedasticity-robust standard errors clustered at the student level. ∗∗∗, ∗∗, and ∗ indicate significance at the 1%, 5%, and 10% levels, respectively.\r\n\r\nAppendix CQualitative Evidence on Student Perspectives on AI Use\r\nIn this section, we analyze student responses to an open-ended question about their use of generative AI. The survey question asked: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” This was an optional question that 48.3 percent of survey respondents answered. Appendix Figure C1 presents a word cloud of the most frequent words in student responses.\r\n\r\nC.1Validating the Open-Ended Response Measure\r\nWe begin by validating our open-ended response measure to show that it contains meaningful signal. To validate this measure, we employ sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner), a widely-used lexicon and rule-based sentiment analysis tool (Hutto and Gilbert, , 2014). We calculate sentiment scores for each response and examine whether these scores correlate with students’ actual AI adoption behaviors. Intuitively, students who express more positive sentiment toward AI in their open-ended responses should be more likely to have adopted AI tools in practice. We test this hypothesis by examining two measures of AI adoption: whether students have ever used generative AI and whether they currently use AI for academic purposes. Appendix Figure C2 presents binned scatterplots showing the relationship between AI adoption rates and standardized sentiment scores.\r\n\r\nSentiment towards generative AI strongly predicts AI adoption. For both outcomes, students with negative sentiment scores (below zero) show substantially lower adoption rates—around 70-85 percent for ever using AI and 70-75 percent for academic use. In contrast, students with positive sentiment scores display markedly higher adoption rates, reaching nearly 100 percent for general use and 95-100 percent for academic purposes among those with the most positive sentiment. The relationship is particularly pronounced for academic AI use (Panel B), where the coefficient of \r\nβ\r\n^\r\n=\r\n0.168\r\n is twice as large as for general adoption.17 These systematic patterns confirm that our open-ended responses capture meaningful variation in student attitudes that corresponds to real behavioral differences.\r\n\r\nC.2How Students use Generative AI\r\nTo analyze these responses systematically, we classified each response using keywords based on their content. For example, if a student mentioned using AI to save time, we tagged the response with the keyword “time-saver.” If a student expressed concerns about AI’s impact on learning, we tagged it with “negative learning.” Responses could receive multiple keywords if they discussed several themes. Appendix Figure C3 shows the frequency of keywords in our classification. The responses reveal how students use AI tools, what motivates this use, and what causes some to avoid or limit their use.\r\n\r\nThe most common use of AI is as an explanatory tool. Nearly 30 percent of responses mentioned using AI to understand course material. Students frequently ask AI to break down complex concepts from readings and lectures, particularly when they find the material difficult to understand. For example, one student reported: “I can ask AI to explain concepts to me that I have a hard time grasping. […] I can keep asking ’simplify’ or ’break down even more.”’ Students also use AI to summarize dense academic readings, which they argue helps them manage heavy reading loads.\r\n\r\nStudents employ AI throughout different stages of the writing process. Some use AI to generate initial drafts that serve as starting points. One student explained: “Helps me get started with a base for most of my essays. It feels easier to edit something already written and make it my own than to write from scratch.” Others use AI more narrowly for brainstorming when stuck on specific problems. As one student noted: “I use it if I am feeling stuck to push me to the right direction (whether a mathematical problem or an essay idea).” Many also report using AI as an editing tool to improve grammar, sentence structure, and overall writing flow. This is especially the case for non-native English speakers. As one student explained: “English is not my first language and it frustrates me sometimes that I cannot find the best way to phrase a certain idea and AI is a useful tool to have to find alternate expressions.”\r\n\r\nStudents frequently mentioned using AI for specific academic tasks. In courses that require coding, students often use AI for debugging code and understanding programming concepts. Students also employ AI for administrative tasks like formatting citations and drafting routine emails. Finally, many students use AI as an enhanced search engine. One student reported: “It has significantly reduced the time it takes to conduct research on new topics and ideas, and helps me by giving me a thorough selection of sources to use for projects of any kind.”\r\n\r\nC.3Why Students Adopt AI\r\nTime savings was the most commonly cited reason for using AI. Nearly 30 percent of responses mentioned using AI to complete work more efficiently. Students often viewed AI as a way to manage demanding course loads. Many students particularly embrace AI assistance for tasks viewed as mechanical or administrative. A student noted they use AI for “Writing emails quick and creating resume/ cover letter templates.” But AI assistance goes beyond grunt work. Some students use it to “spend less time doing assignments and homework.” This is particularly true if students don’t view the work as central to their academic experience. As one student explained: “when I come across work I deem as ineffective, I want to spend as little time as possible doing it.”\r\n\r\nHaving an on-demand tutor for academic support was another key motivation. One-quarter of responses described using AI as an “explainer” when other resources were unavailable or inconvenient. As one student noted, “I use it as a last resort (if there are no office hours, after looking up videos, etc.) if I need extra help. I’d like to think that the way I use it is similar to going to office hours or TA hours.”\r\n\r\nPeer influence also drove AI adoption. Some students reported feeling pressure to use AI to remain competitive with their classmates. One student explained: “I noticed others use it, are getting better grades than me, and they say they learn better with the help of AI, so I gave it a try.” Others worried about being at a competitive disadvantage: “Other people were using it and told me about it. I felt like I would be at a disadvantage if I wasn’t also using it.”\r\n\r\nC.4Concerns and Limitations\r\nStudents expressed several concerns about AI use in academic work. The most frequent worry was about negative impacts on learning. One student who initially used AI extensively reported: “In the past, I have simply plugged and chugged homework assignments into ChatGPT and submitted it. Those assignments feedback from teachers was positive and I was getting good grades, but I definitely felt that my own learning outcomes to be significantly worse.” Other students viewed AI use as fundamentally incompatible with their educational goals. As one humanities student explained: “my task is as a humanities student is to think, not calculate; why should I let AI do the thinking for me? It would defeat the purpose of pursuing my education.”\r\n\r\nMany students described ethical concerns about AI use. Responses suggested uncertainty about appropriate boundaries. One student noted: “I never use it to explicitly write something because that feels like overt cheating, but sometimes I hesitate when it completely solves Econ problems. I understand how it does it, and it helps me to learn, but it still sometimes feels a little morally gray.” Another expressed similar ambivalence: “I tend to only use it when [it] will save me time in a moral way.”\r\n\r\nStudents also emphasized the importance of maintaining ownership of their work. Many expressed pride in producing original work and hesitation about diluting that ownership through AI use. As one student explained: “I don’t have interest in using generative AI for my academic work because I want my work to reflect my own ideas.” Another noted: “It usually would not even occur to me to turn to AI to substitute writing because I want to take credit for my work, and using AI seems to diminish that.”\r\n\r\nTechnical limitations deterred some students from using AI tools. Students reported concerns about inaccurate outputs (“hallucinations”) and poor output quality, particularly for creative writing or complex analytical tasks. For example, one student noted that “In my poetry class we were instructed to use it to come up with poems and they were awful, so that kinda turned me away from using it to do my work for me.”\r\n\r\nC.5Discussion\r\nWe conclude with two overarching themes that emerged from the responses.\r\n\r\nFirst, students vary substantially in how they incorporate AI into their academic lives. Crucially, this heterogeneity largely depends on what students perceive as “appropriate” uses of AI. For activities that they perceive as core to their academic journey—like writing essays or solving problems—many students hesitate to use AI. A student articulated this clearly: “Most of my work is writing or reading. If I’m not doing the writing, what is the purpose of me taking the class?” Yet, students draw different boundaries between central tasks and grunt work. Some use AI extensively, viewing their role as akin to a manager that provides high-level direction while AI handles implementation. Others restrict AI use to specific tasks like brainstorming, editing, or drafting emails. Still others avoid AI entirely for academic work, often for ethical reasons. Even among AI users, adoption patterns reflect individual trade-offs between time savings, learning goals, and academic integrity concerns.\r\n\r\nSecond, there appears to be a fundamental tension between efficiency and learning. The time-saving benefits are easy to observe, quantifiable, and tangible. But these time savings are unlikely to be a free lunch. Some benefits may come at the cost of spending less time with material that requires deeper engagement to digest. As one student noted: “There may be a negative effect in that it eliminates much of the ’struggle’ in learning.” Yet, having an on-demand tutor that explains concepts in relatable ways can also improve learning. As one student explained: “It can explain concepts to me in a way that is tailored to my learning style.” This suggests that the impact of AI use on learning outcomes depends not on whether students use AI—almost all do, to some extent—but rather on how they employ these tools.\r\n\r\nFigure C1:Word Cloud of Student Motivations for Generative AI Use\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 147 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?’\r\n\r\nFigure C2:Relationship Between AI Sentiment and AI Adoption\r\nPanel A. Ever Used Generative AI\r\nRefer to caption\r\nPanel B. Uses AI for Academic Purposes\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents the relationship between AI sentiment and AI adoption. Panel A shows the proportion of respondents who have ever used generative AI, while Panel B shows the proportion who use AI for academic purposes. Each point represents the mean adoption rate for respondents within sentiment score bins of width 0.2. Sentiment scores are standardized compound scores computed using Hutto and Gilbert, (2014)’s VADER algorithm applied to responses to an open-ended question about generative AI. Positive values indicate positive sentiment and negative values indicate negative sentiment. The dashed lines show OLS best-fit lines estimated on the microdata, with coefficients and standard errors (in parentheses) displayed in the top-left corner of each panel.\r\n\r\nFigure C3:Frequency of Keywords in Student Motivations for AI Use\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to AI use. The responses come from the question “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” Color coding indicates the category of each theme. Usage type refers to how students use AI tools. Motivation captures what drove students to try AI. Concerns include mentions of course policies and academic integrity, individual reservations about AI use, worries about AI’s impact on education, and AI’s technical limitations.\r\n\r\nAppendix DQualitative Evidence on Student Views of AI Policies\r\nIn this section, we analyze student responses to an open-ended question about Middlebury’s AI policies. The survey asked: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Appendix Figure D1 presents a word cloud of the most frequent words in student responses. To analyze these responses systematically, we classified each response using keywords based on their content. Appendix Figure D2 shows the frequency of keywords in our classification.\r\n\r\nD.1Polarized Views on Generative AI Policy Approaches\r\nStudents expressed markedly different views about appropriate AI policies, revealing fundamental disagreement about the path forward. Some strongly advocated for embracing AI technology. As one student argued, “The tool is there, there is supply and there is demand. Don’t fight another war on drugs. Don’t live in a fake reality”. Others called for significant restrictions, arguing that “the use of generative AI is dishonest and corrosive” and that it “prohibits these organic processes and divorces students from true learning”.\r\n\r\nHowever, the most common position advocated for a balanced approach that would allow beneficial uses while restricting harmful ones. Students distinguished between uses that enhance learning (like concept explanation) and those that substitute for learning (like generating entire essays). One student articulated this nuanced view particularly well: “AI also can really be helpful at explaining a textbook problem that doesn’t make sense, or guiding slightly with homework, or creating study materials, or editing/tightening up your prose. All of those things are good, and universities should figure out how to maximize AI use for those reasons and to minimize students just feeding their problem sets into ChatGPT”\r\n\r\nA recurring theme was the futility of blanket bans. Many students emphasized that prohibition would be ineffective, with one noting “I don’t think anyone really cares what the policy of any given class is. If professors want people to not use it, they need to structure assessments in a way that will discourage use.” Another compared AI bans to restricting internet use, arguing “AI policies seem to be totally irrelevant. It’s like telling people they can’t use the internet as a resource for the class.” This ineffectiveness of bans creates fairness concerns. As one student explained: “I think if it is banned in a class, that should be enforced (and right now it absolutely is not)… As with any form of cheating, those who don’t cheat are put at a disadvantage.” Another student expressed similar frustration: “I find it discouraging when I hear classmates saying they use AI for things such as essays when they use it in dishonest ways.”\r\n\r\nD.2Need for Clear Guidelines\r\nThe most frequently expressed concern was the need for clear guidelines about generative AI use. Several students reported confusion about what constitutes acceptable use, with one noting “I think it should be more clear whether we can use it and how and how to cite it since most professors rarely mention it at all”. Other students emphasized the importance of professors explaining their policies upfront and their rationale, with one stating “I think that Professor’s should be very specific about what is allowed and their reasoning behind their policy”.\r\n\r\nMany students advocated for standardization across classes, observing that “Sometimes its confusing when one class allows it and another doesn’t and the other encourages it and so on so if there was a school wide or department wide policy that could help”. Yet there is also disagreement regarding standardization. Some students preferred leaving the decision to individual professors, arguing that “GenAI is more effective in some classes/majors than others. Making sure professors understand how students use GenAI and how useful GenAI is in their class (given the course structure, nature of assignments/material, etc.) is very important for the class policy.”\r\n\r\nD.3Training and Support Services\r\nStudents strongly emphasized the need for training in appropriate generative AI use. Many suggested that the college should provide guidance on using AI tools effectively while maintaining academic integrity. One student proposed “a workshop that teaches you to effectively use GenAI without violating the honor code”. Students also expressed interest in learning how to leverage AI to enhance their learning experience rather than circumvent it. As one student explained: “I think it could be useful to develop some sort of training. How do we use AI in a way that actually benefits our learning? I tried out some things on my own but I feel that I need more guidance.”\r\n\r\nThis desire for training was often linked to workplace preparedness. Students recognized that AI proficiency would be valuable in their careers, with one noting “As the world uses more and more AI, I think it is an important tool that students should know how to leverage”. Another emphasized: “The moment us students leave campus, we will be using it in the professional world, and when used in combination with one’s own skills, it is merely a tool to maximize efficiency”.\r\n\r\nD.4Discussion\r\nThe responses reveal several key tensions in AI policy implementation. First, students express a desire to use AI in ways that enhance rather than substitute for learning, yet they recognize that blanket bans are ineffective and worry about being disadvantaged if they follow restrictions while others do not.\r\n\r\nSecond, while students desire clear guidelines, they also want flexibility to accommodate legitimate uses that vary by discipline and assignment type. Different courses and majors may find different AI uses appropriate based on their learning objectives and assessment types.\r\n\r\nThird, there is tension between faculty autonomy in setting course policies and students’ desire for consistent institutional standards. While some support letting professors determine appropriate AI use for their specific courses, others argue that varying policies across classes create confusion and enforcement challenges.\r\n\r\nFinally, students report significant variation in faculty attitudes toward AI. Some students perceived faculty fear or misunderstanding of AI tools, noting that categorical bans often reflect a lack of understanding about AI’s capabilities and limitations rather than pedagogical considerations.\r\n\r\nFigure D1:Word Cloud of Student Feedback on Generative AI Policies\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 133 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?”\r\n\r\nFigure D2:Frequency of Keywords in Student Feedback on AI Policies\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to Middlebury’s AI policies. The responses come from the question “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Color coding indicates the category of each theme. Policy Approach captures suggestions about how AI should be regulated at the college. Adoption Views reflect positions on whether and how AI should be integrated into academic work. Training Needs indicates requests for guidance and support. Other Concerns include issues of workplace relevance and fairness.\r\n\r\nSee pages 1 of results/ai-survey-midd.pdf See pages 2- of results//ai-survey-midd.pdf",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== story.txt ===\nI remember the moment that sparked this project. It was during a walk across Middlebury’s campus late one fall afternoon—just after office hours. A student had come in, clearly bright but struggling, and told me they’d been using ChatGPT late at night to help with assignments because they were too anxious to speak up in class. That confession stuck with me.\r\n\r\nLater, over coffee with Germán, we started wondering—how many students were turning to generative AI as a quiet lifeline? Not to cheat, but to learn? That simple question became the seed for this entire study. What we found wasn’t just statistics. It was a story of adaptation, of students reshaping how they learn, and of institutions racing to catch up.\"\n\n==================================================\n\n=== Generative AI in Higher Education E.txt ===\nGenerative AI in Higher Education: Evidence from an Elite College†\r\nZara Contractor\r\nGermán Reyes\r\nAbstract\r\nGenerative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT’s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI’s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI’s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\r\n\r\n1Introduction\r\nThe rapid advancement of generative artificial intelligence (AI) is transforming higher education at an unprecedented pace. The launch of ChatGPT and similar tools has introduced technologies capable of performing tasks central to academic assessment and learning—writing essays, solving complex problems, and explaining intricate concepts—instantly and at near-zero marginal cost. Yet systematic evidence on the nature and implications of this new technology remains scarce. How widespread is generative AI adoption among students, and what factors drive it? Do students primarily use AI to augment their learning or to automate coursework, potentially harming human capital development? Could disparities in access to premium AI resources amplify existing educational inequalities?\r\n\r\nThese questions are at the heart of ongoing debates about the role of AI in education, which have relied heavily on anecdotal evidence and speculation rather than rigorous evidence. As a result, universities have implemented policies that vary dramatically, from outright bans to permissive adoption, often without clear evidence on their effectiveness or unintended consequences (Nolan, , 2023; Xiao et al., , 2023; McDonald et al., , 2025).\r\n\r\nThis paper addresses this gap by systematically examining generative AI adoption at Middlebury College, a highly selective liberal arts college in Vermont. With approximately 2,800 undergraduate students, Middlebury offers 49 majors across the arts, humanities, languages, social sciences, and natural sciences. Over the past decade, Middlebury has consistently ranked among the top 10 liberal arts colleges in U.S. News & World Report, with an average ranking of 8.5 between 2015 and 2025. The college’s selectivity is comparable to that of many elite research universities—Middlebury’s acceptance rate of approximately 10 percent is similar to that of Boston University (11 percent), Georgetown University (13 percent), and the University of Virginia (16 percent).\r\n\r\nOur analysis draws on survey data collected from the student population between December 2024 and February 2025. The survey collected detailed information about AI usage, including the frequency and purpose of use, perceived impacts, and responses to institutional policies. To minimize selection bias, we framed the survey broadly as examining technology use and provided incentives for participation, achieving a 22.9 percent response rate with 634 responses representing a broad cross-section of the student population.\r\n\r\nWe document five main findings. First, generative AI is approaching near-universal adoption at an unprecedented speed. Over 80 percent of students use AI for academic purposes, up from less than 10 percent before Spring 2023. This represents one of the fastest technology adoption episodes ever documented, dramatically exceeding the 40 percent adoption rate among U.S. workers (Bick et al., , 2025) and the 23 percent among all U.S. adults (McClain, , 2024). These levels are consistent with international findings of 50–70 percent adoption in university contexts (Nam, , 2023; Stöhr et al., , 2024; Ravšelj et al., , 2025).\r\n\r\nSecond, AI adoption is markedly unequal across academic disciplines and demographic groups. Field of study is the strongest predictor of adoption, likely reflecting how well AI capabilities align with the academic tasks required across fields. Adoption ranges from 91.1 percent in Natural Sciences majors (including mathematics and computer science) to significantly lower levels in Literature (48.6 percent) and Languages (57.4 percent). Additionally, demographic disparities exist. For example, males adopt AI at higher rates than females (88.7 versus 78.4 percent), a pattern that is consistent with documented AI gender gaps in other contexts (Otis et al., , 2024). Most notably, lower-achieving students have a higher adoption rate than their higher-achieving peers (87.1 versus 80.3 percent). As a result, AI could serve as an equalizing force if it enhances student learning—helping struggling students catch up to their peers—but could also widen achievement gaps if AI undermines skill development.\r\n\r\nThird, generative AI transforms the students’ learning production function by both augmenting student capabilities and automating academic tasks. We classify academic tasks as augmentation when AI enhances human capabilities while maintaining student engagement (e.g., explaining concepts, proofreading) versus automation when AI directly produces outputs with minimal cognitive involvement (e.g., writing essays, creating images), and average usage rates across tasks within each category. We find that 61.2 percent of AI users employ these tool for augmentation purposes, while 41.9 percent use it for automation. Qualitative evidence reinforces these patterns: students describe AI as an “on-demand tutor” for augmentation purposes, particularly valuable when traditional resources like office hours are unavailable, while automation use centers on time savings during periods of overwhelming workload. These self-reported patterns align closely with actual usage data from Claude conversation logs (Handa et al., 2025a,).\r\n\r\nFourth, institutional policies can significantly influence AI adoption, though their effectiveness can be undermined by informational frictions. We find that explicit prohibitions dramatically reduce self-reported intended use 39 percentage points (pp). Importantly, the effects of this policy vary substantially across student groups, with females showing larger reductions in usage under prohibition than males, suggesting that one-size-fits-all policies may inadvertently create disparate impacts across groups. Moreover, we also document substantial informational gaps about institutional AI policies. Many students (19.2 percent) do not understand AI policy rules, few know about institution-provided premium AI resources (10.1 percent) or proper AI citation practices (32.6 percent)—a skill that is necessary for academic integrity.\r\n\r\nFifth, students believe that AI has a positive learning impact and these beliefs strongly predict their own AI usage. Most students believe that AI improves their understanding of course materials (70.2 percent) and learning ability (60.1 percent), though fewer believe it improves grades (41.1 percent). Perhaps unsurprisingly, students who believe that AI improves their academic outcomes are significantly more likely to use these tools. For example, a ten-percentage-point increase in the belief that AI improves learning corresponds to a 4.9 percentage-point increase in adoption, highlighting how perceptions can affect technology diffusion in educational settings.\r\n\r\nTaken together, our findings reveal that generative AI has already reshaped students’ college experience. The substantial information gaps that we document—regarding both permitted uses and available resources—suggest straightforward opportunities for policy intervention. Yet the observed heterogeneity across disciplines and student groups indicates that uniform policies risk unintended consequences: blanket prohibitions may disproportionately disadvantage students who benefit most from AI augmentation, while unrestricted use may encourage automation practices detrimental to skill development. Thus, effective policy requires distinguishing clearly between AI uses that enhance learning and those that undermine it.\r\n\r\nWhile our findings are specific to the population we study—students at an elite liberal arts college—this setting offers valuable into AI’s broader societal impact. The rapid uptake of generative AI in higher education may reflect a generalizable principle regarding the diffusion of generative AI: adoption may be fastest in contexts where AI consolidates fragmented tools into a unified platform. Students have long had access to various tools for the academic tasks for which they now use generative AI.1 What distinguishes AI is its ability to provide all these services through a single, instantly accessible interface at virtually zero marginal cost. This consolidation likely drives students’ perception of enhanced learning efficiency and contributes to the fast adoption. This principle may extend well beyond education: industries with fragmented, specialized tools could experience similar trajectories of rapid AI adoption, even if AI itself does not introduce fundamentally new capabilities.\r\n\r\nOur findings contribute to a rapidly growing literature examining the adoption and impacts of generative AI. Recent work has focused on AI’s effects on worker productivity (Noy and Zhang, , 2023; Dell’Acqua et al., , 2023; Peng et al., , 2023; Cui et al., , 2024; Brynjolfsson et al., , 2025) and its potential to transform occupations (Felten et al., , 2021, 2023; Eloundou et al., , 2024). While several papers document AI adoption in workplace settings (Bick et al., , 2025; Hartley et al., , 2025; Humlum and Vestergaard, , 2025) and at the firm level (McElheran et al., , 2023; Bonney et al., , 2024; Kharazian, , 2025), we examine adoption in higher education—a critical setting where future high-skilled workers develop human capital.\r\n\r\nWe add to an emerging literature examining generative AI in education. Some studies focus on the learning impacts of generative AI, finding mixed impacts depending on the setting (e.g., Lehmann et al., , 2024; Bastani et al., , 2025). A complementary strand of the literature documents adoption patterns across diverse educational contexts, including universities in Australia (Kelly et al., , 2023), Ghana (Bonsu and Baffour-Koduah, , 2023), Norway (Carvajal et al., , 2024), Sweden (Stöhr et al., , 2024), and multi-country studies (Ravšelj et al., , 2025). To the best of our knowledge, this paper provides the first systematic evidence on generative AI adoption in U.S. higher education. Importantly, we focus specifically on how students use AI for academic purposes—unlike studies that measure more general usage including personal lives (e.g., Ravšelj et al., , 2025)—and go beyond documenting adoption rates to examine how students integrate AI into their academic workflows, distinguishing between augmentation and automation, the role of student beliefs about AI’s educational impacts, and how institutional policies shape usage patterns.\r\n\r\nOur findings also contribute to the literature on technology diffusion. Unlike the S-shaped adoption curves documented for most technologies (Griliches, , 1957; Rogers, , 1962), we observe extremely rapid adoption of generative AI, with 80 percent uptake within two years of ChatGPT’s release. This pattern contrasts sharply with the adoption trajectories of historical General Purpose Technologies (GPTs) like electricity, which required decades to achieve widespread use (David, , 1990; Bresnahan and Trajtenberg, , 1995). The rapid diffusion we document aligns with evidence that newer technologies diffuse faster than older ones (Comin and Hobijn, 2010a,; Comin and Hobijn, 2010b,). Two factors likely explain AI’s exceptional adoption speed compared to other GPTs. First, unlike electricity or steam power, generative AI requires minimal physical infrastructure—students access it through existing devices—and is available at no cost to consumers, eliminating the financial barriers that typically slow technology adoption. Second, as noted above, AI consolidates multiple specialized tools into a single platform, making its benefits immediately apparent without requiring specialized training or organizational restructuring, unlike previous GPTs (Brynjolfsson and Hitt, , 2000).\r\n\r\n2Data: Novel Student Survey\r\n2.1Recruitment and Structure\r\nWe conducted the survey from December 2024 to February 2025. All Middlebury College students were contacted via email and received a reminder a few weeks and two months after the initial invitation. To minimize selection bias, the recruitment materials described the survey broadly as a study on students’ use of technology in their academic and personal lives. To incentivize participation, students who completed the survey qualified for entry into a lottery for Amazon gift cards ranging in value from $50 to $500. The full survey instrument is provided in Appendix LABEL:app:survey.\r\n\r\nThe survey contains three main sections (see Appendix Figure A1 for the survey flow). First, we gather demographic and academic information, including gender, race/ethnicity, type of high school attended (private or public), current academic year, and declared or intended major.2 We also collect data on academic inputs and performance through students’ self-reported typical weekly hours spent studying and their first-year GPA.3\r\n\r\nSecond, we measure students’ experience with generative AI tools. We begin by asking whether students have ever used generative AI tools like ChatGPT or Claude. For those who have, we collect information about their usage patterns, including frequency of use during the academic semester, which specific AI models they use, and whether they pay for AI tools. We also gather data on how students use AI for different academic tasks, including writing assistance, learning support, and coding.\r\n\r\nThe final part of the survey elicits students’ beliefs about generative AI’s adoption and impacts. We ask students about their perceptions of AI’s effects on their academic experience across multiple dimensions, including learning, grades, time management, and understanding of course material. We then collect information on the role of institutional policies, asking students how different policy environments influence their likelihood of using AI. We also elicit students’ beliefs about AI usage among their peers, including their estimates of the fraction of Middlebury students who use AI for schoolwork and leisure. The survey concludes with two open-ended questions that allow students to share their experiences with AI in academic settings and provide feedback on the college’s AI policies and support services.\r\n\r\n2.2Sample and Summary Statistics\r\nOut of Middlebury’s 2,760 enrolled students, 739 began the survey. We exclude 105 respondents who left the survey before reaching the generative AI usage module, leaving us with an analysis sample of 634 students (22.9 percent of the student body). This response rate is comparable to that of similar surveys (Wu et al., , 2022). To make our sample more representative of Middlebury’s student body, we construct poststratification weights based on the distribution of declared majors from administrative records.4 Specifically, we weight observations by the ratio of each major’s share in the student population to its share in our survey responses.5\r\n\r\nTable Figures and Tables presents summary statistics for our sample. Column 1 reports unweighted survey averages, column 2 shows averages after applying poststratification weights, and column 3 provides population benchmarks from administrative records where available. Our unweighted sample consists of 44.6 percent male and 50.8 percent females. The racial/ethnic composition includes 61.8 percent white, 15.5 percent Asian, 9.9 percent Hispanic, and 3.6 percent Black students. The majority of students (54.3 percent) attended a public school, while 42.0 percent attended a private high school. Our sample represents 43 different majors across seven fields of study, with 31.1 percent of respondents not yet having declared a major. In our analysis, we group these undeclared students by their intended field of study as reported in the survey.\r\n\r\nComparing our unweighted sample to administrative records reveals notable differences. Our sample overrepresents white students (61.8 versus 53.8 percent) and Asian students (15.5 versus 7.3 percent), while underrepresenting Black students (3.6 versus 5.2 percent) and Hispanic students (9.9 versus 12.4 percent). First-year students are overrepresented (31.1 versus 25.5 percent), while senior students are underrepresented (21.3 versus 28.7 percent). Our weighting procedure partially addresses these discrepancies—particularly in the distribution of academic fields, where the weighted figures closely approximate administrative records—but some demographic differences persist. Despite these differences, our weighted sample provides reasonably close approximations to the college population on most dimensions, particularly for academic characteristics and field distributions.\r\n\r\n3Generative AI Usage Patterns Among Students\r\n3.1Adoption of Generative AI\r\nGenerative AI has achieved a remarkably high adoption rate among Middlebury College students. Figure 1 presents the distribution of AI usage frequency during the academic semester, categorized into four levels: “Rarely” (a few times per semester), “Occasionally” (a few times per month), “Frequently” (a few times per week), and “Very Frequently” (daily or almost daily). Overall, 82.5 percent of students report using generative AI for academic purposes, with substantial variation in usage intensity: 23.5 percent use it rarely, 22.2 percent occasionally, 26.2 percent frequently, and 10.6 percent very frequently.\r\n\r\nThe adoption rate at Middlebury aligns closely with patterns observed across other higher education institutions. A BestColleges survey of U.S. undergraduate and graduate students in fall 2023 found that 56 percent had used AI on assignments or exams (Nam, , 2023). Ravšelj et al., (2025) surveyed higher education students globally between late 2023 and early 2024, finding that 71.4 percent had ever used ChatGPT—though this figure encompasses all usage rather than academic use specifically. Similarly, Stöhr et al., (2024) document that 63 percent of Swedish university students had used ChatGPT by spring 2023, with 35.4 percent reporting regular use and 27.6 percent rare use. Among Norwegian university students, Carvajal et al., (2024) find that 68.9 percent use AI tools occasionally or more frequently.\r\n\r\nThese adoption rates in higher education far exceed those in the general population and workforce. Pew Research finds that only 23 percent of U.S. adults have ever used ChatGPT (McClain, , 2024), while Gallup reports that just one-third of U.S. workers have used AI for work (Gallup, , 2024). Bick et al., (2025) and Hartley et al., (2025) estimate that about 40 percent of the U.S. working-age population used generative AI for work as of late 2024 and early 2025. Both studies document substantial heterogeneity across industries, with information services showing the highest adoption rates at 56–62 percent—still considerably below the 80 percent adoption rate at Middlebury. Even among workers in AI-exposed occupations, Humlum and Vestergaard, (2025) find adoption rates of only 41 percent.6\r\n\r\n3.2Adoption by Student Characteristics and Field of Study\r\nAI adoption varies considerably across demographic groups and academic disciplines (Figure 1 and Appendix Table LABEL:tab:ai_freq_het). Males report higher usage rates than females (88.7 versus 78.4 percent). Usage patterns differ markedly by race/ethnicity: Black students (92.3 percent) and Asian students (91.3 percent) exhibit the highest adoption rates, while white students (80.2 percent) and Hispanic students (77.9 percent) report lower usage. Students from private high schools use AI more frequently than those from public schools (84.1 versus 80.4 percent). Notably, students with below-median GPAs report higher usage rates than their higher-achieving peers (87.1 versus 80.3 percent). Adoption varies widely by field of study: Natural Sciences leads with 91.1 percent usage, followed by Social Sciences at 84.6 percent, while Languages (57.4 percent) and Literature (48.6 percent) show substantially lower adoption rates.\r\n\r\nTo examine how student characteristics jointly relate to AI adoption, we estimate multivariate OLS regressions that include all observed characteristics simultaneously.7 Table LABEL:tab:ai_usage_correlates presents regression estimates using four usage thresholds as outcomes. Each column represents the probability of meeting progressively higher frequency thresholds: any AI use (column 1), at least monthly use (column 2), at least weekly use (column 3), and daily use (column 4).\r\n\r\nThe regression results confirm the patterns observed in the descriptive statistics. Holding other characteristics constant, males are 10.3 pp more likely than females to use AI (column 1, \r\np\r\n<\r\n0.05\r\n), with this gender gap typically widening at higher usage frequencies (columns 2–4, all \r\np\r\n<\r\n0.01\r\n). Black and Asian students show substantially higher adoption rates than white students, at 11.8 and 10.7 pp respectively (column 1, both \r\np\r\n<\r\n0.01\r\n). Students from public high schools are 3.0 pp less likely to use AI than those from private schools, but this difference is not statistically significant. Field of study emerges as the strongest predictor of adoption. Compared to Natural Sciences majors, students in Literature, Languages, Arts, and Humanities all show lower usage rates, with the differences being statistically significant at high usage frequencies (columns 3 and 4). Social Sciences majors exhibit adoption rates similar in magnitude to Natural Sciences majors across all usage frequency thresholds.\r\n\r\nOur findings on heterogeneity in AI adoption align with patterns documented in other settings. The gender gap in AI adoption at Middlebury—10.3 pp higher for males—is consistent with evidence across multiple studies. A meta-analysis of 18 studies by Otis et al., (2024) finds that males are 10–20 percentage points more likely to use generative AI than females. This gender gap in AI adoption appears in all educational studies (Nam, , 2023; Carvajal et al., , 2024; Stöhr et al., , 2024; Ravšelj et al., , 2025). Our finding that students with below-median GPAs are more likely to use AI aligns with Carvajal et al., (2024), who document higher adoption rates among students with lower admission grades. This pattern of greater adoption among lower-achieving students suggests that AI could narrow achievement gaps if it enhances learning and skill development, but could widen these gaps if it undermines the acquisition of fundamental skills.\r\n\r\nThe differences in adoption rates across academic fields at Middlebury mirror patterns documented in other educational settings. Stöhr et al., (2024) find that technology and engineering students exhibit significantly higher ChatGPT usage compared to students in humanities. Similarly, Nam, (2023) report that 62 percent of business majors and 59 percent of STEM majors have used AI tools for coursework, compared to 52 percent of humanities majors. Ravšelj et al., (2025) document comparable disciplinary differences, with applied sciences students showing substantially higher usage rates than arts and humanities students.\r\n\r\nThese academic differences persist into the workforce. Bick et al., (2025) find stark variation by college major: STEM graduates have the highest AI adoption rates (46.0 percent), followed by Business/Economics graduates (40.0 percent), while Liberal Arts graduates show substantially lower rates (22.4 percent). Humlum and Vestergaard, (2025) document similar patterns by occupation, finding that roles requiring strong writing and technical skills—such as marketing specialists and journalists—have the highest adoption rates. These consistent patterns across educational and professional contexts suggest that field-specific factors, particularly the applicability of AI tools to different types of tasks, shape adoption in systematic ways.\r\n\r\n3.3Timing of Generative AI Adoption\r\nThe speed of technology diffusion is a critical determinant of its economic and social impact (David, , 1990; Hall and Khan, , 2003; Stokey, , 2021). In educational contexts, rapid technology adoption can create or exacerbate inequalities between early and late adopters, particularly if the technology confers significant learning advantages (World Bank, , 2016). To track the timing of AI adoption among Middlebury students, we asked them when they first began using generative AI for academic purposes, with options ranging from “This semester (Fall 2024)” to “Before Spring 2023” (as a reference, ChatGPT’s public launch was in November 2022).\r\n\r\nStudents adopted generative AI at an extraordinary pace. Figure 2 shows that the cumulative adoption rate grew dramatically from less than 10 percent before Spring 2023 to slightly above 80 percent by Fall 2024. The pace of adoption has accelerated over time, likely reflecting improvements in AI capabilities. Among current users, 25.7 percent adopted AI in Fall 2024 alone, compared to 19.9 percent in Spring 2024 and 16.3 percent in Fall 2023.\r\n\r\nThe adoption rate among Middlebury students far exceeds that observed in other populations and represents one of the fastest technology adoption episodes ever documented. For comparison, Bick et al., (2025) show that it took over 20 years for computers to reach an 80 percent adoption rate in the U.S. working-age population, and about 15 years for internet adoption to reach similar levels. Even generative AI adoption in the broader population has been markedly slower: Pew Research found that just 23 percent of U.S. adults had ever used ChatGPT as of February 2024, up from 18 percent in July 2023 (McClain, , 2024). The dramatically faster adoption rate we document—reaching over 80 percent in less than two years—suggests academic settings may uniquely accelerate AI diffusion.\r\n\r\nTo identify early versus late adopters of generative AI, in Table LABEL:tab:ai_adopt_correlates and Appendix Figure A2, we analyze how adoption timing varies across student characteristics. The rate of adoption varies substantially across student characteristics. Male students led adoption, with a 8.9 pp higher probability of using AI before Spring 2023 compared to females (column 1, \r\np\r\n<\r\n0.01\r\n)—a gender gap that persists across all periods (columns 2–5). Black and Asian students also adopted earlier than white students, though these differences reach statistical significance only in later periods (columns 4–5). Field of study is a strong predictor of adoption timing. For example, students in Languages consistently lagged behind Natural and Social Sciences majors, with significantly lower adoption rates across nearly all time periods.\r\n\r\n3.4Choice of Generative AI Models\r\nMajor AI companies operate on a freemium model, where free versions coexist with premium subscriptions that offer higher usage limits and access to more advanced models. This tiered structure creates potential for a new form of educational disparities: if paid versions confer substantial academic advantages, students who cannot afford subscriptions may be systematically disadvantaged. To investigate these potential disparities, we presented respondents with a list of options including both free and paid versions of popular models. We also collected information on monthly subscription expenditures, with response options ranging from no active subscription to spending more than $40 monthly (standard subscriptions to major models like ChatGPT cost $20 per month at the time).\r\n\r\nOpenAI’s ChatGPT dominates AI usage among Middlebury AI users, with the free version capturing the largest market share. Figure 3 shows that 89.3 percent of AI users rely on the free version of ChatGPT, making it dramatically more popular than any alternative. Google Gemini (13.5 percent) and Microsoft Copilot (7.7 percent) are distant competitors, while other platforms each capture less than 5 percent of users. This dominance mirrors patterns in other academic settings, where Stöhr et al., (2024) find substantially higher familiarity with ChatGPT compared to alternative platforms, and aligns with broader workforce trends documented by Bick et al., (2025).8\r\n\r\nDespite near-universal AI adoption, only 11.3 percent of AI users pay for any AI service.9 This figure is remarkably similar to the 8 percent found in Ravšelj et al., (2025)’s multi-country survey. This low payment rate suggests that for most students, the premium features of paid versions—primarily higher usage limits and access to more advanced models—do not justify the subscription cost. However, payment patterns reveal significant disparities: males and Asian students are substantially more likely to purchase AI subscriptions (Appendix Table LABEL:tab:ai_model_correlates and Appendix Figure A3), potentially reflecting differences in usage intensity across demographic groups.\r\n\r\n4Generative AI and the Production of Learning\r\n4.1The Use of Generative AI across Academic Tasks\r\nHow is generative AI transforming the traditional inputs to student learning? The educational production function includes inputs like time spent studying (e.g., Stinebrickner and Stinebrickner, , 2008), faculty instruction (e.g., Fairlie et al., , 2014), peer interactions (e.g., Sacerdote, , 2001), and academic support services (e.g., Angrist et al., , 2009). AI tools have the potential to complement or substitute for these traditional inputs. For example, using AI to explain concepts might substitute for faculty office hours, while using it for proofreading might reduce time needed for academic support services.\r\n\r\nTo understand the role of AI in students’ learning production function, we collected information about students’ AI usage across ten common academic tasks: proofreading, generating ideas, writing essays, editing essay drafts, coding assistance, creating images, explaining concepts, composing emails, summarizing materials, and finding information. For each task, students indicated their frequency of AI use on a five-point scale ranging from never to daily. We supplemented this quantitative data with open-ended responses about how AI influences their academic work process.\r\n\r\nStudents use generative AI for a wide range of academic tasks, with the highest adoption rates for learning support and text-processing activities. Figure 4, Panel A shows that explaining concepts is the most common use case, with 80.3 percent of AI users using it for this task. Summarizing texts follows as the second most common task (74.0 percent), followed by finding information and generating ideas (63.1 and 61.9 percent). Writing assistance tasks like proofreading and editing essays are also common, used by 54.1 and 47.3 percent of AI users, respectively. Technical applications like coding help are significant (34.4 percent), considering that many academic degrees involve no programming. Notably, while 23.5 percent of AI users report using it for writing essays, this represents a relatively low adoption rate compared to other academic uses, suggesting students may be more hesitant to use AI for primary content creation. The lowest adoption rate is for creating images at 20.4 percent, likely reflecting fewer academic use cases for this capability.\r\n\r\nAn important limitation of our survey is that it relies on self-reported usage, which may introduce measurement error. For example, students might underreport uses they perceive as academically inappropriate—such as essay writing—while overreporting those viewed as legitimate learning tools (Ling and Imas, , 2025). To assess the validity of our self-reported measures, we compare our findings with Handa et al., 2025a, who analyze actual Claude usage patterns among users with university email addresses. Several caveats apply: their data captures conversation-level interactions rather than student-level usage, and most students in our sample use ChatGPT rather than Claude (Section 3.4). Despite these limitations, the comparison provides a useful benchmark for evaluating our survey responses.\r\n\r\nReassuringly, our results are consistent with Anthropic’s data. Both studies identify explaining concepts and technical problem-solving as primary use cases. Handa et al., 2025a report that the second largest use case (33.5 percent of conversations) involves “technical explanations or solutions for academic assignments,” while we find that 80.3 percent of AI users use it for explaining concepts—a difference likely attributable to our student-level versus their conversation-level measurement. Similarly, the most common usage category in Claude involves “designing practice questions, editing essays, or summarizing academic material” (39.3 percent of conversations), aligning with our findings that 74.0 percent of AI users use it for summarizing texts and 47.3 percent for editing essays. The disciplinary patterns also converge: Handa et al., 2025a finds that computer science, natural sciences, and mathematics conversations are overrepresented, which mirrors our finding that Natural Science majors show significantly higher AI adoption rates (Figure 1). Overall, these convergent findings from self-reported survey data and actual usage logs suggest that our results capture genuine patterns of student AI engagement rather than merely reflecting social desirability in responses.\r\n\r\n4.2Automation versus Augmentation\r\nAre students using generative AI primarily to augment their learning or to automate their coursework? This distinction is crucial for understanding AI’s impacts (Autor and Thompson, , 2025)—augmentation may enhance students’ learning processes while maintaining their active engagement and critical thinking, whereas automation produces fully-formed outputs that could be submitted with minimal student input, potentially harming learning.\r\n\r\nTo examine this empirically, Figure 4, Panel B categorizes the ten measured tasks based on whether they augment or automate academic tasks. We classify tasks as augmentation when they enhance human capabilities (explaining concepts, finding information, proofreading, and editing drafts) and as automation when they directly produce outputs (writing essays, creating images, composing emails, summarizing texts, generating ideas, and coding assistance). We then calculate the percentage of AI users who employ each category at various frequencies.10\r\n\r\nStudents use generative AI for both augmentation and automation, but with markedly different frequency. While 61.2 percent of AI users report using AI for augmentation tasks, 41.9 percent report using it for automation—thus, there is a substantial 19.3 pp difference. This gap is mainly driven by occasional use (19.7 percent for augmentation versus 12.7 percent for automation) and frequent use (16.3 versus 9.0 percent). Most strikingly, students are more than twice as likely to use AI daily for augmentation (5.0 percent) compared to automation (2.5 percent). The higher frequency of augmentation use also suggests that students find augmentation uses more valuable for their day-to-day academic activities. Our findings align closely with patterns observed in actual AI usage data: Handa et al., 2025b analyze real conversation logs from Claude and find that 57 percent of workplace AI interactions involve augmentation while 43 percent involve automation.\r\n\r\nQualitative evidence from open-ended responses provides additional insights into students’ motivations for using AI (see Appendix C for additional results and validation of the open-ended measure). Students’ descriptions align closely with the augmentation-automation framework. For augmentation, many characterize AI as an “on-demand tutor,” particularly valuable when traditional resources like office hours are unavailable. Non-native English speakers frequently mention using AI for proofreading to overcome language barriers, while students from technical majors describe using it to debug code and understand error messages. For automation, time savings emerged as the dominant motivation, with 21.7 percent of open-ended responses explicitly mentioning efficiency benefits (Appendix Figure C3). Students describe turning to AI during periods of overwhelming workload or looming deadlines, using it to generate initial drafts or complete routine assignments. That students automate tasks mostly when time-pressed helps explain why automation is less common than augmentation.11\r\n\r\nOur finding that students favor augmentation over automation extends beyond Middlebury. In Appendix Figure A4, we re-analyze data from Ravšelj et al., (2025)—a multi-country survey of higher education students—to examine how this balance varies across institutional quality.12 We classify universities into quintiles based on their Times Higher Education World University Rankings. Overall, students who use AI worldwide show similar adoption rates for augmentation (64.6 percent) and automation (63.1 percent) tasks—a much smaller gap than at Middlebury (Panel A). However, this aggregate pattern masks heterogeneity by institutional quality. Students at top-quintile universities show a modest preference for augmentation over automation, with this gap narrowing monotonically down the institution quality distribution. By the bottom quintile, augmentation and automation usage are virtually identical (Panel B).\r\n\r\n4.3Heterogeneity in Augmentation versus Automation Usage\r\nUnderstanding whether augmentation and automation patterns vary across student populations may help to design targeted support policies. To assess this, we construct four measures of augmentation and automation usage. First, we create binary indicators for whether students use AI for any augmentation or automation task. Second, we calculate the proportion of tasks in each category for which students employ AI. Third, using the Likert-scale responses, we create intensity measures that capture how frequently students use AI for augmentation and automation purposes. Finally, we compute the difference between augmentation and automation variables, as a measure that directly compare students’ relative preference for augmentation versus automation. Table LABEL:tab:ai_autom_regs presents regression estimates using these measures as outcomes.\r\n\r\nThe balance between augmentation and automation varies substantially by student characteristics. Males show higher adoption of both augmentation and automation tasks, with a similar magnitude in each case—they are 6.0 pp more likely to use augmentation prompts and 5.5 pp more likely to use automation prompts compared to females (columns 1 and 4, \r\np\r\n<\r\n0.05\r\n). Black students have a substantially higher augmentation intensity than white students (0.871 points higher, equivalent to 38.1 percent of the outcome mean, \r\np\r\n<\r\n0.01\r\n), but show no statistically significant differences in automation usage. This results in Black students having the strongest preference for augmentation over automation, with a 19.5 pp higher share of augmentation tasks relative to automation (column 7, \r\np\r\n<\r\n0.01\r\n). Asian students show high usage of both kinds relative to white students. Differences across field of study effects are sizable: humanities, languages, and literature majors tend to show lower usage of both augmentation and automation tools compared to natural science majors.\r\n\r\n5Institutional Policies and AI Adoption\r\n5.1The Role of Institutional Policies in Shaping Student Behavior\r\nInstitutional policies are central to shaping the adoption and diffusion of new technologies (Acemoglu, , 2025). In the context of generative AI in higher education, understanding how policies affect student behavior is essential for guiding evidence-based decision-making. To examine this, we asked students to report their likelihood of using AI under various policy scenarios, ranging from complete prohibition to unrestricted use. This analysis is particularly relevant in light of the ongoing debate about how universities should regulate generative AI use in academic settings (Nolan, , 2023), and the widespread variation in institutional policies across colleges (Xiao et al., , 2023).\r\n\r\nInstitutional policies substantially influence students’ reported likelihood of using generative AI. Figure 5 shows that when generative AI use is unrestricted, 52.4 percent of students report being likely or extremely likely to use it. This likelihood decreases modestly when policies require citation (40.9 percent) or when no explicit policy exists (42.3 percent). However, explicit prohibition creates a dramatic shift: only 13.4 percent of students report they would be likely or extremely likely to use AI when it is banned, while 72.9 percent say they would be unlikely or extremely unlikely to do so (note that these figures include both current AI users and students who do not use generative AI for academic purposes). These results suggest that institutional policies can significantly influence generative AI usage patterns, though a small fraction of students report they would likely use AI even when explicitly prohibited.\r\n\r\nThe magnitude of policy effects in our study aligns closely with findings from other contexts. Carvajal et al., (2024) estimate that banning AI reduces usage by 37.2 pp among females and 20.6 pp among males, for an overall drop of about 28.9 pp. In our survey, a ban leads to a 37.8 pp decline in usage, with a larger decrease among females (49.6 pp) than males (40.1 pp). These parallel results underscore how institutional policies can unintentionally produce disparate effects across gender. Notably, these differential policy effects extend beyond gender. Other demographic and academic characteristics—such as race and field of study—also moderate how students respond to policy restrictions (Appendix Table LABEL:tab:ai_policy_het), underscoring that institutional policies can produce non-neutral impacts along multiple dimensions.\r\n\r\n5.2Understanding of Institutional Policies and Resources\r\nInformation gaps and inattention can significantly affect technology adoption decisions (Duflo et al., , 2011; Hanna et al., , 2014). For example, imperfect information about rules, available resources, or proper usage guidelines could lead to underadoption of beneficial technologies or inadvertent policy violations. To test for the existence of information gaps, we examine three dimensions of policy understanding. First, we asked students whether they find AI policies in their current classes clear. Second, we measured awareness of free access to premium AI tools through the college—a resource that could reduce inequality but only if students know it exists. Third, we assessed whether students know how to properly cite AI when required, a mechanical skill necessary for academic integrity. We supplemented these measures with open-ended feedback about the college’s AI policies and support services.\r\n\r\nStudent understanding of institutional policies is high, though significant gaps remain. Figure 6, Panel A shows that most students (79.1 percent) understand when and where they are allowed to use AI in their classes, but a nontrivial minority (19.2 percent) find AI policies unclear. Moreover, critical knowledge gaps persist elsewhere. Only 10.1 percent know they have free access to Microsoft Copilot through the college (Panel B), and just 32.6 percent understand how to properly cite AI use (Panel C). These gaps vary systematically: females show better policy understanding than males (81.5 versus 75.8 percent), and non-white students demonstrate higher awareness across all three dimensions compared to white students.13\r\n\r\nThe qualitative evidence from open-ended responses reinforces these information frictions and reveals implementation challenges (Appendix D). Students express frustration with vague guidelines, requesting specific examples of acceptable versus unacceptable use cases. Many advocate for formal training, noting that simply knowing policies exist differs from understanding how to effectively integrate AI into their workflow. A particularly striking theme is students’ perception of blanket prohibitions as both ineffective and unfair, as they create a prisoner’s dilemma situation where compliant students are disadvantaged relative to those who secretly violate restrictions. Many responses call for a balanced approach—permitting AI use that supports learning while restricting uses that replace it—though the boundary between the two remains contested.\r\n\r\n6Beliefs About AI’s Educational Impact and Peer Usage\r\n6.1Student Beliefs of AI’s Impact on Educational Outcomes\r\nTechnology adoption decisions are shaped by beliefs about potential returns (Foster and Rosenzweig, , 2010). Students’ perceptions of how AI affects their learning may influence whether they adopt these tools and how they integrate them into their academic workflows. To elicit these beliefs, we asked students to evaluate AI’s impact across four dimensions of their academic performance: understanding of course materials, overall learning ability, time management, and course grades. For each dimension, students rated AI’s effect on a five-point scale ranging from “significantly reduces” to “significantly improves.” These subjective assessments provide insight into the perceived value of AI tools from the student perspective, which may differ from their actual effects on learning outcomes.\r\n\r\nStudents tend to believe that generative AI is beneficial for their academic performance, though the perceived benefits vary across dimensions. Figure 7 shows that the majority of students (70.2 percent) believe that generative AI improves their understanding of course materials, and 60.1 percent report improvement in their ability to grasp concepts, retain information, or learn new skills. Similarly, 59.4 percent report that AI improves their ability to complete assignments on time. Notably, while students believe AI helps their learning and assignment completion, they are less confident about its impact on course grades—41.1 percent believe it improves their grades, while 55.4 percent report no effect and 3.5 percent report negative effects. This pattern suggests that while students perceive generative AI as improving their learning process and workflow—through better understanding, skill development, and timely completion of work—these benefits do not necessarily translate into better course grades.14\r\n\r\nBeliefs about AI’s benefits strongly predict adoption. In Figure 8, we plot the relationship between the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis) for different subgroups of students (e.g., males, white students, public-school students, etc.). Across all four measured academic dimensions, there is a strong and statistically significant relationship between perceiving positive AI effects and AI adoption. The relationship is strongest for beliefs about the ability to improve course grades (Panel D): a 10-pp increase in the belief that AI improves grades is associated with a 5.6 pp increase in AI adoption (\r\np\r\n<\r\n0.01\r\n). Similar positive relationships exist for beliefs about learning ability (4.3 pp, \r\np\r\n<\r\n0.05\r\n), understanding of course materials (4.5 pp, \r\np\r\n<\r\n0.01\r\n), and timely assignment completion (5.3 pp, \r\np\r\n<\r\n0.05\r\n). These findings suggest that student beliefs about AI’s academic benefits—irrespective of the actual benefits—may play a crucial role in shaping adoption decisions.\r\n\r\nThese positive beliefs align with findings from other contexts. Ravšelj et al., (2025) report that the majority of students in their sample believe that ChatGPT improves their general knowledge (68.8 percent) and specific knowledge (62.7 percent)—remarkably similar to our finding that most students believe AI improves understanding of course materials (68.5 percent) and learning ability (57.6 percent). Similarly, 57.4 percent of students in Ravšelj et al., (2025)’s sample believe that ChatGPT helps meet assignment deadlines, while 59.4 percent in our data report AI improves timely assignment completion. Stöhr et al., (2024) provide complementary evidence from Swedish universities, where 47.7 percent of students believe that AI makes them more effective learners, yet only 17.3 percent believe these tools improve their grades—mirroring our finding that perceived learning benefits from AI use exceed perceived effects on course performance.\r\n\r\n6.2Student Beliefs About Peer Use of Generative AI\r\nStudents’ beliefs about their peers’ AI usage may influence their own adoption through multiple channels, including social norms (e.g., Giaccherini et al., , 2019), social learning (e.g., Foster and Rosenzweig, , 1995; Beaman et al., , 2021), peer effects (e.g., Bailey et al., , 2022), and competitive pressure to avoid falling behind (e.g., Goehring et al., , 2024). To measure these beliefs, we asked students to estimate what fraction of their peers use generative AI for different purposes and would use under different policy environments. Figure 9 presents the distribution of these beliefs. Panels A-C show students’ estimates of peer AI usage for schoolwork, leisure, and any purpose respectively. Panels D-F display students’ beliefs about AI usage under three policy environments: when classes have no explicit AI policy, when classes allow AI use, and when classes prohibit AI use.\r\n\r\nStudents systematically underestimate their peers’ AI usage. On average, they believe 65.2 percent of peers regularly use generative AI for schoolwork (Panel A), while our survey reveals an actual usage rate of 82.5 percent—a 17.3 pp gap. This underestimation appears consistent across educational contexts: Stöhr et al., (2024) find that only 38.7 percent of Swedish students believe AI chatbot use is common among peers, despite 63 percent actual usage. When we examine beliefs about policy-contingent behavior, students estimate that 62.7 percent of peers use AI when no explicit policy exists (Panel D), rising to 72.2 percent under permissive policies (Panel E) and falling to 43.5 percent under prohibition (Panel F). These beliefs align directionally with self-reported intentions, though with notable magnitude differences (Appendix Figure A5). For instance, while students believe 43.5 percent of their peers use AI in classes that prohibit it, only 28.4 percent report they themselves would be likely to do so.15\r\n\r\nNotably, students’ perceptions of peer AI adoption are closely linked to their own adoption behavior. Appendix Figure A6 shows a strong positive association between beliefs about peer usage and actual usage rates across student groups. This behavior is consistent with several psychological mechanisms, including the “false consensus effect” (Ross et al., , 1977), selection neglect (Jehiel, , 2018), or interpersonal projection bias (Bushong and Gagnon-Bartsch, , 2024). Still, virtually all groups systematically underestimate peer usage—actual usage rates exceed believed usage for every demographic and academic group we examine. This figure also shows that underestimation varies substantially across student characteristics: males use AI at 88.7 percent but believe only 66.6 percent of peers do so (22.1 pp gap), while females show a smaller gap (77.6 percent actual versus 65.1 percent believed, 13.3 pp gap).\r\n\r\nTaken together, students’ systematic underestimation of peer AI usage coupled with the strong relationship between beliefs and adoption suggests that misperceptions about social norms may shape technology diffusion. If AI enhances learning, then correcting these misperceptions through information provision could accelerate beneficial adoption; conversely, if AI undermines skill development, then students’ underestimation of peer usage may serve as an unintentional safeguard against harmful overadoption.\r\n\r\n7Discussion\r\nThis paper presents the first systematic evidence on generative AI adoption at a highly selective U.S. college. Using novel survey data, we document exceptionally rapid and widespread adoption, substantial shifts in the educational production function through augmentation and automation, and the significant roles of students’ beliefs and institutional policies in shaping AI use.\r\n\r\nOur results offer three implications for institutional policy and ongoing debates about AI in education. First, we identify low-cost opportunities to improve institutional policy effectiveness through targeted information provision. The significant gaps we document in students’ understanding of institutional AI policies, citation practices, and available AI resources suggest that simple interventions—such as clear guidelines, illustrative examples of acceptable uses, and AI literacy programs—can reduce unintentional academic integrity violations and support beneficial AI integration. Qualitative feedback strongly indicates student demand for more explicit guidance on responsible AI use.\r\n\r\nSecond, our evidence challenges alarmist narratives that conflate widespread AI adoption with universal academic dishonesty based on anecdotal accounts.16 Although AI use is indeed near-universal, we find clear evidence that students primarily employ AI as a tool for strategic task management: to enhance learning (augmentation) and selectively automate tasks when facing high time opportunity cost—not solely to circumvent academic effort. This distinction matters: by normalizing academic dishonesty as inevitable and universal, these narratives may shift social norms and encourage students who would otherwise use AI responsibly to engage in prohibited behaviors, believing “everyone else is doing it.”\r\n\r\nThird, our findings caution against policy extremes of either blanket prohibition or unrestricted AI use (Merchant, , 2024; McDonald et al., , 2025). Blanket prohibitions disproportionately harm students who benefit most from AI’s augmentation functions—particularly lower-achieving students—while also creating uneven compliance, placing conscientious students at a disadvantage relative to rule-breakers. Conversely, unrestricted AI use based solely on revealed preference arguments ignores important market failures in educational settings. Students often hold overly optimistic beliefs about AI’s learning benefits despite mixed empirical evidence, potentially leading to unintended negative learning outcomes. Most concerningly, permissive policies risk creating competitive dynamics where students feel compelled to adopt AI not for its learning benefits but simply to avoid falling behind in an educational “arms race” (Goehring et al., , 2024).\r\n\r\nFigures and Tables\r\nFigure 1:The Adoption of Generative AI among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the fraction of students who report using AI during the academic semester, categorized by demographic characteristics, high school type, academic cohort, GPA, and field of study. Usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily).\r\n\r\nThe category “All students” provides the baseline usage rate for the full sample. Gender categories are based on self-identification, with non-binary responses excluded due to a small sample size. “Private HS” refers to students who attended private high schools, while “Public HS” includes public institutions. “Cohort” denotes the student’s academic year, ranging from first-year (“Freshman”) to fourth-year and beyond (“Senior”). GPA categories (“GPA \r\n>\r\n p50” and “GPA \r\n<\r\n p50”) split students into groups above or below the median first-year GPA, as self-reported on a 4.0 scale. See Appendix B.1 for the classification of majors into fields of study.\r\n\r\nFigure 2:The Evolution of Generative AI Adoption among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the cumulative percent of students who reported using generative AI tools for academic purposes over time. The data is based on retrospective self-reports collected in our December 2024 survey, where students were asked “When did you first start using any form of Generative AI for academic purposes?” Response options ranged from “Before Spring 2023” to “This semester (Fall 2024).” The \r\nx\r\n-axis represents academic semesters, while the \r\ny\r\naxis represents the cumulative adoption rate. Vertical lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure 3:Adoption of Generative AI Models Among College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the adoption rates of various AI models as of Fall 2024. The horizontal axis shows the percent of students who reported using each tool, and the vertical axis lists the tools in descending order of adoption rates.\r\n\r\nFigure 4:Academic Uses of Generative AI\r\nPanel A. Across Common Academic Tasks\r\nRefer to caption\r\nPanel B. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of AI students who use generative AI for different academic tasks. For each task, usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). The number at the end of each bar represents the total percent of students who use AI for that purpose at any frequency. Tasks are ordered by total usage, from highest to lowest. Results are based on responses to the question: “For academic purposes, which of the following tasks do you typically use generative AI for?” Sample includes all students who reported using AI during the academic semester.\r\n\r\nFigure 5:Student Reported Likelihood of Using AI under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of students who report different likelihoods of using AI under various policy scenarios. For each policy, responses are categorized on a five-point scale from “Extremely unlikely to use AI” to “Extremely likely to use AI.” The sample includes all survey respondents. The question asked was: “How likely are you to use generative AI in a class with each of the following AI policies?”\r\n\r\nFigure 6:Understanding of Generative AI Policies and Resources\r\nPanel A. Understanding of AI Policies\r\nRefer to caption\r\nPanel B. Awareness of Copilot Access\r\nRefer to caption\r\nPanel C. Knowledge of Citation Requirements\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows students’ understanding of institutional AI policies and resources. Panel A displays the percent of students who report understanding AI policies in their classes, those who report having no explicit policy, and those who find policies unclear, broken down by demographic characteristics. Panel B shows the percent of students who are aware of their free access to Microsoft Copilot through Middlebury College. Panel C presents the percent of students who report knowing how to properly cite AI use in their academic work when required. For Panels B and C, horizontal lines represent 95 percent confidence intervals. Sample includes all survey respondents.\r\n\r\nFigure 7:Student Beliefs about the Impact of AI on their Academic Performance\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of Middlebury students who believe that AI improves, reduces, or has no effect on different aspects of their academic experience. For each outcome, responses are categorized into three groups: “Improves” combines “significantly improves” and “somewhat improves” responses, “Reduces” combines “significantly reduces” and “somewhat reduces” responses, and “No effect” represents neutral responses. Sample includes all students who report using AI during the academic semester. “Don’t know” responses are excluded.\r\n\r\nFigure 8:Relationship Between AI Adoption and Beliefs About AI’s Academic Benefits\r\nPanel A. Learning ability (e.g., ability to grasp concepts, learn new skills, etc.)\r\nRefer to caption\r\nPanel B. Understanding of\r\ncourse materials\r\nRefer to caption\r\nPanel C. Ability to complete\r\nassignments on time\r\nRefer to caption\r\nPanel D. Course grades\r\n \r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the relationship between AI adoption rates and beliefs that AI improves various academic outcomes across different student groups. Each panel plots the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis). Points represent different student groups categorized by demographics (circles), academic characteristics (triangles), and field of study (squares). The dashed line shows the linear fit across all groups. Groups with fewer than 10 students are excluded.\r\n\r\nFigure 9:Student Beliefs about Generative AI Usage at Middlebury College\r\nPanel A. For Schoolwork\r\nRefer to caption\r\nPanel B. For Leisure\r\nRefer to caption\r\nPanel C. For Any Purpose\r\nRefer to caption\r\nPanel D. Classes with No Policy\r\nRefer to caption\r\nPanel E. Classes that Allow AI\r\nRefer to caption\r\nPanel F. Classes that Disallow AI\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the distribution of students’ beliefs about generative AI usage among their peers at Middlebury College. Panels A-C display students’ estimates of the percent of their peers who regularly use AI for schoolwork, leisure activities, and any purpose, respectively. Panels D-F show students’ beliefs about AI usage in classes with different AI policies: those without an explicit policy (Panel D), those that allow AI use (Panel E), and those that prohibit AI use (Panel F). Each panel shows a histogram with bins of width ten percentage points (e.g., responses between 1-10 fall in the 10 bin, 11-20 in the 20 bin, etc.). The red dashed line indicates the mean response. Sample excludes respondents with missing values or who selected the default response for all six categories (which equals zero).\r\n\r\nTable 1:Summary Statistics of Survey Participants\r\nSurvey Sample\tAdmin records\r\nUnweighted\tWeighted\t\r\n(1)\t(2)\t(3)\r\nPanel A. Demographics\r\nMale\t0.446\t0.433\t0.463\r\nFemale\t0.508\t0.516\t0.533\r\nWhite\t0.618\t0.603\t0.538\r\nBlack\t0.036\t0.033\t0.052\r\nHispanic\t0.099\t0.106\t0.124\r\nAsian\t0.155\t0.162\t0.073\r\nPrivate high school\t0.420\t0.399\t–\r\nPublic high school\t0.543\t0.556\t–\r\nPanel B. Academic Characteristics    GPA 3.740 3.736 3.670 Hours spent on academics per week 17.899 17.889 – Freshman 0.311 0.355 0.255 Sophomore 0.273 0.272 0.257 Junior 0.202 0.179 0.201 Senior 0.213 0.194 0.287   Panel C. Field of Study    Arts 0.011 0.021 0.023 Humanities 0.052 0.068 0.073 Languages 0.021 0.020 0.025 Literature 0.035 0.025 0.025 Natural Sciences 0.218 0.249 0.244 Social Sciences 0.353 0.247 0.243 Has not declared major 0.311 0.371 0.364\r\n\r\nN (# degrees) 43 43 49 N (# students) 634 2,760 2,760\r\n\r\n\\justify\r\nNotes: This table presents summary statistics from our survey of college students. Panel A reports demographic characteristics, including the proportion of participants identifying as male, female, white, Black, Hispanic, Asian, or who attended a private or public high school. Panel B provides academic characteristics, such as GPA (only available for non-freshmen), average weekly hours spent on academics, and academic year distribution (Freshman, Sophomore, Junior, and Senior). Note that in column 1–2, GPA refers to self-reported first-year GPA while in column 3 it is the overall GPA during Spring 2024. Panel C summarizes the distribution of participants across different fields of study. Major groups are mutually exclusive.\r\n\r\n0.8250.5890.3680.106 R\r\n−\r\nsquared0.0770.0890.1160.058 N (Students)616616616616 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI usage frequency threshold and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column uses a different threshold for AI usage frequency, categorized as: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). Column 1 defines usage as any nonzero frequency; column 2 includes at least occasional use; column 3 includes frequent or higher use; and column 4 captures only very frequent use.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.0890.1990.3570.5510.801 R\r\n−\r\nsquared0.0450.0610.0880.0800.069 N (Students)633633633633633 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI adoption date and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column presents results for a different threshold of AI adoption. Column 1 shows the probability of adopting AI before Spring 2023; column 2 by Spring 2023; column 3 by Fall 2023; column 4 by Spring 2024; and column 5 by Fall 2024. The dependent variable in each regression is a binary indicator equal to one if the student had adopted AI by the specified time period.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.9120.6122.2850.9130.4191.8010.1930.484 R\r\n−\r\nsquared0.0360.0730.1260.0530.1170.1290.0420.063 N (Students)515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and their use of generative AI for academic tasks. In columns 1 and 4, the outcome is a dummy that equals one if a student reports using AI with any frequency for at least one augmentation or automation task, respectively. In columns 2 and 5, the outcome is the share of tasks within each category for which the student reports any use. In columns 3 and 6, the outcome is a continuous measure capturing average usage frequency for each task category, based on raw Likert-style responses. In columns 7 and 8, the outcome is the difference in average task share and usage intensity between augmentation and automation, respectively. Regressions are weighted and report robust standard errors clustered at the student level. ∗∗∗ \r\np\r\n<\r\n0.01\r\n, ∗∗ \r\np\r\n<\r\n0.05\r\n, ∗ \r\np\r\n<\r\n0.10\r\n.\r\n\r\nReferences\r\nAcemoglu, (2025)\r\nAcemoglu, D. (2025).Nobel lecture: Institutions, technology, and prosperity.American Economic Review, 115(6):1709–1748.\r\nAngrist et al., (2009)\r\nAngrist, J., Lang, D., and Oreopoulos, P. (2009).Incentives and services for college achievement: Evidence from a randomized trial.American Economic Journal: Applied Economics, 1(1):136–163.\r\nAutor and Thompson, (2025)\r\nAutor, D. and Thompson, N. (2025).Expertise.\r\nBailey et al., (2022)\r\nBailey, M., Johnston, D., Kuchler, T., Stroebel, J., and Wong, A. (2022).Peer effects in product adoption.American Economic Journal: Applied Economics, 14(3):488–526.\r\nBastani et al., (2025)\r\nBastani, H., Bastani, O., Sungu, A., Ge, H., Kabakcı, Ö., and Mariman, R. (2025).Generative AI without guardrails can harm learning: Evidence from high school mathematics.Proceedings of the National Academy of Sciences, 122(26):e2422633122.\r\nBeaman et al., (2021)\r\nBeaman, L., BenYishay, A., Magruder, J., and Mobarak, A. M. (2021).Can network theory-based targeting increase technology adoption?American Economic Review, 111(6):1918–1943.\r\nBick et al., (2025)\r\nBick, A., Blandin, A., and Deming, D. J. (2025).The rapid adoption of generative AI.Technical report, National Bureau of Economic Research.\r\nBonney et al., (2024)\r\nBonney, K., Breaux, C., Buffington, C., Dinlersoz, E., Foster, L. S., Goldschlag, N., Haltiwanger, J. C., Kroff, Z., and Savage, K. (2024).Tracking Firm Use of AI in Real Time: A Snapshot from the Business Trends and Outlook Survey.\r\nBonsu and Baffour-Koduah, (2023)\r\nBonsu, E. M. and Baffour-Koduah, D. (2023).From the consumers’ side: Determining students’ perception and intention to use chatgpt in ghanaian higher education.Journal of Education, Society & Multiculturalism, 4(1):1–29.\r\nBresnahan and Trajtenberg, (1995)\r\nBresnahan, T. F. and Trajtenberg, M. (1995).General purpose technologies ‘engines of growth’?Journal of econometrics, 65(1):83–108.\r\nBrynjolfsson and Hitt, (2000)\r\nBrynjolfsson, E. and Hitt, L. M. (2000).Beyond computation: Information technology, organizational transformation and business performance.Journal of Economic Perspectives, 14(4):23–48.\r\nBrynjolfsson et al., (2025)\r\nBrynjolfsson, E., Li, D., and Raymond, L. (2025).Generative AI at Work.The Quarterly Journal of Economics, 140(2):889–942.\r\nBushong and Gagnon-Bartsch, (2024)\r\nBushong, B. and Gagnon-Bartsch, T. (2024).Failures in forecasting: An experiment on interpersonal projection bias.Management Science, 70(12):8735–8752.\r\nCarvajal et al., (2024)\r\nCarvajal, D., Franco, C., and Isaksson, S. (2024).Will artificial intelligence get in the way of achieving gender equality?Discussion Paper 03, NHH Norwegian School of Economics.\r\n(15)\r\nComin, D. and Hobijn, B. (2010a).An exploration of technology diffusion.American Economic Review, 100(5):2031–2059.\r\n(16)\r\nComin, D. and Hobijn, B. (2010b).Technology Diffusion and Postwar Growth, volume 25, pages 209–246.University of Chicago Press.\r\nCui et al., (2024)\r\nCui, Z. K., Demirer, M., Jaffe, S., Musolff, L., Peng, S., and Salz, T. (2024).The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers.\r\nDavid, (1990)\r\nDavid, P. A. (1990).The dynamo and the computer: an historical perspective on the modern productivity paradox.The American economic review, 80(2):355–361.\r\nDell’Acqua et al., (2023)\r\nDell’Acqua, F., McFowland, E., Mollick, E. R., Lifshitz-Assaf, H., Kellogg, K., Rajendran, S., Krayer, L., Candelon, F., and Lakhani, K. R. (2023).Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality.\r\nDuflo et al., (2011)\r\nDuflo, E., Kremer, M., and Robinson, J. (2011).Nudging Farmers to Use Fertilizer: Theory and Experimental Evidence from Kenya.American Economic Review, 101(6):2350–2390.\r\nEloundou et al., (2024)\r\nEloundou, T., Manning, S., Mishkin, P., and Rock, D. (2024).GPTs are GPTs: Labor market impact potential of LLMs.Science, 384(6702):1306–1308.\r\nFairlie et al., (2014)\r\nFairlie, R. W., Hoffmann, F., and Oreopoulos, P. (2014).A community college instructor like me: Race and ethnicity interactions in the classroom.American Economic Review, 104(8):2567–2591.\r\nFelten et al., (2021)\r\nFelten, E. W., Raj, M., and Seamans, R. (2021).Occupational, Industry, and Geographic Exposure to Artificial Intelligence: A Novel Dataset and Its Potential Uses.\r\nFelten et al., (2023)\r\nFelten, E. W., Raj, M., and Seamans, R. (2023).How will Language Modelers like ChatGPT Affect Occupations and Industries?\r\nFoster and Rosenzweig, (1995)\r\nFoster, A. D. and Rosenzweig, M. R. (1995).Learning by doing and learning from others: Human capital and technical change in agriculture.Journal of political Economy, 103(6):1176–1209.\r\nFoster and Rosenzweig, (2010)\r\nFoster, A. D. and Rosenzweig, M. R. (2010).Microeconomics of technology adoption.Annu. Rev. Econ., 2(1):395–424.\r\nGallup, (2024)\r\nGallup (2024).AI in the Workplace: Answering 3 Big Questions.\r\nGiaccherini et al., (2019)\r\nGiaccherini, M., Herberich, D. H., Jimenez-Gomez, D., List, J. A., Ponti, G., and Price, M. K. (2019).The behavioralist goes door-to-door: Understanding household technological diffusion using a theory-driven natural field experiment.Technical report, National Bureau of Economic Research.\r\nGoehring et al., (2024)\r\nGoehring, G., Mezzanotti, F., and Ravid, S. A. A. (2024).Technology adoption and career concerns: Evidence from the adoption of digital technology in motion pictures.Technical report, National Bureau of Economic Research.\r\nGriliches, (1957)\r\nGriliches, Z. (1957).Hybrid corn: An exploration in the economics of technological change.Econometrica, 25(4):501–522.\r\nHall and Khan, (2003)\r\nHall, B. H. and Khan, B. (2003).Adoption of new technology.\r\n(32)\r\nHanda, K., Bent, D., Tamkin, A., McCain, M., Durmus, E., Stern, M., Schiraldi, M., Huang, S., Ritchie, S., Syverud, S., Jagadish, K., Vo, M., Bell, M., and Ganguli, D. (2025a).Anthropic education report: How university students use claude.\r\n(33)\r\nHanda, K., Tamkin, A., McCain, M., Huang, S., Durmus, E., Heck, S., Mueller, J., Hong, J., Ritchie, S., Belonax, T., et al. (2025b).Which economic tasks are performed with ai? evidence from millions of claude conversations.arXiv preprint arXiv:2503.04761.\r\nHanna et al., (2014)\r\nHanna, R., Mullainathan, S., and Schwartzstein, J. (2014).Learning Through Noticing: Theory and Evidence from a Field Experiment.The Quarterly Journal of Economics, 129(3):1311–1353.\r\nHartley et al., (2025)\r\nHartley, J., Jolevski, F., Melo, V., and Moore, B. (2025).The Labor Market Effects of Generative Artificial Intelligence.\r\nHumlum and Vestergaard, (2025)\r\nHumlum, A. and Vestergaard, E. (2025).Large Language Models, Small Labor Market Effects.\r\nHutto and Gilbert, (2014)\r\nHutto, C. and Gilbert, E. (2014).Vader: A parsimonious rule-based model for sentiment analysis of social media text.In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216–225.\r\nJehiel, (2018)\r\nJehiel, P. (2018).Investment strategy and selection bias: An equilibrium perspective on overoptimism.American Economic Review, 108(6):1582–1597.\r\nKelly et al., (2023)\r\nKelly, A., Sullivan, M., and Strampel, K. (2023).Generative artificial intelligence: University student awareness, experience, and confidence in use across disciplines.Journal of University Teaching and Learning Practice, 20(6):1–16.\r\nKharazian, (2025)\r\nKharazian, A. (2025).Ramp ai index: Monthly measurement of ai adoption by american businesses.Technical report, Ramp Economics Lab.\r\nLehmann et al., (2024)\r\nLehmann, M., Cornelius, P. B., and Sting, F. J. (2024).AI Meets the Classroom: When Does ChatGPT Harm Learning?\r\nLing and Imas, (2025)\r\nLing, Y. and Imas, A. (2025).Underreporting of AI use: The role of social desirability bias.Available at SSRN.\r\nLiu and Wang, (2024)\r\nLiu, Y. and Wang, H. (2024).Who on earth is using generative ai?Policy Research Working Paper WPS10870, World Bank Group, Washington, D.C.\r\nMcClain, (2024)\r\nMcClain, C. (2024).Americans’ use of ChatGPT is ticking up, but few trust its election information.\r\nMcDonald et al., (2025)\r\nMcDonald, N., Johri, A., Ali, A., and Collier, A. H. (2025).Generative artificial intelligence in higher education: Evidence from an analysis of institutional policies and guidelines.Computers in Human Behavior: Artificial Humans, 3:100121.\r\nMcElheran et al., (2023)\r\nMcElheran, K., Li, J. F., Brynjolfsson, E., Kroff, Z., Dinlersoz, E., Foster, L. S., and Zolas, N. (2023).AI Adoption in America: Who, What, and Where.\r\nMerchant, (2024)\r\nMerchant, B. (2024).The new luddites aren’t backing down.The Atlantic.Accessed: 2025-06-16.\r\nNam, (2023)\r\nNam, J. (2023).56% of college students have used ai on assignments or exams.Technical report.Edited by Lyss Welding. Accessed June 11, 2025.\r\nNolan, (2023)\r\nNolan, B. (2023).Here are the schools and colleges that have banned chatgpt.Business Insider.Accessed: 2025-01-27.\r\nNoy and Zhang, (2023)\r\nNoy, S. and Zhang, W. (2023).Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence.\r\nOtis et al., (2024)\r\nOtis, N. G., Delecourt, S., Cranney, K., and Koning, R. (2024).Global Evidence on Gender Gaps and Generative AI.Harvard Business School.\r\nPeng et al., (2023)\r\nPeng, S., Kalliamvakou, E., Cihon, P., and Demirer, M. (2023).The Impact of AI on Developer Productivity: Evidence from GitHub Copilot.\r\nRavšelj et al., (2025)\r\nRavšelj, D., Keržič, D., Tomaževič, N., Umek, L., Brezovar, N., A. Iahad, N., Abdulla, A. A., Akopyan, A., Aldana Segura, M. W., AlHumaid, J., Allam, M. F., Alló, M., Andoh, R. P. K., Andronic, O., Arthur, Y. D., Aydın, F., Badran, A., Balbontín-Alvarado, R., Ben Saad, H., Bencsik, A., Benning, I., Besimi, A., Bezerra, D. D. S., Buizza, C., Burro, R., Bwalya, A., Cachero, C., Castillo-Briceno, P., Castro, H., Chai, C. S., Charalambous, C., Chiu, T. K. F., Clipa, O., Colombari, R., Corral Escobedo, L. J. H., Costa, E., Cre\\textcommabelowtulescu, R. G., Crispino, M., Cucari, N., Dalton, F., Demir Kaya, M., Dumić-Čule, I., Dwidienawati, D., Ebardo, R., Egbenya, D. L., Faris, M. E., Fečko, M., Ferrinho, P., Florea, A., Fong, C. Y., Francis, Z., Ghilardi, A., González-Fernández, B., Hau, D., Hossain, M. S., Hug, T., Inasius, F., Ismail, M. J., Jahić, H., Jessa, M. O., Kapanadze, M., Kar, S. K., Kateeb, E. T., Kaya, F., Khadri, H. O., Kikuchi, M., Kobets, V. M., Kostova, K. M., Krasmane, E., Lau, J., Law, W. H. C., Lazăr, F., Lazović-Pita, L., Lee, V. W. Y., Li, J., López-Aguilar, D. V., Luca, A., Luciano, R. G., Machin-Mastromatteo, J. D., Madi, M., Manguele, A. L., Manrique, R. F., Mapulanga, T., Marimon, F., Marinova, G. I., Mas-Machuca, M., Mejía-Rodríguez, O., Meletiou-Mavrotheris, M., Méndez-Prado, S. M., Meza-Cano, J. M., Mirķe, E., Mishra, A., Mital, O., Mollica, C., Morariu, D. I., Mospan, N., Mukuka, A., Navarro Jiménez, S. G., Nikaj, I., Nisheva, M. M., Nisiforou, E., Njiku, J., Nomnian, S., Nuredini-Mehmedi, L., Nyamekye, E., Obadić, A., Okela, A. H., Olenik-Shemesh, D., Ostoj, I., Peralta-Rizzo, K. J., Peštek, A., Pilav-Velić, A., Pires, D. R. M., Rabin, E., Raccanello, D., Ramie, A., Rashid, M. M. U., Reuter, R. A. P., Reyes, V., Rodrigues, A. S., Rodway, P., Ručinská, S., Sadzaglishvili, S., Salem, A. A. M. S., Savić, G., Schepman, A., Shahpo, S. M., Snouber, A., Soler, E., Sonyel, B., Stefanova, E., Stone, A., Strzelecki, A., Tanaka, T., Tapia Cortes, C., Teira-Fachado, A., Tilga, H., Titko, J., Tolmach, M., Turmudi, D., Varela-Candamio, L., Vekiri, I., Vicentini, G., Woyo, E., Yorulmaz, Ö., Yunus, S. A. S., Zamfir, A.-M., Zhou, M., and Aristovnik, A. (2025).Higher education students’ perceptions of ChatGPT: A global study of early reactions.PLOS ONE, 20(2):e0315011.\r\nRogers, (1962)\r\nRogers, E. M. (1962).Diffusion of Innovations.Free Press, New York.\r\nRoss et al., (1977)\r\nRoss, L., Greene, D., and House, P. (1977).The “false consensus effect”: An egocentric bias in social perception and attribution processes.Journal of experimental social psychology, 13(3):279–301.\r\nSacerdote, (2001)\r\nSacerdote, B. (2001).Peer Effects with Random Assignment: Results for Dartmouth Roommates.The Quarterly Journal of Economics, 116(2):681–704.\r\nStinebrickner and Stinebrickner, (2008)\r\nStinebrickner, R. and Stinebrickner, T. R. (2008).The causal effect of studying on academic performance.The BE Journal of Economic Analysis & Policy, 8(1).\r\nStöhr et al., (2024)\r\nStöhr, C., Ou, A. W., and Malmström, H. (2024).Perceptions and usage of AI chatbots among students in higher education across genders, academic levels and fields of study.Computers and Education: Artificial Intelligence, 7:100259.\r\nStokey, (2021)\r\nStokey, N. L. (2021).Technology diffusion.Review of Economic Dynamics, 42:15–36.\r\nWorld Bank, (2016)\r\nWorld Bank (2016).World development report 2016: Digital dividends.World Bank Publications.\r\nWu et al., (2022)\r\nWu, M.-J., Zhao, K., and Fils-Aime, F. (2022).Response rates of online surveys in published research: A meta-analysis.Computers in Human Behavior Reports, 7.\r\nXiao et al., (2023)\r\nXiao, P., Chen, Y., and Bao, W. (2023).Waiting, banning, and embracing: An empirical analysis of adapting policies for generative ai in higher education.arXiv preprint arXiv:2305.18617.\r\nAppendix\r\n\r\nAppendix AAppendix Figures and Tables\r\nFigure A1:Generative AI Usage Survey Design Overview\r\nKey\t\r\nSurvey Structure\r\nSurvey Sections\r\nUsage Data Collection\r\nBeliefs & Perceptions\r\nAI Usage Survey at Middlebury College\r\nDecember 2024\r\nParticipant Recruitment\r\nCampus-wide email invitations, framed as general technology use survey\r\nParticipation Incentives\r\nAmazon gift card lottery ranging from $50-$500\r\nSection 1: Demographics & Academic Information\r\nStudent Characteristics\r\nGender, race/ethnicity, high school type\r\nAcademic Profile\r\nYear, major, self-reported GPA, study hours\r\nSection 2: generative AI Usage Patterns\r\nAdoption Metrics\r\nFrequency, timing of first use, specific AI models used\r\nAcademic Applications\r\nTasks performed with AI, payment for premium services\r\nSection 3: Perceptions & Institutional Policies\r\nImpact Perceptions\r\nEffects on learning, grades, time management\r\nPolicy Responses\r\nUsage likelihood under different policies\r\nPeer Usage Beliefs\r\nEstimates of AI adoption among peers for schoolwork and leisure\r\nOpen-Ended Response Collection\r\nMotivations for AI use, policy feedback, suggestions for improvement\r\nNote: This figure illustrates the structure of the AI usage survey conducted at Middlebury College in December 2024. The survey collected information across three main sections: (1) demographic and academic background, (2) patterns of generative AI usage including adoption timing, frequency, and specific applications, and (3) perceptions of AI’s impact on learning and responses to institutional policies.\r\n\r\nFigure A2:Cumulative Generative AI Use by Student Characteristic\r\nPanel A. By Gender\r\nRefer to caption\r\nPanel B. By Race\r\nRefer to caption\r\nPanel C. By High School Type\r\nRefer to caption\r\nPanel D. By Freshman Year GPA\r\nRefer to caption\r\nPanel E. By Cohort\r\nRefer to caption\r\nPanel F. By Field of Study\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents cumulative AI use based on different student characteristics. Each panel displays the cumulative distribution of AI use based on a specific characteristic: gender, race, school type, first-year GPA, cohort, or field of study. The cumulative percent of students is plotted against usage categories. The legends and colors correspond to subgroups within each demographic variable.\r\n\r\nFigure A3:Percent of Students Who Pay for Generative AI Tools\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of AI users who pay for AI tools (through any platform) across different demographic groups. Horizontal lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure A4:Academic Uses of ChatGPT: Evidence from Global Survey\r\nPanel A. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\nPanel B. Across the College Quality Distribution\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of students who use ChatGPT for different academic tasks based on data from Ravšelj et al., (2025). Panel A displays usage patterns across tasks categorized as augmenting (proofreading, translating, study assistance, research assistance) versus automating (academic writing, professional writing, creative writing, brainstorming, summarizing, calculating, coding assistance, personal assistance) student effort. Panel B shows usage patterns across university quality quintiles based on World University Rankings, with universities ranked in the top 20% (top quintile) showing slightly higher rates of augmentation relative to automation compared to bottom quintile institutions. The analysis includes universities with at least 30 student responses and excludes observations with missing usage data.\r\n\r\nFigure A5:Student Beliefs about AI Usage versus Actual Usage under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure compares students’ beliefs about AI usage with actual usage rates under different AI policies. For each policy type, blue bars show the percent of students who report being “Neutral,” “Likely,” or “Very Likely” to use AI, while red bars show students’ mean beliefs about what percent of their peers use AI in classes with that policy. The sample includes all survey respondents for both actual usage and beliefs measures. “AI Use is Unrestricted” refers to classes with no restrictions on AI use, “No Explicit AI Policy” refers to classes without a stated policy, and “AI Use is Prohibited” refers to classes that ban AI use entirely.\r\n\r\nFigure A6:Relationship Between Beliefs About AI Usage and Actual AI Usage\r\nRefer to caption\\justify\r\nNotes: This figure shows the relationship between students’ beliefs about AI usage among their peers and actual AI usage rates across different demographic groups. Each point represents a different group of students (by demographics, academic characteristics, or field of study). The \r\ny\r\n-axis shows the percent of students in each group who report using AI for academic purposes. The \r\nx\r\n-axis shows the mean belief within each group about what percent of Middlebury students use AI. The dashed line shows the linear fit. Sample includes all survey respondents with at least ten observations per group. Students’ beliefs about AI usage are positively correlated with actual usage patterns, suggesting that students have relatively accurate perceptions of AI adoption among their peers.\r\n\r\nGPA > p50 80.3 26.3 17.4 23.2 13.4 GPA < p50 87.1 18.7 26.3 32.2 9.9 Freshman 81.3 24.3 24.1 24.7 8.1 Sophomore 84.4 25.6 22.6 23.7 12.4 Junior 86.2 21.6 23.1 28.6 12.8 Senior 78.7 21.0 17.4 29.9 10.4\r\n\r\nPanel C. Field of Study    Arts 73.3 21.0 0.0 52.3 0.0 Humanities 74.8 39.3 21.8 12.1 1.7 Languages 57.4 27.0 30.3 0.0 0.0 Literature 48.6 10.0 33.7 5.0 0.0 Natural Sciences 91.1 22.0 21.9 33.8 13.3 Social Sciences 84.6 17.9 20.4 29.1 17.2\r\n\r\n\\justify\r\nNotes: This table presents the percent of students in each demographic group who report using AI at different frequencies during the academic semester. Each cell shows the percent of students within that group. Column 1 reports the total percent who use AI at any frequency. Columns 2 to 5 represent increasing usage frequencies: rarely (1–2 times per semester), occasionally (monthly), frequently (weekly), and very frequently (multiple times per week). Panel A reports percentages by demographic characteristics. Panel B shows percentages by academic characteristics. Panel C presents percentages by field of study.\r\n\r\n0.9730.1350.0770.1100.114 R\r\n−\r\nsquared0.0120.0510.0270.0240.138 N (Students)516516516516516 \\justifyNotes: This table assesses the relationship between AI model adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI model usage (columns 1-4) or payment for AI services (column 5), and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division.\r\n\r\nEach column presents results for a different model or payment outcome. Column 1 shows usage of OpenAI’s ChatGPT, column 2 Google Gemini, column 3 Microsoft Copilot, column 4 any other AI model, and column 5 whether the student pays for any generative AI service.\r\n\r\nThe omitted categories are Natural Sciences for academic division, white students for race/ethnicity, and freshmen for cohort. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.7510.6760.6610.2710.479 R\r\n−\r\nsquared0.0870.1130.0760.0680.041 N (Students)599599599599599 \\justifyNotes: This table examines how student characteristics relate to self-reported likelihood of using generative AI under different policy scenarios. Each column presents results for different policy scenarios. In columns 1-4, the dependent variable equals one if the student reports being“neutral,” “likely” or “extremely likely” to use AI under the specified policy, and zero if they report being “unlikely,” or “extremely unlikely.” Column 5 represents the impact of moving from unrestricted use to complete prohibition.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.5760.6850.3760.4890.564 R\r\n−\r\nsquared0.1020.1190.0840.0370.087 N (Students)561565543560560 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. Each column presents results for beliefs about different academic outcomes: learning ability (e.g., ability to grasp concepts, retain information, or learn new skills) in column 1, understanding of course materials in column 2, course grades in column 3, ability to complete assignments on time in column 4, and time spent on academics in column 5. The dependent variable in each regression equals one if the student believes AI somewhat improves” or significantly improves” the outcome, and zero if they believe it has no effect, reduces, or significantly reduces the outcome. “Don’t know” responses are excluded.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\nAppendix BEmpirical Appendix\r\nB.1Field of Study Classifications\r\nThis appendix details the classification of majors into broad fields of study used in Figure 1:\r\n\r\n• Natural Sciences: Includes Biology, Biochemistry, Chemistry, Computer Science, Earth and Climate Sciences/Geology, Environmental Studies, Mathematics, Molecular Biology & Biochemistry, Neuroscience, Physics, and Statistics.\r\n• Social Sciences: Includes Anthropology, Economics, Education, Geography, International & Global Studies, International Politics & Economics, Political Science, Psychology, and Sociology.\r\n• Humanities: Includes American Studies, Black Studies, Classics, History, Architectural Studies, Art History & Museum Studies, History of Art & Architecture, Philosophy, Religion, and Classical Studies.\r\n• Literature: Includes Comparative Literature, English/English & American Literatures, and Literary Studies.\r\n• Languages: Includes Arabic, Chinese, French & Francophone Studies, German, Japanese Studies, Russian, and Spanish.\r\n• Arts: Includes Film & Media Culture, Music, Studio Art, and Theatre.\r\nB.2Task-Specific Use of Generative AI\r\nThe aggregate patterns documented in Section 4 may mask important heterogeneity across student groups. Different students may adopt AI tools at varying rates depending on their academic needs and field-specific norms. To investigate this heterogeneity systematically, Appendix Table LABEL:tab:ai_purpose_correlates presents regression estimates examining how student characteristics predict AI usage for each of our ten measured academic tasks.\r\n\r\nUsage patterns differ markedly by student characteristics. Male students consistently exhibit higher adoption rates across most applications, with particularly pronounced differences for finding information (16.4 pp; \r\np\r\n<\r\n0.01\r\n), summarizing texts (12.5 pp; \r\np\r\n<\r\n0.01\r\n), and creating images (16.9 pp; \r\np\r\n<\r\n0.01\r\n). Black students show substantially higher usage for information-gathering tasks, being 23.8 pp more likely to use AI for finding information compared to white students (\r\np\r\n<\r\n0.05\r\n). They also exhibit dramatically higher adoption of writing assistance tools, with 27.6 pp higher usage for editing text (\r\np\r\n<\r\n0.05\r\n) and 23.6 pp for writing emails (\r\np\r\n<\r\n0.10\r\n). Latino students demonstrate significantly higher usage for generating ideas (24.0 pp; \r\np\r\n<\r\n0.01\r\n) and writing emails (20.7 pp; \r\np\r\n<\r\n0.01\r\n), while Asian students show markedly higher adoption for writing emails (24.5 pp; \r\np\r\n<\r\n0.01\r\n) and explaining concepts (10.9 pp, \r\np\r\n<\r\n0.05\r\n). Students from public high schools report lower usage rates for several applications, particularly for concept explanation (7.3 pp lower; \r\np\r\n<\r\n0.10\r\n) and writing emails (7.4 pp lower; \r\np\r\n<\r\n0.10\r\n), but higher usage for proofreading (8.7 pp higher; \r\np\r\n<\r\n0.10\r\n).\r\n\r\nField of study emerges as a particularly strong predictor of usage patterns. Arts majors show significantly lower adoption across multiple tasks compared to Natural Science students, with gaps of 49.7 pp for finding information (\r\np\r\n<\r\n0.01\r\n), 44.0 pp for generating ideas (\r\np\r\n<\r\n0.05\r\n), and 62.1 pp for coding assistance (\r\np\r\n<\r\n0.01\r\n). Similarly, humanities majors are 36.8 pp less likely to use AI for generating ideas (\r\np\r\n<\r\n0.01\r\n) and 30.8 pp less likely for coding assistance (\r\np\r\n<\r\n0.01\r\n). Languages majors exhibit the most pronounced differences, being 52.4 pp less likely to use AI for summarizing texts (\r\np\r\n<\r\n0.01\r\n) and 46.9 pp less likely for generating ideas (\r\np\r\n<\r\n0.01\r\n). By contrast, social science students show higher usage for explaining concepts (8.0 pp; \r\np\r\n<\r\n0.10\r\n) and summarizing texts (14.7 pp; \r\np\r\n<\r\n0.01\r\n).\r\n\r\n0.8030.7400.6310.6190.5410.4730.3710.3440.2350.204 R\r\n−\r\nsquared0.0780.0870.0770.1160.0280.0540.0830.1350.0400.094 N (Students)515515515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and use of AI for specific academic tasks. Each column shows the result for a different academic task. The omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Regressions are weighted and use heteroskedasticity-robust standard errors clustered at the student level. ∗∗∗, ∗∗, and ∗ indicate significance at the 1%, 5%, and 10% levels, respectively.\r\n\r\nAppendix CQualitative Evidence on Student Perspectives on AI Use\r\nIn this section, we analyze student responses to an open-ended question about their use of generative AI. The survey question asked: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” This was an optional question that 48.3 percent of survey respondents answered. Appendix Figure C1 presents a word cloud of the most frequent words in student responses.\r\n\r\nC.1Validating the Open-Ended Response Measure\r\nWe begin by validating our open-ended response measure to show that it contains meaningful signal. To validate this measure, we employ sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner), a widely-used lexicon and rule-based sentiment analysis tool (Hutto and Gilbert, , 2014). We calculate sentiment scores for each response and examine whether these scores correlate with students’ actual AI adoption behaviors. Intuitively, students who express more positive sentiment toward AI in their open-ended responses should be more likely to have adopted AI tools in practice. We test this hypothesis by examining two measures of AI adoption: whether students have ever used generative AI and whether they currently use AI for academic purposes. Appendix Figure C2 presents binned scatterplots showing the relationship between AI adoption rates and standardized sentiment scores.\r\n\r\nSentiment towards generative AI strongly predicts AI adoption. For both outcomes, students with negative sentiment scores (below zero) show substantially lower adoption rates—around 70-85 percent for ever using AI and 70-75 percent for academic use. In contrast, students with positive sentiment scores display markedly higher adoption rates, reaching nearly 100 percent for general use and 95-100 percent for academic purposes among those with the most positive sentiment. The relationship is particularly pronounced for academic AI use (Panel B), where the coefficient of \r\nβ\r\n^\r\n=\r\n0.168\r\n is twice as large as for general adoption.17 These systematic patterns confirm that our open-ended responses capture meaningful variation in student attitudes that corresponds to real behavioral differences.\r\n\r\nC.2How Students use Generative AI\r\nTo analyze these responses systematically, we classified each response using keywords based on their content. For example, if a student mentioned using AI to save time, we tagged the response with the keyword “time-saver.” If a student expressed concerns about AI’s impact on learning, we tagged it with “negative learning.” Responses could receive multiple keywords if they discussed several themes. Appendix Figure C3 shows the frequency of keywords in our classification. The responses reveal how students use AI tools, what motivates this use, and what causes some to avoid or limit their use.\r\n\r\nThe most common use of AI is as an explanatory tool. Nearly 30 percent of responses mentioned using AI to understand course material. Students frequently ask AI to break down complex concepts from readings and lectures, particularly when they find the material difficult to understand. For example, one student reported: “I can ask AI to explain concepts to me that I have a hard time grasping. […] I can keep asking ’simplify’ or ’break down even more.”’ Students also use AI to summarize dense academic readings, which they argue helps them manage heavy reading loads.\r\n\r\nStudents employ AI throughout different stages of the writing process. Some use AI to generate initial drafts that serve as starting points. One student explained: “Helps me get started with a base for most of my essays. It feels easier to edit something already written and make it my own than to write from scratch.” Others use AI more narrowly for brainstorming when stuck on specific problems. As one student noted: “I use it if I am feeling stuck to push me to the right direction (whether a mathematical problem or an essay idea).” Many also report using AI as an editing tool to improve grammar, sentence structure, and overall writing flow. This is especially the case for non-native English speakers. As one student explained: “English is not my first language and it frustrates me sometimes that I cannot find the best way to phrase a certain idea and AI is a useful tool to have to find alternate expressions.”\r\n\r\nStudents frequently mentioned using AI for specific academic tasks. In courses that require coding, students often use AI for debugging code and understanding programming concepts. Students also employ AI for administrative tasks like formatting citations and drafting routine emails. Finally, many students use AI as an enhanced search engine. One student reported: “It has significantly reduced the time it takes to conduct research on new topics and ideas, and helps me by giving me a thorough selection of sources to use for projects of any kind.”\r\n\r\nC.3Why Students Adopt AI\r\nTime savings was the most commonly cited reason for using AI. Nearly 30 percent of responses mentioned using AI to complete work more efficiently. Students often viewed AI as a way to manage demanding course loads. Many students particularly embrace AI assistance for tasks viewed as mechanical or administrative. A student noted they use AI for “Writing emails quick and creating resume/ cover letter templates.” But AI assistance goes beyond grunt work. Some students use it to “spend less time doing assignments and homework.” This is particularly true if students don’t view the work as central to their academic experience. As one student explained: “when I come across work I deem as ineffective, I want to spend as little time as possible doing it.”\r\n\r\nHaving an on-demand tutor for academic support was another key motivation. One-quarter of responses described using AI as an “explainer” when other resources were unavailable or inconvenient. As one student noted, “I use it as a last resort (if there are no office hours, after looking up videos, etc.) if I need extra help. I’d like to think that the way I use it is similar to going to office hours or TA hours.”\r\n\r\nPeer influence also drove AI adoption. Some students reported feeling pressure to use AI to remain competitive with their classmates. One student explained: “I noticed others use it, are getting better grades than me, and they say they learn better with the help of AI, so I gave it a try.” Others worried about being at a competitive disadvantage: “Other people were using it and told me about it. I felt like I would be at a disadvantage if I wasn’t also using it.”\r\n\r\nC.4Concerns and Limitations\r\nStudents expressed several concerns about AI use in academic work. The most frequent worry was about negative impacts on learning. One student who initially used AI extensively reported: “In the past, I have simply plugged and chugged homework assignments into ChatGPT and submitted it. Those assignments feedback from teachers was positive and I was getting good grades, but I definitely felt that my own learning outcomes to be significantly worse.” Other students viewed AI use as fundamentally incompatible with their educational goals. As one humanities student explained: “my task is as a humanities student is to think, not calculate; why should I let AI do the thinking for me? It would defeat the purpose of pursuing my education.”\r\n\r\nMany students described ethical concerns about AI use. Responses suggested uncertainty about appropriate boundaries. One student noted: “I never use it to explicitly write something because that feels like overt cheating, but sometimes I hesitate when it completely solves Econ problems. I understand how it does it, and it helps me to learn, but it still sometimes feels a little morally gray.” Another expressed similar ambivalence: “I tend to only use it when [it] will save me time in a moral way.”\r\n\r\nStudents also emphasized the importance of maintaining ownership of their work. Many expressed pride in producing original work and hesitation about diluting that ownership through AI use. As one student explained: “I don’t have interest in using generative AI for my academic work because I want my work to reflect my own ideas.” Another noted: “It usually would not even occur to me to turn to AI to substitute writing because I want to take credit for my work, and using AI seems to diminish that.”\r\n\r\nTechnical limitations deterred some students from using AI tools. Students reported concerns about inaccurate outputs (“hallucinations”) and poor output quality, particularly for creative writing or complex analytical tasks. For example, one student noted that “In my poetry class we were instructed to use it to come up with poems and they were awful, so that kinda turned me away from using it to do my work for me.”\r\n\r\nC.5Discussion\r\nWe conclude with two overarching themes that emerged from the responses.\r\n\r\nFirst, students vary substantially in how they incorporate AI into their academic lives. Crucially, this heterogeneity largely depends on what students perceive as “appropriate” uses of AI. For activities that they perceive as core to their academic journey—like writing essays or solving problems—many students hesitate to use AI. A student articulated this clearly: “Most of my work is writing or reading. If I’m not doing the writing, what is the purpose of me taking the class?” Yet, students draw different boundaries between central tasks and grunt work. Some use AI extensively, viewing their role as akin to a manager that provides high-level direction while AI handles implementation. Others restrict AI use to specific tasks like brainstorming, editing, or drafting emails. Still others avoid AI entirely for academic work, often for ethical reasons. Even among AI users, adoption patterns reflect individual trade-offs between time savings, learning goals, and academic integrity concerns.\r\n\r\nSecond, there appears to be a fundamental tension between efficiency and learning. The time-saving benefits are easy to observe, quantifiable, and tangible. But these time savings are unlikely to be a free lunch. Some benefits may come at the cost of spending less time with material that requires deeper engagement to digest. As one student noted: “There may be a negative effect in that it eliminates much of the ’struggle’ in learning.” Yet, having an on-demand tutor that explains concepts in relatable ways can also improve learning. As one student explained: “It can explain concepts to me in a way that is tailored to my learning style.” This suggests that the impact of AI use on learning outcomes depends not on whether students use AI—almost all do, to some extent—but rather on how they employ these tools.\r\n\r\nFigure C1:Word Cloud of Student Motivations for Generative AI Use\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 147 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?’\r\n\r\nFigure C2:Relationship Between AI Sentiment and AI Adoption\r\nPanel A. Ever Used Generative AI\r\nRefer to caption\r\nPanel B. Uses AI for Academic Purposes\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents the relationship between AI sentiment and AI adoption. Panel A shows the proportion of respondents who have ever used generative AI, while Panel B shows the proportion who use AI for academic purposes. Each point represents the mean adoption rate for respondents within sentiment score bins of width 0.2. Sentiment scores are standardized compound scores computed using Hutto and Gilbert, (2014)’s VADER algorithm applied to responses to an open-ended question about generative AI. Positive values indicate positive sentiment and negative values indicate negative sentiment. The dashed lines show OLS best-fit lines estimated on the microdata, with coefficients and standard errors (in parentheses) displayed in the top-left corner of each panel.\r\n\r\nFigure C3:Frequency of Keywords in Student Motivations for AI Use\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to AI use. The responses come from the question “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” Color coding indicates the category of each theme. Usage type refers to how students use AI tools. Motivation captures what drove students to try AI. Concerns include mentions of course policies and academic integrity, individual reservations about AI use, worries about AI’s impact on education, and AI’s technical limitations.\r\n\r\nAppendix DQualitative Evidence on Student Views of AI Policies\r\nIn this section, we analyze student responses to an open-ended question about Middlebury’s AI policies. The survey asked: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Appendix Figure D1 presents a word cloud of the most frequent words in student responses. To analyze these responses systematically, we classified each response using keywords based on their content. Appendix Figure D2 shows the frequency of keywords in our classification.\r\n\r\nD.1Polarized Views on Generative AI Policy Approaches\r\nStudents expressed markedly different views about appropriate AI policies, revealing fundamental disagreement about the path forward. Some strongly advocated for embracing AI technology. As one student argued, “The tool is there, there is supply and there is demand. Don’t fight another war on drugs. Don’t live in a fake reality”. Others called for significant restrictions, arguing that “the use of generative AI is dishonest and corrosive” and that it “prohibits these organic processes and divorces students from true learning”.\r\n\r\nHowever, the most common position advocated for a balanced approach that would allow beneficial uses while restricting harmful ones. Students distinguished between uses that enhance learning (like concept explanation) and those that substitute for learning (like generating entire essays). One student articulated this nuanced view particularly well: “AI also can really be helpful at explaining a textbook problem that doesn’t make sense, or guiding slightly with homework, or creating study materials, or editing/tightening up your prose. All of those things are good, and universities should figure out how to maximize AI use for those reasons and to minimize students just feeding their problem sets into ChatGPT”\r\n\r\nA recurring theme was the futility of blanket bans. Many students emphasized that prohibition would be ineffective, with one noting “I don’t think anyone really cares what the policy of any given class is. If professors want people to not use it, they need to structure assessments in a way that will discourage use.” Another compared AI bans to restricting internet use, arguing “AI policies seem to be totally irrelevant. It’s like telling people they can’t use the internet as a resource for the class.” This ineffectiveness of bans creates fairness concerns. As one student explained: “I think if it is banned in a class, that should be enforced (and right now it absolutely is not)… As with any form of cheating, those who don’t cheat are put at a disadvantage.” Another student expressed similar frustration: “I find it discouraging when I hear classmates saying they use AI for things such as essays when they use it in dishonest ways.”\r\n\r\nD.2Need for Clear Guidelines\r\nThe most frequently expressed concern was the need for clear guidelines about generative AI use. Several students reported confusion about what constitutes acceptable use, with one noting “I think it should be more clear whether we can use it and how and how to cite it since most professors rarely mention it at all”. Other students emphasized the importance of professors explaining their policies upfront and their rationale, with one stating “I think that Professor’s should be very specific about what is allowed and their reasoning behind their policy”.\r\n\r\nMany students advocated for standardization across classes, observing that “Sometimes its confusing when one class allows it and another doesn’t and the other encourages it and so on so if there was a school wide or department wide policy that could help”. Yet there is also disagreement regarding standardization. Some students preferred leaving the decision to individual professors, arguing that “GenAI is more effective in some classes/majors than others. Making sure professors understand how students use GenAI and how useful GenAI is in their class (given the course structure, nature of assignments/material, etc.) is very important for the class policy.”\r\n\r\nD.3Training and Support Services\r\nStudents strongly emphasized the need for training in appropriate generative AI use. Many suggested that the college should provide guidance on using AI tools effectively while maintaining academic integrity. One student proposed “a workshop that teaches you to effectively use GenAI without violating the honor code”. Students also expressed interest in learning how to leverage AI to enhance their learning experience rather than circumvent it. As one student explained: “I think it could be useful to develop some sort of training. How do we use AI in a way that actually benefits our learning? I tried out some things on my own but I feel that I need more guidance.”\r\n\r\nThis desire for training was often linked to workplace preparedness. Students recognized that AI proficiency would be valuable in their careers, with one noting “As the world uses more and more AI, I think it is an important tool that students should know how to leverage”. Another emphasized: “The moment us students leave campus, we will be using it in the professional world, and when used in combination with one’s own skills, it is merely a tool to maximize efficiency”.\r\n\r\nD.4Discussion\r\nThe responses reveal several key tensions in AI policy implementation. First, students express a desire to use AI in ways that enhance rather than substitute for learning, yet they recognize that blanket bans are ineffective and worry about being disadvantaged if they follow restrictions while others do not.\r\n\r\nSecond, while students desire clear guidelines, they also want flexibility to accommodate legitimate uses that vary by discipline and assignment type. Different courses and majors may find different AI uses appropriate based on their learning objectives and assessment types.\r\n\r\nThird, there is tension between faculty autonomy in setting course policies and students’ desire for consistent institutional standards. While some support letting professors determine appropriate AI use for their specific courses, others argue that varying policies across classes create confusion and enforcement challenges.\r\n\r\nFinally, students report significant variation in faculty attitudes toward AI. Some students perceived faculty fear or misunderstanding of AI tools, noting that categorical bans often reflect a lack of understanding about AI’s capabilities and limitations rather than pedagogical considerations.\r\n\r\nFigure D1:Word Cloud of Student Feedback on Generative AI Policies\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 133 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?”\r\n\r\nFigure D2:Frequency of Keywords in Student Feedback on AI Policies\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to Middlebury’s AI policies. The responses come from the question “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Color coding indicates the category of each theme. Policy Approach captures suggestions about how AI should be regulated at the college. Adoption Views reflect positions on whether and how AI should be integrated into academic work. Training Needs indicates requests for guidance and support. Other Concerns include issues of workplace relevance and fairness.\r\n\r\nSee pages 1 of results/ai-survey-midd.pdf See pages 2- of results//ai-survey-midd.pdf\n",
    "totalFiles": 2
  },
  {
    "id": "extraction_1754349767051_0szgqurh9",
    "timestamp": "2025-08-04T23:22:47.051Z",
    "files": [
      {
        "name": "story.txt",
        "size": 730,
        "type": "text/plain",
        "extractedText": "I remember the moment that sparked this project. It was during a walk across Middlebury’s campus late one fall afternoon—just after office hours. A student had come in, clearly bright but struggling, and told me they’d been using ChatGPT late at night to help with assignments because they were too anxious to speak up in class. That confession stuck with me.\r\n\r\nLater, over coffee with Germán, we started wondering—how many students were turning to generative AI as a quiet lifeline? Not to cheat, but to learn? That simple question became the seed for this entire study. What we found wasn’t just statistics. It was a story of adaptation, of students reshaping how they learn, and of institutions racing to catch up.\"",
        "extractionStatus": "success"
      },
      {
        "name": "Generative AI in Higher Education E.txt",
        "size": 117470,
        "type": "text/plain",
        "extractedText": "Generative AI in Higher Education: Evidence from an Elite College†\r\nZara Contractor\r\nGermán Reyes\r\nAbstract\r\nGenerative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT’s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI’s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI’s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\r\n\r\n1Introduction\r\nThe rapid advancement of generative artificial intelligence (AI) is transforming higher education at an unprecedented pace. The launch of ChatGPT and similar tools has introduced technologies capable of performing tasks central to academic assessment and learning—writing essays, solving complex problems, and explaining intricate concepts—instantly and at near-zero marginal cost. Yet systematic evidence on the nature and implications of this new technology remains scarce. How widespread is generative AI adoption among students, and what factors drive it? Do students primarily use AI to augment their learning or to automate coursework, potentially harming human capital development? Could disparities in access to premium AI resources amplify existing educational inequalities?\r\n\r\nThese questions are at the heart of ongoing debates about the role of AI in education, which have relied heavily on anecdotal evidence and speculation rather than rigorous evidence. As a result, universities have implemented policies that vary dramatically, from outright bans to permissive adoption, often without clear evidence on their effectiveness or unintended consequences (Nolan, , 2023; Xiao et al., , 2023; McDonald et al., , 2025).\r\n\r\nThis paper addresses this gap by systematically examining generative AI adoption at Middlebury College, a highly selective liberal arts college in Vermont. With approximately 2,800 undergraduate students, Middlebury offers 49 majors across the arts, humanities, languages, social sciences, and natural sciences. Over the past decade, Middlebury has consistently ranked among the top 10 liberal arts colleges in U.S. News & World Report, with an average ranking of 8.5 between 2015 and 2025. The college’s selectivity is comparable to that of many elite research universities—Middlebury’s acceptance rate of approximately 10 percent is similar to that of Boston University (11 percent), Georgetown University (13 percent), and the University of Virginia (16 percent).\r\n\r\nOur analysis draws on survey data collected from the student population between December 2024 and February 2025. The survey collected detailed information about AI usage, including the frequency and purpose of use, perceived impacts, and responses to institutional policies. To minimize selection bias, we framed the survey broadly as examining technology use and provided incentives for participation, achieving a 22.9 percent response rate with 634 responses representing a broad cross-section of the student population.\r\n\r\nWe document five main findings. First, generative AI is approaching near-universal adoption at an unprecedented speed. Over 80 percent of students use AI for academic purposes, up from less than 10 percent before Spring 2023. This represents one of the fastest technology adoption episodes ever documented, dramatically exceeding the 40 percent adoption rate among U.S. workers (Bick et al., , 2025) and the 23 percent among all U.S. adults (McClain, , 2024). These levels are consistent with international findings of 50–70 percent adoption in university contexts (Nam, , 2023; Stöhr et al., , 2024; Ravšelj et al., , 2025).\r\n\r\nSecond, AI adoption is markedly unequal across academic disciplines and demographic groups. Field of study is the strongest predictor of adoption, likely reflecting how well AI capabilities align with the academic tasks required across fields. Adoption ranges from 91.1 percent in Natural Sciences majors (including mathematics and computer science) to significantly lower levels in Literature (48.6 percent) and Languages (57.4 percent). Additionally, demographic disparities exist. For example, males adopt AI at higher rates than females (88.7 versus 78.4 percent), a pattern that is consistent with documented AI gender gaps in other contexts (Otis et al., , 2024). Most notably, lower-achieving students have a higher adoption rate than their higher-achieving peers (87.1 versus 80.3 percent). As a result, AI could serve as an equalizing force if it enhances student learning—helping struggling students catch up to their peers—but could also widen achievement gaps if AI undermines skill development.\r\n\r\nThird, generative AI transforms the students’ learning production function by both augmenting student capabilities and automating academic tasks. We classify academic tasks as augmentation when AI enhances human capabilities while maintaining student engagement (e.g., explaining concepts, proofreading) versus automation when AI directly produces outputs with minimal cognitive involvement (e.g., writing essays, creating images), and average usage rates across tasks within each category. We find that 61.2 percent of AI users employ these tool for augmentation purposes, while 41.9 percent use it for automation. Qualitative evidence reinforces these patterns: students describe AI as an “on-demand tutor” for augmentation purposes, particularly valuable when traditional resources like office hours are unavailable, while automation use centers on time savings during periods of overwhelming workload. These self-reported patterns align closely with actual usage data from Claude conversation logs (Handa et al., 2025a,).\r\n\r\nFourth, institutional policies can significantly influence AI adoption, though their effectiveness can be undermined by informational frictions. We find that explicit prohibitions dramatically reduce self-reported intended use 39 percentage points (pp). Importantly, the effects of this policy vary substantially across student groups, with females showing larger reductions in usage under prohibition than males, suggesting that one-size-fits-all policies may inadvertently create disparate impacts across groups. Moreover, we also document substantial informational gaps about institutional AI policies. Many students (19.2 percent) do not understand AI policy rules, few know about institution-provided premium AI resources (10.1 percent) or proper AI citation practices (32.6 percent)—a skill that is necessary for academic integrity.\r\n\r\nFifth, students believe that AI has a positive learning impact and these beliefs strongly predict their own AI usage. Most students believe that AI improves their understanding of course materials (70.2 percent) and learning ability (60.1 percent), though fewer believe it improves grades (41.1 percent). Perhaps unsurprisingly, students who believe that AI improves their academic outcomes are significantly more likely to use these tools. For example, a ten-percentage-point increase in the belief that AI improves learning corresponds to a 4.9 percentage-point increase in adoption, highlighting how perceptions can affect technology diffusion in educational settings.\r\n\r\nTaken together, our findings reveal that generative AI has already reshaped students’ college experience. The substantial information gaps that we document—regarding both permitted uses and available resources—suggest straightforward opportunities for policy intervention. Yet the observed heterogeneity across disciplines and student groups indicates that uniform policies risk unintended consequences: blanket prohibitions may disproportionately disadvantage students who benefit most from AI augmentation, while unrestricted use may encourage automation practices detrimental to skill development. Thus, effective policy requires distinguishing clearly between AI uses that enhance learning and those that undermine it.\r\n\r\nWhile our findings are specific to the population we study—students at an elite liberal arts college—this setting offers valuable into AI’s broader societal impact. The rapid uptake of generative AI in higher education may reflect a generalizable principle regarding the diffusion of generative AI: adoption may be fastest in contexts where AI consolidates fragmented tools into a unified platform. Students have long had access to various tools for the academic tasks for which they now use generative AI.1 What distinguishes AI is its ability to provide all these services through a single, instantly accessible interface at virtually zero marginal cost. This consolidation likely drives students’ perception of enhanced learning efficiency and contributes to the fast adoption. This principle may extend well beyond education: industries with fragmented, specialized tools could experience similar trajectories of rapid AI adoption, even if AI itself does not introduce fundamentally new capabilities.\r\n\r\nOur findings contribute to a rapidly growing literature examining the adoption and impacts of generative AI. Recent work has focused on AI’s effects on worker productivity (Noy and Zhang, , 2023; Dell’Acqua et al., , 2023; Peng et al., , 2023; Cui et al., , 2024; Brynjolfsson et al., , 2025) and its potential to transform occupations (Felten et al., , 2021, 2023; Eloundou et al., , 2024). While several papers document AI adoption in workplace settings (Bick et al., , 2025; Hartley et al., , 2025; Humlum and Vestergaard, , 2025) and at the firm level (McElheran et al., , 2023; Bonney et al., , 2024; Kharazian, , 2025), we examine adoption in higher education—a critical setting where future high-skilled workers develop human capital.\r\n\r\nWe add to an emerging literature examining generative AI in education. Some studies focus on the learning impacts of generative AI, finding mixed impacts depending on the setting (e.g., Lehmann et al., , 2024; Bastani et al., , 2025). A complementary strand of the literature documents adoption patterns across diverse educational contexts, including universities in Australia (Kelly et al., , 2023), Ghana (Bonsu and Baffour-Koduah, , 2023), Norway (Carvajal et al., , 2024), Sweden (Stöhr et al., , 2024), and multi-country studies (Ravšelj et al., , 2025). To the best of our knowledge, this paper provides the first systematic evidence on generative AI adoption in U.S. higher education. Importantly, we focus specifically on how students use AI for academic purposes—unlike studies that measure more general usage including personal lives (e.g., Ravšelj et al., , 2025)—and go beyond documenting adoption rates to examine how students integrate AI into their academic workflows, distinguishing between augmentation and automation, the role of student beliefs about AI’s educational impacts, and how institutional policies shape usage patterns.\r\n\r\nOur findings also contribute to the literature on technology diffusion. Unlike the S-shaped adoption curves documented for most technologies (Griliches, , 1957; Rogers, , 1962), we observe extremely rapid adoption of generative AI, with 80 percent uptake within two years of ChatGPT’s release. This pattern contrasts sharply with the adoption trajectories of historical General Purpose Technologies (GPTs) like electricity, which required decades to achieve widespread use (David, , 1990; Bresnahan and Trajtenberg, , 1995). The rapid diffusion we document aligns with evidence that newer technologies diffuse faster than older ones (Comin and Hobijn, 2010a,; Comin and Hobijn, 2010b,). Two factors likely explain AI’s exceptional adoption speed compared to other GPTs. First, unlike electricity or steam power, generative AI requires minimal physical infrastructure—students access it through existing devices—and is available at no cost to consumers, eliminating the financial barriers that typically slow technology adoption. Second, as noted above, AI consolidates multiple specialized tools into a single platform, making its benefits immediately apparent without requiring specialized training or organizational restructuring, unlike previous GPTs (Brynjolfsson and Hitt, , 2000).\r\n\r\n2Data: Novel Student Survey\r\n2.1Recruitment and Structure\r\nWe conducted the survey from December 2024 to February 2025. All Middlebury College students were contacted via email and received a reminder a few weeks and two months after the initial invitation. To minimize selection bias, the recruitment materials described the survey broadly as a study on students’ use of technology in their academic and personal lives. To incentivize participation, students who completed the survey qualified for entry into a lottery for Amazon gift cards ranging in value from $50 to $500. The full survey instrument is provided in Appendix LABEL:app:survey.\r\n\r\nThe survey contains three main sections (see Appendix Figure A1 for the survey flow). First, we gather demographic and academic information, including gender, race/ethnicity, type of high school attended (private or public), current academic year, and declared or intended major.2 We also collect data on academic inputs and performance through students’ self-reported typical weekly hours spent studying and their first-year GPA.3\r\n\r\nSecond, we measure students’ experience with generative AI tools. We begin by asking whether students have ever used generative AI tools like ChatGPT or Claude. For those who have, we collect information about their usage patterns, including frequency of use during the academic semester, which specific AI models they use, and whether they pay for AI tools. We also gather data on how students use AI for different academic tasks, including writing assistance, learning support, and coding.\r\n\r\nThe final part of the survey elicits students’ beliefs about generative AI’s adoption and impacts. We ask students about their perceptions of AI’s effects on their academic experience across multiple dimensions, including learning, grades, time management, and understanding of course material. We then collect information on the role of institutional policies, asking students how different policy environments influence their likelihood of using AI. We also elicit students’ beliefs about AI usage among their peers, including their estimates of the fraction of Middlebury students who use AI for schoolwork and leisure. The survey concludes with two open-ended questions that allow students to share their experiences with AI in academic settings and provide feedback on the college’s AI policies and support services.\r\n\r\n2.2Sample and Summary Statistics\r\nOut of Middlebury’s 2,760 enrolled students, 739 began the survey. We exclude 105 respondents who left the survey before reaching the generative AI usage module, leaving us with an analysis sample of 634 students (22.9 percent of the student body). This response rate is comparable to that of similar surveys (Wu et al., , 2022). To make our sample more representative of Middlebury’s student body, we construct poststratification weights based on the distribution of declared majors from administrative records.4 Specifically, we weight observations by the ratio of each major’s share in the student population to its share in our survey responses.5\r\n\r\nTable Figures and Tables presents summary statistics for our sample. Column 1 reports unweighted survey averages, column 2 shows averages after applying poststratification weights, and column 3 provides population benchmarks from administrative records where available. Our unweighted sample consists of 44.6 percent male and 50.8 percent females. The racial/ethnic composition includes 61.8 percent white, 15.5 percent Asian, 9.9 percent Hispanic, and 3.6 percent Black students. The majority of students (54.3 percent) attended a public school, while 42.0 percent attended a private high school. Our sample represents 43 different majors across seven fields of study, with 31.1 percent of respondents not yet having declared a major. In our analysis, we group these undeclared students by their intended field of study as reported in the survey.\r\n\r\nComparing our unweighted sample to administrative records reveals notable differences. Our sample overrepresents white students (61.8 versus 53.8 percent) and Asian students (15.5 versus 7.3 percent), while underrepresenting Black students (3.6 versus 5.2 percent) and Hispanic students (9.9 versus 12.4 percent). First-year students are overrepresented (31.1 versus 25.5 percent), while senior students are underrepresented (21.3 versus 28.7 percent). Our weighting procedure partially addresses these discrepancies—particularly in the distribution of academic fields, where the weighted figures closely approximate administrative records—but some demographic differences persist. Despite these differences, our weighted sample provides reasonably close approximations to the college population on most dimensions, particularly for academic characteristics and field distributions.\r\n\r\n3Generative AI Usage Patterns Among Students\r\n3.1Adoption of Generative AI\r\nGenerative AI has achieved a remarkably high adoption rate among Middlebury College students. Figure 1 presents the distribution of AI usage frequency during the academic semester, categorized into four levels: “Rarely” (a few times per semester), “Occasionally” (a few times per month), “Frequently” (a few times per week), and “Very Frequently” (daily or almost daily). Overall, 82.5 percent of students report using generative AI for academic purposes, with substantial variation in usage intensity: 23.5 percent use it rarely, 22.2 percent occasionally, 26.2 percent frequently, and 10.6 percent very frequently.\r\n\r\nThe adoption rate at Middlebury aligns closely with patterns observed across other higher education institutions. A BestColleges survey of U.S. undergraduate and graduate students in fall 2023 found that 56 percent had used AI on assignments or exams (Nam, , 2023). Ravšelj et al., (2025) surveyed higher education students globally between late 2023 and early 2024, finding that 71.4 percent had ever used ChatGPT—though this figure encompasses all usage rather than academic use specifically. Similarly, Stöhr et al., (2024) document that 63 percent of Swedish university students had used ChatGPT by spring 2023, with 35.4 percent reporting regular use and 27.6 percent rare use. Among Norwegian university students, Carvajal et al., (2024) find that 68.9 percent use AI tools occasionally or more frequently.\r\n\r\nThese adoption rates in higher education far exceed those in the general population and workforce. Pew Research finds that only 23 percent of U.S. adults have ever used ChatGPT (McClain, , 2024), while Gallup reports that just one-third of U.S. workers have used AI for work (Gallup, , 2024). Bick et al., (2025) and Hartley et al., (2025) estimate that about 40 percent of the U.S. working-age population used generative AI for work as of late 2024 and early 2025. Both studies document substantial heterogeneity across industries, with information services showing the highest adoption rates at 56–62 percent—still considerably below the 80 percent adoption rate at Middlebury. Even among workers in AI-exposed occupations, Humlum and Vestergaard, (2025) find adoption rates of only 41 percent.6\r\n\r\n3.2Adoption by Student Characteristics and Field of Study\r\nAI adoption varies considerably across demographic groups and academic disciplines (Figure 1 and Appendix Table LABEL:tab:ai_freq_het). Males report higher usage rates than females (88.7 versus 78.4 percent). Usage patterns differ markedly by race/ethnicity: Black students (92.3 percent) and Asian students (91.3 percent) exhibit the highest adoption rates, while white students (80.2 percent) and Hispanic students (77.9 percent) report lower usage. Students from private high schools use AI more frequently than those from public schools (84.1 versus 80.4 percent). Notably, students with below-median GPAs report higher usage rates than their higher-achieving peers (87.1 versus 80.3 percent). Adoption varies widely by field of study: Natural Sciences leads with 91.1 percent usage, followed by Social Sciences at 84.6 percent, while Languages (57.4 percent) and Literature (48.6 percent) show substantially lower adoption rates.\r\n\r\nTo examine how student characteristics jointly relate to AI adoption, we estimate multivariate OLS regressions that include all observed characteristics simultaneously.7 Table LABEL:tab:ai_usage_correlates presents regression estimates using four usage thresholds as outcomes. Each column represents the probability of meeting progressively higher frequency thresholds: any AI use (column 1), at least monthly use (column 2), at least weekly use (column 3), and daily use (column 4).\r\n\r\nThe regression results confirm the patterns observed in the descriptive statistics. Holding other characteristics constant, males are 10.3 pp more likely than females to use AI (column 1, \r\np\r\n<\r\n0.05\r\n), with this gender gap typically widening at higher usage frequencies (columns 2–4, all \r\np\r\n<\r\n0.01\r\n). Black and Asian students show substantially higher adoption rates than white students, at 11.8 and 10.7 pp respectively (column 1, both \r\np\r\n<\r\n0.01\r\n). Students from public high schools are 3.0 pp less likely to use AI than those from private schools, but this difference is not statistically significant. Field of study emerges as the strongest predictor of adoption. Compared to Natural Sciences majors, students in Literature, Languages, Arts, and Humanities all show lower usage rates, with the differences being statistically significant at high usage frequencies (columns 3 and 4). Social Sciences majors exhibit adoption rates similar in magnitude to Natural Sciences majors across all usage frequency thresholds.\r\n\r\nOur findings on heterogeneity in AI adoption align with patterns documented in other settings. The gender gap in AI adoption at Middlebury—10.3 pp higher for males—is consistent with evidence across multiple studies. A meta-analysis of 18 studies by Otis et al., (2024) finds that males are 10–20 percentage points more likely to use generative AI than females. This gender gap in AI adoption appears in all educational studies (Nam, , 2023; Carvajal et al., , 2024; Stöhr et al., , 2024; Ravšelj et al., , 2025). Our finding that students with below-median GPAs are more likely to use AI aligns with Carvajal et al., (2024), who document higher adoption rates among students with lower admission grades. This pattern of greater adoption among lower-achieving students suggests that AI could narrow achievement gaps if it enhances learning and skill development, but could widen these gaps if it undermines the acquisition of fundamental skills.\r\n\r\nThe differences in adoption rates across academic fields at Middlebury mirror patterns documented in other educational settings. Stöhr et al., (2024) find that technology and engineering students exhibit significantly higher ChatGPT usage compared to students in humanities. Similarly, Nam, (2023) report that 62 percent of business majors and 59 percent of STEM majors have used AI tools for coursework, compared to 52 percent of humanities majors. Ravšelj et al., (2025) document comparable disciplinary differences, with applied sciences students showing substantially higher usage rates than arts and humanities students.\r\n\r\nThese academic differences persist into the workforce. Bick et al., (2025) find stark variation by college major: STEM graduates have the highest AI adoption rates (46.0 percent), followed by Business/Economics graduates (40.0 percent), while Liberal Arts graduates show substantially lower rates (22.4 percent). Humlum and Vestergaard, (2025) document similar patterns by occupation, finding that roles requiring strong writing and technical skills—such as marketing specialists and journalists—have the highest adoption rates. These consistent patterns across educational and professional contexts suggest that field-specific factors, particularly the applicability of AI tools to different types of tasks, shape adoption in systematic ways.\r\n\r\n3.3Timing of Generative AI Adoption\r\nThe speed of technology diffusion is a critical determinant of its economic and social impact (David, , 1990; Hall and Khan, , 2003; Stokey, , 2021). In educational contexts, rapid technology adoption can create or exacerbate inequalities between early and late adopters, particularly if the technology confers significant learning advantages (World Bank, , 2016). To track the timing of AI adoption among Middlebury students, we asked them when they first began using generative AI for academic purposes, with options ranging from “This semester (Fall 2024)” to “Before Spring 2023” (as a reference, ChatGPT’s public launch was in November 2022).\r\n\r\nStudents adopted generative AI at an extraordinary pace. Figure 2 shows that the cumulative adoption rate grew dramatically from less than 10 percent before Spring 2023 to slightly above 80 percent by Fall 2024. The pace of adoption has accelerated over time, likely reflecting improvements in AI capabilities. Among current users, 25.7 percent adopted AI in Fall 2024 alone, compared to 19.9 percent in Spring 2024 and 16.3 percent in Fall 2023.\r\n\r\nThe adoption rate among Middlebury students far exceeds that observed in other populations and represents one of the fastest technology adoption episodes ever documented. For comparison, Bick et al., (2025) show that it took over 20 years for computers to reach an 80 percent adoption rate in the U.S. working-age population, and about 15 years for internet adoption to reach similar levels. Even generative AI adoption in the broader population has been markedly slower: Pew Research found that just 23 percent of U.S. adults had ever used ChatGPT as of February 2024, up from 18 percent in July 2023 (McClain, , 2024). The dramatically faster adoption rate we document—reaching over 80 percent in less than two years—suggests academic settings may uniquely accelerate AI diffusion.\r\n\r\nTo identify early versus late adopters of generative AI, in Table LABEL:tab:ai_adopt_correlates and Appendix Figure A2, we analyze how adoption timing varies across student characteristics. The rate of adoption varies substantially across student characteristics. Male students led adoption, with a 8.9 pp higher probability of using AI before Spring 2023 compared to females (column 1, \r\np\r\n<\r\n0.01\r\n)—a gender gap that persists across all periods (columns 2–5). Black and Asian students also adopted earlier than white students, though these differences reach statistical significance only in later periods (columns 4–5). Field of study is a strong predictor of adoption timing. For example, students in Languages consistently lagged behind Natural and Social Sciences majors, with significantly lower adoption rates across nearly all time periods.\r\n\r\n3.4Choice of Generative AI Models\r\nMajor AI companies operate on a freemium model, where free versions coexist with premium subscriptions that offer higher usage limits and access to more advanced models. This tiered structure creates potential for a new form of educational disparities: if paid versions confer substantial academic advantages, students who cannot afford subscriptions may be systematically disadvantaged. To investigate these potential disparities, we presented respondents with a list of options including both free and paid versions of popular models. We also collected information on monthly subscription expenditures, with response options ranging from no active subscription to spending more than $40 monthly (standard subscriptions to major models like ChatGPT cost $20 per month at the time).\r\n\r\nOpenAI’s ChatGPT dominates AI usage among Middlebury AI users, with the free version capturing the largest market share. Figure 3 shows that 89.3 percent of AI users rely on the free version of ChatGPT, making it dramatically more popular than any alternative. Google Gemini (13.5 percent) and Microsoft Copilot (7.7 percent) are distant competitors, while other platforms each capture less than 5 percent of users. This dominance mirrors patterns in other academic settings, where Stöhr et al., (2024) find substantially higher familiarity with ChatGPT compared to alternative platforms, and aligns with broader workforce trends documented by Bick et al., (2025).8\r\n\r\nDespite near-universal AI adoption, only 11.3 percent of AI users pay for any AI service.9 This figure is remarkably similar to the 8 percent found in Ravšelj et al., (2025)’s multi-country survey. This low payment rate suggests that for most students, the premium features of paid versions—primarily higher usage limits and access to more advanced models—do not justify the subscription cost. However, payment patterns reveal significant disparities: males and Asian students are substantially more likely to purchase AI subscriptions (Appendix Table LABEL:tab:ai_model_correlates and Appendix Figure A3), potentially reflecting differences in usage intensity across demographic groups.\r\n\r\n4Generative AI and the Production of Learning\r\n4.1The Use of Generative AI across Academic Tasks\r\nHow is generative AI transforming the traditional inputs to student learning? The educational production function includes inputs like time spent studying (e.g., Stinebrickner and Stinebrickner, , 2008), faculty instruction (e.g., Fairlie et al., , 2014), peer interactions (e.g., Sacerdote, , 2001), and academic support services (e.g., Angrist et al., , 2009). AI tools have the potential to complement or substitute for these traditional inputs. For example, using AI to explain concepts might substitute for faculty office hours, while using it for proofreading might reduce time needed for academic support services.\r\n\r\nTo understand the role of AI in students’ learning production function, we collected information about students’ AI usage across ten common academic tasks: proofreading, generating ideas, writing essays, editing essay drafts, coding assistance, creating images, explaining concepts, composing emails, summarizing materials, and finding information. For each task, students indicated their frequency of AI use on a five-point scale ranging from never to daily. We supplemented this quantitative data with open-ended responses about how AI influences their academic work process.\r\n\r\nStudents use generative AI for a wide range of academic tasks, with the highest adoption rates for learning support and text-processing activities. Figure 4, Panel A shows that explaining concepts is the most common use case, with 80.3 percent of AI users using it for this task. Summarizing texts follows as the second most common task (74.0 percent), followed by finding information and generating ideas (63.1 and 61.9 percent). Writing assistance tasks like proofreading and editing essays are also common, used by 54.1 and 47.3 percent of AI users, respectively. Technical applications like coding help are significant (34.4 percent), considering that many academic degrees involve no programming. Notably, while 23.5 percent of AI users report using it for writing essays, this represents a relatively low adoption rate compared to other academic uses, suggesting students may be more hesitant to use AI for primary content creation. The lowest adoption rate is for creating images at 20.4 percent, likely reflecting fewer academic use cases for this capability.\r\n\r\nAn important limitation of our survey is that it relies on self-reported usage, which may introduce measurement error. For example, students might underreport uses they perceive as academically inappropriate—such as essay writing—while overreporting those viewed as legitimate learning tools (Ling and Imas, , 2025). To assess the validity of our self-reported measures, we compare our findings with Handa et al., 2025a, who analyze actual Claude usage patterns among users with university email addresses. Several caveats apply: their data captures conversation-level interactions rather than student-level usage, and most students in our sample use ChatGPT rather than Claude (Section 3.4). Despite these limitations, the comparison provides a useful benchmark for evaluating our survey responses.\r\n\r\nReassuringly, our results are consistent with Anthropic’s data. Both studies identify explaining concepts and technical problem-solving as primary use cases. Handa et al., 2025a report that the second largest use case (33.5 percent of conversations) involves “technical explanations or solutions for academic assignments,” while we find that 80.3 percent of AI users use it for explaining concepts—a difference likely attributable to our student-level versus their conversation-level measurement. Similarly, the most common usage category in Claude involves “designing practice questions, editing essays, or summarizing academic material” (39.3 percent of conversations), aligning with our findings that 74.0 percent of AI users use it for summarizing texts and 47.3 percent for editing essays. The disciplinary patterns also converge: Handa et al., 2025a finds that computer science, natural sciences, and mathematics conversations are overrepresented, which mirrors our finding that Natural Science majors show significantly higher AI adoption rates (Figure 1). Overall, these convergent findings from self-reported survey data and actual usage logs suggest that our results capture genuine patterns of student AI engagement rather than merely reflecting social desirability in responses.\r\n\r\n4.2Automation versus Augmentation\r\nAre students using generative AI primarily to augment their learning or to automate their coursework? This distinction is crucial for understanding AI’s impacts (Autor and Thompson, , 2025)—augmentation may enhance students’ learning processes while maintaining their active engagement and critical thinking, whereas automation produces fully-formed outputs that could be submitted with minimal student input, potentially harming learning.\r\n\r\nTo examine this empirically, Figure 4, Panel B categorizes the ten measured tasks based on whether they augment or automate academic tasks. We classify tasks as augmentation when they enhance human capabilities (explaining concepts, finding information, proofreading, and editing drafts) and as automation when they directly produce outputs (writing essays, creating images, composing emails, summarizing texts, generating ideas, and coding assistance). We then calculate the percentage of AI users who employ each category at various frequencies.10\r\n\r\nStudents use generative AI for both augmentation and automation, but with markedly different frequency. While 61.2 percent of AI users report using AI for augmentation tasks, 41.9 percent report using it for automation—thus, there is a substantial 19.3 pp difference. This gap is mainly driven by occasional use (19.7 percent for augmentation versus 12.7 percent for automation) and frequent use (16.3 versus 9.0 percent). Most strikingly, students are more than twice as likely to use AI daily for augmentation (5.0 percent) compared to automation (2.5 percent). The higher frequency of augmentation use also suggests that students find augmentation uses more valuable for their day-to-day academic activities. Our findings align closely with patterns observed in actual AI usage data: Handa et al., 2025b analyze real conversation logs from Claude and find that 57 percent of workplace AI interactions involve augmentation while 43 percent involve automation.\r\n\r\nQualitative evidence from open-ended responses provides additional insights into students’ motivations for using AI (see Appendix C for additional results and validation of the open-ended measure). Students’ descriptions align closely with the augmentation-automation framework. For augmentation, many characterize AI as an “on-demand tutor,” particularly valuable when traditional resources like office hours are unavailable. Non-native English speakers frequently mention using AI for proofreading to overcome language barriers, while students from technical majors describe using it to debug code and understand error messages. For automation, time savings emerged as the dominant motivation, with 21.7 percent of open-ended responses explicitly mentioning efficiency benefits (Appendix Figure C3). Students describe turning to AI during periods of overwhelming workload or looming deadlines, using it to generate initial drafts or complete routine assignments. That students automate tasks mostly when time-pressed helps explain why automation is less common than augmentation.11\r\n\r\nOur finding that students favor augmentation over automation extends beyond Middlebury. In Appendix Figure A4, we re-analyze data from Ravšelj et al., (2025)—a multi-country survey of higher education students—to examine how this balance varies across institutional quality.12 We classify universities into quintiles based on their Times Higher Education World University Rankings. Overall, students who use AI worldwide show similar adoption rates for augmentation (64.6 percent) and automation (63.1 percent) tasks—a much smaller gap than at Middlebury (Panel A). However, this aggregate pattern masks heterogeneity by institutional quality. Students at top-quintile universities show a modest preference for augmentation over automation, with this gap narrowing monotonically down the institution quality distribution. By the bottom quintile, augmentation and automation usage are virtually identical (Panel B).\r\n\r\n4.3Heterogeneity in Augmentation versus Automation Usage\r\nUnderstanding whether augmentation and automation patterns vary across student populations may help to design targeted support policies. To assess this, we construct four measures of augmentation and automation usage. First, we create binary indicators for whether students use AI for any augmentation or automation task. Second, we calculate the proportion of tasks in each category for which students employ AI. Third, using the Likert-scale responses, we create intensity measures that capture how frequently students use AI for augmentation and automation purposes. Finally, we compute the difference between augmentation and automation variables, as a measure that directly compare students’ relative preference for augmentation versus automation. Table LABEL:tab:ai_autom_regs presents regression estimates using these measures as outcomes.\r\n\r\nThe balance between augmentation and automation varies substantially by student characteristics. Males show higher adoption of both augmentation and automation tasks, with a similar magnitude in each case—they are 6.0 pp more likely to use augmentation prompts and 5.5 pp more likely to use automation prompts compared to females (columns 1 and 4, \r\np\r\n<\r\n0.05\r\n). Black students have a substantially higher augmentation intensity than white students (0.871 points higher, equivalent to 38.1 percent of the outcome mean, \r\np\r\n<\r\n0.01\r\n), but show no statistically significant differences in automation usage. This results in Black students having the strongest preference for augmentation over automation, with a 19.5 pp higher share of augmentation tasks relative to automation (column 7, \r\np\r\n<\r\n0.01\r\n). Asian students show high usage of both kinds relative to white students. Differences across field of study effects are sizable: humanities, languages, and literature majors tend to show lower usage of both augmentation and automation tools compared to natural science majors.\r\n\r\n5Institutional Policies and AI Adoption\r\n5.1The Role of Institutional Policies in Shaping Student Behavior\r\nInstitutional policies are central to shaping the adoption and diffusion of new technologies (Acemoglu, , 2025). In the context of generative AI in higher education, understanding how policies affect student behavior is essential for guiding evidence-based decision-making. To examine this, we asked students to report their likelihood of using AI under various policy scenarios, ranging from complete prohibition to unrestricted use. This analysis is particularly relevant in light of the ongoing debate about how universities should regulate generative AI use in academic settings (Nolan, , 2023), and the widespread variation in institutional policies across colleges (Xiao et al., , 2023).\r\n\r\nInstitutional policies substantially influence students’ reported likelihood of using generative AI. Figure 5 shows that when generative AI use is unrestricted, 52.4 percent of students report being likely or extremely likely to use it. This likelihood decreases modestly when policies require citation (40.9 percent) or when no explicit policy exists (42.3 percent). However, explicit prohibition creates a dramatic shift: only 13.4 percent of students report they would be likely or extremely likely to use AI when it is banned, while 72.9 percent say they would be unlikely or extremely unlikely to do so (note that these figures include both current AI users and students who do not use generative AI for academic purposes). These results suggest that institutional policies can significantly influence generative AI usage patterns, though a small fraction of students report they would likely use AI even when explicitly prohibited.\r\n\r\nThe magnitude of policy effects in our study aligns closely with findings from other contexts. Carvajal et al., (2024) estimate that banning AI reduces usage by 37.2 pp among females and 20.6 pp among males, for an overall drop of about 28.9 pp. In our survey, a ban leads to a 37.8 pp decline in usage, with a larger decrease among females (49.6 pp) than males (40.1 pp). These parallel results underscore how institutional policies can unintentionally produce disparate effects across gender. Notably, these differential policy effects extend beyond gender. Other demographic and academic characteristics—such as race and field of study—also moderate how students respond to policy restrictions (Appendix Table LABEL:tab:ai_policy_het), underscoring that institutional policies can produce non-neutral impacts along multiple dimensions.\r\n\r\n5.2Understanding of Institutional Policies and Resources\r\nInformation gaps and inattention can significantly affect technology adoption decisions (Duflo et al., , 2011; Hanna et al., , 2014). For example, imperfect information about rules, available resources, or proper usage guidelines could lead to underadoption of beneficial technologies or inadvertent policy violations. To test for the existence of information gaps, we examine three dimensions of policy understanding. First, we asked students whether they find AI policies in their current classes clear. Second, we measured awareness of free access to premium AI tools through the college—a resource that could reduce inequality but only if students know it exists. Third, we assessed whether students know how to properly cite AI when required, a mechanical skill necessary for academic integrity. We supplemented these measures with open-ended feedback about the college’s AI policies and support services.\r\n\r\nStudent understanding of institutional policies is high, though significant gaps remain. Figure 6, Panel A shows that most students (79.1 percent) understand when and where they are allowed to use AI in their classes, but a nontrivial minority (19.2 percent) find AI policies unclear. Moreover, critical knowledge gaps persist elsewhere. Only 10.1 percent know they have free access to Microsoft Copilot through the college (Panel B), and just 32.6 percent understand how to properly cite AI use (Panel C). These gaps vary systematically: females show better policy understanding than males (81.5 versus 75.8 percent), and non-white students demonstrate higher awareness across all three dimensions compared to white students.13\r\n\r\nThe qualitative evidence from open-ended responses reinforces these information frictions and reveals implementation challenges (Appendix D). Students express frustration with vague guidelines, requesting specific examples of acceptable versus unacceptable use cases. Many advocate for formal training, noting that simply knowing policies exist differs from understanding how to effectively integrate AI into their workflow. A particularly striking theme is students’ perception of blanket prohibitions as both ineffective and unfair, as they create a prisoner’s dilemma situation where compliant students are disadvantaged relative to those who secretly violate restrictions. Many responses call for a balanced approach—permitting AI use that supports learning while restricting uses that replace it—though the boundary between the two remains contested.\r\n\r\n6Beliefs About AI’s Educational Impact and Peer Usage\r\n6.1Student Beliefs of AI’s Impact on Educational Outcomes\r\nTechnology adoption decisions are shaped by beliefs about potential returns (Foster and Rosenzweig, , 2010). Students’ perceptions of how AI affects their learning may influence whether they adopt these tools and how they integrate them into their academic workflows. To elicit these beliefs, we asked students to evaluate AI’s impact across four dimensions of their academic performance: understanding of course materials, overall learning ability, time management, and course grades. For each dimension, students rated AI’s effect on a five-point scale ranging from “significantly reduces” to “significantly improves.” These subjective assessments provide insight into the perceived value of AI tools from the student perspective, which may differ from their actual effects on learning outcomes.\r\n\r\nStudents tend to believe that generative AI is beneficial for their academic performance, though the perceived benefits vary across dimensions. Figure 7 shows that the majority of students (70.2 percent) believe that generative AI improves their understanding of course materials, and 60.1 percent report improvement in their ability to grasp concepts, retain information, or learn new skills. Similarly, 59.4 percent report that AI improves their ability to complete assignments on time. Notably, while students believe AI helps their learning and assignment completion, they are less confident about its impact on course grades—41.1 percent believe it improves their grades, while 55.4 percent report no effect and 3.5 percent report negative effects. This pattern suggests that while students perceive generative AI as improving their learning process and workflow—through better understanding, skill development, and timely completion of work—these benefits do not necessarily translate into better course grades.14\r\n\r\nBeliefs about AI’s benefits strongly predict adoption. In Figure 8, we plot the relationship between the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis) for different subgroups of students (e.g., males, white students, public-school students, etc.). Across all four measured academic dimensions, there is a strong and statistically significant relationship between perceiving positive AI effects and AI adoption. The relationship is strongest for beliefs about the ability to improve course grades (Panel D): a 10-pp increase in the belief that AI improves grades is associated with a 5.6 pp increase in AI adoption (\r\np\r\n<\r\n0.01\r\n). Similar positive relationships exist for beliefs about learning ability (4.3 pp, \r\np\r\n<\r\n0.05\r\n), understanding of course materials (4.5 pp, \r\np\r\n<\r\n0.01\r\n), and timely assignment completion (5.3 pp, \r\np\r\n<\r\n0.05\r\n). These findings suggest that student beliefs about AI’s academic benefits—irrespective of the actual benefits—may play a crucial role in shaping adoption decisions.\r\n\r\nThese positive beliefs align with findings from other contexts. Ravšelj et al., (2025) report that the majority of students in their sample believe that ChatGPT improves their general knowledge (68.8 percent) and specific knowledge (62.7 percent)—remarkably similar to our finding that most students believe AI improves understanding of course materials (68.5 percent) and learning ability (57.6 percent). Similarly, 57.4 percent of students in Ravšelj et al., (2025)’s sample believe that ChatGPT helps meet assignment deadlines, while 59.4 percent in our data report AI improves timely assignment completion. Stöhr et al., (2024) provide complementary evidence from Swedish universities, where 47.7 percent of students believe that AI makes them more effective learners, yet only 17.3 percent believe these tools improve their grades—mirroring our finding that perceived learning benefits from AI use exceed perceived effects on course performance.\r\n\r\n6.2Student Beliefs About Peer Use of Generative AI\r\nStudents’ beliefs about their peers’ AI usage may influence their own adoption through multiple channels, including social norms (e.g., Giaccherini et al., , 2019), social learning (e.g., Foster and Rosenzweig, , 1995; Beaman et al., , 2021), peer effects (e.g., Bailey et al., , 2022), and competitive pressure to avoid falling behind (e.g., Goehring et al., , 2024). To measure these beliefs, we asked students to estimate what fraction of their peers use generative AI for different purposes and would use under different policy environments. Figure 9 presents the distribution of these beliefs. Panels A-C show students’ estimates of peer AI usage for schoolwork, leisure, and any purpose respectively. Panels D-F display students’ beliefs about AI usage under three policy environments: when classes have no explicit AI policy, when classes allow AI use, and when classes prohibit AI use.\r\n\r\nStudents systematically underestimate their peers’ AI usage. On average, they believe 65.2 percent of peers regularly use generative AI for schoolwork (Panel A), while our survey reveals an actual usage rate of 82.5 percent—a 17.3 pp gap. This underestimation appears consistent across educational contexts: Stöhr et al., (2024) find that only 38.7 percent of Swedish students believe AI chatbot use is common among peers, despite 63 percent actual usage. When we examine beliefs about policy-contingent behavior, students estimate that 62.7 percent of peers use AI when no explicit policy exists (Panel D), rising to 72.2 percent under permissive policies (Panel E) and falling to 43.5 percent under prohibition (Panel F). These beliefs align directionally with self-reported intentions, though with notable magnitude differences (Appendix Figure A5). For instance, while students believe 43.5 percent of their peers use AI in classes that prohibit it, only 28.4 percent report they themselves would be likely to do so.15\r\n\r\nNotably, students’ perceptions of peer AI adoption are closely linked to their own adoption behavior. Appendix Figure A6 shows a strong positive association between beliefs about peer usage and actual usage rates across student groups. This behavior is consistent with several psychological mechanisms, including the “false consensus effect” (Ross et al., , 1977), selection neglect (Jehiel, , 2018), or interpersonal projection bias (Bushong and Gagnon-Bartsch, , 2024). Still, virtually all groups systematically underestimate peer usage—actual usage rates exceed believed usage for every demographic and academic group we examine. This figure also shows that underestimation varies substantially across student characteristics: males use AI at 88.7 percent but believe only 66.6 percent of peers do so (22.1 pp gap), while females show a smaller gap (77.6 percent actual versus 65.1 percent believed, 13.3 pp gap).\r\n\r\nTaken together, students’ systematic underestimation of peer AI usage coupled with the strong relationship between beliefs and adoption suggests that misperceptions about social norms may shape technology diffusion. If AI enhances learning, then correcting these misperceptions through information provision could accelerate beneficial adoption; conversely, if AI undermines skill development, then students’ underestimation of peer usage may serve as an unintentional safeguard against harmful overadoption.\r\n\r\n7Discussion\r\nThis paper presents the first systematic evidence on generative AI adoption at a highly selective U.S. college. Using novel survey data, we document exceptionally rapid and widespread adoption, substantial shifts in the educational production function through augmentation and automation, and the significant roles of students’ beliefs and institutional policies in shaping AI use.\r\n\r\nOur results offer three implications for institutional policy and ongoing debates about AI in education. First, we identify low-cost opportunities to improve institutional policy effectiveness through targeted information provision. The significant gaps we document in students’ understanding of institutional AI policies, citation practices, and available AI resources suggest that simple interventions—such as clear guidelines, illustrative examples of acceptable uses, and AI literacy programs—can reduce unintentional academic integrity violations and support beneficial AI integration. Qualitative feedback strongly indicates student demand for more explicit guidance on responsible AI use.\r\n\r\nSecond, our evidence challenges alarmist narratives that conflate widespread AI adoption with universal academic dishonesty based on anecdotal accounts.16 Although AI use is indeed near-universal, we find clear evidence that students primarily employ AI as a tool for strategic task management: to enhance learning (augmentation) and selectively automate tasks when facing high time opportunity cost—not solely to circumvent academic effort. This distinction matters: by normalizing academic dishonesty as inevitable and universal, these narratives may shift social norms and encourage students who would otherwise use AI responsibly to engage in prohibited behaviors, believing “everyone else is doing it.”\r\n\r\nThird, our findings caution against policy extremes of either blanket prohibition or unrestricted AI use (Merchant, , 2024; McDonald et al., , 2025). Blanket prohibitions disproportionately harm students who benefit most from AI’s augmentation functions—particularly lower-achieving students—while also creating uneven compliance, placing conscientious students at a disadvantage relative to rule-breakers. Conversely, unrestricted AI use based solely on revealed preference arguments ignores important market failures in educational settings. Students often hold overly optimistic beliefs about AI’s learning benefits despite mixed empirical evidence, potentially leading to unintended negative learning outcomes. Most concerningly, permissive policies risk creating competitive dynamics where students feel compelled to adopt AI not for its learning benefits but simply to avoid falling behind in an educational “arms race” (Goehring et al., , 2024).\r\n\r\nFigures and Tables\r\nFigure 1:The Adoption of Generative AI among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the fraction of students who report using AI during the academic semester, categorized by demographic characteristics, high school type, academic cohort, GPA, and field of study. Usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily).\r\n\r\nThe category “All students” provides the baseline usage rate for the full sample. Gender categories are based on self-identification, with non-binary responses excluded due to a small sample size. “Private HS” refers to students who attended private high schools, while “Public HS” includes public institutions. “Cohort” denotes the student’s academic year, ranging from first-year (“Freshman”) to fourth-year and beyond (“Senior”). GPA categories (“GPA \r\n>\r\n p50” and “GPA \r\n<\r\n p50”) split students into groups above or below the median first-year GPA, as self-reported on a 4.0 scale. See Appendix B.1 for the classification of majors into fields of study.\r\n\r\nFigure 2:The Evolution of Generative AI Adoption among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the cumulative percent of students who reported using generative AI tools for academic purposes over time. The data is based on retrospective self-reports collected in our December 2024 survey, where students were asked “When did you first start using any form of Generative AI for academic purposes?” Response options ranged from “Before Spring 2023” to “This semester (Fall 2024).” The \r\nx\r\n-axis represents academic semesters, while the \r\ny\r\naxis represents the cumulative adoption rate. Vertical lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure 3:Adoption of Generative AI Models Among College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the adoption rates of various AI models as of Fall 2024. The horizontal axis shows the percent of students who reported using each tool, and the vertical axis lists the tools in descending order of adoption rates.\r\n\r\nFigure 4:Academic Uses of Generative AI\r\nPanel A. Across Common Academic Tasks\r\nRefer to caption\r\nPanel B. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of AI students who use generative AI for different academic tasks. For each task, usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). The number at the end of each bar represents the total percent of students who use AI for that purpose at any frequency. Tasks are ordered by total usage, from highest to lowest. Results are based on responses to the question: “For academic purposes, which of the following tasks do you typically use generative AI for?” Sample includes all students who reported using AI during the academic semester.\r\n\r\nFigure 5:Student Reported Likelihood of Using AI under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of students who report different likelihoods of using AI under various policy scenarios. For each policy, responses are categorized on a five-point scale from “Extremely unlikely to use AI” to “Extremely likely to use AI.” The sample includes all survey respondents. The question asked was: “How likely are you to use generative AI in a class with each of the following AI policies?”\r\n\r\nFigure 6:Understanding of Generative AI Policies and Resources\r\nPanel A. Understanding of AI Policies\r\nRefer to caption\r\nPanel B. Awareness of Copilot Access\r\nRefer to caption\r\nPanel C. Knowledge of Citation Requirements\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows students’ understanding of institutional AI policies and resources. Panel A displays the percent of students who report understanding AI policies in their classes, those who report having no explicit policy, and those who find policies unclear, broken down by demographic characteristics. Panel B shows the percent of students who are aware of their free access to Microsoft Copilot through Middlebury College. Panel C presents the percent of students who report knowing how to properly cite AI use in their academic work when required. For Panels B and C, horizontal lines represent 95 percent confidence intervals. Sample includes all survey respondents.\r\n\r\nFigure 7:Student Beliefs about the Impact of AI on their Academic Performance\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of Middlebury students who believe that AI improves, reduces, or has no effect on different aspects of their academic experience. For each outcome, responses are categorized into three groups: “Improves” combines “significantly improves” and “somewhat improves” responses, “Reduces” combines “significantly reduces” and “somewhat reduces” responses, and “No effect” represents neutral responses. Sample includes all students who report using AI during the academic semester. “Don’t know” responses are excluded.\r\n\r\nFigure 8:Relationship Between AI Adoption and Beliefs About AI’s Academic Benefits\r\nPanel A. Learning ability (e.g., ability to grasp concepts, learn new skills, etc.)\r\nRefer to caption\r\nPanel B. Understanding of\r\ncourse materials\r\nRefer to caption\r\nPanel C. Ability to complete\r\nassignments on time\r\nRefer to caption\r\nPanel D. Course grades\r\n \r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the relationship between AI adoption rates and beliefs that AI improves various academic outcomes across different student groups. Each panel plots the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis). Points represent different student groups categorized by demographics (circles), academic characteristics (triangles), and field of study (squares). The dashed line shows the linear fit across all groups. Groups with fewer than 10 students are excluded.\r\n\r\nFigure 9:Student Beliefs about Generative AI Usage at Middlebury College\r\nPanel A. For Schoolwork\r\nRefer to caption\r\nPanel B. For Leisure\r\nRefer to caption\r\nPanel C. For Any Purpose\r\nRefer to caption\r\nPanel D. Classes with No Policy\r\nRefer to caption\r\nPanel E. Classes that Allow AI\r\nRefer to caption\r\nPanel F. Classes that Disallow AI\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the distribution of students’ beliefs about generative AI usage among their peers at Middlebury College. Panels A-C display students’ estimates of the percent of their peers who regularly use AI for schoolwork, leisure activities, and any purpose, respectively. Panels D-F show students’ beliefs about AI usage in classes with different AI policies: those without an explicit policy (Panel D), those that allow AI use (Panel E), and those that prohibit AI use (Panel F). Each panel shows a histogram with bins of width ten percentage points (e.g., responses between 1-10 fall in the 10 bin, 11-20 in the 20 bin, etc.). The red dashed line indicates the mean response. Sample excludes respondents with missing values or who selected the default response for all six categories (which equals zero).\r\n\r\nTable 1:Summary Statistics of Survey Participants\r\nSurvey Sample\tAdmin records\r\nUnweighted\tWeighted\t\r\n(1)\t(2)\t(3)\r\nPanel A. Demographics\r\nMale\t0.446\t0.433\t0.463\r\nFemale\t0.508\t0.516\t0.533\r\nWhite\t0.618\t0.603\t0.538\r\nBlack\t0.036\t0.033\t0.052\r\nHispanic\t0.099\t0.106\t0.124\r\nAsian\t0.155\t0.162\t0.073\r\nPrivate high school\t0.420\t0.399\t–\r\nPublic high school\t0.543\t0.556\t–\r\nPanel B. Academic Characteristics    GPA 3.740 3.736 3.670 Hours spent on academics per week 17.899 17.889 – Freshman 0.311 0.355 0.255 Sophomore 0.273 0.272 0.257 Junior 0.202 0.179 0.201 Senior 0.213 0.194 0.287   Panel C. Field of Study    Arts 0.011 0.021 0.023 Humanities 0.052 0.068 0.073 Languages 0.021 0.020 0.025 Literature 0.035 0.025 0.025 Natural Sciences 0.218 0.249 0.244 Social Sciences 0.353 0.247 0.243 Has not declared major 0.311 0.371 0.364\r\n\r\nN (# degrees) 43 43 49 N (# students) 634 2,760 2,760\r\n\r\n\\justify\r\nNotes: This table presents summary statistics from our survey of college students. Panel A reports demographic characteristics, including the proportion of participants identifying as male, female, white, Black, Hispanic, Asian, or who attended a private or public high school. Panel B provides academic characteristics, such as GPA (only available for non-freshmen), average weekly hours spent on academics, and academic year distribution (Freshman, Sophomore, Junior, and Senior). Note that in column 1–2, GPA refers to self-reported first-year GPA while in column 3 it is the overall GPA during Spring 2024. Panel C summarizes the distribution of participants across different fields of study. Major groups are mutually exclusive.\r\n\r\n0.8250.5890.3680.106 R\r\n−\r\nsquared0.0770.0890.1160.058 N (Students)616616616616 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI usage frequency threshold and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column uses a different threshold for AI usage frequency, categorized as: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). Column 1 defines usage as any nonzero frequency; column 2 includes at least occasional use; column 3 includes frequent or higher use; and column 4 captures only very frequent use.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.0890.1990.3570.5510.801 R\r\n−\r\nsquared0.0450.0610.0880.0800.069 N (Students)633633633633633 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI adoption date and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column presents results for a different threshold of AI adoption. Column 1 shows the probability of adopting AI before Spring 2023; column 2 by Spring 2023; column 3 by Fall 2023; column 4 by Spring 2024; and column 5 by Fall 2024. The dependent variable in each regression is a binary indicator equal to one if the student had adopted AI by the specified time period.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.9120.6122.2850.9130.4191.8010.1930.484 R\r\n−\r\nsquared0.0360.0730.1260.0530.1170.1290.0420.063 N (Students)515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and their use of generative AI for academic tasks. In columns 1 and 4, the outcome is a dummy that equals one if a student reports using AI with any frequency for at least one augmentation or automation task, respectively. In columns 2 and 5, the outcome is the share of tasks within each category for which the student reports any use. In columns 3 and 6, the outcome is a continuous measure capturing average usage frequency for each task category, based on raw Likert-style responses. In columns 7 and 8, the outcome is the difference in average task share and usage intensity between augmentation and automation, respectively. Regressions are weighted and report robust standard errors clustered at the student level. ∗∗∗ \r\np\r\n<\r\n0.01\r\n, ∗∗ \r\np\r\n<\r\n0.05\r\n, ∗ \r\np\r\n<\r\n0.10\r\n.\r\n\r\nReferences\r\nAcemoglu, (2025)\r\nAcemoglu, D. (2025).Nobel lecture: Institutions, technology, and prosperity.American Economic Review, 115(6):1709–1748.\r\nAngrist et al., (2009)\r\nAngrist, J., Lang, D., and Oreopoulos, P. (2009).Incentives and services for college achievement: Evidence from a randomized trial.American Economic Journal: Applied Economics, 1(1):136–163.\r\nAutor and Thompson, (2025)\r\nAutor, D. and Thompson, N. (2025).Expertise.\r\nBailey et al., (2022)\r\nBailey, M., Johnston, D., Kuchler, T., Stroebel, J., and Wong, A. (2022).Peer effects in product adoption.American Economic Journal: Applied Economics, 14(3):488–526.\r\nBastani et al., (2025)\r\nBastani, H., Bastani, O., Sungu, A., Ge, H., Kabakcı, Ö., and Mariman, R. (2025).Generative AI without guardrails can harm learning: Evidence from high school mathematics.Proceedings of the National Academy of Sciences, 122(26):e2422633122.\r\nBeaman et al., (2021)\r\nBeaman, L., BenYishay, A., Magruder, J., and Mobarak, A. M. (2021).Can network theory-based targeting increase technology adoption?American Economic Review, 111(6):1918–1943.\r\nBick et al., (2025)\r\nBick, A., Blandin, A., and Deming, D. J. (2025).The rapid adoption of generative AI.Technical report, National Bureau of Economic Research.\r\nBonney et al., (2024)\r\nBonney, K., Breaux, C., Buffington, C., Dinlersoz, E., Foster, L. S., Goldschlag, N., Haltiwanger, J. C., Kroff, Z., and Savage, K. (2024).Tracking Firm Use of AI in Real Time: A Snapshot from the Business Trends and Outlook Survey.\r\nBonsu and Baffour-Koduah, (2023)\r\nBonsu, E. M. and Baffour-Koduah, D. (2023).From the consumers’ side: Determining students’ perception and intention to use chatgpt in ghanaian higher education.Journal of Education, Society & Multiculturalism, 4(1):1–29.\r\nBresnahan and Trajtenberg, (1995)\r\nBresnahan, T. F. and Trajtenberg, M. (1995).General purpose technologies ‘engines of growth’?Journal of econometrics, 65(1):83–108.\r\nBrynjolfsson and Hitt, (2000)\r\nBrynjolfsson, E. and Hitt, L. M. (2000).Beyond computation: Information technology, organizational transformation and business performance.Journal of Economic Perspectives, 14(4):23–48.\r\nBrynjolfsson et al., (2025)\r\nBrynjolfsson, E., Li, D., and Raymond, L. (2025).Generative AI at Work.The Quarterly Journal of Economics, 140(2):889–942.\r\nBushong and Gagnon-Bartsch, (2024)\r\nBushong, B. and Gagnon-Bartsch, T. (2024).Failures in forecasting: An experiment on interpersonal projection bias.Management Science, 70(12):8735–8752.\r\nCarvajal et al., (2024)\r\nCarvajal, D., Franco, C., and Isaksson, S. (2024).Will artificial intelligence get in the way of achieving gender equality?Discussion Paper 03, NHH Norwegian School of Economics.\r\n(15)\r\nComin, D. and Hobijn, B. (2010a).An exploration of technology diffusion.American Economic Review, 100(5):2031–2059.\r\n(16)\r\nComin, D. and Hobijn, B. (2010b).Technology Diffusion and Postwar Growth, volume 25, pages 209–246.University of Chicago Press.\r\nCui et al., (2024)\r\nCui, Z. K., Demirer, M., Jaffe, S., Musolff, L., Peng, S., and Salz, T. (2024).The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers.\r\nDavid, (1990)\r\nDavid, P. A. (1990).The dynamo and the computer: an historical perspective on the modern productivity paradox.The American economic review, 80(2):355–361.\r\nDell’Acqua et al., (2023)\r\nDell’Acqua, F., McFowland, E., Mollick, E. R., Lifshitz-Assaf, H., Kellogg, K., Rajendran, S., Krayer, L., Candelon, F., and Lakhani, K. R. (2023).Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality.\r\nDuflo et al., (2011)\r\nDuflo, E., Kremer, M., and Robinson, J. (2011).Nudging Farmers to Use Fertilizer: Theory and Experimental Evidence from Kenya.American Economic Review, 101(6):2350–2390.\r\nEloundou et al., (2024)\r\nEloundou, T., Manning, S., Mishkin, P., and Rock, D. (2024).GPTs are GPTs: Labor market impact potential of LLMs.Science, 384(6702):1306–1308.\r\nFairlie et al., (2014)\r\nFairlie, R. W., Hoffmann, F., and Oreopoulos, P. (2014).A community college instructor like me: Race and ethnicity interactions in the classroom.American Economic Review, 104(8):2567–2591.\r\nFelten et al., (2021)\r\nFelten, E. W., Raj, M., and Seamans, R. (2021).Occupational, Industry, and Geographic Exposure to Artificial Intelligence: A Novel Dataset and Its Potential Uses.\r\nFelten et al., (2023)\r\nFelten, E. W., Raj, M., and Seamans, R. (2023).How will Language Modelers like ChatGPT Affect Occupations and Industries?\r\nFoster and Rosenzweig, (1995)\r\nFoster, A. D. and Rosenzweig, M. R. (1995).Learning by doing and learning from others: Human capital and technical change in agriculture.Journal of political Economy, 103(6):1176–1209.\r\nFoster and Rosenzweig, (2010)\r\nFoster, A. D. and Rosenzweig, M. R. (2010).Microeconomics of technology adoption.Annu. Rev. Econ., 2(1):395–424.\r\nGallup, (2024)\r\nGallup (2024).AI in the Workplace: Answering 3 Big Questions.\r\nGiaccherini et al., (2019)\r\nGiaccherini, M., Herberich, D. H., Jimenez-Gomez, D., List, J. A., Ponti, G., and Price, M. K. (2019).The behavioralist goes door-to-door: Understanding household technological diffusion using a theory-driven natural field experiment.Technical report, National Bureau of Economic Research.\r\nGoehring et al., (2024)\r\nGoehring, G., Mezzanotti, F., and Ravid, S. A. A. (2024).Technology adoption and career concerns: Evidence from the adoption of digital technology in motion pictures.Technical report, National Bureau of Economic Research.\r\nGriliches, (1957)\r\nGriliches, Z. (1957).Hybrid corn: An exploration in the economics of technological change.Econometrica, 25(4):501–522.\r\nHall and Khan, (2003)\r\nHall, B. H. and Khan, B. (2003).Adoption of new technology.\r\n(32)\r\nHanda, K., Bent, D., Tamkin, A., McCain, M., Durmus, E., Stern, M., Schiraldi, M., Huang, S., Ritchie, S., Syverud, S., Jagadish, K., Vo, M., Bell, M., and Ganguli, D. (2025a).Anthropic education report: How university students use claude.\r\n(33)\r\nHanda, K., Tamkin, A., McCain, M., Huang, S., Durmus, E., Heck, S., Mueller, J., Hong, J., Ritchie, S., Belonax, T., et al. (2025b).Which economic tasks are performed with ai? evidence from millions of claude conversations.arXiv preprint arXiv:2503.04761.\r\nHanna et al., (2014)\r\nHanna, R., Mullainathan, S., and Schwartzstein, J. (2014).Learning Through Noticing: Theory and Evidence from a Field Experiment.The Quarterly Journal of Economics, 129(3):1311–1353.\r\nHartley et al., (2025)\r\nHartley, J., Jolevski, F., Melo, V., and Moore, B. (2025).The Labor Market Effects of Generative Artificial Intelligence.\r\nHumlum and Vestergaard, (2025)\r\nHumlum, A. and Vestergaard, E. (2025).Large Language Models, Small Labor Market Effects.\r\nHutto and Gilbert, (2014)\r\nHutto, C. and Gilbert, E. (2014).Vader: A parsimonious rule-based model for sentiment analysis of social media text.In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216–225.\r\nJehiel, (2018)\r\nJehiel, P. (2018).Investment strategy and selection bias: An equilibrium perspective on overoptimism.American Economic Review, 108(6):1582–1597.\r\nKelly et al., (2023)\r\nKelly, A., Sullivan, M., and Strampel, K. (2023).Generative artificial intelligence: University student awareness, experience, and confidence in use across disciplines.Journal of University Teaching and Learning Practice, 20(6):1–16.\r\nKharazian, (2025)\r\nKharazian, A. (2025).Ramp ai index: Monthly measurement of ai adoption by american businesses.Technical report, Ramp Economics Lab.\r\nLehmann et al., (2024)\r\nLehmann, M., Cornelius, P. B., and Sting, F. J. (2024).AI Meets the Classroom: When Does ChatGPT Harm Learning?\r\nLing and Imas, (2025)\r\nLing, Y. and Imas, A. (2025).Underreporting of AI use: The role of social desirability bias.Available at SSRN.\r\nLiu and Wang, (2024)\r\nLiu, Y. and Wang, H. (2024).Who on earth is using generative ai?Policy Research Working Paper WPS10870, World Bank Group, Washington, D.C.\r\nMcClain, (2024)\r\nMcClain, C. (2024).Americans’ use of ChatGPT is ticking up, but few trust its election information.\r\nMcDonald et al., (2025)\r\nMcDonald, N., Johri, A., Ali, A., and Collier, A. H. (2025).Generative artificial intelligence in higher education: Evidence from an analysis of institutional policies and guidelines.Computers in Human Behavior: Artificial Humans, 3:100121.\r\nMcElheran et al., (2023)\r\nMcElheran, K., Li, J. F., Brynjolfsson, E., Kroff, Z., Dinlersoz, E., Foster, L. S., and Zolas, N. (2023).AI Adoption in America: Who, What, and Where.\r\nMerchant, (2024)\r\nMerchant, B. (2024).The new luddites aren’t backing down.The Atlantic.Accessed: 2025-06-16.\r\nNam, (2023)\r\nNam, J. (2023).56% of college students have used ai on assignments or exams.Technical report.Edited by Lyss Welding. Accessed June 11, 2025.\r\nNolan, (2023)\r\nNolan, B. (2023).Here are the schools and colleges that have banned chatgpt.Business Insider.Accessed: 2025-01-27.\r\nNoy and Zhang, (2023)\r\nNoy, S. and Zhang, W. (2023).Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence.\r\nOtis et al., (2024)\r\nOtis, N. G., Delecourt, S., Cranney, K., and Koning, R. (2024).Global Evidence on Gender Gaps and Generative AI.Harvard Business School.\r\nPeng et al., (2023)\r\nPeng, S., Kalliamvakou, E., Cihon, P., and Demirer, M. (2023).The Impact of AI on Developer Productivity: Evidence from GitHub Copilot.\r\nRavšelj et al., (2025)\r\nRavšelj, D., Keržič, D., Tomaževič, N., Umek, L., Brezovar, N., A. Iahad, N., Abdulla, A. A., Akopyan, A., Aldana Segura, M. W., AlHumaid, J., Allam, M. F., Alló, M., Andoh, R. P. K., Andronic, O., Arthur, Y. D., Aydın, F., Badran, A., Balbontín-Alvarado, R., Ben Saad, H., Bencsik, A., Benning, I., Besimi, A., Bezerra, D. D. S., Buizza, C., Burro, R., Bwalya, A., Cachero, C., Castillo-Briceno, P., Castro, H., Chai, C. S., Charalambous, C., Chiu, T. K. F., Clipa, O., Colombari, R., Corral Escobedo, L. J. H., Costa, E., Cre\\textcommabelowtulescu, R. G., Crispino, M., Cucari, N., Dalton, F., Demir Kaya, M., Dumić-Čule, I., Dwidienawati, D., Ebardo, R., Egbenya, D. L., Faris, M. E., Fečko, M., Ferrinho, P., Florea, A., Fong, C. Y., Francis, Z., Ghilardi, A., González-Fernández, B., Hau, D., Hossain, M. S., Hug, T., Inasius, F., Ismail, M. J., Jahić, H., Jessa, M. O., Kapanadze, M., Kar, S. K., Kateeb, E. T., Kaya, F., Khadri, H. O., Kikuchi, M., Kobets, V. M., Kostova, K. M., Krasmane, E., Lau, J., Law, W. H. C., Lazăr, F., Lazović-Pita, L., Lee, V. W. Y., Li, J., López-Aguilar, D. V., Luca, A., Luciano, R. G., Machin-Mastromatteo, J. D., Madi, M., Manguele, A. L., Manrique, R. F., Mapulanga, T., Marimon, F., Marinova, G. I., Mas-Machuca, M., Mejía-Rodríguez, O., Meletiou-Mavrotheris, M., Méndez-Prado, S. M., Meza-Cano, J. M., Mirķe, E., Mishra, A., Mital, O., Mollica, C., Morariu, D. I., Mospan, N., Mukuka, A., Navarro Jiménez, S. G., Nikaj, I., Nisheva, M. M., Nisiforou, E., Njiku, J., Nomnian, S., Nuredini-Mehmedi, L., Nyamekye, E., Obadić, A., Okela, A. H., Olenik-Shemesh, D., Ostoj, I., Peralta-Rizzo, K. J., Peštek, A., Pilav-Velić, A., Pires, D. R. M., Rabin, E., Raccanello, D., Ramie, A., Rashid, M. M. U., Reuter, R. A. P., Reyes, V., Rodrigues, A. S., Rodway, P., Ručinská, S., Sadzaglishvili, S., Salem, A. A. M. S., Savić, G., Schepman, A., Shahpo, S. M., Snouber, A., Soler, E., Sonyel, B., Stefanova, E., Stone, A., Strzelecki, A., Tanaka, T., Tapia Cortes, C., Teira-Fachado, A., Tilga, H., Titko, J., Tolmach, M., Turmudi, D., Varela-Candamio, L., Vekiri, I., Vicentini, G., Woyo, E., Yorulmaz, Ö., Yunus, S. A. S., Zamfir, A.-M., Zhou, M., and Aristovnik, A. (2025).Higher education students’ perceptions of ChatGPT: A global study of early reactions.PLOS ONE, 20(2):e0315011.\r\nRogers, (1962)\r\nRogers, E. M. (1962).Diffusion of Innovations.Free Press, New York.\r\nRoss et al., (1977)\r\nRoss, L., Greene, D., and House, P. (1977).The “false consensus effect”: An egocentric bias in social perception and attribution processes.Journal of experimental social psychology, 13(3):279–301.\r\nSacerdote, (2001)\r\nSacerdote, B. (2001).Peer Effects with Random Assignment: Results for Dartmouth Roommates.The Quarterly Journal of Economics, 116(2):681–704.\r\nStinebrickner and Stinebrickner, (2008)\r\nStinebrickner, R. and Stinebrickner, T. R. (2008).The causal effect of studying on academic performance.The BE Journal of Economic Analysis & Policy, 8(1).\r\nStöhr et al., (2024)\r\nStöhr, C., Ou, A. W., and Malmström, H. (2024).Perceptions and usage of AI chatbots among students in higher education across genders, academic levels and fields of study.Computers and Education: Artificial Intelligence, 7:100259.\r\nStokey, (2021)\r\nStokey, N. L. (2021).Technology diffusion.Review of Economic Dynamics, 42:15–36.\r\nWorld Bank, (2016)\r\nWorld Bank (2016).World development report 2016: Digital dividends.World Bank Publications.\r\nWu et al., (2022)\r\nWu, M.-J., Zhao, K., and Fils-Aime, F. (2022).Response rates of online surveys in published research: A meta-analysis.Computers in Human Behavior Reports, 7.\r\nXiao et al., (2023)\r\nXiao, P., Chen, Y., and Bao, W. (2023).Waiting, banning, and embracing: An empirical analysis of adapting policies for generative ai in higher education.arXiv preprint arXiv:2305.18617.\r\nAppendix\r\n\r\nAppendix AAppendix Figures and Tables\r\nFigure A1:Generative AI Usage Survey Design Overview\r\nKey\t\r\nSurvey Structure\r\nSurvey Sections\r\nUsage Data Collection\r\nBeliefs & Perceptions\r\nAI Usage Survey at Middlebury College\r\nDecember 2024\r\nParticipant Recruitment\r\nCampus-wide email invitations, framed as general technology use survey\r\nParticipation Incentives\r\nAmazon gift card lottery ranging from $50-$500\r\nSection 1: Demographics & Academic Information\r\nStudent Characteristics\r\nGender, race/ethnicity, high school type\r\nAcademic Profile\r\nYear, major, self-reported GPA, study hours\r\nSection 2: generative AI Usage Patterns\r\nAdoption Metrics\r\nFrequency, timing of first use, specific AI models used\r\nAcademic Applications\r\nTasks performed with AI, payment for premium services\r\nSection 3: Perceptions & Institutional Policies\r\nImpact Perceptions\r\nEffects on learning, grades, time management\r\nPolicy Responses\r\nUsage likelihood under different policies\r\nPeer Usage Beliefs\r\nEstimates of AI adoption among peers for schoolwork and leisure\r\nOpen-Ended Response Collection\r\nMotivations for AI use, policy feedback, suggestions for improvement\r\nNote: This figure illustrates the structure of the AI usage survey conducted at Middlebury College in December 2024. The survey collected information across three main sections: (1) demographic and academic background, (2) patterns of generative AI usage including adoption timing, frequency, and specific applications, and (3) perceptions of AI’s impact on learning and responses to institutional policies.\r\n\r\nFigure A2:Cumulative Generative AI Use by Student Characteristic\r\nPanel A. By Gender\r\nRefer to caption\r\nPanel B. By Race\r\nRefer to caption\r\nPanel C. By High School Type\r\nRefer to caption\r\nPanel D. By Freshman Year GPA\r\nRefer to caption\r\nPanel E. By Cohort\r\nRefer to caption\r\nPanel F. By Field of Study\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents cumulative AI use based on different student characteristics. Each panel displays the cumulative distribution of AI use based on a specific characteristic: gender, race, school type, first-year GPA, cohort, or field of study. The cumulative percent of students is plotted against usage categories. The legends and colors correspond to subgroups within each demographic variable.\r\n\r\nFigure A3:Percent of Students Who Pay for Generative AI Tools\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of AI users who pay for AI tools (through any platform) across different demographic groups. Horizontal lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure A4:Academic Uses of ChatGPT: Evidence from Global Survey\r\nPanel A. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\nPanel B. Across the College Quality Distribution\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of students who use ChatGPT for different academic tasks based on data from Ravšelj et al., (2025). Panel A displays usage patterns across tasks categorized as augmenting (proofreading, translating, study assistance, research assistance) versus automating (academic writing, professional writing, creative writing, brainstorming, summarizing, calculating, coding assistance, personal assistance) student effort. Panel B shows usage patterns across university quality quintiles based on World University Rankings, with universities ranked in the top 20% (top quintile) showing slightly higher rates of augmentation relative to automation compared to bottom quintile institutions. The analysis includes universities with at least 30 student responses and excludes observations with missing usage data.\r\n\r\nFigure A5:Student Beliefs about AI Usage versus Actual Usage under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure compares students’ beliefs about AI usage with actual usage rates under different AI policies. For each policy type, blue bars show the percent of students who report being “Neutral,” “Likely,” or “Very Likely” to use AI, while red bars show students’ mean beliefs about what percent of their peers use AI in classes with that policy. The sample includes all survey respondents for both actual usage and beliefs measures. “AI Use is Unrestricted” refers to classes with no restrictions on AI use, “No Explicit AI Policy” refers to classes without a stated policy, and “AI Use is Prohibited” refers to classes that ban AI use entirely.\r\n\r\nFigure A6:Relationship Between Beliefs About AI Usage and Actual AI Usage\r\nRefer to caption\\justify\r\nNotes: This figure shows the relationship between students’ beliefs about AI usage among their peers and actual AI usage rates across different demographic groups. Each point represents a different group of students (by demographics, academic characteristics, or field of study). The \r\ny\r\n-axis shows the percent of students in each group who report using AI for academic purposes. The \r\nx\r\n-axis shows the mean belief within each group about what percent of Middlebury students use AI. The dashed line shows the linear fit. Sample includes all survey respondents with at least ten observations per group. Students’ beliefs about AI usage are positively correlated with actual usage patterns, suggesting that students have relatively accurate perceptions of AI adoption among their peers.\r\n\r\nGPA > p50 80.3 26.3 17.4 23.2 13.4 GPA < p50 87.1 18.7 26.3 32.2 9.9 Freshman 81.3 24.3 24.1 24.7 8.1 Sophomore 84.4 25.6 22.6 23.7 12.4 Junior 86.2 21.6 23.1 28.6 12.8 Senior 78.7 21.0 17.4 29.9 10.4\r\n\r\nPanel C. Field of Study    Arts 73.3 21.0 0.0 52.3 0.0 Humanities 74.8 39.3 21.8 12.1 1.7 Languages 57.4 27.0 30.3 0.0 0.0 Literature 48.6 10.0 33.7 5.0 0.0 Natural Sciences 91.1 22.0 21.9 33.8 13.3 Social Sciences 84.6 17.9 20.4 29.1 17.2\r\n\r\n\\justify\r\nNotes: This table presents the percent of students in each demographic group who report using AI at different frequencies during the academic semester. Each cell shows the percent of students within that group. Column 1 reports the total percent who use AI at any frequency. Columns 2 to 5 represent increasing usage frequencies: rarely (1–2 times per semester), occasionally (monthly), frequently (weekly), and very frequently (multiple times per week). Panel A reports percentages by demographic characteristics. Panel B shows percentages by academic characteristics. Panel C presents percentages by field of study.\r\n\r\n0.9730.1350.0770.1100.114 R\r\n−\r\nsquared0.0120.0510.0270.0240.138 N (Students)516516516516516 \\justifyNotes: This table assesses the relationship between AI model adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI model usage (columns 1-4) or payment for AI services (column 5), and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division.\r\n\r\nEach column presents results for a different model or payment outcome. Column 1 shows usage of OpenAI’s ChatGPT, column 2 Google Gemini, column 3 Microsoft Copilot, column 4 any other AI model, and column 5 whether the student pays for any generative AI service.\r\n\r\nThe omitted categories are Natural Sciences for academic division, white students for race/ethnicity, and freshmen for cohort. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.7510.6760.6610.2710.479 R\r\n−\r\nsquared0.0870.1130.0760.0680.041 N (Students)599599599599599 \\justifyNotes: This table examines how student characteristics relate to self-reported likelihood of using generative AI under different policy scenarios. Each column presents results for different policy scenarios. In columns 1-4, the dependent variable equals one if the student reports being“neutral,” “likely” or “extremely likely” to use AI under the specified policy, and zero if they report being “unlikely,” or “extremely unlikely.” Column 5 represents the impact of moving from unrestricted use to complete prohibition.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.5760.6850.3760.4890.564 R\r\n−\r\nsquared0.1020.1190.0840.0370.087 N (Students)561565543560560 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. Each column presents results for beliefs about different academic outcomes: learning ability (e.g., ability to grasp concepts, retain information, or learn new skills) in column 1, understanding of course materials in column 2, course grades in column 3, ability to complete assignments on time in column 4, and time spent on academics in column 5. The dependent variable in each regression equals one if the student believes AI somewhat improves” or significantly improves” the outcome, and zero if they believe it has no effect, reduces, or significantly reduces the outcome. “Don’t know” responses are excluded.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\nAppendix BEmpirical Appendix\r\nB.1Field of Study Classifications\r\nThis appendix details the classification of majors into broad fields of study used in Figure 1:\r\n\r\n• Natural Sciences: Includes Biology, Biochemistry, Chemistry, Computer Science, Earth and Climate Sciences/Geology, Environmental Studies, Mathematics, Molecular Biology & Biochemistry, Neuroscience, Physics, and Statistics.\r\n• Social Sciences: Includes Anthropology, Economics, Education, Geography, International & Global Studies, International Politics & Economics, Political Science, Psychology, and Sociology.\r\n• Humanities: Includes American Studies, Black Studies, Classics, History, Architectural Studies, Art History & Museum Studies, History of Art & Architecture, Philosophy, Religion, and Classical Studies.\r\n• Literature: Includes Comparative Literature, English/English & American Literatures, and Literary Studies.\r\n• Languages: Includes Arabic, Chinese, French & Francophone Studies, German, Japanese Studies, Russian, and Spanish.\r\n• Arts: Includes Film & Media Culture, Music, Studio Art, and Theatre.\r\nB.2Task-Specific Use of Generative AI\r\nThe aggregate patterns documented in Section 4 may mask important heterogeneity across student groups. Different students may adopt AI tools at varying rates depending on their academic needs and field-specific norms. To investigate this heterogeneity systematically, Appendix Table LABEL:tab:ai_purpose_correlates presents regression estimates examining how student characteristics predict AI usage for each of our ten measured academic tasks.\r\n\r\nUsage patterns differ markedly by student characteristics. Male students consistently exhibit higher adoption rates across most applications, with particularly pronounced differences for finding information (16.4 pp; \r\np\r\n<\r\n0.01\r\n), summarizing texts (12.5 pp; \r\np\r\n<\r\n0.01\r\n), and creating images (16.9 pp; \r\np\r\n<\r\n0.01\r\n). Black students show substantially higher usage for information-gathering tasks, being 23.8 pp more likely to use AI for finding information compared to white students (\r\np\r\n<\r\n0.05\r\n). They also exhibit dramatically higher adoption of writing assistance tools, with 27.6 pp higher usage for editing text (\r\np\r\n<\r\n0.05\r\n) and 23.6 pp for writing emails (\r\np\r\n<\r\n0.10\r\n). Latino students demonstrate significantly higher usage for generating ideas (24.0 pp; \r\np\r\n<\r\n0.01\r\n) and writing emails (20.7 pp; \r\np\r\n<\r\n0.01\r\n), while Asian students show markedly higher adoption for writing emails (24.5 pp; \r\np\r\n<\r\n0.01\r\n) and explaining concepts (10.9 pp, \r\np\r\n<\r\n0.05\r\n). Students from public high schools report lower usage rates for several applications, particularly for concept explanation (7.3 pp lower; \r\np\r\n<\r\n0.10\r\n) and writing emails (7.4 pp lower; \r\np\r\n<\r\n0.10\r\n), but higher usage for proofreading (8.7 pp higher; \r\np\r\n<\r\n0.10\r\n).\r\n\r\nField of study emerges as a particularly strong predictor of usage patterns. Arts majors show significantly lower adoption across multiple tasks compared to Natural Science students, with gaps of 49.7 pp for finding information (\r\np\r\n<\r\n0.01\r\n), 44.0 pp for generating ideas (\r\np\r\n<\r\n0.05\r\n), and 62.1 pp for coding assistance (\r\np\r\n<\r\n0.01\r\n). Similarly, humanities majors are 36.8 pp less likely to use AI for generating ideas (\r\np\r\n<\r\n0.01\r\n) and 30.8 pp less likely for coding assistance (\r\np\r\n<\r\n0.01\r\n). Languages majors exhibit the most pronounced differences, being 52.4 pp less likely to use AI for summarizing texts (\r\np\r\n<\r\n0.01\r\n) and 46.9 pp less likely for generating ideas (\r\np\r\n<\r\n0.01\r\n). By contrast, social science students show higher usage for explaining concepts (8.0 pp; \r\np\r\n<\r\n0.10\r\n) and summarizing texts (14.7 pp; \r\np\r\n<\r\n0.01\r\n).\r\n\r\n0.8030.7400.6310.6190.5410.4730.3710.3440.2350.204 R\r\n−\r\nsquared0.0780.0870.0770.1160.0280.0540.0830.1350.0400.094 N (Students)515515515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and use of AI for specific academic tasks. Each column shows the result for a different academic task. The omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Regressions are weighted and use heteroskedasticity-robust standard errors clustered at the student level. ∗∗∗, ∗∗, and ∗ indicate significance at the 1%, 5%, and 10% levels, respectively.\r\n\r\nAppendix CQualitative Evidence on Student Perspectives on AI Use\r\nIn this section, we analyze student responses to an open-ended question about their use of generative AI. The survey question asked: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” This was an optional question that 48.3 percent of survey respondents answered. Appendix Figure C1 presents a word cloud of the most frequent words in student responses.\r\n\r\nC.1Validating the Open-Ended Response Measure\r\nWe begin by validating our open-ended response measure to show that it contains meaningful signal. To validate this measure, we employ sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner), a widely-used lexicon and rule-based sentiment analysis tool (Hutto and Gilbert, , 2014). We calculate sentiment scores for each response and examine whether these scores correlate with students’ actual AI adoption behaviors. Intuitively, students who express more positive sentiment toward AI in their open-ended responses should be more likely to have adopted AI tools in practice. We test this hypothesis by examining two measures of AI adoption: whether students have ever used generative AI and whether they currently use AI for academic purposes. Appendix Figure C2 presents binned scatterplots showing the relationship between AI adoption rates and standardized sentiment scores.\r\n\r\nSentiment towards generative AI strongly predicts AI adoption. For both outcomes, students with negative sentiment scores (below zero) show substantially lower adoption rates—around 70-85 percent for ever using AI and 70-75 percent for academic use. In contrast, students with positive sentiment scores display markedly higher adoption rates, reaching nearly 100 percent for general use and 95-100 percent for academic purposes among those with the most positive sentiment. The relationship is particularly pronounced for academic AI use (Panel B), where the coefficient of \r\nβ\r\n^\r\n=\r\n0.168\r\n is twice as large as for general adoption.17 These systematic patterns confirm that our open-ended responses capture meaningful variation in student attitudes that corresponds to real behavioral differences.\r\n\r\nC.2How Students use Generative AI\r\nTo analyze these responses systematically, we classified each response using keywords based on their content. For example, if a student mentioned using AI to save time, we tagged the response with the keyword “time-saver.” If a student expressed concerns about AI’s impact on learning, we tagged it with “negative learning.” Responses could receive multiple keywords if they discussed several themes. Appendix Figure C3 shows the frequency of keywords in our classification. The responses reveal how students use AI tools, what motivates this use, and what causes some to avoid or limit their use.\r\n\r\nThe most common use of AI is as an explanatory tool. Nearly 30 percent of responses mentioned using AI to understand course material. Students frequently ask AI to break down complex concepts from readings and lectures, particularly when they find the material difficult to understand. For example, one student reported: “I can ask AI to explain concepts to me that I have a hard time grasping. […] I can keep asking ’simplify’ or ’break down even more.”’ Students also use AI to summarize dense academic readings, which they argue helps them manage heavy reading loads.\r\n\r\nStudents employ AI throughout different stages of the writing process. Some use AI to generate initial drafts that serve as starting points. One student explained: “Helps me get started with a base for most of my essays. It feels easier to edit something already written and make it my own than to write from scratch.” Others use AI more narrowly for brainstorming when stuck on specific problems. As one student noted: “I use it if I am feeling stuck to push me to the right direction (whether a mathematical problem or an essay idea).” Many also report using AI as an editing tool to improve grammar, sentence structure, and overall writing flow. This is especially the case for non-native English speakers. As one student explained: “English is not my first language and it frustrates me sometimes that I cannot find the best way to phrase a certain idea and AI is a useful tool to have to find alternate expressions.”\r\n\r\nStudents frequently mentioned using AI for specific academic tasks. In courses that require coding, students often use AI for debugging code and understanding programming concepts. Students also employ AI for administrative tasks like formatting citations and drafting routine emails. Finally, many students use AI as an enhanced search engine. One student reported: “It has significantly reduced the time it takes to conduct research on new topics and ideas, and helps me by giving me a thorough selection of sources to use for projects of any kind.”\r\n\r\nC.3Why Students Adopt AI\r\nTime savings was the most commonly cited reason for using AI. Nearly 30 percent of responses mentioned using AI to complete work more efficiently. Students often viewed AI as a way to manage demanding course loads. Many students particularly embrace AI assistance for tasks viewed as mechanical or administrative. A student noted they use AI for “Writing emails quick and creating resume/ cover letter templates.” But AI assistance goes beyond grunt work. Some students use it to “spend less time doing assignments and homework.” This is particularly true if students don’t view the work as central to their academic experience. As one student explained: “when I come across work I deem as ineffective, I want to spend as little time as possible doing it.”\r\n\r\nHaving an on-demand tutor for academic support was another key motivation. One-quarter of responses described using AI as an “explainer” when other resources were unavailable or inconvenient. As one student noted, “I use it as a last resort (if there are no office hours, after looking up videos, etc.) if I need extra help. I’d like to think that the way I use it is similar to going to office hours or TA hours.”\r\n\r\nPeer influence also drove AI adoption. Some students reported feeling pressure to use AI to remain competitive with their classmates. One student explained: “I noticed others use it, are getting better grades than me, and they say they learn better with the help of AI, so I gave it a try.” Others worried about being at a competitive disadvantage: “Other people were using it and told me about it. I felt like I would be at a disadvantage if I wasn’t also using it.”\r\n\r\nC.4Concerns and Limitations\r\nStudents expressed several concerns about AI use in academic work. The most frequent worry was about negative impacts on learning. One student who initially used AI extensively reported: “In the past, I have simply plugged and chugged homework assignments into ChatGPT and submitted it. Those assignments feedback from teachers was positive and I was getting good grades, but I definitely felt that my own learning outcomes to be significantly worse.” Other students viewed AI use as fundamentally incompatible with their educational goals. As one humanities student explained: “my task is as a humanities student is to think, not calculate; why should I let AI do the thinking for me? It would defeat the purpose of pursuing my education.”\r\n\r\nMany students described ethical concerns about AI use. Responses suggested uncertainty about appropriate boundaries. One student noted: “I never use it to explicitly write something because that feels like overt cheating, but sometimes I hesitate when it completely solves Econ problems. I understand how it does it, and it helps me to learn, but it still sometimes feels a little morally gray.” Another expressed similar ambivalence: “I tend to only use it when [it] will save me time in a moral way.”\r\n\r\nStudents also emphasized the importance of maintaining ownership of their work. Many expressed pride in producing original work and hesitation about diluting that ownership through AI use. As one student explained: “I don’t have interest in using generative AI for my academic work because I want my work to reflect my own ideas.” Another noted: “It usually would not even occur to me to turn to AI to substitute writing because I want to take credit for my work, and using AI seems to diminish that.”\r\n\r\nTechnical limitations deterred some students from using AI tools. Students reported concerns about inaccurate outputs (“hallucinations”) and poor output quality, particularly for creative writing or complex analytical tasks. For example, one student noted that “In my poetry class we were instructed to use it to come up with poems and they were awful, so that kinda turned me away from using it to do my work for me.”\r\n\r\nC.5Discussion\r\nWe conclude with two overarching themes that emerged from the responses.\r\n\r\nFirst, students vary substantially in how they incorporate AI into their academic lives. Crucially, this heterogeneity largely depends on what students perceive as “appropriate” uses of AI. For activities that they perceive as core to their academic journey—like writing essays or solving problems—many students hesitate to use AI. A student articulated this clearly: “Most of my work is writing or reading. If I’m not doing the writing, what is the purpose of me taking the class?” Yet, students draw different boundaries between central tasks and grunt work. Some use AI extensively, viewing their role as akin to a manager that provides high-level direction while AI handles implementation. Others restrict AI use to specific tasks like brainstorming, editing, or drafting emails. Still others avoid AI entirely for academic work, often for ethical reasons. Even among AI users, adoption patterns reflect individual trade-offs between time savings, learning goals, and academic integrity concerns.\r\n\r\nSecond, there appears to be a fundamental tension between efficiency and learning. The time-saving benefits are easy to observe, quantifiable, and tangible. But these time savings are unlikely to be a free lunch. Some benefits may come at the cost of spending less time with material that requires deeper engagement to digest. As one student noted: “There may be a negative effect in that it eliminates much of the ’struggle’ in learning.” Yet, having an on-demand tutor that explains concepts in relatable ways can also improve learning. As one student explained: “It can explain concepts to me in a way that is tailored to my learning style.” This suggests that the impact of AI use on learning outcomes depends not on whether students use AI—almost all do, to some extent—but rather on how they employ these tools.\r\n\r\nFigure C1:Word Cloud of Student Motivations for Generative AI Use\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 147 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?’\r\n\r\nFigure C2:Relationship Between AI Sentiment and AI Adoption\r\nPanel A. Ever Used Generative AI\r\nRefer to caption\r\nPanel B. Uses AI for Academic Purposes\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents the relationship between AI sentiment and AI adoption. Panel A shows the proportion of respondents who have ever used generative AI, while Panel B shows the proportion who use AI for academic purposes. Each point represents the mean adoption rate for respondents within sentiment score bins of width 0.2. Sentiment scores are standardized compound scores computed using Hutto and Gilbert, (2014)’s VADER algorithm applied to responses to an open-ended question about generative AI. Positive values indicate positive sentiment and negative values indicate negative sentiment. The dashed lines show OLS best-fit lines estimated on the microdata, with coefficients and standard errors (in parentheses) displayed in the top-left corner of each panel.\r\n\r\nFigure C3:Frequency of Keywords in Student Motivations for AI Use\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to AI use. The responses come from the question “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” Color coding indicates the category of each theme. Usage type refers to how students use AI tools. Motivation captures what drove students to try AI. Concerns include mentions of course policies and academic integrity, individual reservations about AI use, worries about AI’s impact on education, and AI’s technical limitations.\r\n\r\nAppendix DQualitative Evidence on Student Views of AI Policies\r\nIn this section, we analyze student responses to an open-ended question about Middlebury’s AI policies. The survey asked: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Appendix Figure D1 presents a word cloud of the most frequent words in student responses. To analyze these responses systematically, we classified each response using keywords based on their content. Appendix Figure D2 shows the frequency of keywords in our classification.\r\n\r\nD.1Polarized Views on Generative AI Policy Approaches\r\nStudents expressed markedly different views about appropriate AI policies, revealing fundamental disagreement about the path forward. Some strongly advocated for embracing AI technology. As one student argued, “The tool is there, there is supply and there is demand. Don’t fight another war on drugs. Don’t live in a fake reality”. Others called for significant restrictions, arguing that “the use of generative AI is dishonest and corrosive” and that it “prohibits these organic processes and divorces students from true learning”.\r\n\r\nHowever, the most common position advocated for a balanced approach that would allow beneficial uses while restricting harmful ones. Students distinguished between uses that enhance learning (like concept explanation) and those that substitute for learning (like generating entire essays). One student articulated this nuanced view particularly well: “AI also can really be helpful at explaining a textbook problem that doesn’t make sense, or guiding slightly with homework, or creating study materials, or editing/tightening up your prose. All of those things are good, and universities should figure out how to maximize AI use for those reasons and to minimize students just feeding their problem sets into ChatGPT”\r\n\r\nA recurring theme was the futility of blanket bans. Many students emphasized that prohibition would be ineffective, with one noting “I don’t think anyone really cares what the policy of any given class is. If professors want people to not use it, they need to structure assessments in a way that will discourage use.” Another compared AI bans to restricting internet use, arguing “AI policies seem to be totally irrelevant. It’s like telling people they can’t use the internet as a resource for the class.” This ineffectiveness of bans creates fairness concerns. As one student explained: “I think if it is banned in a class, that should be enforced (and right now it absolutely is not)… As with any form of cheating, those who don’t cheat are put at a disadvantage.” Another student expressed similar frustration: “I find it discouraging when I hear classmates saying they use AI for things such as essays when they use it in dishonest ways.”\r\n\r\nD.2Need for Clear Guidelines\r\nThe most frequently expressed concern was the need for clear guidelines about generative AI use. Several students reported confusion about what constitutes acceptable use, with one noting “I think it should be more clear whether we can use it and how and how to cite it since most professors rarely mention it at all”. Other students emphasized the importance of professors explaining their policies upfront and their rationale, with one stating “I think that Professor’s should be very specific about what is allowed and their reasoning behind their policy”.\r\n\r\nMany students advocated for standardization across classes, observing that “Sometimes its confusing when one class allows it and another doesn’t and the other encourages it and so on so if there was a school wide or department wide policy that could help”. Yet there is also disagreement regarding standardization. Some students preferred leaving the decision to individual professors, arguing that “GenAI is more effective in some classes/majors than others. Making sure professors understand how students use GenAI and how useful GenAI is in their class (given the course structure, nature of assignments/material, etc.) is very important for the class policy.”\r\n\r\nD.3Training and Support Services\r\nStudents strongly emphasized the need for training in appropriate generative AI use. Many suggested that the college should provide guidance on using AI tools effectively while maintaining academic integrity. One student proposed “a workshop that teaches you to effectively use GenAI without violating the honor code”. Students also expressed interest in learning how to leverage AI to enhance their learning experience rather than circumvent it. As one student explained: “I think it could be useful to develop some sort of training. How do we use AI in a way that actually benefits our learning? I tried out some things on my own but I feel that I need more guidance.”\r\n\r\nThis desire for training was often linked to workplace preparedness. Students recognized that AI proficiency would be valuable in their careers, with one noting “As the world uses more and more AI, I think it is an important tool that students should know how to leverage”. Another emphasized: “The moment us students leave campus, we will be using it in the professional world, and when used in combination with one’s own skills, it is merely a tool to maximize efficiency”.\r\n\r\nD.4Discussion\r\nThe responses reveal several key tensions in AI policy implementation. First, students express a desire to use AI in ways that enhance rather than substitute for learning, yet they recognize that blanket bans are ineffective and worry about being disadvantaged if they follow restrictions while others do not.\r\n\r\nSecond, while students desire clear guidelines, they also want flexibility to accommodate legitimate uses that vary by discipline and assignment type. Different courses and majors may find different AI uses appropriate based on their learning objectives and assessment types.\r\n\r\nThird, there is tension between faculty autonomy in setting course policies and students’ desire for consistent institutional standards. While some support letting professors determine appropriate AI use for their specific courses, others argue that varying policies across classes create confusion and enforcement challenges.\r\n\r\nFinally, students report significant variation in faculty attitudes toward AI. Some students perceived faculty fear or misunderstanding of AI tools, noting that categorical bans often reflect a lack of understanding about AI’s capabilities and limitations rather than pedagogical considerations.\r\n\r\nFigure D1:Word Cloud of Student Feedback on Generative AI Policies\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 133 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?”\r\n\r\nFigure D2:Frequency of Keywords in Student Feedback on AI Policies\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to Middlebury’s AI policies. The responses come from the question “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Color coding indicates the category of each theme. Policy Approach captures suggestions about how AI should be regulated at the college. Adoption Views reflect positions on whether and how AI should be integrated into academic work. Training Needs indicates requests for guidance and support. Other Concerns include issues of workplace relevance and fairness.\r\n\r\nSee pages 1 of results/ai-survey-midd.pdf See pages 2- of results//ai-survey-midd.pdf",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== story.txt ===\nI remember the moment that sparked this project. It was during a walk across Middlebury’s campus late one fall afternoon—just after office hours. A student had come in, clearly bright but struggling, and told me they’d been using ChatGPT late at night to help with assignments because they were too anxious to speak up in class. That confession stuck with me.\r\n\r\nLater, over coffee with Germán, we started wondering—how many students were turning to generative AI as a quiet lifeline? Not to cheat, but to learn? That simple question became the seed for this entire study. What we found wasn’t just statistics. It was a story of adaptation, of students reshaping how they learn, and of institutions racing to catch up.\"\n\n==================================================\n\n=== Generative AI in Higher Education E.txt ===\nGenerative AI in Higher Education: Evidence from an Elite College†\r\nZara Contractor\r\nGermán Reyes\r\nAbstract\r\nGenerative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT’s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI’s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI’s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\r\n\r\n1Introduction\r\nThe rapid advancement of generative artificial intelligence (AI) is transforming higher education at an unprecedented pace. The launch of ChatGPT and similar tools has introduced technologies capable of performing tasks central to academic assessment and learning—writing essays, solving complex problems, and explaining intricate concepts—instantly and at near-zero marginal cost. Yet systematic evidence on the nature and implications of this new technology remains scarce. How widespread is generative AI adoption among students, and what factors drive it? Do students primarily use AI to augment their learning or to automate coursework, potentially harming human capital development? Could disparities in access to premium AI resources amplify existing educational inequalities?\r\n\r\nThese questions are at the heart of ongoing debates about the role of AI in education, which have relied heavily on anecdotal evidence and speculation rather than rigorous evidence. As a result, universities have implemented policies that vary dramatically, from outright bans to permissive adoption, often without clear evidence on their effectiveness or unintended consequences (Nolan, , 2023; Xiao et al., , 2023; McDonald et al., , 2025).\r\n\r\nThis paper addresses this gap by systematically examining generative AI adoption at Middlebury College, a highly selective liberal arts college in Vermont. With approximately 2,800 undergraduate students, Middlebury offers 49 majors across the arts, humanities, languages, social sciences, and natural sciences. Over the past decade, Middlebury has consistently ranked among the top 10 liberal arts colleges in U.S. News & World Report, with an average ranking of 8.5 between 2015 and 2025. The college’s selectivity is comparable to that of many elite research universities—Middlebury’s acceptance rate of approximately 10 percent is similar to that of Boston University (11 percent), Georgetown University (13 percent), and the University of Virginia (16 percent).\r\n\r\nOur analysis draws on survey data collected from the student population between December 2024 and February 2025. The survey collected detailed information about AI usage, including the frequency and purpose of use, perceived impacts, and responses to institutional policies. To minimize selection bias, we framed the survey broadly as examining technology use and provided incentives for participation, achieving a 22.9 percent response rate with 634 responses representing a broad cross-section of the student population.\r\n\r\nWe document five main findings. First, generative AI is approaching near-universal adoption at an unprecedented speed. Over 80 percent of students use AI for academic purposes, up from less than 10 percent before Spring 2023. This represents one of the fastest technology adoption episodes ever documented, dramatically exceeding the 40 percent adoption rate among U.S. workers (Bick et al., , 2025) and the 23 percent among all U.S. adults (McClain, , 2024). These levels are consistent with international findings of 50–70 percent adoption in university contexts (Nam, , 2023; Stöhr et al., , 2024; Ravšelj et al., , 2025).\r\n\r\nSecond, AI adoption is markedly unequal across academic disciplines and demographic groups. Field of study is the strongest predictor of adoption, likely reflecting how well AI capabilities align with the academic tasks required across fields. Adoption ranges from 91.1 percent in Natural Sciences majors (including mathematics and computer science) to significantly lower levels in Literature (48.6 percent) and Languages (57.4 percent). Additionally, demographic disparities exist. For example, males adopt AI at higher rates than females (88.7 versus 78.4 percent), a pattern that is consistent with documented AI gender gaps in other contexts (Otis et al., , 2024). Most notably, lower-achieving students have a higher adoption rate than their higher-achieving peers (87.1 versus 80.3 percent). As a result, AI could serve as an equalizing force if it enhances student learning—helping struggling students catch up to their peers—but could also widen achievement gaps if AI undermines skill development.\r\n\r\nThird, generative AI transforms the students’ learning production function by both augmenting student capabilities and automating academic tasks. We classify academic tasks as augmentation when AI enhances human capabilities while maintaining student engagement (e.g., explaining concepts, proofreading) versus automation when AI directly produces outputs with minimal cognitive involvement (e.g., writing essays, creating images), and average usage rates across tasks within each category. We find that 61.2 percent of AI users employ these tool for augmentation purposes, while 41.9 percent use it for automation. Qualitative evidence reinforces these patterns: students describe AI as an “on-demand tutor” for augmentation purposes, particularly valuable when traditional resources like office hours are unavailable, while automation use centers on time savings during periods of overwhelming workload. These self-reported patterns align closely with actual usage data from Claude conversation logs (Handa et al., 2025a,).\r\n\r\nFourth, institutional policies can significantly influence AI adoption, though their effectiveness can be undermined by informational frictions. We find that explicit prohibitions dramatically reduce self-reported intended use 39 percentage points (pp). Importantly, the effects of this policy vary substantially across student groups, with females showing larger reductions in usage under prohibition than males, suggesting that one-size-fits-all policies may inadvertently create disparate impacts across groups. Moreover, we also document substantial informational gaps about institutional AI policies. Many students (19.2 percent) do not understand AI policy rules, few know about institution-provided premium AI resources (10.1 percent) or proper AI citation practices (32.6 percent)—a skill that is necessary for academic integrity.\r\n\r\nFifth, students believe that AI has a positive learning impact and these beliefs strongly predict their own AI usage. Most students believe that AI improves their understanding of course materials (70.2 percent) and learning ability (60.1 percent), though fewer believe it improves grades (41.1 percent). Perhaps unsurprisingly, students who believe that AI improves their academic outcomes are significantly more likely to use these tools. For example, a ten-percentage-point increase in the belief that AI improves learning corresponds to a 4.9 percentage-point increase in adoption, highlighting how perceptions can affect technology diffusion in educational settings.\r\n\r\nTaken together, our findings reveal that generative AI has already reshaped students’ college experience. The substantial information gaps that we document—regarding both permitted uses and available resources—suggest straightforward opportunities for policy intervention. Yet the observed heterogeneity across disciplines and student groups indicates that uniform policies risk unintended consequences: blanket prohibitions may disproportionately disadvantage students who benefit most from AI augmentation, while unrestricted use may encourage automation practices detrimental to skill development. Thus, effective policy requires distinguishing clearly between AI uses that enhance learning and those that undermine it.\r\n\r\nWhile our findings are specific to the population we study—students at an elite liberal arts college—this setting offers valuable into AI’s broader societal impact. The rapid uptake of generative AI in higher education may reflect a generalizable principle regarding the diffusion of generative AI: adoption may be fastest in contexts where AI consolidates fragmented tools into a unified platform. Students have long had access to various tools for the academic tasks for which they now use generative AI.1 What distinguishes AI is its ability to provide all these services through a single, instantly accessible interface at virtually zero marginal cost. This consolidation likely drives students’ perception of enhanced learning efficiency and contributes to the fast adoption. This principle may extend well beyond education: industries with fragmented, specialized tools could experience similar trajectories of rapid AI adoption, even if AI itself does not introduce fundamentally new capabilities.\r\n\r\nOur findings contribute to a rapidly growing literature examining the adoption and impacts of generative AI. Recent work has focused on AI’s effects on worker productivity (Noy and Zhang, , 2023; Dell’Acqua et al., , 2023; Peng et al., , 2023; Cui et al., , 2024; Brynjolfsson et al., , 2025) and its potential to transform occupations (Felten et al., , 2021, 2023; Eloundou et al., , 2024). While several papers document AI adoption in workplace settings (Bick et al., , 2025; Hartley et al., , 2025; Humlum and Vestergaard, , 2025) and at the firm level (McElheran et al., , 2023; Bonney et al., , 2024; Kharazian, , 2025), we examine adoption in higher education—a critical setting where future high-skilled workers develop human capital.\r\n\r\nWe add to an emerging literature examining generative AI in education. Some studies focus on the learning impacts of generative AI, finding mixed impacts depending on the setting (e.g., Lehmann et al., , 2024; Bastani et al., , 2025). A complementary strand of the literature documents adoption patterns across diverse educational contexts, including universities in Australia (Kelly et al., , 2023), Ghana (Bonsu and Baffour-Koduah, , 2023), Norway (Carvajal et al., , 2024), Sweden (Stöhr et al., , 2024), and multi-country studies (Ravšelj et al., , 2025). To the best of our knowledge, this paper provides the first systematic evidence on generative AI adoption in U.S. higher education. Importantly, we focus specifically on how students use AI for academic purposes—unlike studies that measure more general usage including personal lives (e.g., Ravšelj et al., , 2025)—and go beyond documenting adoption rates to examine how students integrate AI into their academic workflows, distinguishing between augmentation and automation, the role of student beliefs about AI’s educational impacts, and how institutional policies shape usage patterns.\r\n\r\nOur findings also contribute to the literature on technology diffusion. Unlike the S-shaped adoption curves documented for most technologies (Griliches, , 1957; Rogers, , 1962), we observe extremely rapid adoption of generative AI, with 80 percent uptake within two years of ChatGPT’s release. This pattern contrasts sharply with the adoption trajectories of historical General Purpose Technologies (GPTs) like electricity, which required decades to achieve widespread use (David, , 1990; Bresnahan and Trajtenberg, , 1995). The rapid diffusion we document aligns with evidence that newer technologies diffuse faster than older ones (Comin and Hobijn, 2010a,; Comin and Hobijn, 2010b,). Two factors likely explain AI’s exceptional adoption speed compared to other GPTs. First, unlike electricity or steam power, generative AI requires minimal physical infrastructure—students access it through existing devices—and is available at no cost to consumers, eliminating the financial barriers that typically slow technology adoption. Second, as noted above, AI consolidates multiple specialized tools into a single platform, making its benefits immediately apparent without requiring specialized training or organizational restructuring, unlike previous GPTs (Brynjolfsson and Hitt, , 2000).\r\n\r\n2Data: Novel Student Survey\r\n2.1Recruitment and Structure\r\nWe conducted the survey from December 2024 to February 2025. All Middlebury College students were contacted via email and received a reminder a few weeks and two months after the initial invitation. To minimize selection bias, the recruitment materials described the survey broadly as a study on students’ use of technology in their academic and personal lives. To incentivize participation, students who completed the survey qualified for entry into a lottery for Amazon gift cards ranging in value from $50 to $500. The full survey instrument is provided in Appendix LABEL:app:survey.\r\n\r\nThe survey contains three main sections (see Appendix Figure A1 for the survey flow). First, we gather demographic and academic information, including gender, race/ethnicity, type of high school attended (private or public), current academic year, and declared or intended major.2 We also collect data on academic inputs and performance through students’ self-reported typical weekly hours spent studying and their first-year GPA.3\r\n\r\nSecond, we measure students’ experience with generative AI tools. We begin by asking whether students have ever used generative AI tools like ChatGPT or Claude. For those who have, we collect information about their usage patterns, including frequency of use during the academic semester, which specific AI models they use, and whether they pay for AI tools. We also gather data on how students use AI for different academic tasks, including writing assistance, learning support, and coding.\r\n\r\nThe final part of the survey elicits students’ beliefs about generative AI’s adoption and impacts. We ask students about their perceptions of AI’s effects on their academic experience across multiple dimensions, including learning, grades, time management, and understanding of course material. We then collect information on the role of institutional policies, asking students how different policy environments influence their likelihood of using AI. We also elicit students’ beliefs about AI usage among their peers, including their estimates of the fraction of Middlebury students who use AI for schoolwork and leisure. The survey concludes with two open-ended questions that allow students to share their experiences with AI in academic settings and provide feedback on the college’s AI policies and support services.\r\n\r\n2.2Sample and Summary Statistics\r\nOut of Middlebury’s 2,760 enrolled students, 739 began the survey. We exclude 105 respondents who left the survey before reaching the generative AI usage module, leaving us with an analysis sample of 634 students (22.9 percent of the student body). This response rate is comparable to that of similar surveys (Wu et al., , 2022). To make our sample more representative of Middlebury’s student body, we construct poststratification weights based on the distribution of declared majors from administrative records.4 Specifically, we weight observations by the ratio of each major’s share in the student population to its share in our survey responses.5\r\n\r\nTable Figures and Tables presents summary statistics for our sample. Column 1 reports unweighted survey averages, column 2 shows averages after applying poststratification weights, and column 3 provides population benchmarks from administrative records where available. Our unweighted sample consists of 44.6 percent male and 50.8 percent females. The racial/ethnic composition includes 61.8 percent white, 15.5 percent Asian, 9.9 percent Hispanic, and 3.6 percent Black students. The majority of students (54.3 percent) attended a public school, while 42.0 percent attended a private high school. Our sample represents 43 different majors across seven fields of study, with 31.1 percent of respondents not yet having declared a major. In our analysis, we group these undeclared students by their intended field of study as reported in the survey.\r\n\r\nComparing our unweighted sample to administrative records reveals notable differences. Our sample overrepresents white students (61.8 versus 53.8 percent) and Asian students (15.5 versus 7.3 percent), while underrepresenting Black students (3.6 versus 5.2 percent) and Hispanic students (9.9 versus 12.4 percent). First-year students are overrepresented (31.1 versus 25.5 percent), while senior students are underrepresented (21.3 versus 28.7 percent). Our weighting procedure partially addresses these discrepancies—particularly in the distribution of academic fields, where the weighted figures closely approximate administrative records—but some demographic differences persist. Despite these differences, our weighted sample provides reasonably close approximations to the college population on most dimensions, particularly for academic characteristics and field distributions.\r\n\r\n3Generative AI Usage Patterns Among Students\r\n3.1Adoption of Generative AI\r\nGenerative AI has achieved a remarkably high adoption rate among Middlebury College students. Figure 1 presents the distribution of AI usage frequency during the academic semester, categorized into four levels: “Rarely” (a few times per semester), “Occasionally” (a few times per month), “Frequently” (a few times per week), and “Very Frequently” (daily or almost daily). Overall, 82.5 percent of students report using generative AI for academic purposes, with substantial variation in usage intensity: 23.5 percent use it rarely, 22.2 percent occasionally, 26.2 percent frequently, and 10.6 percent very frequently.\r\n\r\nThe adoption rate at Middlebury aligns closely with patterns observed across other higher education institutions. A BestColleges survey of U.S. undergraduate and graduate students in fall 2023 found that 56 percent had used AI on assignments or exams (Nam, , 2023). Ravšelj et al., (2025) surveyed higher education students globally between late 2023 and early 2024, finding that 71.4 percent had ever used ChatGPT—though this figure encompasses all usage rather than academic use specifically. Similarly, Stöhr et al., (2024) document that 63 percent of Swedish university students had used ChatGPT by spring 2023, with 35.4 percent reporting regular use and 27.6 percent rare use. Among Norwegian university students, Carvajal et al., (2024) find that 68.9 percent use AI tools occasionally or more frequently.\r\n\r\nThese adoption rates in higher education far exceed those in the general population and workforce. Pew Research finds that only 23 percent of U.S. adults have ever used ChatGPT (McClain, , 2024), while Gallup reports that just one-third of U.S. workers have used AI for work (Gallup, , 2024). Bick et al., (2025) and Hartley et al., (2025) estimate that about 40 percent of the U.S. working-age population used generative AI for work as of late 2024 and early 2025. Both studies document substantial heterogeneity across industries, with information services showing the highest adoption rates at 56–62 percent—still considerably below the 80 percent adoption rate at Middlebury. Even among workers in AI-exposed occupations, Humlum and Vestergaard, (2025) find adoption rates of only 41 percent.6\r\n\r\n3.2Adoption by Student Characteristics and Field of Study\r\nAI adoption varies considerably across demographic groups and academic disciplines (Figure 1 and Appendix Table LABEL:tab:ai_freq_het). Males report higher usage rates than females (88.7 versus 78.4 percent). Usage patterns differ markedly by race/ethnicity: Black students (92.3 percent) and Asian students (91.3 percent) exhibit the highest adoption rates, while white students (80.2 percent) and Hispanic students (77.9 percent) report lower usage. Students from private high schools use AI more frequently than those from public schools (84.1 versus 80.4 percent). Notably, students with below-median GPAs report higher usage rates than their higher-achieving peers (87.1 versus 80.3 percent). Adoption varies widely by field of study: Natural Sciences leads with 91.1 percent usage, followed by Social Sciences at 84.6 percent, while Languages (57.4 percent) and Literature (48.6 percent) show substantially lower adoption rates.\r\n\r\nTo examine how student characteristics jointly relate to AI adoption, we estimate multivariate OLS regressions that include all observed characteristics simultaneously.7 Table LABEL:tab:ai_usage_correlates presents regression estimates using four usage thresholds as outcomes. Each column represents the probability of meeting progressively higher frequency thresholds: any AI use (column 1), at least monthly use (column 2), at least weekly use (column 3), and daily use (column 4).\r\n\r\nThe regression results confirm the patterns observed in the descriptive statistics. Holding other characteristics constant, males are 10.3 pp more likely than females to use AI (column 1, \r\np\r\n<\r\n0.05\r\n), with this gender gap typically widening at higher usage frequencies (columns 2–4, all \r\np\r\n<\r\n0.01\r\n). Black and Asian students show substantially higher adoption rates than white students, at 11.8 and 10.7 pp respectively (column 1, both \r\np\r\n<\r\n0.01\r\n). Students from public high schools are 3.0 pp less likely to use AI than those from private schools, but this difference is not statistically significant. Field of study emerges as the strongest predictor of adoption. Compared to Natural Sciences majors, students in Literature, Languages, Arts, and Humanities all show lower usage rates, with the differences being statistically significant at high usage frequencies (columns 3 and 4). Social Sciences majors exhibit adoption rates similar in magnitude to Natural Sciences majors across all usage frequency thresholds.\r\n\r\nOur findings on heterogeneity in AI adoption align with patterns documented in other settings. The gender gap in AI adoption at Middlebury—10.3 pp higher for males—is consistent with evidence across multiple studies. A meta-analysis of 18 studies by Otis et al., (2024) finds that males are 10–20 percentage points more likely to use generative AI than females. This gender gap in AI adoption appears in all educational studies (Nam, , 2023; Carvajal et al., , 2024; Stöhr et al., , 2024; Ravšelj et al., , 2025). Our finding that students with below-median GPAs are more likely to use AI aligns with Carvajal et al., (2024), who document higher adoption rates among students with lower admission grades. This pattern of greater adoption among lower-achieving students suggests that AI could narrow achievement gaps if it enhances learning and skill development, but could widen these gaps if it undermines the acquisition of fundamental skills.\r\n\r\nThe differences in adoption rates across academic fields at Middlebury mirror patterns documented in other educational settings. Stöhr et al., (2024) find that technology and engineering students exhibit significantly higher ChatGPT usage compared to students in humanities. Similarly, Nam, (2023) report that 62 percent of business majors and 59 percent of STEM majors have used AI tools for coursework, compared to 52 percent of humanities majors. Ravšelj et al., (2025) document comparable disciplinary differences, with applied sciences students showing substantially higher usage rates than arts and humanities students.\r\n\r\nThese academic differences persist into the workforce. Bick et al., (2025) find stark variation by college major: STEM graduates have the highest AI adoption rates (46.0 percent), followed by Business/Economics graduates (40.0 percent), while Liberal Arts graduates show substantially lower rates (22.4 percent). Humlum and Vestergaard, (2025) document similar patterns by occupation, finding that roles requiring strong writing and technical skills—such as marketing specialists and journalists—have the highest adoption rates. These consistent patterns across educational and professional contexts suggest that field-specific factors, particularly the applicability of AI tools to different types of tasks, shape adoption in systematic ways.\r\n\r\n3.3Timing of Generative AI Adoption\r\nThe speed of technology diffusion is a critical determinant of its economic and social impact (David, , 1990; Hall and Khan, , 2003; Stokey, , 2021). In educational contexts, rapid technology adoption can create or exacerbate inequalities between early and late adopters, particularly if the technology confers significant learning advantages (World Bank, , 2016). To track the timing of AI adoption among Middlebury students, we asked them when they first began using generative AI for academic purposes, with options ranging from “This semester (Fall 2024)” to “Before Spring 2023” (as a reference, ChatGPT’s public launch was in November 2022).\r\n\r\nStudents adopted generative AI at an extraordinary pace. Figure 2 shows that the cumulative adoption rate grew dramatically from less than 10 percent before Spring 2023 to slightly above 80 percent by Fall 2024. The pace of adoption has accelerated over time, likely reflecting improvements in AI capabilities. Among current users, 25.7 percent adopted AI in Fall 2024 alone, compared to 19.9 percent in Spring 2024 and 16.3 percent in Fall 2023.\r\n\r\nThe adoption rate among Middlebury students far exceeds that observed in other populations and represents one of the fastest technology adoption episodes ever documented. For comparison, Bick et al., (2025) show that it took over 20 years for computers to reach an 80 percent adoption rate in the U.S. working-age population, and about 15 years for internet adoption to reach similar levels. Even generative AI adoption in the broader population has been markedly slower: Pew Research found that just 23 percent of U.S. adults had ever used ChatGPT as of February 2024, up from 18 percent in July 2023 (McClain, , 2024). The dramatically faster adoption rate we document—reaching over 80 percent in less than two years—suggests academic settings may uniquely accelerate AI diffusion.\r\n\r\nTo identify early versus late adopters of generative AI, in Table LABEL:tab:ai_adopt_correlates and Appendix Figure A2, we analyze how adoption timing varies across student characteristics. The rate of adoption varies substantially across student characteristics. Male students led adoption, with a 8.9 pp higher probability of using AI before Spring 2023 compared to females (column 1, \r\np\r\n<\r\n0.01\r\n)—a gender gap that persists across all periods (columns 2–5). Black and Asian students also adopted earlier than white students, though these differences reach statistical significance only in later periods (columns 4–5). Field of study is a strong predictor of adoption timing. For example, students in Languages consistently lagged behind Natural and Social Sciences majors, with significantly lower adoption rates across nearly all time periods.\r\n\r\n3.4Choice of Generative AI Models\r\nMajor AI companies operate on a freemium model, where free versions coexist with premium subscriptions that offer higher usage limits and access to more advanced models. This tiered structure creates potential for a new form of educational disparities: if paid versions confer substantial academic advantages, students who cannot afford subscriptions may be systematically disadvantaged. To investigate these potential disparities, we presented respondents with a list of options including both free and paid versions of popular models. We also collected information on monthly subscription expenditures, with response options ranging from no active subscription to spending more than $40 monthly (standard subscriptions to major models like ChatGPT cost $20 per month at the time).\r\n\r\nOpenAI’s ChatGPT dominates AI usage among Middlebury AI users, with the free version capturing the largest market share. Figure 3 shows that 89.3 percent of AI users rely on the free version of ChatGPT, making it dramatically more popular than any alternative. Google Gemini (13.5 percent) and Microsoft Copilot (7.7 percent) are distant competitors, while other platforms each capture less than 5 percent of users. This dominance mirrors patterns in other academic settings, where Stöhr et al., (2024) find substantially higher familiarity with ChatGPT compared to alternative platforms, and aligns with broader workforce trends documented by Bick et al., (2025).8\r\n\r\nDespite near-universal AI adoption, only 11.3 percent of AI users pay for any AI service.9 This figure is remarkably similar to the 8 percent found in Ravšelj et al., (2025)’s multi-country survey. This low payment rate suggests that for most students, the premium features of paid versions—primarily higher usage limits and access to more advanced models—do not justify the subscription cost. However, payment patterns reveal significant disparities: males and Asian students are substantially more likely to purchase AI subscriptions (Appendix Table LABEL:tab:ai_model_correlates and Appendix Figure A3), potentially reflecting differences in usage intensity across demographic groups.\r\n\r\n4Generative AI and the Production of Learning\r\n4.1The Use of Generative AI across Academic Tasks\r\nHow is generative AI transforming the traditional inputs to student learning? The educational production function includes inputs like time spent studying (e.g., Stinebrickner and Stinebrickner, , 2008), faculty instruction (e.g., Fairlie et al., , 2014), peer interactions (e.g., Sacerdote, , 2001), and academic support services (e.g., Angrist et al., , 2009). AI tools have the potential to complement or substitute for these traditional inputs. For example, using AI to explain concepts might substitute for faculty office hours, while using it for proofreading might reduce time needed for academic support services.\r\n\r\nTo understand the role of AI in students’ learning production function, we collected information about students’ AI usage across ten common academic tasks: proofreading, generating ideas, writing essays, editing essay drafts, coding assistance, creating images, explaining concepts, composing emails, summarizing materials, and finding information. For each task, students indicated their frequency of AI use on a five-point scale ranging from never to daily. We supplemented this quantitative data with open-ended responses about how AI influences their academic work process.\r\n\r\nStudents use generative AI for a wide range of academic tasks, with the highest adoption rates for learning support and text-processing activities. Figure 4, Panel A shows that explaining concepts is the most common use case, with 80.3 percent of AI users using it for this task. Summarizing texts follows as the second most common task (74.0 percent), followed by finding information and generating ideas (63.1 and 61.9 percent). Writing assistance tasks like proofreading and editing essays are also common, used by 54.1 and 47.3 percent of AI users, respectively. Technical applications like coding help are significant (34.4 percent), considering that many academic degrees involve no programming. Notably, while 23.5 percent of AI users report using it for writing essays, this represents a relatively low adoption rate compared to other academic uses, suggesting students may be more hesitant to use AI for primary content creation. The lowest adoption rate is for creating images at 20.4 percent, likely reflecting fewer academic use cases for this capability.\r\n\r\nAn important limitation of our survey is that it relies on self-reported usage, which may introduce measurement error. For example, students might underreport uses they perceive as academically inappropriate—such as essay writing—while overreporting those viewed as legitimate learning tools (Ling and Imas, , 2025). To assess the validity of our self-reported measures, we compare our findings with Handa et al., 2025a, who analyze actual Claude usage patterns among users with university email addresses. Several caveats apply: their data captures conversation-level interactions rather than student-level usage, and most students in our sample use ChatGPT rather than Claude (Section 3.4). Despite these limitations, the comparison provides a useful benchmark for evaluating our survey responses.\r\n\r\nReassuringly, our results are consistent with Anthropic’s data. Both studies identify explaining concepts and technical problem-solving as primary use cases. Handa et al., 2025a report that the second largest use case (33.5 percent of conversations) involves “technical explanations or solutions for academic assignments,” while we find that 80.3 percent of AI users use it for explaining concepts—a difference likely attributable to our student-level versus their conversation-level measurement. Similarly, the most common usage category in Claude involves “designing practice questions, editing essays, or summarizing academic material” (39.3 percent of conversations), aligning with our findings that 74.0 percent of AI users use it for summarizing texts and 47.3 percent for editing essays. The disciplinary patterns also converge: Handa et al., 2025a finds that computer science, natural sciences, and mathematics conversations are overrepresented, which mirrors our finding that Natural Science majors show significantly higher AI adoption rates (Figure 1). Overall, these convergent findings from self-reported survey data and actual usage logs suggest that our results capture genuine patterns of student AI engagement rather than merely reflecting social desirability in responses.\r\n\r\n4.2Automation versus Augmentation\r\nAre students using generative AI primarily to augment their learning or to automate their coursework? This distinction is crucial for understanding AI’s impacts (Autor and Thompson, , 2025)—augmentation may enhance students’ learning processes while maintaining their active engagement and critical thinking, whereas automation produces fully-formed outputs that could be submitted with minimal student input, potentially harming learning.\r\n\r\nTo examine this empirically, Figure 4, Panel B categorizes the ten measured tasks based on whether they augment or automate academic tasks. We classify tasks as augmentation when they enhance human capabilities (explaining concepts, finding information, proofreading, and editing drafts) and as automation when they directly produce outputs (writing essays, creating images, composing emails, summarizing texts, generating ideas, and coding assistance). We then calculate the percentage of AI users who employ each category at various frequencies.10\r\n\r\nStudents use generative AI for both augmentation and automation, but with markedly different frequency. While 61.2 percent of AI users report using AI for augmentation tasks, 41.9 percent report using it for automation—thus, there is a substantial 19.3 pp difference. This gap is mainly driven by occasional use (19.7 percent for augmentation versus 12.7 percent for automation) and frequent use (16.3 versus 9.0 percent). Most strikingly, students are more than twice as likely to use AI daily for augmentation (5.0 percent) compared to automation (2.5 percent). The higher frequency of augmentation use also suggests that students find augmentation uses more valuable for their day-to-day academic activities. Our findings align closely with patterns observed in actual AI usage data: Handa et al., 2025b analyze real conversation logs from Claude and find that 57 percent of workplace AI interactions involve augmentation while 43 percent involve automation.\r\n\r\nQualitative evidence from open-ended responses provides additional insights into students’ motivations for using AI (see Appendix C for additional results and validation of the open-ended measure). Students’ descriptions align closely with the augmentation-automation framework. For augmentation, many characterize AI as an “on-demand tutor,” particularly valuable when traditional resources like office hours are unavailable. Non-native English speakers frequently mention using AI for proofreading to overcome language barriers, while students from technical majors describe using it to debug code and understand error messages. For automation, time savings emerged as the dominant motivation, with 21.7 percent of open-ended responses explicitly mentioning efficiency benefits (Appendix Figure C3). Students describe turning to AI during periods of overwhelming workload or looming deadlines, using it to generate initial drafts or complete routine assignments. That students automate tasks mostly when time-pressed helps explain why automation is less common than augmentation.11\r\n\r\nOur finding that students favor augmentation over automation extends beyond Middlebury. In Appendix Figure A4, we re-analyze data from Ravšelj et al., (2025)—a multi-country survey of higher education students—to examine how this balance varies across institutional quality.12 We classify universities into quintiles based on their Times Higher Education World University Rankings. Overall, students who use AI worldwide show similar adoption rates for augmentation (64.6 percent) and automation (63.1 percent) tasks—a much smaller gap than at Middlebury (Panel A). However, this aggregate pattern masks heterogeneity by institutional quality. Students at top-quintile universities show a modest preference for augmentation over automation, with this gap narrowing monotonically down the institution quality distribution. By the bottom quintile, augmentation and automation usage are virtually identical (Panel B).\r\n\r\n4.3Heterogeneity in Augmentation versus Automation Usage\r\nUnderstanding whether augmentation and automation patterns vary across student populations may help to design targeted support policies. To assess this, we construct four measures of augmentation and automation usage. First, we create binary indicators for whether students use AI for any augmentation or automation task. Second, we calculate the proportion of tasks in each category for which students employ AI. Third, using the Likert-scale responses, we create intensity measures that capture how frequently students use AI for augmentation and automation purposes. Finally, we compute the difference between augmentation and automation variables, as a measure that directly compare students’ relative preference for augmentation versus automation. Table LABEL:tab:ai_autom_regs presents regression estimates using these measures as outcomes.\r\n\r\nThe balance between augmentation and automation varies substantially by student characteristics. Males show higher adoption of both augmentation and automation tasks, with a similar magnitude in each case—they are 6.0 pp more likely to use augmentation prompts and 5.5 pp more likely to use automation prompts compared to females (columns 1 and 4, \r\np\r\n<\r\n0.05\r\n). Black students have a substantially higher augmentation intensity than white students (0.871 points higher, equivalent to 38.1 percent of the outcome mean, \r\np\r\n<\r\n0.01\r\n), but show no statistically significant differences in automation usage. This results in Black students having the strongest preference for augmentation over automation, with a 19.5 pp higher share of augmentation tasks relative to automation (column 7, \r\np\r\n<\r\n0.01\r\n). Asian students show high usage of both kinds relative to white students. Differences across field of study effects are sizable: humanities, languages, and literature majors tend to show lower usage of both augmentation and automation tools compared to natural science majors.\r\n\r\n5Institutional Policies and AI Adoption\r\n5.1The Role of Institutional Policies in Shaping Student Behavior\r\nInstitutional policies are central to shaping the adoption and diffusion of new technologies (Acemoglu, , 2025). In the context of generative AI in higher education, understanding how policies affect student behavior is essential for guiding evidence-based decision-making. To examine this, we asked students to report their likelihood of using AI under various policy scenarios, ranging from complete prohibition to unrestricted use. This analysis is particularly relevant in light of the ongoing debate about how universities should regulate generative AI use in academic settings (Nolan, , 2023), and the widespread variation in institutional policies across colleges (Xiao et al., , 2023).\r\n\r\nInstitutional policies substantially influence students’ reported likelihood of using generative AI. Figure 5 shows that when generative AI use is unrestricted, 52.4 percent of students report being likely or extremely likely to use it. This likelihood decreases modestly when policies require citation (40.9 percent) or when no explicit policy exists (42.3 percent). However, explicit prohibition creates a dramatic shift: only 13.4 percent of students report they would be likely or extremely likely to use AI when it is banned, while 72.9 percent say they would be unlikely or extremely unlikely to do so (note that these figures include both current AI users and students who do not use generative AI for academic purposes). These results suggest that institutional policies can significantly influence generative AI usage patterns, though a small fraction of students report they would likely use AI even when explicitly prohibited.\r\n\r\nThe magnitude of policy effects in our study aligns closely with findings from other contexts. Carvajal et al., (2024) estimate that banning AI reduces usage by 37.2 pp among females and 20.6 pp among males, for an overall drop of about 28.9 pp. In our survey, a ban leads to a 37.8 pp decline in usage, with a larger decrease among females (49.6 pp) than males (40.1 pp). These parallel results underscore how institutional policies can unintentionally produce disparate effects across gender. Notably, these differential policy effects extend beyond gender. Other demographic and academic characteristics—such as race and field of study—also moderate how students respond to policy restrictions (Appendix Table LABEL:tab:ai_policy_het), underscoring that institutional policies can produce non-neutral impacts along multiple dimensions.\r\n\r\n5.2Understanding of Institutional Policies and Resources\r\nInformation gaps and inattention can significantly affect technology adoption decisions (Duflo et al., , 2011; Hanna et al., , 2014). For example, imperfect information about rules, available resources, or proper usage guidelines could lead to underadoption of beneficial technologies or inadvertent policy violations. To test for the existence of information gaps, we examine three dimensions of policy understanding. First, we asked students whether they find AI policies in their current classes clear. Second, we measured awareness of free access to premium AI tools through the college—a resource that could reduce inequality but only if students know it exists. Third, we assessed whether students know how to properly cite AI when required, a mechanical skill necessary for academic integrity. We supplemented these measures with open-ended feedback about the college’s AI policies and support services.\r\n\r\nStudent understanding of institutional policies is high, though significant gaps remain. Figure 6, Panel A shows that most students (79.1 percent) understand when and where they are allowed to use AI in their classes, but a nontrivial minority (19.2 percent) find AI policies unclear. Moreover, critical knowledge gaps persist elsewhere. Only 10.1 percent know they have free access to Microsoft Copilot through the college (Panel B), and just 32.6 percent understand how to properly cite AI use (Panel C). These gaps vary systematically: females show better policy understanding than males (81.5 versus 75.8 percent), and non-white students demonstrate higher awareness across all three dimensions compared to white students.13\r\n\r\nThe qualitative evidence from open-ended responses reinforces these information frictions and reveals implementation challenges (Appendix D). Students express frustration with vague guidelines, requesting specific examples of acceptable versus unacceptable use cases. Many advocate for formal training, noting that simply knowing policies exist differs from understanding how to effectively integrate AI into their workflow. A particularly striking theme is students’ perception of blanket prohibitions as both ineffective and unfair, as they create a prisoner’s dilemma situation where compliant students are disadvantaged relative to those who secretly violate restrictions. Many responses call for a balanced approach—permitting AI use that supports learning while restricting uses that replace it—though the boundary between the two remains contested.\r\n\r\n6Beliefs About AI’s Educational Impact and Peer Usage\r\n6.1Student Beliefs of AI’s Impact on Educational Outcomes\r\nTechnology adoption decisions are shaped by beliefs about potential returns (Foster and Rosenzweig, , 2010). Students’ perceptions of how AI affects their learning may influence whether they adopt these tools and how they integrate them into their academic workflows. To elicit these beliefs, we asked students to evaluate AI’s impact across four dimensions of their academic performance: understanding of course materials, overall learning ability, time management, and course grades. For each dimension, students rated AI’s effect on a five-point scale ranging from “significantly reduces” to “significantly improves.” These subjective assessments provide insight into the perceived value of AI tools from the student perspective, which may differ from their actual effects on learning outcomes.\r\n\r\nStudents tend to believe that generative AI is beneficial for their academic performance, though the perceived benefits vary across dimensions. Figure 7 shows that the majority of students (70.2 percent) believe that generative AI improves their understanding of course materials, and 60.1 percent report improvement in their ability to grasp concepts, retain information, or learn new skills. Similarly, 59.4 percent report that AI improves their ability to complete assignments on time. Notably, while students believe AI helps their learning and assignment completion, they are less confident about its impact on course grades—41.1 percent believe it improves their grades, while 55.4 percent report no effect and 3.5 percent report negative effects. This pattern suggests that while students perceive generative AI as improving their learning process and workflow—through better understanding, skill development, and timely completion of work—these benefits do not necessarily translate into better course grades.14\r\n\r\nBeliefs about AI’s benefits strongly predict adoption. In Figure 8, we plot the relationship between the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis) for different subgroups of students (e.g., males, white students, public-school students, etc.). Across all four measured academic dimensions, there is a strong and statistically significant relationship between perceiving positive AI effects and AI adoption. The relationship is strongest for beliefs about the ability to improve course grades (Panel D): a 10-pp increase in the belief that AI improves grades is associated with a 5.6 pp increase in AI adoption (\r\np\r\n<\r\n0.01\r\n). Similar positive relationships exist for beliefs about learning ability (4.3 pp, \r\np\r\n<\r\n0.05\r\n), understanding of course materials (4.5 pp, \r\np\r\n<\r\n0.01\r\n), and timely assignment completion (5.3 pp, \r\np\r\n<\r\n0.05\r\n). These findings suggest that student beliefs about AI’s academic benefits—irrespective of the actual benefits—may play a crucial role in shaping adoption decisions.\r\n\r\nThese positive beliefs align with findings from other contexts. Ravšelj et al., (2025) report that the majority of students in their sample believe that ChatGPT improves their general knowledge (68.8 percent) and specific knowledge (62.7 percent)—remarkably similar to our finding that most students believe AI improves understanding of course materials (68.5 percent) and learning ability (57.6 percent). Similarly, 57.4 percent of students in Ravšelj et al., (2025)’s sample believe that ChatGPT helps meet assignment deadlines, while 59.4 percent in our data report AI improves timely assignment completion. Stöhr et al., (2024) provide complementary evidence from Swedish universities, where 47.7 percent of students believe that AI makes them more effective learners, yet only 17.3 percent believe these tools improve their grades—mirroring our finding that perceived learning benefits from AI use exceed perceived effects on course performance.\r\n\r\n6.2Student Beliefs About Peer Use of Generative AI\r\nStudents’ beliefs about their peers’ AI usage may influence their own adoption through multiple channels, including social norms (e.g., Giaccherini et al., , 2019), social learning (e.g., Foster and Rosenzweig, , 1995; Beaman et al., , 2021), peer effects (e.g., Bailey et al., , 2022), and competitive pressure to avoid falling behind (e.g., Goehring et al., , 2024). To measure these beliefs, we asked students to estimate what fraction of their peers use generative AI for different purposes and would use under different policy environments. Figure 9 presents the distribution of these beliefs. Panels A-C show students’ estimates of peer AI usage for schoolwork, leisure, and any purpose respectively. Panels D-F display students’ beliefs about AI usage under three policy environments: when classes have no explicit AI policy, when classes allow AI use, and when classes prohibit AI use.\r\n\r\nStudents systematically underestimate their peers’ AI usage. On average, they believe 65.2 percent of peers regularly use generative AI for schoolwork (Panel A), while our survey reveals an actual usage rate of 82.5 percent—a 17.3 pp gap. This underestimation appears consistent across educational contexts: Stöhr et al., (2024) find that only 38.7 percent of Swedish students believe AI chatbot use is common among peers, despite 63 percent actual usage. When we examine beliefs about policy-contingent behavior, students estimate that 62.7 percent of peers use AI when no explicit policy exists (Panel D), rising to 72.2 percent under permissive policies (Panel E) and falling to 43.5 percent under prohibition (Panel F). These beliefs align directionally with self-reported intentions, though with notable magnitude differences (Appendix Figure A5). For instance, while students believe 43.5 percent of their peers use AI in classes that prohibit it, only 28.4 percent report they themselves would be likely to do so.15\r\n\r\nNotably, students’ perceptions of peer AI adoption are closely linked to their own adoption behavior. Appendix Figure A6 shows a strong positive association between beliefs about peer usage and actual usage rates across student groups. This behavior is consistent with several psychological mechanisms, including the “false consensus effect” (Ross et al., , 1977), selection neglect (Jehiel, , 2018), or interpersonal projection bias (Bushong and Gagnon-Bartsch, , 2024). Still, virtually all groups systematically underestimate peer usage—actual usage rates exceed believed usage for every demographic and academic group we examine. This figure also shows that underestimation varies substantially across student characteristics: males use AI at 88.7 percent but believe only 66.6 percent of peers do so (22.1 pp gap), while females show a smaller gap (77.6 percent actual versus 65.1 percent believed, 13.3 pp gap).\r\n\r\nTaken together, students’ systematic underestimation of peer AI usage coupled with the strong relationship between beliefs and adoption suggests that misperceptions about social norms may shape technology diffusion. If AI enhances learning, then correcting these misperceptions through information provision could accelerate beneficial adoption; conversely, if AI undermines skill development, then students’ underestimation of peer usage may serve as an unintentional safeguard against harmful overadoption.\r\n\r\n7Discussion\r\nThis paper presents the first systematic evidence on generative AI adoption at a highly selective U.S. college. Using novel survey data, we document exceptionally rapid and widespread adoption, substantial shifts in the educational production function through augmentation and automation, and the significant roles of students’ beliefs and institutional policies in shaping AI use.\r\n\r\nOur results offer three implications for institutional policy and ongoing debates about AI in education. First, we identify low-cost opportunities to improve institutional policy effectiveness through targeted information provision. The significant gaps we document in students’ understanding of institutional AI policies, citation practices, and available AI resources suggest that simple interventions—such as clear guidelines, illustrative examples of acceptable uses, and AI literacy programs—can reduce unintentional academic integrity violations and support beneficial AI integration. Qualitative feedback strongly indicates student demand for more explicit guidance on responsible AI use.\r\n\r\nSecond, our evidence challenges alarmist narratives that conflate widespread AI adoption with universal academic dishonesty based on anecdotal accounts.16 Although AI use is indeed near-universal, we find clear evidence that students primarily employ AI as a tool for strategic task management: to enhance learning (augmentation) and selectively automate tasks when facing high time opportunity cost—not solely to circumvent academic effort. This distinction matters: by normalizing academic dishonesty as inevitable and universal, these narratives may shift social norms and encourage students who would otherwise use AI responsibly to engage in prohibited behaviors, believing “everyone else is doing it.”\r\n\r\nThird, our findings caution against policy extremes of either blanket prohibition or unrestricted AI use (Merchant, , 2024; McDonald et al., , 2025). Blanket prohibitions disproportionately harm students who benefit most from AI’s augmentation functions—particularly lower-achieving students—while also creating uneven compliance, placing conscientious students at a disadvantage relative to rule-breakers. Conversely, unrestricted AI use based solely on revealed preference arguments ignores important market failures in educational settings. Students often hold overly optimistic beliefs about AI’s learning benefits despite mixed empirical evidence, potentially leading to unintended negative learning outcomes. Most concerningly, permissive policies risk creating competitive dynamics where students feel compelled to adopt AI not for its learning benefits but simply to avoid falling behind in an educational “arms race” (Goehring et al., , 2024).\r\n\r\nFigures and Tables\r\nFigure 1:The Adoption of Generative AI among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the fraction of students who report using AI during the academic semester, categorized by demographic characteristics, high school type, academic cohort, GPA, and field of study. Usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily).\r\n\r\nThe category “All students” provides the baseline usage rate for the full sample. Gender categories are based on self-identification, with non-binary responses excluded due to a small sample size. “Private HS” refers to students who attended private high schools, while “Public HS” includes public institutions. “Cohort” denotes the student’s academic year, ranging from first-year (“Freshman”) to fourth-year and beyond (“Senior”). GPA categories (“GPA \r\n>\r\n p50” and “GPA \r\n<\r\n p50”) split students into groups above or below the median first-year GPA, as self-reported on a 4.0 scale. See Appendix B.1 for the classification of majors into fields of study.\r\n\r\nFigure 2:The Evolution of Generative AI Adoption among Middlebury College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the cumulative percent of students who reported using generative AI tools for academic purposes over time. The data is based on retrospective self-reports collected in our December 2024 survey, where students were asked “When did you first start using any form of Generative AI for academic purposes?” Response options ranged from “Before Spring 2023” to “This semester (Fall 2024).” The \r\nx\r\n-axis represents academic semesters, while the \r\ny\r\naxis represents the cumulative adoption rate. Vertical lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure 3:Adoption of Generative AI Models Among College Students\r\nRefer to caption\\justify\r\nNotes: This figure shows the adoption rates of various AI models as of Fall 2024. The horizontal axis shows the percent of students who reported using each tool, and the vertical axis lists the tools in descending order of adoption rates.\r\n\r\nFigure 4:Academic Uses of Generative AI\r\nPanel A. Across Common Academic Tasks\r\nRefer to caption\r\nPanel B. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of AI students who use generative AI for different academic tasks. For each task, usage frequency is divided into four levels: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). The number at the end of each bar represents the total percent of students who use AI for that purpose at any frequency. Tasks are ordered by total usage, from highest to lowest. Results are based on responses to the question: “For academic purposes, which of the following tasks do you typically use generative AI for?” Sample includes all students who reported using AI during the academic semester.\r\n\r\nFigure 5:Student Reported Likelihood of Using AI under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of students who report different likelihoods of using AI under various policy scenarios. For each policy, responses are categorized on a five-point scale from “Extremely unlikely to use AI” to “Extremely likely to use AI.” The sample includes all survey respondents. The question asked was: “How likely are you to use generative AI in a class with each of the following AI policies?”\r\n\r\nFigure 6:Understanding of Generative AI Policies and Resources\r\nPanel A. Understanding of AI Policies\r\nRefer to caption\r\nPanel B. Awareness of Copilot Access\r\nRefer to caption\r\nPanel C. Knowledge of Citation Requirements\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows students’ understanding of institutional AI policies and resources. Panel A displays the percent of students who report understanding AI policies in their classes, those who report having no explicit policy, and those who find policies unclear, broken down by demographic characteristics. Panel B shows the percent of students who are aware of their free access to Microsoft Copilot through Middlebury College. Panel C presents the percent of students who report knowing how to properly cite AI use in their academic work when required. For Panels B and C, horizontal lines represent 95 percent confidence intervals. Sample includes all survey respondents.\r\n\r\nFigure 7:Student Beliefs about the Impact of AI on their Academic Performance\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of Middlebury students who believe that AI improves, reduces, or has no effect on different aspects of their academic experience. For each outcome, responses are categorized into three groups: “Improves” combines “significantly improves” and “somewhat improves” responses, “Reduces” combines “significantly reduces” and “somewhat reduces” responses, and “No effect” represents neutral responses. Sample includes all students who report using AI during the academic semester. “Don’t know” responses are excluded.\r\n\r\nFigure 8:Relationship Between AI Adoption and Beliefs About AI’s Academic Benefits\r\nPanel A. Learning ability (e.g., ability to grasp concepts, learn new skills, etc.)\r\nRefer to caption\r\nPanel B. Understanding of\r\ncourse materials\r\nRefer to caption\r\nPanel C. Ability to complete\r\nassignments on time\r\nRefer to caption\r\nPanel D. Course grades\r\n \r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the relationship between AI adoption rates and beliefs that AI improves various academic outcomes across different student groups. Each panel plots the percent of students who use AI (\r\nx\r\n-axis) against the percentage who believe AI improves a specific outcome (\r\ny\r\n-axis). Points represent different student groups categorized by demographics (circles), academic characteristics (triangles), and field of study (squares). The dashed line shows the linear fit across all groups. Groups with fewer than 10 students are excluded.\r\n\r\nFigure 9:Student Beliefs about Generative AI Usage at Middlebury College\r\nPanel A. For Schoolwork\r\nRefer to caption\r\nPanel B. For Leisure\r\nRefer to caption\r\nPanel C. For Any Purpose\r\nRefer to caption\r\nPanel D. Classes with No Policy\r\nRefer to caption\r\nPanel E. Classes that Allow AI\r\nRefer to caption\r\nPanel F. Classes that Disallow AI\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the distribution of students’ beliefs about generative AI usage among their peers at Middlebury College. Panels A-C display students’ estimates of the percent of their peers who regularly use AI for schoolwork, leisure activities, and any purpose, respectively. Panels D-F show students’ beliefs about AI usage in classes with different AI policies: those without an explicit policy (Panel D), those that allow AI use (Panel E), and those that prohibit AI use (Panel F). Each panel shows a histogram with bins of width ten percentage points (e.g., responses between 1-10 fall in the 10 bin, 11-20 in the 20 bin, etc.). The red dashed line indicates the mean response. Sample excludes respondents with missing values or who selected the default response for all six categories (which equals zero).\r\n\r\nTable 1:Summary Statistics of Survey Participants\r\nSurvey Sample\tAdmin records\r\nUnweighted\tWeighted\t\r\n(1)\t(2)\t(3)\r\nPanel A. Demographics\r\nMale\t0.446\t0.433\t0.463\r\nFemale\t0.508\t0.516\t0.533\r\nWhite\t0.618\t0.603\t0.538\r\nBlack\t0.036\t0.033\t0.052\r\nHispanic\t0.099\t0.106\t0.124\r\nAsian\t0.155\t0.162\t0.073\r\nPrivate high school\t0.420\t0.399\t–\r\nPublic high school\t0.543\t0.556\t–\r\nPanel B. Academic Characteristics    GPA 3.740 3.736 3.670 Hours spent on academics per week 17.899 17.889 – Freshman 0.311 0.355 0.255 Sophomore 0.273 0.272 0.257 Junior 0.202 0.179 0.201 Senior 0.213 0.194 0.287   Panel C. Field of Study    Arts 0.011 0.021 0.023 Humanities 0.052 0.068 0.073 Languages 0.021 0.020 0.025 Literature 0.035 0.025 0.025 Natural Sciences 0.218 0.249 0.244 Social Sciences 0.353 0.247 0.243 Has not declared major 0.311 0.371 0.364\r\n\r\nN (# degrees) 43 43 49 N (# students) 634 2,760 2,760\r\n\r\n\\justify\r\nNotes: This table presents summary statistics from our survey of college students. Panel A reports demographic characteristics, including the proportion of participants identifying as male, female, white, Black, Hispanic, Asian, or who attended a private or public high school. Panel B provides academic characteristics, such as GPA (only available for non-freshmen), average weekly hours spent on academics, and academic year distribution (Freshman, Sophomore, Junior, and Senior). Note that in column 1–2, GPA refers to self-reported first-year GPA while in column 3 it is the overall GPA during Spring 2024. Panel C summarizes the distribution of participants across different fields of study. Major groups are mutually exclusive.\r\n\r\n0.8250.5890.3680.106 R\r\n−\r\nsquared0.0770.0890.1160.058 N (Students)616616616616 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI usage frequency threshold and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column uses a different threshold for AI usage frequency, categorized as: “Rarely” (a few times a semester), “Occasionally” (a few times a month), “Frequently” (a few times a week), and “Very Frequently” (daily or almost daily). Column 1 defines usage as any nonzero frequency; column 2 includes at least occasional use; column 3 includes frequent or higher use; and column 4 captures only very frequent use.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.0890.1990.3570.5510.801 R\r\n−\r\nsquared0.0450.0610.0880.0800.069 N (Students)633633633633633 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI adoption date and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division. Students who have not declared their major are classified into fields of study based on their intended major.\r\n\r\nEach column presents results for a different threshold of AI adoption. Column 1 shows the probability of adopting AI before Spring 2023; column 2 by Spring 2023; column 3 by Fall 2023; column 4 by Spring 2024; and column 5 by Fall 2024. The dependent variable in each regression is a binary indicator equal to one if the student had adopted AI by the specified time period.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.9120.6122.2850.9130.4191.8010.1930.484 R\r\n−\r\nsquared0.0360.0730.1260.0530.1170.1290.0420.063 N (Students)515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and their use of generative AI for academic tasks. In columns 1 and 4, the outcome is a dummy that equals one if a student reports using AI with any frequency for at least one augmentation or automation task, respectively. In columns 2 and 5, the outcome is the share of tasks within each category for which the student reports any use. In columns 3 and 6, the outcome is a continuous measure capturing average usage frequency for each task category, based on raw Likert-style responses. In columns 7 and 8, the outcome is the difference in average task share and usage intensity between augmentation and automation, respectively. Regressions are weighted and report robust standard errors clustered at the student level. ∗∗∗ \r\np\r\n<\r\n0.01\r\n, ∗∗ \r\np\r\n<\r\n0.05\r\n, ∗ \r\np\r\n<\r\n0.10\r\n.\r\n\r\nReferences\r\nAcemoglu, (2025)\r\nAcemoglu, D. (2025).Nobel lecture: Institutions, technology, and prosperity.American Economic Review, 115(6):1709–1748.\r\nAngrist et al., (2009)\r\nAngrist, J., Lang, D., and Oreopoulos, P. (2009).Incentives and services for college achievement: Evidence from a randomized trial.American Economic Journal: Applied Economics, 1(1):136–163.\r\nAutor and Thompson, (2025)\r\nAutor, D. and Thompson, N. (2025).Expertise.\r\nBailey et al., (2022)\r\nBailey, M., Johnston, D., Kuchler, T., Stroebel, J., and Wong, A. (2022).Peer effects in product adoption.American Economic Journal: Applied Economics, 14(3):488–526.\r\nBastani et al., (2025)\r\nBastani, H., Bastani, O., Sungu, A., Ge, H., Kabakcı, Ö., and Mariman, R. (2025).Generative AI without guardrails can harm learning: Evidence from high school mathematics.Proceedings of the National Academy of Sciences, 122(26):e2422633122.\r\nBeaman et al., (2021)\r\nBeaman, L., BenYishay, A., Magruder, J., and Mobarak, A. M. (2021).Can network theory-based targeting increase technology adoption?American Economic Review, 111(6):1918–1943.\r\nBick et al., (2025)\r\nBick, A., Blandin, A., and Deming, D. J. (2025).The rapid adoption of generative AI.Technical report, National Bureau of Economic Research.\r\nBonney et al., (2024)\r\nBonney, K., Breaux, C., Buffington, C., Dinlersoz, E., Foster, L. S., Goldschlag, N., Haltiwanger, J. C., Kroff, Z., and Savage, K. (2024).Tracking Firm Use of AI in Real Time: A Snapshot from the Business Trends and Outlook Survey.\r\nBonsu and Baffour-Koduah, (2023)\r\nBonsu, E. M. and Baffour-Koduah, D. (2023).From the consumers’ side: Determining students’ perception and intention to use chatgpt in ghanaian higher education.Journal of Education, Society & Multiculturalism, 4(1):1–29.\r\nBresnahan and Trajtenberg, (1995)\r\nBresnahan, T. F. and Trajtenberg, M. (1995).General purpose technologies ‘engines of growth’?Journal of econometrics, 65(1):83–108.\r\nBrynjolfsson and Hitt, (2000)\r\nBrynjolfsson, E. and Hitt, L. M. (2000).Beyond computation: Information technology, organizational transformation and business performance.Journal of Economic Perspectives, 14(4):23–48.\r\nBrynjolfsson et al., (2025)\r\nBrynjolfsson, E., Li, D., and Raymond, L. (2025).Generative AI at Work.The Quarterly Journal of Economics, 140(2):889–942.\r\nBushong and Gagnon-Bartsch, (2024)\r\nBushong, B. and Gagnon-Bartsch, T. (2024).Failures in forecasting: An experiment on interpersonal projection bias.Management Science, 70(12):8735–8752.\r\nCarvajal et al., (2024)\r\nCarvajal, D., Franco, C., and Isaksson, S. (2024).Will artificial intelligence get in the way of achieving gender equality?Discussion Paper 03, NHH Norwegian School of Economics.\r\n(15)\r\nComin, D. and Hobijn, B. (2010a).An exploration of technology diffusion.American Economic Review, 100(5):2031–2059.\r\n(16)\r\nComin, D. and Hobijn, B. (2010b).Technology Diffusion and Postwar Growth, volume 25, pages 209–246.University of Chicago Press.\r\nCui et al., (2024)\r\nCui, Z. K., Demirer, M., Jaffe, S., Musolff, L., Peng, S., and Salz, T. (2024).The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers.\r\nDavid, (1990)\r\nDavid, P. A. (1990).The dynamo and the computer: an historical perspective on the modern productivity paradox.The American economic review, 80(2):355–361.\r\nDell’Acqua et al., (2023)\r\nDell’Acqua, F., McFowland, E., Mollick, E. R., Lifshitz-Assaf, H., Kellogg, K., Rajendran, S., Krayer, L., Candelon, F., and Lakhani, K. R. (2023).Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality.\r\nDuflo et al., (2011)\r\nDuflo, E., Kremer, M., and Robinson, J. (2011).Nudging Farmers to Use Fertilizer: Theory and Experimental Evidence from Kenya.American Economic Review, 101(6):2350–2390.\r\nEloundou et al., (2024)\r\nEloundou, T., Manning, S., Mishkin, P., and Rock, D. (2024).GPTs are GPTs: Labor market impact potential of LLMs.Science, 384(6702):1306–1308.\r\nFairlie et al., (2014)\r\nFairlie, R. W., Hoffmann, F., and Oreopoulos, P. (2014).A community college instructor like me: Race and ethnicity interactions in the classroom.American Economic Review, 104(8):2567–2591.\r\nFelten et al., (2021)\r\nFelten, E. W., Raj, M., and Seamans, R. (2021).Occupational, Industry, and Geographic Exposure to Artificial Intelligence: A Novel Dataset and Its Potential Uses.\r\nFelten et al., (2023)\r\nFelten, E. W., Raj, M., and Seamans, R. (2023).How will Language Modelers like ChatGPT Affect Occupations and Industries?\r\nFoster and Rosenzweig, (1995)\r\nFoster, A. D. and Rosenzweig, M. R. (1995).Learning by doing and learning from others: Human capital and technical change in agriculture.Journal of political Economy, 103(6):1176–1209.\r\nFoster and Rosenzweig, (2010)\r\nFoster, A. D. and Rosenzweig, M. R. (2010).Microeconomics of technology adoption.Annu. Rev. Econ., 2(1):395–424.\r\nGallup, (2024)\r\nGallup (2024).AI in the Workplace: Answering 3 Big Questions.\r\nGiaccherini et al., (2019)\r\nGiaccherini, M., Herberich, D. H., Jimenez-Gomez, D., List, J. A., Ponti, G., and Price, M. K. (2019).The behavioralist goes door-to-door: Understanding household technological diffusion using a theory-driven natural field experiment.Technical report, National Bureau of Economic Research.\r\nGoehring et al., (2024)\r\nGoehring, G., Mezzanotti, F., and Ravid, S. A. A. (2024).Technology adoption and career concerns: Evidence from the adoption of digital technology in motion pictures.Technical report, National Bureau of Economic Research.\r\nGriliches, (1957)\r\nGriliches, Z. (1957).Hybrid corn: An exploration in the economics of technological change.Econometrica, 25(4):501–522.\r\nHall and Khan, (2003)\r\nHall, B. H. and Khan, B. (2003).Adoption of new technology.\r\n(32)\r\nHanda, K., Bent, D., Tamkin, A., McCain, M., Durmus, E., Stern, M., Schiraldi, M., Huang, S., Ritchie, S., Syverud, S., Jagadish, K., Vo, M., Bell, M., and Ganguli, D. (2025a).Anthropic education report: How university students use claude.\r\n(33)\r\nHanda, K., Tamkin, A., McCain, M., Huang, S., Durmus, E., Heck, S., Mueller, J., Hong, J., Ritchie, S., Belonax, T., et al. (2025b).Which economic tasks are performed with ai? evidence from millions of claude conversations.arXiv preprint arXiv:2503.04761.\r\nHanna et al., (2014)\r\nHanna, R., Mullainathan, S., and Schwartzstein, J. (2014).Learning Through Noticing: Theory and Evidence from a Field Experiment.The Quarterly Journal of Economics, 129(3):1311–1353.\r\nHartley et al., (2025)\r\nHartley, J., Jolevski, F., Melo, V., and Moore, B. (2025).The Labor Market Effects of Generative Artificial Intelligence.\r\nHumlum and Vestergaard, (2025)\r\nHumlum, A. and Vestergaard, E. (2025).Large Language Models, Small Labor Market Effects.\r\nHutto and Gilbert, (2014)\r\nHutto, C. and Gilbert, E. (2014).Vader: A parsimonious rule-based model for sentiment analysis of social media text.In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216–225.\r\nJehiel, (2018)\r\nJehiel, P. (2018).Investment strategy and selection bias: An equilibrium perspective on overoptimism.American Economic Review, 108(6):1582–1597.\r\nKelly et al., (2023)\r\nKelly, A., Sullivan, M., and Strampel, K. (2023).Generative artificial intelligence: University student awareness, experience, and confidence in use across disciplines.Journal of University Teaching and Learning Practice, 20(6):1–16.\r\nKharazian, (2025)\r\nKharazian, A. (2025).Ramp ai index: Monthly measurement of ai adoption by american businesses.Technical report, Ramp Economics Lab.\r\nLehmann et al., (2024)\r\nLehmann, M., Cornelius, P. B., and Sting, F. J. (2024).AI Meets the Classroom: When Does ChatGPT Harm Learning?\r\nLing and Imas, (2025)\r\nLing, Y. and Imas, A. (2025).Underreporting of AI use: The role of social desirability bias.Available at SSRN.\r\nLiu and Wang, (2024)\r\nLiu, Y. and Wang, H. (2024).Who on earth is using generative ai?Policy Research Working Paper WPS10870, World Bank Group, Washington, D.C.\r\nMcClain, (2024)\r\nMcClain, C. (2024).Americans’ use of ChatGPT is ticking up, but few trust its election information.\r\nMcDonald et al., (2025)\r\nMcDonald, N., Johri, A., Ali, A., and Collier, A. H. (2025).Generative artificial intelligence in higher education: Evidence from an analysis of institutional policies and guidelines.Computers in Human Behavior: Artificial Humans, 3:100121.\r\nMcElheran et al., (2023)\r\nMcElheran, K., Li, J. F., Brynjolfsson, E., Kroff, Z., Dinlersoz, E., Foster, L. S., and Zolas, N. (2023).AI Adoption in America: Who, What, and Where.\r\nMerchant, (2024)\r\nMerchant, B. (2024).The new luddites aren’t backing down.The Atlantic.Accessed: 2025-06-16.\r\nNam, (2023)\r\nNam, J. (2023).56% of college students have used ai on assignments or exams.Technical report.Edited by Lyss Welding. Accessed June 11, 2025.\r\nNolan, (2023)\r\nNolan, B. (2023).Here are the schools and colleges that have banned chatgpt.Business Insider.Accessed: 2025-01-27.\r\nNoy and Zhang, (2023)\r\nNoy, S. and Zhang, W. (2023).Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence.\r\nOtis et al., (2024)\r\nOtis, N. G., Delecourt, S., Cranney, K., and Koning, R. (2024).Global Evidence on Gender Gaps and Generative AI.Harvard Business School.\r\nPeng et al., (2023)\r\nPeng, S., Kalliamvakou, E., Cihon, P., and Demirer, M. (2023).The Impact of AI on Developer Productivity: Evidence from GitHub Copilot.\r\nRavšelj et al., (2025)\r\nRavšelj, D., Keržič, D., Tomaževič, N., Umek, L., Brezovar, N., A. Iahad, N., Abdulla, A. A., Akopyan, A., Aldana Segura, M. W., AlHumaid, J., Allam, M. F., Alló, M., Andoh, R. P. K., Andronic, O., Arthur, Y. D., Aydın, F., Badran, A., Balbontín-Alvarado, R., Ben Saad, H., Bencsik, A., Benning, I., Besimi, A., Bezerra, D. D. S., Buizza, C., Burro, R., Bwalya, A., Cachero, C., Castillo-Briceno, P., Castro, H., Chai, C. S., Charalambous, C., Chiu, T. K. F., Clipa, O., Colombari, R., Corral Escobedo, L. J. H., Costa, E., Cre\\textcommabelowtulescu, R. G., Crispino, M., Cucari, N., Dalton, F., Demir Kaya, M., Dumić-Čule, I., Dwidienawati, D., Ebardo, R., Egbenya, D. L., Faris, M. E., Fečko, M., Ferrinho, P., Florea, A., Fong, C. Y., Francis, Z., Ghilardi, A., González-Fernández, B., Hau, D., Hossain, M. S., Hug, T., Inasius, F., Ismail, M. J., Jahić, H., Jessa, M. O., Kapanadze, M., Kar, S. K., Kateeb, E. T., Kaya, F., Khadri, H. O., Kikuchi, M., Kobets, V. M., Kostova, K. M., Krasmane, E., Lau, J., Law, W. H. C., Lazăr, F., Lazović-Pita, L., Lee, V. W. Y., Li, J., López-Aguilar, D. V., Luca, A., Luciano, R. G., Machin-Mastromatteo, J. D., Madi, M., Manguele, A. L., Manrique, R. F., Mapulanga, T., Marimon, F., Marinova, G. I., Mas-Machuca, M., Mejía-Rodríguez, O., Meletiou-Mavrotheris, M., Méndez-Prado, S. M., Meza-Cano, J. M., Mirķe, E., Mishra, A., Mital, O., Mollica, C., Morariu, D. I., Mospan, N., Mukuka, A., Navarro Jiménez, S. G., Nikaj, I., Nisheva, M. M., Nisiforou, E., Njiku, J., Nomnian, S., Nuredini-Mehmedi, L., Nyamekye, E., Obadić, A., Okela, A. H., Olenik-Shemesh, D., Ostoj, I., Peralta-Rizzo, K. J., Peštek, A., Pilav-Velić, A., Pires, D. R. M., Rabin, E., Raccanello, D., Ramie, A., Rashid, M. M. U., Reuter, R. A. P., Reyes, V., Rodrigues, A. S., Rodway, P., Ručinská, S., Sadzaglishvili, S., Salem, A. A. M. S., Savić, G., Schepman, A., Shahpo, S. M., Snouber, A., Soler, E., Sonyel, B., Stefanova, E., Stone, A., Strzelecki, A., Tanaka, T., Tapia Cortes, C., Teira-Fachado, A., Tilga, H., Titko, J., Tolmach, M., Turmudi, D., Varela-Candamio, L., Vekiri, I., Vicentini, G., Woyo, E., Yorulmaz, Ö., Yunus, S. A. S., Zamfir, A.-M., Zhou, M., and Aristovnik, A. (2025).Higher education students’ perceptions of ChatGPT: A global study of early reactions.PLOS ONE, 20(2):e0315011.\r\nRogers, (1962)\r\nRogers, E. M. (1962).Diffusion of Innovations.Free Press, New York.\r\nRoss et al., (1977)\r\nRoss, L., Greene, D., and House, P. (1977).The “false consensus effect”: An egocentric bias in social perception and attribution processes.Journal of experimental social psychology, 13(3):279–301.\r\nSacerdote, (2001)\r\nSacerdote, B. (2001).Peer Effects with Random Assignment: Results for Dartmouth Roommates.The Quarterly Journal of Economics, 116(2):681–704.\r\nStinebrickner and Stinebrickner, (2008)\r\nStinebrickner, R. and Stinebrickner, T. R. (2008).The causal effect of studying on academic performance.The BE Journal of Economic Analysis & Policy, 8(1).\r\nStöhr et al., (2024)\r\nStöhr, C., Ou, A. W., and Malmström, H. (2024).Perceptions and usage of AI chatbots among students in higher education across genders, academic levels and fields of study.Computers and Education: Artificial Intelligence, 7:100259.\r\nStokey, (2021)\r\nStokey, N. L. (2021).Technology diffusion.Review of Economic Dynamics, 42:15–36.\r\nWorld Bank, (2016)\r\nWorld Bank (2016).World development report 2016: Digital dividends.World Bank Publications.\r\nWu et al., (2022)\r\nWu, M.-J., Zhao, K., and Fils-Aime, F. (2022).Response rates of online surveys in published research: A meta-analysis.Computers in Human Behavior Reports, 7.\r\nXiao et al., (2023)\r\nXiao, P., Chen, Y., and Bao, W. (2023).Waiting, banning, and embracing: An empirical analysis of adapting policies for generative ai in higher education.arXiv preprint arXiv:2305.18617.\r\nAppendix\r\n\r\nAppendix AAppendix Figures and Tables\r\nFigure A1:Generative AI Usage Survey Design Overview\r\nKey\t\r\nSurvey Structure\r\nSurvey Sections\r\nUsage Data Collection\r\nBeliefs & Perceptions\r\nAI Usage Survey at Middlebury College\r\nDecember 2024\r\nParticipant Recruitment\r\nCampus-wide email invitations, framed as general technology use survey\r\nParticipation Incentives\r\nAmazon gift card lottery ranging from $50-$500\r\nSection 1: Demographics & Academic Information\r\nStudent Characteristics\r\nGender, race/ethnicity, high school type\r\nAcademic Profile\r\nYear, major, self-reported GPA, study hours\r\nSection 2: generative AI Usage Patterns\r\nAdoption Metrics\r\nFrequency, timing of first use, specific AI models used\r\nAcademic Applications\r\nTasks performed with AI, payment for premium services\r\nSection 3: Perceptions & Institutional Policies\r\nImpact Perceptions\r\nEffects on learning, grades, time management\r\nPolicy Responses\r\nUsage likelihood under different policies\r\nPeer Usage Beliefs\r\nEstimates of AI adoption among peers for schoolwork and leisure\r\nOpen-Ended Response Collection\r\nMotivations for AI use, policy feedback, suggestions for improvement\r\nNote: This figure illustrates the structure of the AI usage survey conducted at Middlebury College in December 2024. The survey collected information across three main sections: (1) demographic and academic background, (2) patterns of generative AI usage including adoption timing, frequency, and specific applications, and (3) perceptions of AI’s impact on learning and responses to institutional policies.\r\n\r\nFigure A2:Cumulative Generative AI Use by Student Characteristic\r\nPanel A. By Gender\r\nRefer to caption\r\nPanel B. By Race\r\nRefer to caption\r\nPanel C. By High School Type\r\nRefer to caption\r\nPanel D. By Freshman Year GPA\r\nRefer to caption\r\nPanel E. By Cohort\r\nRefer to caption\r\nPanel F. By Field of Study\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents cumulative AI use based on different student characteristics. Each panel displays the cumulative distribution of AI use based on a specific characteristic: gender, race, school type, first-year GPA, cohort, or field of study. The cumulative percent of students is plotted against usage categories. The legends and colors correspond to subgroups within each demographic variable.\r\n\r\nFigure A3:Percent of Students Who Pay for Generative AI Tools\r\nRefer to caption\\justify\r\nNotes: This figure shows the percent of AI users who pay for AI tools (through any platform) across different demographic groups. Horizontal lines represent 95 percent confidence intervals calculated with heteroskedasticity-robust standard errors clustered at the student level.\r\n\r\nFigure A4:Academic Uses of ChatGPT: Evidence from Global Survey\r\nPanel A. Across Tasks that Augment versus Automate Student Effort\r\nRefer to caption\r\nPanel B. Across the College Quality Distribution\r\nRefer to caption\r\n\\justify\r\nNotes: This figure shows the percent of students who use ChatGPT for different academic tasks based on data from Ravšelj et al., (2025). Panel A displays usage patterns across tasks categorized as augmenting (proofreading, translating, study assistance, research assistance) versus automating (academic writing, professional writing, creative writing, brainstorming, summarizing, calculating, coding assistance, personal assistance) student effort. Panel B shows usage patterns across university quality quintiles based on World University Rankings, with universities ranked in the top 20% (top quintile) showing slightly higher rates of augmentation relative to automation compared to bottom quintile institutions. The analysis includes universities with at least 30 student responses and excludes observations with missing usage data.\r\n\r\nFigure A5:Student Beliefs about AI Usage versus Actual Usage under Different Policies\r\nRefer to caption\\justify\r\nNotes: This figure compares students’ beliefs about AI usage with actual usage rates under different AI policies. For each policy type, blue bars show the percent of students who report being “Neutral,” “Likely,” or “Very Likely” to use AI, while red bars show students’ mean beliefs about what percent of their peers use AI in classes with that policy. The sample includes all survey respondents for both actual usage and beliefs measures. “AI Use is Unrestricted” refers to classes with no restrictions on AI use, “No Explicit AI Policy” refers to classes without a stated policy, and “AI Use is Prohibited” refers to classes that ban AI use entirely.\r\n\r\nFigure A6:Relationship Between Beliefs About AI Usage and Actual AI Usage\r\nRefer to caption\\justify\r\nNotes: This figure shows the relationship between students’ beliefs about AI usage among their peers and actual AI usage rates across different demographic groups. Each point represents a different group of students (by demographics, academic characteristics, or field of study). The \r\ny\r\n-axis shows the percent of students in each group who report using AI for academic purposes. The \r\nx\r\n-axis shows the mean belief within each group about what percent of Middlebury students use AI. The dashed line shows the linear fit. Sample includes all survey respondents with at least ten observations per group. Students’ beliefs about AI usage are positively correlated with actual usage patterns, suggesting that students have relatively accurate perceptions of AI adoption among their peers.\r\n\r\nGPA > p50 80.3 26.3 17.4 23.2 13.4 GPA < p50 87.1 18.7 26.3 32.2 9.9 Freshman 81.3 24.3 24.1 24.7 8.1 Sophomore 84.4 25.6 22.6 23.7 12.4 Junior 86.2 21.6 23.1 28.6 12.8 Senior 78.7 21.0 17.4 29.9 10.4\r\n\r\nPanel C. Field of Study    Arts 73.3 21.0 0.0 52.3 0.0 Humanities 74.8 39.3 21.8 12.1 1.7 Languages 57.4 27.0 30.3 0.0 0.0 Literature 48.6 10.0 33.7 5.0 0.0 Natural Sciences 91.1 22.0 21.9 33.8 13.3 Social Sciences 84.6 17.9 20.4 29.1 17.2\r\n\r\n\\justify\r\nNotes: This table presents the percent of students in each demographic group who report using AI at different frequencies during the academic semester. Each cell shows the percent of students within that group. Column 1 reports the total percent who use AI at any frequency. Columns 2 to 5 represent increasing usage frequencies: rarely (1–2 times per semester), occasionally (monthly), frequently (weekly), and very frequently (multiple times per week). Panel A reports percentages by demographic characteristics. Panel B shows percentages by academic characteristics. Panel C presents percentages by field of study.\r\n\r\n0.9730.1350.0770.1100.114 R\r\n−\r\nsquared0.0120.0510.0270.0240.138 N (Students)516516516516516 \\justifyNotes: This table assesses the relationship between AI model adoption and student characteristics. We estimate: Y_i = α+ βX_i + ε_i, where \r\nY\r\ni\r\n is a binary indicator of AI model usage (columns 1-4) or payment for AI services (column 5), and \r\nX\r\ni\r\n is a vector of student characteristics including gender, race/ethnicity, high school type, cohort indicators, and academic division.\r\n\r\nEach column presents results for a different model or payment outcome. Column 1 shows usage of OpenAI’s ChatGPT, column 2 Google Gemini, column 3 Microsoft Copilot, column 4 any other AI model, and column 5 whether the student pays for any generative AI service.\r\n\r\nThe omitted categories are Natural Sciences for academic division, white students for race/ethnicity, and freshmen for cohort. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.7510.6760.6610.2710.479 R\r\n−\r\nsquared0.0870.1130.0760.0680.041 N (Students)599599599599599 \\justifyNotes: This table examines how student characteristics relate to self-reported likelihood of using generative AI under different policy scenarios. Each column presents results for different policy scenarios. In columns 1-4, the dependent variable equals one if the student reports being“neutral,” “likely” or “extremely likely” to use AI under the specified policy, and zero if they report being “unlikely,” or “extremely unlikely.” Column 5 represents the impact of moving from unrestricted use to complete prohibition.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\n0.5760.6850.3760.4890.564 R\r\n−\r\nsquared0.1020.1190.0840.0370.087 N (Students)561565543560560 \\justifyNotes: This table assesses the relationship between AI adoption and student characteristics. Each column presents results for beliefs about different academic outcomes: learning ability (e.g., ability to grasp concepts, retain information, or learn new skills) in column 1, understanding of course materials in column 2, course grades in column 3, ability to complete assignments on time in column 4, and time spent on academics in column 5. The dependent variable in each regression equals one if the student believes AI somewhat improves” or significantly improves” the outcome, and zero if they believe it has no effect, reduces, or significantly reduces the outcome. “Don’t know” responses are excluded.\r\n\r\nThe omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Heteroskedasticity-robust standard errors clustered at the student level in parentheses. Observations are weighted to adjust for sampling. * \r\np\r\n<\r\n0.10\r\n, ** \r\np\r\n<\r\n0.05\r\n, *** \r\np\r\n<\r\n0.01\r\n.\r\n\r\nAppendix BEmpirical Appendix\r\nB.1Field of Study Classifications\r\nThis appendix details the classification of majors into broad fields of study used in Figure 1:\r\n\r\n• Natural Sciences: Includes Biology, Biochemistry, Chemistry, Computer Science, Earth and Climate Sciences/Geology, Environmental Studies, Mathematics, Molecular Biology & Biochemistry, Neuroscience, Physics, and Statistics.\r\n• Social Sciences: Includes Anthropology, Economics, Education, Geography, International & Global Studies, International Politics & Economics, Political Science, Psychology, and Sociology.\r\n• Humanities: Includes American Studies, Black Studies, Classics, History, Architectural Studies, Art History & Museum Studies, History of Art & Architecture, Philosophy, Religion, and Classical Studies.\r\n• Literature: Includes Comparative Literature, English/English & American Literatures, and Literary Studies.\r\n• Languages: Includes Arabic, Chinese, French & Francophone Studies, German, Japanese Studies, Russian, and Spanish.\r\n• Arts: Includes Film & Media Culture, Music, Studio Art, and Theatre.\r\nB.2Task-Specific Use of Generative AI\r\nThe aggregate patterns documented in Section 4 may mask important heterogeneity across student groups. Different students may adopt AI tools at varying rates depending on their academic needs and field-specific norms. To investigate this heterogeneity systematically, Appendix Table LABEL:tab:ai_purpose_correlates presents regression estimates examining how student characteristics predict AI usage for each of our ten measured academic tasks.\r\n\r\nUsage patterns differ markedly by student characteristics. Male students consistently exhibit higher adoption rates across most applications, with particularly pronounced differences for finding information (16.4 pp; \r\np\r\n<\r\n0.01\r\n), summarizing texts (12.5 pp; \r\np\r\n<\r\n0.01\r\n), and creating images (16.9 pp; \r\np\r\n<\r\n0.01\r\n). Black students show substantially higher usage for information-gathering tasks, being 23.8 pp more likely to use AI for finding information compared to white students (\r\np\r\n<\r\n0.05\r\n). They also exhibit dramatically higher adoption of writing assistance tools, with 27.6 pp higher usage for editing text (\r\np\r\n<\r\n0.05\r\n) and 23.6 pp for writing emails (\r\np\r\n<\r\n0.10\r\n). Latino students demonstrate significantly higher usage for generating ideas (24.0 pp; \r\np\r\n<\r\n0.01\r\n) and writing emails (20.7 pp; \r\np\r\n<\r\n0.01\r\n), while Asian students show markedly higher adoption for writing emails (24.5 pp; \r\np\r\n<\r\n0.01\r\n) and explaining concepts (10.9 pp, \r\np\r\n<\r\n0.05\r\n). Students from public high schools report lower usage rates for several applications, particularly for concept explanation (7.3 pp lower; \r\np\r\n<\r\n0.10\r\n) and writing emails (7.4 pp lower; \r\np\r\n<\r\n0.10\r\n), but higher usage for proofreading (8.7 pp higher; \r\np\r\n<\r\n0.10\r\n).\r\n\r\nField of study emerges as a particularly strong predictor of usage patterns. Arts majors show significantly lower adoption across multiple tasks compared to Natural Science students, with gaps of 49.7 pp for finding information (\r\np\r\n<\r\n0.01\r\n), 44.0 pp for generating ideas (\r\np\r\n<\r\n0.05\r\n), and 62.1 pp for coding assistance (\r\np\r\n<\r\n0.01\r\n). Similarly, humanities majors are 36.8 pp less likely to use AI for generating ideas (\r\np\r\n<\r\n0.01\r\n) and 30.8 pp less likely for coding assistance (\r\np\r\n<\r\n0.01\r\n). Languages majors exhibit the most pronounced differences, being 52.4 pp less likely to use AI for summarizing texts (\r\np\r\n<\r\n0.01\r\n) and 46.9 pp less likely for generating ideas (\r\np\r\n<\r\n0.01\r\n). By contrast, social science students show higher usage for explaining concepts (8.0 pp; \r\np\r\n<\r\n0.10\r\n) and summarizing texts (14.7 pp; \r\np\r\n<\r\n0.01\r\n).\r\n\r\n0.8030.7400.6310.6190.5410.4730.3710.3440.2350.204 R\r\n−\r\nsquared0.0780.0870.0770.1160.0280.0540.0830.1350.0400.094 N (Students)515515515515515515515515515515 \\justifyNotes: This table reports estimated associations between student characteristics and use of AI for specific academic tasks. Each column shows the result for a different academic task. The omitted categories are: Natural Sciences for academic division, white students for race/ethnicity, freshmen for cohort, female for gender, and private high school for school type. Regressions are weighted and use heteroskedasticity-robust standard errors clustered at the student level. ∗∗∗, ∗∗, and ∗ indicate significance at the 1%, 5%, and 10% levels, respectively.\r\n\r\nAppendix CQualitative Evidence on Student Perspectives on AI Use\r\nIn this section, we analyze student responses to an open-ended question about their use of generative AI. The survey question asked: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” This was an optional question that 48.3 percent of survey respondents answered. Appendix Figure C1 presents a word cloud of the most frequent words in student responses.\r\n\r\nC.1Validating the Open-Ended Response Measure\r\nWe begin by validating our open-ended response measure to show that it contains meaningful signal. To validate this measure, we employ sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner), a widely-used lexicon and rule-based sentiment analysis tool (Hutto and Gilbert, , 2014). We calculate sentiment scores for each response and examine whether these scores correlate with students’ actual AI adoption behaviors. Intuitively, students who express more positive sentiment toward AI in their open-ended responses should be more likely to have adopted AI tools in practice. We test this hypothesis by examining two measures of AI adoption: whether students have ever used generative AI and whether they currently use AI for academic purposes. Appendix Figure C2 presents binned scatterplots showing the relationship between AI adoption rates and standardized sentiment scores.\r\n\r\nSentiment towards generative AI strongly predicts AI adoption. For both outcomes, students with negative sentiment scores (below zero) show substantially lower adoption rates—around 70-85 percent for ever using AI and 70-75 percent for academic use. In contrast, students with positive sentiment scores display markedly higher adoption rates, reaching nearly 100 percent for general use and 95-100 percent for academic purposes among those with the most positive sentiment. The relationship is particularly pronounced for academic AI use (Panel B), where the coefficient of \r\nβ\r\n^\r\n=\r\n0.168\r\n is twice as large as for general adoption.17 These systematic patterns confirm that our open-ended responses capture meaningful variation in student attitudes that corresponds to real behavioral differences.\r\n\r\nC.2How Students use Generative AI\r\nTo analyze these responses systematically, we classified each response using keywords based on their content. For example, if a student mentioned using AI to save time, we tagged the response with the keyword “time-saver.” If a student expressed concerns about AI’s impact on learning, we tagged it with “negative learning.” Responses could receive multiple keywords if they discussed several themes. Appendix Figure C3 shows the frequency of keywords in our classification. The responses reveal how students use AI tools, what motivates this use, and what causes some to avoid or limit their use.\r\n\r\nThe most common use of AI is as an explanatory tool. Nearly 30 percent of responses mentioned using AI to understand course material. Students frequently ask AI to break down complex concepts from readings and lectures, particularly when they find the material difficult to understand. For example, one student reported: “I can ask AI to explain concepts to me that I have a hard time grasping. […] I can keep asking ’simplify’ or ’break down even more.”’ Students also use AI to summarize dense academic readings, which they argue helps them manage heavy reading loads.\r\n\r\nStudents employ AI throughout different stages of the writing process. Some use AI to generate initial drafts that serve as starting points. One student explained: “Helps me get started with a base for most of my essays. It feels easier to edit something already written and make it my own than to write from scratch.” Others use AI more narrowly for brainstorming when stuck on specific problems. As one student noted: “I use it if I am feeling stuck to push me to the right direction (whether a mathematical problem or an essay idea).” Many also report using AI as an editing tool to improve grammar, sentence structure, and overall writing flow. This is especially the case for non-native English speakers. As one student explained: “English is not my first language and it frustrates me sometimes that I cannot find the best way to phrase a certain idea and AI is a useful tool to have to find alternate expressions.”\r\n\r\nStudents frequently mentioned using AI for specific academic tasks. In courses that require coding, students often use AI for debugging code and understanding programming concepts. Students also employ AI for administrative tasks like formatting citations and drafting routine emails. Finally, many students use AI as an enhanced search engine. One student reported: “It has significantly reduced the time it takes to conduct research on new topics and ideas, and helps me by giving me a thorough selection of sources to use for projects of any kind.”\r\n\r\nC.3Why Students Adopt AI\r\nTime savings was the most commonly cited reason for using AI. Nearly 30 percent of responses mentioned using AI to complete work more efficiently. Students often viewed AI as a way to manage demanding course loads. Many students particularly embrace AI assistance for tasks viewed as mechanical or administrative. A student noted they use AI for “Writing emails quick and creating resume/ cover letter templates.” But AI assistance goes beyond grunt work. Some students use it to “spend less time doing assignments and homework.” This is particularly true if students don’t view the work as central to their academic experience. As one student explained: “when I come across work I deem as ineffective, I want to spend as little time as possible doing it.”\r\n\r\nHaving an on-demand tutor for academic support was another key motivation. One-quarter of responses described using AI as an “explainer” when other resources were unavailable or inconvenient. As one student noted, “I use it as a last resort (if there are no office hours, after looking up videos, etc.) if I need extra help. I’d like to think that the way I use it is similar to going to office hours or TA hours.”\r\n\r\nPeer influence also drove AI adoption. Some students reported feeling pressure to use AI to remain competitive with their classmates. One student explained: “I noticed others use it, are getting better grades than me, and they say they learn better with the help of AI, so I gave it a try.” Others worried about being at a competitive disadvantage: “Other people were using it and told me about it. I felt like I would be at a disadvantage if I wasn’t also using it.”\r\n\r\nC.4Concerns and Limitations\r\nStudents expressed several concerns about AI use in academic work. The most frequent worry was about negative impacts on learning. One student who initially used AI extensively reported: “In the past, I have simply plugged and chugged homework assignments into ChatGPT and submitted it. Those assignments feedback from teachers was positive and I was getting good grades, but I definitely felt that my own learning outcomes to be significantly worse.” Other students viewed AI use as fundamentally incompatible with their educational goals. As one humanities student explained: “my task is as a humanities student is to think, not calculate; why should I let AI do the thinking for me? It would defeat the purpose of pursuing my education.”\r\n\r\nMany students described ethical concerns about AI use. Responses suggested uncertainty about appropriate boundaries. One student noted: “I never use it to explicitly write something because that feels like overt cheating, but sometimes I hesitate when it completely solves Econ problems. I understand how it does it, and it helps me to learn, but it still sometimes feels a little morally gray.” Another expressed similar ambivalence: “I tend to only use it when [it] will save me time in a moral way.”\r\n\r\nStudents also emphasized the importance of maintaining ownership of their work. Many expressed pride in producing original work and hesitation about diluting that ownership through AI use. As one student explained: “I don’t have interest in using generative AI for my academic work because I want my work to reflect my own ideas.” Another noted: “It usually would not even occur to me to turn to AI to substitute writing because I want to take credit for my work, and using AI seems to diminish that.”\r\n\r\nTechnical limitations deterred some students from using AI tools. Students reported concerns about inaccurate outputs (“hallucinations”) and poor output quality, particularly for creative writing or complex analytical tasks. For example, one student noted that “In my poetry class we were instructed to use it to come up with poems and they were awful, so that kinda turned me away from using it to do my work for me.”\r\n\r\nC.5Discussion\r\nWe conclude with two overarching themes that emerged from the responses.\r\n\r\nFirst, students vary substantially in how they incorporate AI into their academic lives. Crucially, this heterogeneity largely depends on what students perceive as “appropriate” uses of AI. For activities that they perceive as core to their academic journey—like writing essays or solving problems—many students hesitate to use AI. A student articulated this clearly: “Most of my work is writing or reading. If I’m not doing the writing, what is the purpose of me taking the class?” Yet, students draw different boundaries between central tasks and grunt work. Some use AI extensively, viewing their role as akin to a manager that provides high-level direction while AI handles implementation. Others restrict AI use to specific tasks like brainstorming, editing, or drafting emails. Still others avoid AI entirely for academic work, often for ethical reasons. Even among AI users, adoption patterns reflect individual trade-offs between time savings, learning goals, and academic integrity concerns.\r\n\r\nSecond, there appears to be a fundamental tension between efficiency and learning. The time-saving benefits are easy to observe, quantifiable, and tangible. But these time savings are unlikely to be a free lunch. Some benefits may come at the cost of spending less time with material that requires deeper engagement to digest. As one student noted: “There may be a negative effect in that it eliminates much of the ’struggle’ in learning.” Yet, having an on-demand tutor that explains concepts in relatable ways can also improve learning. As one student explained: “It can explain concepts to me in a way that is tailored to my learning style.” This suggests that the impact of AI use on learning outcomes depends not on whether students use AI—almost all do, to some extent—but rather on how they employ these tools.\r\n\r\nFigure C1:Word Cloud of Student Motivations for Generative AI Use\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 147 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?’\r\n\r\nFigure C2:Relationship Between AI Sentiment and AI Adoption\r\nPanel A. Ever Used Generative AI\r\nRefer to caption\r\nPanel B. Uses AI for Academic Purposes\r\nRefer to caption\r\n\\justify\r\nNotes: This figure presents the relationship between AI sentiment and AI adoption. Panel A shows the proportion of respondents who have ever used generative AI, while Panel B shows the proportion who use AI for academic purposes. Each point represents the mean adoption rate for respondents within sentiment score bins of width 0.2. Sentiment scores are standardized compound scores computed using Hutto and Gilbert, (2014)’s VADER algorithm applied to responses to an open-ended question about generative AI. Positive values indicate positive sentiment and negative values indicate negative sentiment. The dashed lines show OLS best-fit lines estimated on the microdata, with coefficients and standard errors (in parentheses) displayed in the top-left corner of each panel.\r\n\r\nFigure C3:Frequency of Keywords in Student Motivations for AI Use\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to AI use. The responses come from the question “Please describe the factors that have personally influenced your use of generative AI in your academic work. What initially led you to try it, what has motivated you to use it or caused you to hesitate?” Color coding indicates the category of each theme. Usage type refers to how students use AI tools. Motivation captures what drove students to try AI. Concerns include mentions of course policies and academic integrity, individual reservations about AI use, worries about AI’s impact on education, and AI’s technical limitations.\r\n\r\nAppendix DQualitative Evidence on Student Views of AI Policies\r\nIn this section, we analyze student responses to an open-ended question about Middlebury’s AI policies. The survey asked: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Appendix Figure D1 presents a word cloud of the most frequent words in student responses. To analyze these responses systematically, we classified each response using keywords based on their content. Appendix Figure D2 shows the frequency of keywords in our classification.\r\n\r\nD.1Polarized Views on Generative AI Policy Approaches\r\nStudents expressed markedly different views about appropriate AI policies, revealing fundamental disagreement about the path forward. Some strongly advocated for embracing AI technology. As one student argued, “The tool is there, there is supply and there is demand. Don’t fight another war on drugs. Don’t live in a fake reality”. Others called for significant restrictions, arguing that “the use of generative AI is dishonest and corrosive” and that it “prohibits these organic processes and divorces students from true learning”.\r\n\r\nHowever, the most common position advocated for a balanced approach that would allow beneficial uses while restricting harmful ones. Students distinguished between uses that enhance learning (like concept explanation) and those that substitute for learning (like generating entire essays). One student articulated this nuanced view particularly well: “AI also can really be helpful at explaining a textbook problem that doesn’t make sense, or guiding slightly with homework, or creating study materials, or editing/tightening up your prose. All of those things are good, and universities should figure out how to maximize AI use for those reasons and to minimize students just feeding their problem sets into ChatGPT”\r\n\r\nA recurring theme was the futility of blanket bans. Many students emphasized that prohibition would be ineffective, with one noting “I don’t think anyone really cares what the policy of any given class is. If professors want people to not use it, they need to structure assessments in a way that will discourage use.” Another compared AI bans to restricting internet use, arguing “AI policies seem to be totally irrelevant. It’s like telling people they can’t use the internet as a resource for the class.” This ineffectiveness of bans creates fairness concerns. As one student explained: “I think if it is banned in a class, that should be enforced (and right now it absolutely is not)… As with any form of cheating, those who don’t cheat are put at a disadvantage.” Another student expressed similar frustration: “I find it discouraging when I hear classmates saying they use AI for things such as essays when they use it in dishonest ways.”\r\n\r\nD.2Need for Clear Guidelines\r\nThe most frequently expressed concern was the need for clear guidelines about generative AI use. Several students reported confusion about what constitutes acceptable use, with one noting “I think it should be more clear whether we can use it and how and how to cite it since most professors rarely mention it at all”. Other students emphasized the importance of professors explaining their policies upfront and their rationale, with one stating “I think that Professor’s should be very specific about what is allowed and their reasoning behind their policy”.\r\n\r\nMany students advocated for standardization across classes, observing that “Sometimes its confusing when one class allows it and another doesn’t and the other encourages it and so on so if there was a school wide or department wide policy that could help”. Yet there is also disagreement regarding standardization. Some students preferred leaving the decision to individual professors, arguing that “GenAI is more effective in some classes/majors than others. Making sure professors understand how students use GenAI and how useful GenAI is in their class (given the course structure, nature of assignments/material, etc.) is very important for the class policy.”\r\n\r\nD.3Training and Support Services\r\nStudents strongly emphasized the need for training in appropriate generative AI use. Many suggested that the college should provide guidance on using AI tools effectively while maintaining academic integrity. One student proposed “a workshop that teaches you to effectively use GenAI without violating the honor code”. Students also expressed interest in learning how to leverage AI to enhance their learning experience rather than circumvent it. As one student explained: “I think it could be useful to develop some sort of training. How do we use AI in a way that actually benefits our learning? I tried out some things on my own but I feel that I need more guidance.”\r\n\r\nThis desire for training was often linked to workplace preparedness. Students recognized that AI proficiency would be valuable in their careers, with one noting “As the world uses more and more AI, I think it is an important tool that students should know how to leverage”. Another emphasized: “The moment us students leave campus, we will be using it in the professional world, and when used in combination with one’s own skills, it is merely a tool to maximize efficiency”.\r\n\r\nD.4Discussion\r\nThe responses reveal several key tensions in AI policy implementation. First, students express a desire to use AI in ways that enhance rather than substitute for learning, yet they recognize that blanket bans are ineffective and worry about being disadvantaged if they follow restrictions while others do not.\r\n\r\nSecond, while students desire clear guidelines, they also want flexibility to accommodate legitimate uses that vary by discipline and assignment type. Different courses and majors may find different AI uses appropriate based on their learning objectives and assessment types.\r\n\r\nThird, there is tension between faculty autonomy in setting course policies and students’ desire for consistent institutional standards. While some support letting professors determine appropriate AI use for their specific courses, others argue that varying policies across classes create confusion and enforcement challenges.\r\n\r\nFinally, students report significant variation in faculty attitudes toward AI. Some students perceived faculty fear or misunderstanding of AI tools, noting that categorical bans often reflect a lack of understanding about AI’s capabilities and limitations rather than pedagogical considerations.\r\n\r\nFigure D1:Word Cloud of Student Feedback on Generative AI Policies\r\nRefer to caption\\justify\r\nNotes: Word cloud displaying words that appear at least five times in 133 student responses after removing common English stop words and the word “AI”. Text size is proportional to word frequency. The visualization is based on responses to the question: “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?”\r\n\r\nFigure D2:Frequency of Keywords in Student Feedback on AI Policies\r\nRefer to caption\\justify\r\nNotes: The figure shows the share of open-ended responses that mentioned different themes related to Middlebury’s AI policies. The responses come from the question “Do you have any specific feedback or suggestions about Middlebury’s generative AI policies, resources, or support services?” Color coding indicates the category of each theme. Policy Approach captures suggestions about how AI should be regulated at the college. Adoption Views reflect positions on whether and how AI should be integrated into academic work. Training Needs indicates requests for guidance and support. Other Concerns include issues of workplace relevance and fairness.\r\n\r\nSee pages 1 of results/ai-survey-midd.pdf See pages 2- of results//ai-survey-midd.pdf\n",
    "totalFiles": 2
  },
  {
    "id": "extraction_1754350188668_fnl5ybj8q",
    "timestamp": "2025-08-04T23:29:48.668Z",
    "files": [
      {
        "name": "Kurt paper.txt",
        "size": 103748,
        "type": "text/plain",
        "extractedText": "Photo Steward: A Deliberative Collective Intelligence Workflow for Validating Historical Archives\r\nVikram Mohanty, Computer Science, Virginia Tech, USA, vikrammohanty@vt.edu\r\nKurt Luther, Computer Science, Virginia Tech, USA, kluther@vt.edu\r\n\r\nDOI: https://doi.org/10.1145/3582269.3615600\r\nCI '23: Collective Intelligence Conference, Delft, Netherlands, November 2023\r\nHistorical photographs of people generate significant cultural and economic value, but correctly identifying the subjects of photos can be a difficult task, requiring careful attention to detail while synthesizing large amounts of data from diverse sources. When photos are misidentified, the negative consequences can include financial losses and inaccuracies in the historical record, and even the spread of mis- and disinformation. To address this challenge, we introduce Photo Steward, an information stewardship architecture that leverages a deliberative workflow for validating historical photo IDs. We explored Photo Steward in the context of Civil War Photo Sleuth (CWPS), a popular online community dedicated to identifying photos from the American Civil War era (1861–65) using facial recognition and crowdsourcing. While the platform has been successful in identifying hundreds of unknown photographs, there have been concerns about unverified identifications and misidentifications. Our exploratory evaluation of Photo Steward on CWPS showed that its validation workflow encouraged users to deliberate while making photo ID decisions. Further, its stewardship visualizations helped users to assess photo ID information accurately, while fostering diverse forms of stigmergic collaboration.\r\nCCS Concepts: • Human-centered computing → Collaborative and social computing design and evaluation methods; • Human-centered computing → Interactive systems and tools;\r\n\r\nKeywords: crowdsourcing, human-AI interaction, online deliberation, community stewardship, information assessability, online communities, history, person identification, facial recognition, stigmergic collaboration\r\n\r\nACM Reference Format:\r\nVikram Mohanty and Kurt Luther. 2023. Photo Steward: A Deliberative Collective Intelligence Workflow for Validating Historical Archives. In Collective Intelligence Conference (CI '23), November 06--09, 2023, Delft, Netherlands. ACM, New York, NY, USA 19 Pages. https://doi.org/10.1145/3582269.3615600\r\n\r\nFigure 1: Workflow of Photo Steward. (1) Decision-Making: The user compares photos for facial similarity and make decisions on the photo ID using Photo Steward's deliberative validation interface. (2) Access: The user accesses the validation interface from Civil War Photo Sleuth's search results and photo page. (3) Evidence: The community's responses from the validation interface feed into stewardship visualizations that are visible on the Photo Page, which subsequently foster a form of stigmergic collaboration among the users.\r\n1 INTRODUCTION\r\nThe task of correctly identifying individuals in historical photos holds great cultural and economic importance [3, 19, 39, 61]. This identification process is analogous to solving a complex mystery. It often involves corroboration of multiple research processes such as investigating visual clues in a photo, finding relevant reference resources, and comparing multiple low-resolution reference photos [31, 32, 37]. Historical photos pose many challenges, including low-resolution images, scattered reference materials, limited domain expertise, and lack of suitable verification tools. These hurdles often result in misidentifications which can have negative consequences, ranging from distorting historical narratives [60] and fueling conspiracy theories [12] to spreading disinformation [16] and unwarranted financial gains from inaccurate representations [21]. As online platforms such as Ancestry.com, Find-a-Grave, and FamilySearch democratize historical and genealogical research, the risk of misidentification is further amplified due to factors such as inadequate experience, confirmation bias, and automation bias introduced by imperfect automated tools [44, 75].\r\nTo address these challenges, we introduce Photo Steward, a deliberative workflow that leverages collective intelligence to validate historical photo identifications (IDs). Photo Steward's architecture builds upon the concept of information stewardship [18, 72], which involves community-driven validation of content, as seen in online communities like iNaturalist and Wikipedia. We designed and applied the Photo Steward architecture to Civil War Photo Sleuth (CWPS),1 an AI-infused online platform for identifying historical photos. CWPS has over 20,000 registered users and over 25,000 identified Civil War portraits, and faces the problem of historical photo misidentification [43]. Photo Steward provides a validation workflow that promotes careful deliberation during facial similarity comparison and photo ID verification, while enabling users to share opinions. It also visualizes community opinions on the reliability of photo IDs and facial similarity, fostering information stewardship at multiple levels.\r\nWe publicly released Photo Steward on CWPS and conducted an exploratory evaluation of twelve months of usage, including interviews with users of different expertise levels and log analysis of stewardship behaviors on the platform. We found that Photo Steward's stewardship visualizations helped users to find additional evidence (e.g., external sources, uniform clues matching service records, etc.) for assessing the reliability of photo IDs. Users validated hundreds of different IDs on the platform, and found the workflow to be useful for deliberating on facial similarity comparison and fine-grained photo ID decisions.\r\nWe also discuss the implications of community participation, deliberative human-AI interaction, and assessable designs for historical photo identification.\r\n2 RELATED WORK\r\n2.1 Misinformation in a Historical Context\r\nNumerous online communities, forums, and websites have emerged in recent years for archiving and documenting history [57], generating family histories [74, 75], identifying and sharing historical photos [43], trading antiques [1, 8], and facilitating discussions around history [20]. Much like popular social media platforms such as Facebook and Twitter, these history-based platforms are also prone to the problem of misinformation, albeit in a more specialized historical research context.\r\nPrior work has shown that erroneous family history trees were being disseminated across Find-a-Grave and Ancestry, two popular genealogy research communities, as a result of the platform's low bar to entry and inexperienced contributors' over-reliance on inaccurate automated features [75]. Mohanty et al. [43] showed that despite successful identifications on Civil War Photo Sleuth (CWPS), several photos were misidentified in the first month, particularly for photos without period inscriptions or duplicate views (12 misidentifications out of 37). Multiple factors — the correct candidate not present in the search pool, or the user incorrectly assessing facial similarity [53] and picking the wrong match — can lead to incorrect IDs. A follow-up benchmarking study of the underlying face recognition algorithm [44] highlighted its low precision (i.e., it retrieves over hundreds of search results), raising the possibility of errors due to automation bias, i.e., the tendency of users to over-rely on automation for making a decision [47, 50, 64].\r\nIdentifying historical photos is a complex investigative process, often involving the corroboration of multiple evidence pieces [31, 33] and can be seen analogous to \"finding a needle in a haystack\". As a result, historical photo IDs run a high risk of getting misidentified even with the best of intentions. Multiple Civil War photos have also been misidentified in the collections of professionally managed museums and archives, such as the US Library of Congress [36] and the Abraham Lincoln Presidential Library [34]. At the same time, historical photo IDs also have the potential to generate significant monetary value [1, 3, 8, 10], and such financial incentives might also lead to falsified identifications [21]. Validating these historical photo IDs, which are a result of complex, subjective original research, becomes tricky without the lack of domain expertise and access to investigative tools.\r\nWe addressed these challenges in Photo Steward by designing a stewardship architecture that allows users to share their expertise with others. To address the impact of automation, we introduced a two-step validation workflow for the users to deliberate on decisions while interacting with the AI's recommendations.\r\n2.2 Data Validation in Online Communities\r\nMultiple online platforms have leveraged the strengths of crowdsourced contributions for validating the quality of data generated on those sites. Elliott discusses how stigmergic collaboration, where indirect coordination within a community stimulates subsequent actions, plays a role in maintaining articles on Wikipedia [13]. This concept was observed by Wiggins et al. in their study of iNaturalist, an online platform for identifying species, wherein community stewardship behaviors were seen as users agreed on organism identifications to influence the platform's quality grade status [72]. Prior work has shown that stewardship visualizations on Wikipedia (i.e., article quality) [18] and iNaturalist (i.e., ID research grade status) [72] have a positive impact on users' assessment of the information.\r\nAlong these lines, we also built DoubleCheck [42], a quality assessment framework that builds upon the concepts of provenance and stewardship for verifying historical photo IDs. DoubleCheck focused on displaying quality indicator badges for historical photo IDs by capturing accurate provenance information and combining the source trustworthiness information with community opinions on the ID. In this work, we focus solely on the underlying stewardship architecture that helped facilitate the community opinions. Both DoubleCheck and Photo Steward were evaluated in the same lab study, but there is no overlapping data.\r\nVisualizations displaying (surrogate) quality metrics, such as popularity among expert users, social reputation, and content coverage, have been effective in helping users assess the credibility of websites and search results [62]. Prior work has also shown that visualizing the history of edits for a Wikipedia article can have a significant impact on users' perceived trustworthiness of the article [54, 67]. Similarly, Chevalier et al. [7] showed that visualizing the number of contributors, length of the article and discussion, and the history of edits helped users assess the quality of Wikipedia articles faster. On the other hand, Towne et al. [68] found that being exposed to editor conflicts in the discussion of a Wikipedia article lowered the perception of the article's quality, even though the users reported that the transparency raised their perceptions of the page and Wikipedia in general. Morris et al. [45] found that Twitter users relied on the author information for making assessments about the credibility of information in a tweet.\r\nDrawing from this prior work, we designed Photo Steward's stewardship visualizations to highlight the role of collective intelligence, while fostering stigmergic collaboration on CWPS to validate the quality of photo identifications.\r\n2.3 Background: Civil War Photo Sleuthing\r\nThe American Civil War (1861–65) was one of the first major conflicts to be extensively photographed. Over 3 million soldiers fought in the war, with many of them having been photographed at least once. Over 150 years, many of these photos have survived in museums, libraries, and personal collections, but only 10–20% are are identified [69, 77]. Civil War photography has garnered a lot of interest among historians, collectors, dealers, genealogists, archivists, and other experts, who often try to identify unknown photos for personal, cultural, and economic reasons. However, the identification process is complex and challenging, which often involves identifying visual clues in a photo and manually scanning through hundreds of low-resolution photos, military records, and reference books for corroborating evidence [31, 33, 38].\r\n3 ENHANCING CIVIL WAR PHOTO SLEUTH: DESIGN OPPORTUNITIES\r\n\r\nFigure 2: CWPS Haystack Model: Person Identification Pipeline [43]\r\nCivil War Photo Sleuth (CWPS) is a free, public website where users can identify unknown portraits from the American Civil War era using a person identification pipeline that combines crowdsourced human expertise and face recognition [43]. Drawing analogies to finding a needle in a haystack, Mohanty et al. propose a 'haystack model' to describe CWPS's person identification pipeline. In this pipeline, a user begins the identification process by first tagging a photo for uniform clues, which then generates search filters based on service records, and then facial recognition returns facially similar-looking results from a pool of potential candidates, ordered by similarity to the query photo, that satisfy the search filters (see Figure 2).\r\nThe CWPS haystack model is designed to prevent misidentifications by placing human decision-making at the forefront and treating AI as a supportive tool. It avoids automatically selecting the best match or displaying the algorithm's inconsistent confidence levels [41, 44]. Instead, the user carefully inspects search results for potential matches based on facial similarity and corresponding biographical details. Once a photo is identified, CWPS links the face and identity together and displays the ID on the photo page.\r\nDespite these measures, the open participation model of CWPS, which lacks verification, has raised concerns about the trustworthiness of proposed identities and the potential increase of \"false positives\" as the site grows [22]. To address these concerns, we enumerate three design goals which draw upon prior work on Civil War photo identifications and CWPS system designs, evaluations, and critiques [22, 30, 33, 35, 41, 43, 44], as well as our own observations and experiences using the publicly available version of the website. We provide further details in Appendix A.\r\nDesign Goal 1: Decouple facial similarity comparison from the overall task of person identification.. The current CWPS workflow conflates facial similarity and person identification into a single decision-making process (see Figure 2-C). Facial similarity, while important, can conflict with the identity suggested by personal details like biographical information and service records. The facial recognition algorithm's low precision [44] adds to the complexity, with the possibility of users interacting with false positives. In order to discourage over-reliance on facial similarity, we propose this design goal of separating both these tasks, allowing users to deliberate on the facial similarity and other person identification attributes separately, thereby minimizing inaccuracies.\r\nDesign Goal 2: Support fine-grained, deliberative decision-making.. With the current CWPS interface only permitting binary feedback during photo identification, there is a heightened risk of misrepresentation and misidentifications (see Figure 2-C). Historical photo identification is intricate, often demanding careful corroboration of numerous evidence pieces, with user confidence varying according to the evidence at hand [33]. To more effectively facilitate this complex process, we advocate for a design that enables users to express their level of certainty in their identification decisions. This design goal seeks to foster more accurate and nuanced user feedback, as well as reflective and deliberate decision-making [28, 29].\r\nDesign Goal 3: Encourage community contribution and transparency for validating photo IDs.. Identifying individuals in photos can be considered a subjective process and often benefits from multiple perspectives [41]. Currently, CWPS lacks mechanisms for community feedback and transparency about the process of photo identification, leaving potential misidentifications unchecked (see Figure 8 in Appendix). To support accurate original historical research, the platform should encourage community participation in validating identifications and promoting transparency around the roles of community contribution and facial recognition [18, 72]. This approach also encourages collective responsibility, facilitating stigmergic collaboration [13, 14], where user contributions guide future validation efforts.\r\n4 SYSTEM DESCRIPTION: PHOTO STEWARD\r\nWe developed Photo Steward, an information stewardship architecture that integrates a deliberative workflow for the community to validate historical photo identifications, which we then integrated into CWPS. Photo Steward's architecture has three main components (see Figure 1): 1) a deliberative decision-making interface for facial similarity comparison and photo identification, 2) new access points for validating photo identities, and 3) stewardship evidence for fostering stigmergic collaborations.\r\n4.1 DECISION-MAKING: Deliberating on facial similarity and photo identification\r\nAs part of Photo Steward, we introduce a multi-step \"Validation Interface\" (see Figures 3 and 4) to replace CWPS's single-step comparison interface. Photo Steward's validation interface allows users to deliberate while interacting with the facial recognition results. Meeting Design Goal 1, the validation workflow separates the task of facial similarity comparison from the overall goal of identifying the photo.\r\nTo inform our design, we draw on evidence-based decision-making [11], a model primarily used in healthcare, policymaking, and judicial sectors, which advocates for justifying decisions (photo IDs in this case) by gathering available evidence (facial similarity as visual evidence here). In the first step, the user compares the query photo to all other photos with the same identity for facial similarity. After deliberating on the facial similarity evidence, the user then votes on whether the query photo fits the target identity in the second step (which is the user's primary goal).\r\nThe validation interface is divided into four columns (from left to right): 1) the task description, 2) the query photo, 3) the evidence that is being weighed, and 4) the biographical information. The query photo and evidence are positioned in the two middle panels for easy side-by-side comparison. The task description panel displays the rating question for both the facial similarity comparison and the identification steps. Here, we used structured feedback to capture both the user's facial similarity comparison and their confidence on the photo ID, in an effort for encouraging users to exercise personal deliberation on all available evidence before making a decision on the ID. The interface updates the task description and evidence column depending on which task the user is performing.\r\nTo investigate the identity of a query photo, the user opens the validation interface which loads all the photos and biographical information available for the target identity.\r\n4.1.1 Validation Step 1: Facial Similarity Comparison. For the first step of the validation process, the interface displays the target photo in the evidence column next to the query photo for easy facial similarity comparison (see Figure 3).\r\n\r\nFigure 3: Step 1 of Photo Steward's Validation Interface. Users can compare two photos and answer whether they show the same person or not. They have the option of selecting whether the two photos are a facial match (i.e., same person, different views) or a replica (i.e., same person, same view). Here, the user is comparing whether Photo 41929 and Photo 1 show the same person or not. If multiple faces are available for the same ID, they appear one after the other in the order in which they were uploaded to CWPS.\r\n\r\nThe user's task is to determine whether both photos show the same person (regardless of whether the identity is known). Users can select from the following options: No (Different Person), Not Sure, Yes (Facial Match) and Yes (Replica). Mohanty et al. found that photos correctly identified on CWPS were either facial matches (i.e., same person, different view) or replicas (i.e., same person, same view) [43], which informed the design of this input scale. Since facial similarity does not have any standard scale and users may perceive the similarity or dissimilarity of two faces differently [40, 76], we chose not to capture any further granularity in their responses for facial match, replica or different person as this might lead to inconsistent data collection.\r\nCapturing these responses in a structured way allows users to deliberate on the task of facial similarity; this becomes more critical as users are also interacting with the results of a low-precision facial recognition algorithm [44]. In this step, the user compares facial similarity of the query photo with all available photos of the target identity, one photo at a time.\r\n4.1.2 Validation Step 2: Fine-Grained Photo Identification. In this step, the user analyzes the biographical information and incorporates the facial similarity evidence from the previous step to make a decision on the photo's identity.\r\nThe validation interface displays information in the same four-column layout (see Figure 4), with the evidence column now displaying a summary of the user's responses about facial similarity between the query photo and the target photo(s). The biography column shows the name and the service records for the user to analyze.\r\n\r\nFigure 4: Step 2 of Photo Steward's Validation Interface. Users vote on whether the query photo can be identified as the target identity by expressing their confidence. They can also add an optional note to justify their decision. The evidence panel displays a summary of the user's responses from the first step, where the faces are displayed next to the user's facial similarity comparison with the query photo. The faces are ordered in the way they appear for comparison, i.e., the order in which they were uploaded to CWPS.\r\n\r\nThe user now decides whether the query photo can be identified as the target identity (see Figure 4). The instruction above nudges the user to factor in the prior photo comparison evidence and the biography information. Meeting Design Goal 2, users indicate their confidence about the task question by selecting one of the five options displayed in radio buttons: No (Highly Confident), No (Slightly Confident), Not Sure, Yes (Slightly Confident) and Yes (Highly Confident). This scale, which offers more nuance than a binary decision, serves as a proxy measure for the ID's reliability given the complexities of quantifying accuracy in a historical photo ID investigation. These options reflect the varying degrees of confidence users have based on the quality and quantity of corroborating evidence, such as reputable sources, facial similarity to additional photos, and expert opinions. Users also have the option to elaborate their decision rationale in a free-text note.\r\n4.2 ACCESS: Expanding Validation Opportunities for Photo Identifications\r\nPhoto Steward provides stewardship capabilities for the CWPS user community by allowing them to access and use the validation interface at different stages of the photo identification process from multiple gateways (see Figure 1). On the \"Search Results\" page, it can be used for identifying a photo from a pool of potential similar-looking candidates, or ruling out some potential candidates. After a photo has been identified, users can also access it on the \"Photo Page\" to either validate an existing ID or dispute an incorrect one, and collaborate with other users in a stigmergic manner (Design Goal 3).\r\n4.2.1 Search Results Page: Matching and Ruling Out Candidates. While identifying a query photo, users can now inspect potential matches on the search results page with the help of the validation interface. The \"Compare\" button on a search result brings up the validation interface, loading all the target information for the corresponding search result. The target identities in the validation interface will update as the users check new search candidates for matches. The interface allows users to make two types of decisions, depending on their confidence response: 1) either of the \"Yes\" responses will match the photo with the target identity with varying degrees of confidence, and 2) either the \"No\" or \"Not Sure\" responses will rule out the search candidate as a potential match for the current user.\r\n4.2.2 Photo Page: Validating and Disputing Existing IDs. After a photo has been identified, Photo Steward allows users to review opinions from other users (described in Section 4.3) and contribute their own for a given photo ID on CWPS's photo page, fulfilling Design Goal 3. By clicking the \"Give Your Opinion\" button, users launch the validation interface featuring the query photo, the linked target identity, and target photos of the same ID, a new feature previously absent from CWPS. The two-step process mirrors that on the search results page, enabling community deliberation on the validity of an ID. Users can validate the facial match among photos linked to the same ID, express their agreement or disagreement on an ID with varying confidence levels, and optionally add a note explaining their decision. Thus, each vote contributes to a stigmergic collaboration, enhancing the reliability of photo IDs on CWPS. Consistent with CWPS's open participation model, Photo Steward allows any registered user to share their opinion on an identification.\r\n4.3 EVIDENCE: Visualizing Information Stewardship\r\nAs part of Photo Steward, we designed stewardship visualizations to help users assess the reliability of 1) facial matches (i.e., photos that were matched to each other by the user), and 2) photo IDs. The CWPS community's opinions on facial similarity comparison and photo IDs, captured through the validation interface, feeds into these reliability visualizations. These visualizations not only promote user accountability through social translucence [15], but also serve as deliberative evidence for subsequent stigmergic user collaborations (Design Goal 3).\r\n4.3.1 Reliability of Facial Similarity. For each photo pair that has been compared, the system aggregates the community's decisions for the visual match type and generates a distribution, which is displayed in the form of an interactive horizontal bar chart on the photo page. This chart appears next to the corresponding photo matched to the query photo (see Figure 5). Users can click the \"View Details\" button or an individual bar to see how each user voted. When multiple photos have been matched to the query photo, the matched photos appear one below the other, with each having its own visualization next to it. The bar charts are stacked vertically above each other to allow users to easily see and compare the reliability of every match.\r\nTo complement the community stewardship visualization, we also added an AI stewardship badge that indicates whether the particular match is supported by facial recognition (see Figure 5). On the search results page, CWPS retrieves those search results that have a facial similarity score greater than 0.50, so we use the same threshold here. However, the badge intentionally does not display the exact similarity scores (which have been found to be inconsistent [41, 44]) to avoid a false perception of precision, and cautions users to carefully analyze all the context and evidence, as there is a possibility of false positives with face recognition.\r\n\r\nFigure 5: Facial Similarity Reliability Visualization on the Photo Page. The photo matched to the query photo is displayed here, whereas the query photo is displayed on top of the page. Other photos matched to the query photo are displayed vertically one below the other. Users can bring up the query photo and the matched photo side-by-side by clicking the \"Compare\" button. Each matched photo has its own visualization next to it, and the responses are visible to everyone.\r\n\r\n4.3.2 Reliability of Photo Identifications. Similar to the facial similarity visualization, the system aggregates the community's confidence levels for an identification and generates a distribution, displayed in the form of an interactive horizontal bar chart. This visualization is displayed for every proposed identity on the photo page, below the biography subsection (see Figure 6a). If users propose multiple IDs for a given photo, they are displayed one below the other; each ID will have its own visualizations. The community's confidence votes are aggregated to order these IDs. Users can click the \"View Details\" button or the individual bars to view a modal dialog box with each user's confidence scores and optional text comments (see Figure 6b).\r\n\r\nFigure 6: Photo ID Reliability Visualization on the Photo Page for a given ID. If multiple IDs are present, they are displayed vertically one below the other, ordered in terms of aggregate votes. Each ID will have a separate visualization listed under the respective IDs.\r\n\r\nThis visualization reflects the community's expertise, and is intended for users to quickly assess the reliability of an identification. Prior work on crowdsourced person identification [41] suggests that airtight identifications are likely to show consensus from the community, whereas potential misidentifications are likely to reflect disagreement from the community. Further, the comments may reflect the voters' decision rationale and any external research they conducted before giving their decision on the photo's identification, allowing users to build on the work of others in making their own assessment [17].\r\n4.4 Summary\r\nPhoto Steward augments the CWPS platform with an information stewardship architecture to support community validation of historical photo IDs in a stigmergic manner. We summarize all the changes in the Appendix (see Table 6).\r\n5 EVALUATION\r\nWe obtained permission to publicly launch Photo Steward on CWPS in December 2020. We conducted a mixed-methods, exploratory evaluation study to understand how well users with different expertise levels could validate and assess Civil War photo IDs using CWPS with Photo Steward. Specifically, we wanted to understand 1) how users validated photo IDs using Photo Steward, 2) how the stewardship visualizations (i.e., ID and facial similarity reliability visualizations) impacted users' assessment of an ID. The study was approved by our university's IRB.\r\n5.1 Log Analysis\r\nTo understand the community's stewardship behaviors, we analyzed website logs of all user activities for a year after new features were launched, which included 5843 voting instances on 5672 photos for 5355 unique IDs. Our analysis included categorization of user deliberations as 'pre-identified' (i.e., the user knew the ID of the photo ahead of time and therefore, might be looking for a specific target on the search results page) or 'post-identified' (i.e., the user did not know the ID of the photo and therefore, may not have a specific target while analyzing the search results), coding of user comments (see Appendix C), and comparison of community's facial similarity comparisons against facial recognition scores. Details of these analysis methods are provided in Appendix B.\r\n5.2 Lab Study\r\nIn order to understand how well Photo Steward supports diverse users in validating the quality of photo identifications, we also conducted an exploratory lab study.\r\n5.2.1 Participants. We recruited 15 participants representing the three major expertise levels: 5 history students, 5 amateur experts (experienced users of CWPS), and 5 expert historians. Participant details can be found in Appendix D. We anonymize these groups with the following identifiers, respectively: S1–S5, C1—C5, and H1–H5.\r\n5.2.2 Dataset. For the study, we created a dataset of 10 different photos identified on CWPS. Three of these photos had an ID conflict, i.e., multiple identities were proposed. For two of these photos, one ID was correct and the other one was incorrect. The community had already researched both photos, voted on the correct ID, and left credible evidence in the comments. Both IDs were linked to additional photos as well. The third photo was one of the seeded photos on CWPS, but was originally misidentified. We added another false ID, making both IDs for the third photo incorrect. All photos had multiple photos matched to them; eight of them were linked via facial matches, while two of them had replicas.\r\n5.2.3 Procedure. The entire study was conducted online via recorded Zoom sessions, with at least one researcher attending each session. Each participant first completed a consent form and a pre-survey describing their demographics and Civil War photography experience.\r\nAs part of the study, participants reviewed three randomly assigned photos from the dataset one-by-one in the original CWPS system first, followed by the same photos on the Photo Steward version. Participants used a think-aloud protocol while using the two systems; after the completion of the task with each system, they were asked a few semi-structured questions about their experience. Finally, the participants completed a summative post-survey of standard usability questions (e.g., ease of use, usefulness of features, instruction clarity, preferred system, etc.) (see Appendix E)\r\nWe maintained this sequence (original CWPS first, CWPS with Photo Steward second) for all the participants, rather than using a randomized sequence, for two reasons. First, we did not want participants' assessments to be biased in favor of Photo Steward after seeing additional features in the new interface. This design allowed us to observe if the original interface misled the participants towards incorrect assessments, and if, subsequently, the Photo Steward interface helped correct them. Second, in a randomized sequence, Photo Steward would expose the participants to new information in the form of prior user votes and responses, and therefore, may confound how they assess the information on the original CWPS version.\r\n5.2.4 Data Analysis. The first author fully transcribed and analyzed the interviews and think-aloud recordings using an inductive qualitative thematic approach [4]. The transcript sections were first divided according to the interface in question (i.e., original CWPS or Photo Steward), followed by an open coding of the transcripts using MAXQDA 2020 [65]. The first author iterated and settled on a total of 28 codes (e.g., change in opinions, comparison interface, source trustworthiness, etc.) for 634 coded segments across all the transcripts. These codes were then organized into themes as described in Section 6 after discussing with the co-author.\r\n5.2.5 Limitations. We conducted a qualitative lab study to understand how users with different backgrounds and expertises validated photo IDs using Photo Steward and hit theoretical saturation. However, there are a couple of limitations with the study: 1) limited insights on the role of expertise, and 2) the task sequence could have order effects. Further, the large-scale analysis of Photo Steward logs provided us with insights of its usage amongst users. However, it lacked an expert-prepared gold standard dataset, which hindered our ability to conduct specific performance analyses as part of this study.\r\n6 FINDINGS\r\nUsing the methods above, we evaluated how well Photo Steward's stewardship architecture supported CWPS users in validating photo identifications, compared to the original version of CWPS.\r\n6.1 Validation Interface\r\nUsers found Photo Steward's validation interface to be useful for comparing different photos.. While assessing the IDs with the original interface, participants would go back and forth between different photos to compare whether they are the same person or not. Some participants opened the photos in two different browser windows and kept them side-by-side. While using the validation interface in the new system, participants appreciated being able to see the photos side-by-side at the same time.\r\nH1 said, \"As an historian using this, this is really great to see them both together. It just makes a comparison a lot easy for me to do. I mean, this is the same gentleman, he's got a little dark facial hair. It looks a little bit different there and the photo on the right, but the facial match is definitely there. \" This was also echoed by C3, who said, \"This, I really find extremely useful, especially when I'm trying to do facial recognition. I can zoom in and have them side by side here. [...] Where in the past, I would have to go back and forth between tabs or cut and paste them into a different document to look at them side by side.\"\r\nFrom our logs, we found that 223 users had compared 2319 unique photo pairs for facial similarity, with 156 pairs receiving comparisons from at least 2 different users. The facial similarity responses were distributed as follows: 763 replicas, 1232 facial matches, 283 unsure, and 280 different people.\r\nTable 1: Distribution of User Votes.\r\n\r\nVotes\r\n/ ID\r\n# of IDs\r\nNote\r\nPresent\r\nNegative\r\nVotes\r\n1\r\n5650\r\n511\r\n61\r\n2\r\n157\r\n(Agreement: 119)\r\n(Disagreement: 38)\r\n83\r\n12\r\n3+\r\n36\r\n(Agreement: 21)\r\n(Disagreement: 15)\r\n31\r\n4\r\nTable 2: Distribution of Confidence Levels.\r\n\r\nMean\r\nConfidence\r\n1\r\nVote / ID\r\n2\r\nVotes / ID\r\n3+\r\nVotes / ID\r\n-2 (No - Highly Confident)\r\nto\r\n-1 (No - Slightly Confident)\r\n45\r\n3\r\n2\r\n-1 (No - Slightly Confident)\r\nto\r\n0 (Not Sure)\r\n16\r\n3\r\n2\r\n0 (Not Sure)\r\n99\r\n8\r\n4\r\n0 (Not Sure)\r\nto\r\n1 (Yes - Slightly Confident)\r\n760\r\n15\r\n5\r\n1 (Yes - Slightly Confident)\r\nto\r\n2 (Yes - Highly Confident)\r\n4730\r\n128\r\n23\r\nUsers preferred the ability to provide granular feedback for photo IDs using Photo Steward's validation interface.. All participants expressed preference for the fine-grained confidence levels, including the ability to dispute an ID, in Photo Steward's validation interface, appreciating how it more accurately mirrored the inherent uncertainty present when assessing photo IDs. S3 said, \"I definitely like the five levels. I think it leaves more room for interpretation. Like sometimes it's kind of hard to just say yes or a hard no because so much goes into it. Especially because a lot of this stuff was so long ago, there's so many unanswered questions.\" H4 initially defended the original interface's binary vote, but changed her mind after experiencing Photo Steward's confidence levels: \"[W]hat I had said has this very black and white feel to it, you're wrong or you're right. I like these degrees of disagreement or agreement. I think that's way more helpful broadly.\"\r\nThe usage of the voting feature was reflected in our logs, which showed 5843 voting instances from 328 unique users (see Table 1). Table 2 shows that while users utilized the full range of confidence levels, including when they were unsure (mean confidence = 0) or slightly confident (mean confidence = 0 to 1) about the ID, the vast majority of the votes were highly confident ones (mean confidence = 1 to 2). A small proportion of votes (77) were cast for disputing an ID.\r\nUsers justified their voting decisions through notes covering an extensive range of topics..\r\nTable 3: Distribution of Note Topics. The table also displays how the notes are distributed for different user confidence levels.\r\n\r\nCategory\r\nSub-Category\r\nTotal\r\nNumber of\r\nNotes\r\nNo\r\n(Highly\r\nConfident)\r\nNo\r\n(Slightly\r\nConfident)\r\nNot\r\nSure\r\nYes\r\n(Slightly\r\nConfident)\r\nYes\r\n(Highly\r\nConfident)\r\nPhoto\r\nComparison\r\nHigh-Level\r\nComparison\r\n206\r\n9\r\n5\r\n9\r\n20\r\n163\r\n\r\nDescribing\r\nFacial Features\r\n37\r\n4\r\n3\r\n9\r\n8\r\n13\r\nWord-of-Mouth\r\nDescendant\r\n55\r\n1\r\n0\r\n0\r\n8\r\n46\r\n\r\nOwnership\r\n47\r\n1\r\n0\r\n0\r\n0\r\n46\r\n\r\nFamiliarity\r\n18\r\n0\r\n0\r\n0\r\n2\r\n16\r\nVisual\r\nEvidence\r\nUniform\r\n73\r\n7\r\n1\r\n8\r\n17\r\n40\r\n\r\nInscription\r\n145\r\n4\r\n3\r\n3\r\n6\r\n129\r\n\r\nOther\r\nVisual Clues\r\n17\r\n1\r\n0\r\n1\r\n2\r\n13\r\nExternal\r\nInformation\r\nExternal URL\r\n47\r\n1\r\n0\r\n1\r\n4\r\n41\r\n\r\nOther Sources\r\n172\r\n9\r\n1\r\n1\r\n21\r\n140\r\n\r\nLack of\r\nInformation\r\n21\r\n0\r\n2\r\n13\r\n6\r\n0\r\nProviding\r\nAdditional\r\nInformation\r\nBiographical\r\nInformation\r\n46\r\n11\r\n2\r\n3\r\n3\r\n27\r\n\r\nAdditional\r\nContext\r\n117\r\n5\r\n0\r\n2\r\n11\r\n99\r\nFrom our logs, we found that 155 users had left 682 notes for 600 different photos. However, as Table 1 shows, around 10% of the votes had a note. Table 3 shows the different topics covered by the notes. We observe that users' voting patterns are significantly influenced by the availability and quality of evidence, with clear facial similarity, period inscriptions, personal anecdotes, visible clues in the photograph, and added biographical context often leading to high confidence \"Yes\" votes, while lack of information typically results in \"Not Sure\" votes.\r\nUsers most frequently left a comment attributing facial similarity (after comparing it in the first step of the validation process) to be the reason for their decision (e.g., \"Identical to the other CDV\"). In some instances, they would expound on it by discussing facial features: \"The eyes, nose, cheek bones, shape of face, all look similar to George Pickett, although possibly reversed based on hair part\". In many instances, we found users inferring biographical information (service records, location, etc.) from visual evidence in the photo, be it uniform or backmarks (e.g., \"Initials MN on chinstrap (brass letters). Signature on verso is made out to Marlin's oldest sister. Style of insignia is consistent with other 1862 recruits for Co. B 1st USSS\").\r\nInterestingly, we also observed a large number of word-of-mouth evidence notes for justifying the user's decisions, such as claiming to be a descendant, or owning the original copy of the photo, or having seen the photo somewhere. For example, one user noted, \"He is my great-great grandfather and this photo has been passed down through the generations to me and was identified by his son John Albert Johnson, my father's grandfather.\" Users also left external URLs and source details in the notes as evidence. Sometimes, they provided additional context (e.g., \"This image came with a group of 7th Iowa images. The majority were of Company G., but there is only one person in the entire 7th Iowa Infantry that could be identified by the first or last name of 'Nelson.'\").\r\nThe validation workflow encouraged users to exercise careful deliberation while making photo ID decisions.. Users felt that the questions in each step of the validation process helped them to carefully weigh in all the evidence and deliberate while voting on the ID (Q4, mean = 4.60, SD = 0.49). C3 explained why the two steps were necessary: \"It's two separate things. One is asking, do you think that this face is the same face? Then the second is, do you think that this face matches this name? I think that that is a necessary question for both of those scenarios. I don't think it's redundant, I think it's necessary.\" H2 appreciated the thoughtfulness that the two-step process encouraged, saying, \"It could be the same guy, but it might be a different guy, but now that you know the other interface kind of forces me to slow down a little bit and think more carefully, because it's asking specific questions about things.\" A couple of participants, however, expressed initial confusion over the separation and order of these two tasks.\r\n\r\nFigure 7: Deliberation in case of post-identification voting. The table shows the distribution of confidence votes (second step of the validation process) against the user's facial similarity comparison (first step of the validation process). The comparison scores were computed by aggregating all the facial similarity comparisons in the first step (replica = 2, facial match = 1, not sure = 0, and different people = -1).\r\n\r\nOur logs affirm the deliberative aspect of the two-step validation process. We observed that when users compared one or more photos in the first step, they factored in the facial similarity evidence for their ID vote decision. Figure 7 shows that when the cumulative comparison scores are positive (i.e., majority of the comparisons were a facial match or a replica), the confidence scores are generally positive. Similarly, when the cumulative comparison scores are negative, the confidence votes are also largely negative. When the users are unsure about the facial similarity comparison, it reflects in their final ID vote. Notably, when there were mismatches between comparison scores and ID votes, users provided reasons in the notes, citing visual evidence and an unsure photo comparison.\r\nPhoto Steward's stewardship architecture fostered diverse forms of meaningful stigmergic collaboration amongst users.. Although most IDs received only a single vote, about 3% (193 IDs) received multiple votes (see Table 1). Among these, 140 IDs saw total consensus among voters on confidence scores, with half involving an explanatory note. When the initial vote lacked a note, subsequent voters often added information like external sources or context. Almost all IDs where voters agreed positively were supported by metadata such as period inscriptions, scholarly sources, or additional comparison photos, underscoring the credibility of these agreements.\r\nWhen users disagreed on an ID, they would almost always justify in notes why they differed from the previous voters' opinions (43 out of 53 disagreement instances). From analyzing the notes, we found that the subsequent voters provided additional information about external sources or service records to disagree (e.g., \"Please see Military Images magazine, MI, Volume XVI, Number 3, November - December 1994 for the correct identification of this image. The correct identification via the Michael McAfee collection is Richard Cramer, 4th Michigan Infantry\"). Among 18 instances of conflicting IDs for the same photo, seven saw resolution through a process of voting one ID over another, often accompanied by external evidence in notes or citing facial similarity.\r\nUser agreement was particularly strong in facial similarity comparisons. Of the 2200 unique photo pairs compared, 141 received multiple user reviews, with 114 seeing total agreement. The remaining cases typically involved user errors in distinguishing between 'facial match' and 'replica', or uncertainty about facial similarity.\r\n6.2 Stewardship Visualizations\r\nStewardship visualizations reflecting community insights enhanced the accuracy of photo ID assessments.. Participants found Photo Steward's bar chart visualization showing the five confidence levels to be simple and easy to understand (Q1, mean = 4.93, SD = 0.25). S1 said, \"I'm a visual person. Bar graphs or charts like statistical data helps me put things into a better perspective or gives me an idea of what I am working with versus just something more plain [like] the other interface.\"\r\nWhen participants were uncertain about an ID, they saw the additional evidence and justification that the community members had provided along with their vote as essential to taking the vote seriously. S5 said, \"If there's other users giving comments like 'I've used this source,' and you know you get information from a source where they found it, I'm probably gonna agree with them. If they're just voting yes without anything else, then I am probably more likely to go out and find sources for the photo myself and make my own decision.\"\r\nIn three instances, students (S1, S3, and S5) initially made incorrect assessments on the original interface but rectified these after using Photo Steward. The community's consensus and evidence in Photo Steward were pivotal in these corrections. S5, confronted with ID conflicts, stated after using Photo Steward: \"You know that helps a lot with what people are thinking and presumably these people have also gone to the Maine State Archives or something and verified and then given their opinion so that helps.\" Similarly, historian H2 and collector C3 initially made incorrect choices but amended their decisions after analyzing community-based evidence on Photo Steward. H2 remarked: \"So I believe it's the first person, of course, because it's based on the Maine State Archives. And then you've got the piece on there that said that you looked and found a different man. With that name in the actual regimental history. I say that's fairly accurate information.\" This highlights the critical role of community consensus in resolving photo ID conflicts.\r\nOn the other hand, when participants were confident about an ID, seeing the community's opinions affirmed their own assessments, for better or worse. In the words of H5: \"I really liked the very clear community consensus, and the ability to be able to see the identities of the people who were looking at these images. It was something that (gave) a boost of confidence in terms of my final decision.\" Overall, the participants found the community opinions to be useful for assessing the IDs (Q2, mean = 4.93, SD = 0.25).\r\nUsers gave additional weight to the opinions of members they were familiar with and desired more contextual information about all contributors.. Participants examined the bar chart visualization details and gave higher weight to the opinions of prominent names from the Civil War photography community while assessing the ID. C3 said, \"Here's <name redacted>, period inscription with valediction, the uniform matches his service record. <name redacted> said the same thing. These two uploaders I hold in very high regard to their opinions on this site. If they're saying that they think highly confident of this identification, that gives me a lot of confidence as well.\" H5 became further unsure about an ID after seeing a fellow historian's vote: \"And, you know, sort of knowing <name redacted>, someone who I know deals with primary source material a lot, and sort of being on the fence with it as well, leaves me in that unsure position.\"\r\nParticipants also sought additional details about community members, proposing indicators of professional status or active participation in the CWPS community. H1 emphasized his appreciation of amateur experts and opposition to gatekeeping, yet he suggested having some kind of credential indicator next to the username would be helpful: \"It will be great if, you know, somebody was an academic historian or a published Civil War author, if there was some way to just say some little tidbit next to <name redacted> 'Oh, saying, hey, I'm from <university redacted>, Professor or, you know, author of whatever.'\" Others preferred indicators of community activity levels or personal connections (i.e., descendants) to the identified individual in the photo.\r\nOur log analysis showed revealed the presence of certain active voters, with 9 out of 328 unique voters voting on more than 50 photos each, and 2 voting on over 2000 photos each (mostly ones that they had uploaded and identified). If we consider only votes on photos identified by someone else, we have 8 users who have voted on more than 15 IDs each, with 1 user voting on over 150 IDs.\r\nThe community opinions made the platform feel more engaging, but users had mixed thoughts about the ideal number of votes per photo.. Participants, in general, felt the community opinions made Photo Steward more engaging compared to the original system. H5 said, \"I thought it appeared more user-centric and user-friendly and it felt more participatory.\" C3 saw the benefits of increased engagement for making IDs more reliable: \"I think for this crowdsourcing project that we're building on this database [it] is very important to have those comments, those feedback, that we see in the voting system. It only makes this ID stronger and makes the project and the database a more trustworthy and reliable source.\"\r\nUsers wanted to see more community opinions and comments for IDs that had few votes. S1, on seeing only one vote for an ID, said, \"That doesn't make me feel as confident because that's not too many for me to give an answer.\" S3 said it would have been easier to assess some photos if \"there was more input from other people.\" Users had mixed opinions about how many votes they wanted to see for verifying an ID. In general, they wanted to see consensus among the community for an identity and at least three votes. C3 said, \"I usually shoot for like three to four [votes] as the lowest where I take some good quality out of those votes. [...] If there's only one or two, and especially if there's two that are split, that is not as reliable to me.\"\r\nParticipants' concerns about spreading voters too thin were borne out in the log data (see Table 1). We analyzed the logs to check how often CWPS users vote on the photos they are browsing. We found 1784 instances (out of 5843 voting instances) where the number of \"lurkers\" for a given photo page exceeded the number of voters on that photo.\r\nTable 4: Face Recognition Similarity Score Stats vs. User Comparisons (Facial Similarity).\r\n\r\nUser Comparison\r\nMean\r\nMedian\r\nSD\r\nCount\r\nReplica\r\n0.88\r\n0.91\r\n0.16\r\n659\r\nFacial Match\r\n0.52\r\n0.65\r\n0.31\r\n1015\r\nDifferent Person\r\n0.55\r\n0.59\r\n0.16\r\n265\r\nNot Sure\r\n0.51\r\n0.58\r\n0.23\r\n261\r\nUsers found the face recognition badge and community's opinions complementary for assessing the reliability of matched photos..\r\nTable 5: User Comparisons (Facial Similarity) vs. Face Recognition.\r\n\r\n\r\n# of Photo Pairs\r\n# of Photo Pairs\r\ncompared by 1 user\r\n# of Photo Pairs\r\ncompared by 2 users\r\n# of Photo Pairs\r\ncompared by 2+ users\r\nSupported by users\r\nand face recognition\r\n1408\r\n1281\r\n109\r\n18\r\nDisputed by users\r\nand face recognition\r\n26\r\n24\r\n0\r\n1\r\nUsers unsure,\r\nface recognition disputes\r\n42\r\n42\r\n0\r\n0\r\nUsers unsure,\r\nface recognition supports\r\n219\r\n217\r\n2\r\n0\r\nUsers dispute,\r\nface recognition supports\r\n239\r\n238\r\n1\r\n0\r\nUsers support,\r\nface recognition disputes\r\n266\r\n257\r\n8\r\n1\r\nFrom our logs, we found that 1408 photo pair comparisons were supported by both users and facial recognition, by far the most common outcome (see Table 5). This information would be visible to the larger user community in the form of stewardship visualizations (see Figure 5). Interestingly, we also see 266 cases where a comparison is supported by users, but disputed by facial recognition (similarity confidence score < 0.50). That outcome was approximately as common as when users disputed a comparison but face recognition supported it (239 pairs) or when a user was unsure but face recognition supported it (219 pairs). However, it was far less common for facial recognition to dispute a comparison when a user also disputed it (26 pairs) or was unsure (42 pairs).\r\nWhen we analyze the face recognition's confidence scores in more detail, Table 4 shows that there is a clear separation between replicas and other types of user comparison scores. Face recognition confidence scores for photo pairs that users labeled as replicas were much higher in terms of both mean and median (0.88 and 0.91, respectively, versus scores in the 0.50s and 0.60s for all others). While these fine-grained scores are not displayed to users — they intentionally see only the face recognition badge — the very close mean and median confidence scores for \"facial match\" versus \"different person\" illustrate the difficulty of automatically identifying non-replica matches and offer support for a hybrid human-AI approach (cf. Section 4.3.1).\r\nParticipants found the community's opinions to be helpful for assessing whether two photos were facial matches or not. S4 said, \"I mean, I think it's cool to see what the community is saying, because I do feel when it comes to saying 'Is this the same person in both these pictures?', that's really the best way to do it if you don't have any [other] information.\" C1 said that he found the community opinions for facial matches especially helpful because he is \"face blind\": \"I can't really identify the face-to-face, but the hair and the mustache and all the stuff that, in addition, it helps me with that for sure. It's good because other than having to find somebody close to me and be like, 'Do you think this is the same people?' [I] have that community right there.\"\r\nParticipants had mixed opinions about facial recognition technology, but most found the badge indicating whether it supported the two photos being a match to be a useful data point. S3 said, \"The facial recognition saying they are similar — I would go ahead and trust that but I don't know if I would trust it enough to make a verification on my own.\" In general, participants found the strengths of facial recognition and the community to be complementary in determining whether two photos showed the same person, and liked seeing both results together. H4 felt the community and the technology had separate roles:\r\nThis is facial recognition, and this is the historical background. I trust facial recognition, but it makes me feel better to have that historical background. I think the human eye can be tricked by different hairstyles and different beards. Just to have this outside historical verification to say like, 'Okay, maybe you or I was tricked, but the machine was not,' I think that's really helpful.\r\nS1 relied on both the community and facial recognition to make a decision on a facial match: \"I think both give me kind of an idea. Okay, there's this facial recognition technology being used, but also there's other users that are leaning towards that this is the same person.\" H5 got a similar boost of confidence: \"This is certainly reassuring seeing not only the AI match, but also in terms of the community — seeing that seven users have said that this is a facial match as well. I'd be quite convinced by this.\"\r\n7 DISCUSSION\r\n7.1 Leveraging Collective Intelligence for Validating Person Identification\r\nPrior work has raised concerns about misinformation in online history communities [43, 75]. To address these problems on CWPS, we built Photo Steward for supporting community-based validation of photo IDs. Users found Photo Steward's stewardship visualizations not only helpful for affirming their own assessment, but also for discovering new knowledge and correcting their decisions, if need be. These visualizations, combined with the validation workflow, exhibit a form of stigmergic collaboration, where users build on prior knowledge left by the community and leave their own assessment for other users [14, 17, 27, 56].\r\nPhoto Steward allowed users to express how confident they are about an ID in a fine-grained manner, in contrast to the binary agreements or disagreements observed on iNaturalist by Wiggins et al. [72]. The CWPS community preferred this nuanced form of stewardship as users are likely to have different degrees of confidence based on the evidence available for identifying a photo, thus demonstrating the effectiveness of Design Goal 2. As S3 pointed out, users often experience difficulty in making a binary decision about individuals who lived 150 years back due to the lack of surviving documentation.\r\nBeyond its basic usefulness, Photo Steward's full potential can best be realized through sustained community participation, but most IDs on CWPS only received one vote. To address this challenge, we can leverage different crowdsourcing and online community strategies. For example, we can draw the community's attention towards IDs that are \"more of a puzzle,\" as H5 suggested, similar to Twitter's Birdwatch promoting tweets for fact-checking [52]. Designing nudges to encourage lurkers to vote on the IDs they are viewing can further help in these efforts. Organizing community events can help foster interest and participation in collaboratively verifying IDs, drawing inspiration from crowdsourcing events like CrowdSolve, where experts and novices collaborate on solving missing persons cold cases [70]. Incentive mechanisms such as leaderboards and challenges [46] can drive extrinsic motivation within the community for verifying the IDs. Finding users who are more likely to vote on an ID, based on their skills and interests, can also be an effective collaboration strategy [71]. In future work, we plan to integrate these strategies and introduce explicit \"calls to action\" [51, 58] on the home feed, guiding the community's attention towards IDs that require validation and fostering more sustainable, collaborative participation in historical photo identification.\r\n7.2 Exercising Deliberation in Human-AI Teams\r\nWe found that Photo Steward's validation workflow was effective not only for voting on the IDs and comparing the photos side-by-side, but also encouraged users to deliberate on their decision, drawing parallels to other social computing systems that support reflection and deliberation (e.g., [28, 29]). This deliberative intervention was non-trivial as users on CWPS follow an identification pipeline which is powered by facial recognition, an AI algorithm that is far from perfect [44]. Users are trying to find the correct match, if present at all, from a pool of potential candidates, which are largely comprised of similar-looking false positives — akin to finding a needle in a haystack. Further, the task of comparing photos of people is by no means an easy task for humans, even in a modern context [53]. While Photo Steward can not completely curb automation bias, an issue that has been previously observed in multiple online history communities [44, 75], its multi-step, validation workflow with structured feedback interventions encouraged users to deliberate over AI suggestions before making a decision.\r\nPhoto Steward's workflow also compartmentalizes the tasks that AI is good at – such as quickly retrieving similar-looking candidates from a large search pool – from the tasks where the AI makes more errors – such as verifying whether two faces show the same person or not [5, 55]. Decoupling facial similarity comparison from the person identification task (Design Goal 1) allows the users to now focus on the face verification task. In doing so, Photo Steward's workflow supports effective human-AI teaming in the context of person identification by allowing the user to make a granular assessment for the face verification task instead of the AI, while also ensuring that an AI-retrieved, similar-looking potential candidate is being compared against.\r\nAs imperfect AI algorithms get deployed in high-stakes scenarios such as medical imaging, law enforcement, etc. [6], it becomes more critical to reduce automation bias and encourage more deliberative decision-making. Amershi et al. recommend granular user feedback while interacting with AI systems as part of their \"Guidelines for Human-AI Interaction\" [2]. Similarly, other forms of design interventions, such as counterfactual AI explanations [63], chatbots [26], and community opinions [59] can also be explored for encouraging deliberative decision-making with AI assistance.\r\nPrior work in human-face recognition teams has shown that algorithmic suggestions can have a significant biasing effect on a user's decision [23]. Our findings showed that Photo Steward was able to encourage users to exercise deliberation while interacting with results retrieved by facial recognition. At the same time, Table 4 also showed that users can differ from the algorithm's suggestions, thus necessitating a deeper dive analysis of this dissonance as part of future work.\r\n7.3 Assessing Quality in Crowdsourced Original Historical Research\r\nWe found that Photo Steward's stewardship visualizations helped users assess the reliability of photo IDs on CWPS, which were a result of Design Goal 3. However, crowdsourced identifications always run the risk of groupthink [24, 25], which can eventually mislead users into believing and amplifying misidentifications, a concern also raised by H4. Public deliberation of modern photo IDs on social media can have profound negative consequences for false targets, as exemplified by the Boston Marathon bombing [49, 66] and the recent US Capitol riot [48]. This raises the question: are Photo Steward's stewardship visualizations sufficient for assessing the quality of photo identifications made on CWPS?\r\nPrior work on crowdsourced scholarship suggests an answer. Rosenzweig [57] analyzed Wikipedia as a source of historical scholarship, noting its policy against original research, and advocating for it as a tool for teaching the limitations of information sources and critical analysis of primary and secondary sources. Motivated along similar lines, Forte et al [18] proposed the assessability framework for designing assessable participatory information systems, based on information provenance and stewardship. The concept of provenance, extensively used in history and archival studies, describes information that makes it possible to trace the ownership or origins of the content, while stewardship refers to the processes that were used for maintaining the content, including its authorship. In the case of Wikipedia, Forte et al. found that visualizing provenance (i.e., citation types) and stewardship (i.e., article quality) had a significant impact on assessments of articles and Wikipedia as an information source.\r\nWhile Photo Steward enables information stewardship on CWPS, there is an opportunity for incorporating provenance into the CWPS platform to make it a truly assessable online platform. A significant proportion of the notes left by users on Photo Steward qualified as provenance information, namely comments about period inscriptions, family trees, external sources and URLs. The challenges of assessing IDs on CWPS are, however, different from assessing information on Wikipedia, primarily because CWPS supports original research unlike Wikipedia's no original research policy [73]. This was also the reason why we designed Photo Steward to be a review system rather than a single editable output such as Wikipedia; original research such as historical photo identifications is often times an evolving investigation rather than a final decision. To assess the reliability of original photo IDs made on CWPS, users may want to factor in the provenance of the reference photos that were used in the identification process. In such cases, Photo Steward's stewardship visualizations (i.e., facial similarity reliability) can further help the user in assessing whether the reference photos can be used as reliable provenance or not.\r\n8 CONCLUSION\r\nPhoto Steward attempts to help users assess and validate photo IDs better on CWPS. We present an information stewardship architecture, and adapt it for the task of historical person identification. We demonstrate the effectiveness of Photo Steward on CWPS, an existing online platform, where users found the stewardship visualizations, which included the community opinions and the AI verdict, useful for making accurate assessments of photo IDs on the platform. Further, users found Photo Sleuth's multi-step, structured validation workflow to help them deliberate before making decisions about the photo's identity. This work opens doors for exploring new ways to leverage collective intelligence and AI in creating assessable online information systems for historical archives.\r\nACKNOWLEDGMENTS\r\nWe wish to thank Ron Coddington, Paul Quigley, Liling Yuan, and our study participants. This research was supported by NSF IIS-1651969 and a Virginia Tech ICTAS Junior Faculty Award.\r\nREFERENCES\r\n2021. Heritage Auctions: World's Largest Collectibles Auctioneer. https://www.ha.com/ Navigate tocitation 1citation 2\r\nSaleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. 2019. Guidelines for human-AI interaction. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–13. Navigate tocitation 1\r\nBrakkton Booker. 2015. $2 photo found at Junk Store has Billy the kid in it, could be worth $5M. https://www.npr.org/sections/thetwo-way/2015/10/15/448993361/-2-photo-found-at-junk-store-has-billy-the-kid-in-it-could-be-worth-5-million Navigate tocitation 1citation 2\r\nVirginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3, 2 (2006), 77–101. Navigate tocitation 1\r\nJoy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Conference on Fairness, Accountability and Transparency, FAT 2018, 23-24 February 2018, New York, NY, USA. 77–91. http://proceedings.mlr.press/v81/buolamwini18a.html Navigate tocitation 1\r\nCarrie J Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg S Corrado, Martin C Stumpe, et al. 2019. Human-centered tools for coping with imperfect algorithms during medical decision-making. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–14. Navigate tocitation 1\r\nFanny Chevalier, Stéphane Huot, and Jean-Daniel Fekete. 2010. Wikipediaviz: Conveying article quality for casual wikipedia readers. In 2010 IEEE Pacific Visualization Symposium (PacificVis). IEEE, 49–56. Navigate tocitation 1\r\nH Jason Combs. 2005. The Internet's Impact on the Antiques Trade. Material Culture (2005), 26–41. Navigate tocitation 1citation 2\r\nAnthony DeBartolo. 1975. Appraisers can tell when old photo really may be golden. https://www.chicagotribune.com/news/ct-xpm-1985-12-27-8503300067-story.html Navigate tocitation 1\r\nAnthony DeBartolo. 2021. Appraisers can tell when old photo really may be golden. https://www.chicagotribune.com/news/ct-xpm-1985-12-27-8503300067-story.html Navigate tocitation 1\r\nHarley D Dickinson. 1998. Evidence-based decision-making: an argumentative approach. International Journal of Medical Informatics 51, 2-3 (1998), 71–81. Navigate tocitation 1\r\nFor The Inquirer Edward Colimore. 2019. Did John Wilkes Booth get away with murdering President Abraham Lincoln?https://www.inquirer.com/news/john-wilkes-booth-lincoln-conspiracy-photo-recognition-20190415.html Navigate tocitation 1\r\nMark Elliott. 2006. Stigmergic Collaboration: The Evolution of Group Work: Introduction. m/c journal 9, 2 (2006). Navigate tocitation 1citation 2citation 3\r\nMark Elliott. 2016. Stigmergic collaboration: A framework for understanding and designing mass collaboration. In Mass collaboration and education. Springer, 65–84. Navigate tocitation 1citation 2citation 3\r\nThomas Erickson and Wendy A Kellogg. 2000. Social translucence: an approach to designing systems that support social processes. ACM transactions on computer-human interaction (TOCHI) 7, 1 (2000), 59–83. Navigate tocitation 1\r\nDan Evon. 2020. Did Joe Biden's Great-Grandfather Own Slaves?https://www.snopes.com/fact-check/joe-biden-slaves-great-grandfather/ Navigate tocitation 1\r\nKristie Fisher, Scott Counts, and Aniket Kittur. 2012. Distributed sensemaking: improving sensemaking by leveraging the efforts of previous users. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 247–256. Navigate tocitation 1citation 2\r\nAndrea Forte, Nazanin Andalibi, Thomas Park, and Heather Willever-Farr. 2014. Designing information savvy societies: an introduction to assessability. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2471–2480. Navigate tocitation 1citation 2citation 3citation 4\r\nJacey Fortin. 2018. She Was the Only Woman in a Photo of 38 Scientists, and Now She's Been Identified. The New York Times (Mar 2018). https://www.nytimes.com/2018/03/19/us/twitter-mystery-photo.html Navigate tocitation 1\r\nSarah A Gilbert. 2020. \" I run the world's largest historical outreach project and it's on a cesspool of a website.\" Moderating a Public Scholarship Site on Reddit: A Case Study of r/AskHistorians. Proceedings of the ACM on Human-Computer Interaction 4, CSCW1 (2020), 1–27. Navigate tocitation 1\r\nJerome S Handler and Michael L Tuite. 2007. Retouching History: The Modern Falsification of a Civil War Photograph. Navigate tocitation 1citation 2\r\nM. Keith Harris. 2019. Civil War Photo Sleuth. Journal of American History 106, 2 (2019), 544–546. https://doi.org/10.1093/jahist/jaz498 Navigate tocitation 1citation 2\r\nJohn J Howard, Laura R Rabbitt, and Yevgeniy B Sirotin. 2020. Human-algorithm teaming in face recognition: How algorithm outcomes cognitively bias human decision-making. Plos one 15, 8 (2020), e0237855. Navigate tocitation 1\r\nPan Hui and Sonja Buchegger. 2009. Groupthink and peer pressure: Social influence in online social network groups. In 2009 International Conference on Advances in Social Network Analysis and Mining. IEEE, 53–59. Navigate tocitation 1\r\nNassim JafariNaimi and Eric M Meyers. 2015. Collective intelligence or group think? Engaging participation patterns in World Without Oil. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing. 1872–1881. Navigate tocitation 1\r\nSoomin Kim, Jinsu Eun, Joseph Seering, and Joonhwan Lee. 2021. Moderator chatbot for deliberative discussion: Effects of discussion structure and discussant facilitation. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–26. Navigate tocitation 1\r\nAniket Kittur, Andrew M Peters, Abdigani Diriye, and Michael Bove. 2014. Standing on the schemas of giants: socially augmented information foraging. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. 999–1010. Navigate tocitation 1\r\nTravis Kriplean, Caitlin Bonnar, Alan Borning, Bo Kinney, and Brian Gill. 2014. Integrating On-demand Fact-checking with Public Dialogue. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing(CSCW '14). ACM, New York, NY, USA, 1188–1199. https://doi.org/10.1145/2531602.2531677 Navigate tocitation 1citation 2citation 3\r\nTravis Kriplean, Michael Toomim, Jonathan Morgan, Alan Borning, and Andrew Ko. 2012. Is This What You Meant?: Promoting Listening on the Web with Reflect. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems(CHI '12). ACM, New York, NY, USA, 1559–1568. https://doi.org/10.1145/2207676.2208621 Navigate tocitation 1citation 2citation 3\r\nKurt Luther. 2016. How Fellow Collectors, Field Photos and Muttonchops Identified an Unknown Officer. Military Images 34, 1 (2016), 29–31. Navigate tocitation 1\r\nKurt Luther. 2017. Merrill Carbine Leads to a Soldier's Identification. Military Images 35, 2 (2017), 64–65. Navigate tocitation 1citation 2citation 3\r\nKurt Luther. 2018. Non-Traditional Research Tools—and Serendipity. Military Images 36, 3 (2018), 12–13. Navigate tocitation 1\r\nKurt Luther. 2018. What are the odds? Photo sleuthing by the numbers. Military Images 36, 1 (2018), 12–15. Navigate tocitation 1citation 2citation 3citation 4citation 5\r\nKurt Luther. 2019. What to Do When Gold Standards Go Wrong?Military Images 37, 1 (2019), 8–9. https://www.jstor.org/stable/26532101 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: How to Trust the Worthiness of an Identification. Military Images 38, 3 (213) (2020), 8–11. https://www.jstor.org/stable/26914966 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: Lost and Found in the Library of Congress. Military Images 38, 2 (212) (2020), 10–13. https://www.jstor.org/stable/26890126 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: The Art of Photo Sleuthing. Military Images 38, 4 (214) (2020), 8–11. https://www.jstor.org/stable/26925454 Navigate tocitation 1\r\nRamona Martinez. 2012. Photo mystery solved, then doubted, then deciphered, thanks to readers. https://www.npr.org/sections/pictureshow/2012/04/17/150801239/photo-mystery-solved-then-doubted-then-resolved-thanks-to-readers Navigate tocitation 1\r\nRamona Martinez. 2012. Unknown No More: Identifying A Civil War Soldier. http://www.npr.org/2012/04/11/150288978/unknown-no-more-identifying-a-civil-war-soldier Navigate tocitation 1\r\nChristian A Meissner and John C Brigham. 2001. Thirty years of investigating the own-race bias in memory for faces: A meta-analytic review.Psychology, Public Policy, and Law 7, 1 (2001), 3. Navigate tocitation 1\r\nVikram Mohanty, Kareem Abdol-Hamid, Courtney Ebersohl, and Kurt Luther. 2019. Second opinion: Supporting last-mile person identification with crowdsourcing and face recognition. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 86–96. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6\r\nVikram Mohanty and Kurt Luther. 2023. DoubleCheck: Designing Community-based Assessability for Historical Person Identification. ACM Journal on Computing and Cultural Heritage (JOCCH) (to appear) (2023). Navigate tocitation 1\r\nVikram Mohanty, David Thames, Sneha Mehta, and Kurt Luther. 2019. Photo sleuth: Combining human expertise and face recognition to identify historical portraits. In Proceedings of the 24th International Conference on Intelligent User Interfaces. 547–557. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6citation 7citation 8\r\nVikram Mohanty, David Thames, Sneha Mehta, and Kurt Luther. 2020. Photo Sleuth: Identifying Historical Portraits with Face Recognition and Crowdsourced Human Expertise. ACM Transactions on Interactive Intelligent Systems (TiiS) 10, 4 (2020), 1–36. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6citation 7citation 8citation 9citation 10\r\nMeredith Ringel Morris, Scott Counts, Asta Roseway, Aaron Hoff, and Julia Schwarz. 2012. Tweeting is believing? Understanding microblog credibility perceptions. In Proceedings of the ACM 2012 conference on computer supported cooperative work. 441–450. Navigate tocitation 1\r\nBenedikt Morschheuser, Juho Hamari, and Jonna Koivisto. 2016. Gamification in crowdsourcing: a review. In 2016 49th Hawaii International Conference on System Sciences (HICSS). IEEE, 4375–4384. Navigate tocitation 1\r\nKathleen L Mosier and Linda J Skitka. 1999. Automation use and automation bias. In Proceedings of the human factors and ergonomics society annual meeting, Vol. 43. SAGE Publications Sage CA: Los Angeles, CA, 344–348. Navigate tocitation 1citation 2\r\nGreg Myre. 2021. How Online Sleuths Identified Rioters At The Capitol. https://www.npr.org/2021/01/11/955513539/how-online-sleuths-identified-rioters-at-the-capitol Navigate tocitation 1\r\nJohnny Nhan, Laura Huey, and Ryan Broll. 2017. Digilantism: An analysis of crowdsourcing and the Boston marathon bombings. The British journal of criminology 57, 2 (2017), 341–361. Navigate tocitation 1\r\nRaja Parasuraman and Dietrich H Manzey. 2010. Complacency and bias in human use of automation: An attentional integration. Human factors 52, 3 (2010), 381–410. Navigate tocitation 1\r\nJunwon Park, Ranjay Krishna, Pranav Khadpe, Li Fei-Fei, and Michael Bernstein. 2019. AI-based request augmentation to increase crowdsourcing participation. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 115–124. Navigate tocitation 1\r\nSarah Perez. 2022. Twitter to show 'Birdwatch' community fact-checks to more users, following criticism. https://techcrunch.com/2022/03/03/twitter-to-show-birdwatch-community-fact-checks-to-more-users-following-criticism/ Navigate tocitation 1\r\nP Jonathon Phillips, Amy N Yates, Ying Hu, Carina A Hahn, Eilidh Noyes, Kelsey Jackson, Jacqueline G Cavazos, Géraldine Jeckeln, Rajeev Ranjan, Swami Sankaranarayanan, et al. 2018. Face recognition accuracy of forensic examiners, superrecognizers, and face recognition algorithms. Proceedings of the National Academy of Sciences 115, 24 (2018), 6171–6176. Navigate tocitation 1citation 2\r\nPeter Pirolli, Evelin Wollny, and Bongwon Suh. 2009. So you know you're getting the best possible information: a tool that increases Wikipedia credibility. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1505–1508. Navigate tocitation 1\r\nInioluwa Deborah Raji and Joy Buolamwini. 2019. Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products. Navigate tocitation 1\r\nAmira Rezgui and Kevin Crowston. 2018. Stigmergic coordination in Wikipedia. In Proceedings of the 14th International Symposium on Open Collaboration. 1–12. Navigate tocitation 1\r\nRoy Rosenzweig. 2006. Can History Be Open Source? Wikipedia and the Future of the Past. Journal of American History 93, 1 (June 2006), 117–146. Navigate tocitation 1citation 2\r\nSaiph Savage, Andres Monroy-Hernandez, and Tobias Höllerer. 2016. Botivist: Calling volunteers to action using online bots. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing. 813–822. Navigate tocitation 1\r\nMike Schaekermann, Joslin Goh, Kate Larson, and Edith Law. 2018. Resolvable vs. irresolvable disagreement: A study on worker deliberation in crowd work. Proceedings of the ACM on Human-Computer Interaction 2, CSCW (2018), 1–19. Navigate tocitation 1\r\nMichael S. Schmidt. 2018. 'Flags of Our Fathers' Author Now Doubts His Father Was in Iwo Jima Photo. The New York Times (Jan 2018). https://www.nytimes.com/2016/05/04/us/iwo-jima-marines-bradley.html Navigate tocitation 1\r\nJennifer Schuessler. 2017. Found: Oldest Known Photo of a U.S. President (Socks and All). https://www.nytimes.com/2017/08/16/arts/design/john-quincy-adams-daguerreotype-sothebys-auction.html Navigate tocitation 1\r\nJulia Schwarz and Meredith Morris. 2011. Augmenting web pages and search results to support credibility assessment. In Proceedings of the SIGCHI conference on human factors in computing systems. 1245–1254. Navigate tocitation 1\r\nRuoxi Shang, KJ Kevin Feng, and Chirag Shah. 2022. Why Am I Not Seeing It? Understanding Users' Needs for Counterfactual Explanations in Everyday Recommendations. In 2022 ACM Conference on Fairness, Accountability, and Transparency. 1330–1340. Navigate tocitation 1\r\nLinda J Skitka, Kathleen L Mosier, and Mark Burdick. 1999. Does automation bias decision-making?International Journal of Human-Computer Studies 51, 5 (1999), 991–1006. Navigate tocitation 1\r\nVerbi Software. 2019. MAXQDA 2020 [computer software]. VERBI Software. Available from maxqda.com. Navigate tocitation 1\r\nNPR Staff. 2016. How Social Media Smeared A Missing Student As A Terrorism Suspect. https://www.npr.org/sections/codeswitch/2016/04/18/474671097/how-social-media-smeared-a-missing-student-as-a-terrorism-suspect Navigate tocitation 1\r\nBongwon Suh, Ed H Chi, Aniket Kittur, and Bryan A Pendleton. 2008. Lifting the veil: improving accountability and social transparency in Wikipedia with wikidashboard. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1037–1040. Navigate tocitation 1\r\nW Ben Towne, Aniket Kittur, Peter Kinnaird, and James Herbsleb. 2013. Your process is showing: controversy management and perceived quality in Wikipedia. In Proceedings of the 2013 conference on Computer supported cooperative work. 1059–1068. Navigate tocitation 1\r\nCivil War Trust. 2021. Military Images Magazine | Interview with Ron Coddington. https://www.battlefields.org/learn/articles/military-images-magazine Navigate tocitation 1\r\nSukrit Venkatagiri, Aakash Gautam, and Kurt Luther. 2021. CrowdSolve: Managing Tensions in an Expert-Led Crowdsourced Investigation. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–30. Navigate tocitation 1\r\nShaun Wallace, Lucy Van Kleunen, Marianne Aubin-Le Quere, Abraham Peterkin, Yirui Huang, and Jeff Huang. 2017. Drafty: Enlisting Users To Be Editors Who Maintain Structured Data. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 5. Navigate tocitation 1\r\nAndrea Wiggins and Yurong He. 2016. Community-based data validation practices in citizen science. In Proceedings of the 19th ACM Conference on computer-supported cooperative work & social computing. 1548–1559. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6\r\nFoundation Wikimedia. 2022. No original research. https://en.wikipedia.org/wiki/Wikipedia:No_original_research Navigate tocitation 1\r\nHeather Willever-Farr, Lisl Zach, and Andrea Forte. 2012. Tell me about my family: A study of cooperative research on Ancestry. com. In Proceedings of the 2012 iConference. ACM, 303–310. Navigate tocitation 1\r\nHeather L Willever-Farr and Andrea Forte. 2014. Family matters: Control and conflict in online family history production. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. ACM, 475–486. Navigate tocitation 1citation 2citation 3citation 4citation 5\r\nJeremy B Wilmer. 2017. Individual differences in face recognition: A decade of discovery. Current Directions in Psychological Science 26, 3 (2017), 225–230. Navigate tocitation 1\r\nBob Zeller. 2022. Searching for photos of Civil War Soldiers | David Wynn Vaughan. https://www.civilwarphotography.org/searching-for-photos-of-civil-war-soldiers/ Navigate tocitation 1\r\nA CIVIL WAR PHOTO SLEUTH: DESIGN CHALLENGES AND OPPORTUNITIES\r\n \r\nHere, we provide details of three key challenges posed by Civil War Photo Sleuth that might have contributed towards misidentification and subsequently, inaccurate assessments, and how they present design opportunities for Photo Steward.\r\nA.1 Conflating facial similarity with photo identification\r\nA.1.1 Challenges: Mohanty et al. conducted a benchmarking study of CWPS [44], and found the face recognition algorithm to be of low precision; i.e., it retrieved hundreds of search results which may look similar to the query photo but are actually different people (false positives). Low precision increases the chances that users will interact with a lot of false positives. In such cases, one may need to garner additional information (i.e., comparing biographical information) before making a decision. However, it is plausible that automation bias may play a role in non-expert users making a match solely based on facial similarity [47]. As a result, there is strong potential for misidentification (see Figure 8).\r\nA.1.2 Current Workflow: CWPS's compare interface allows users to closely inspect the search results for a potential match, but does not make any distinction between facial similarity comparison and photo identification. Both are conflated into a one-step process, with one \"Identify\" button for the users to make their decisions (see Figure 2-C). Yet, users may want to indicate agreement with just the facial similarity (i.e., query photo and the search result showing the same person) but not the identity (i.e., name and biographical information), or vice versa.\r\nA.1.3 Design Goal 1: To support accurate investigation of photo identifications, users should be able to deliberate on the different aspects of the decision-making process. Providing users with a decision-making workflow that decouples facial similarity comparison from the overall photo identification task would allow them to focus on these tasks separately, while discouraging them from making decisions solely on the basis of facial similarity.\r\n\r\nFigure 8: An example of a misidentified photo on CWPS. This photo was identified as John C Whiteside based on facial similarity with the top search result: an identified reference photo of John C Whiteside. However, other visual clues, such as the photographer studio, do not align with Whiteside's biographical information.\r\n\r\nA.2 Lacking support for fine-grained, deliberative decision making\r\nA.2.1 Challenges: Historical photo identification is a complex task, where experts often corroborate multiple pieces of evidence, including facial similarity comparison, before reaching a decision about the identity of the photo [33]. While confirming an identity, experts may be highly confident if the source, military records, uniform clues, and additional photos of the same person all line up, or slightly confident if they need additional evidence. Conflicting evidence pieces may also affect their confidence levels. Similarly, they may have different degrees of certainty while ruling out an identity for a photo. A lack of support for expressing and displaying granularity in these photo identification decisions can lead to varying degrees of uncertainty being captured and misinterpreted as a confirmation, and eventually propagating misidentifications. Further, while it is safe to assume the vast majority of the Civil War photography community care about the accuracy of the photo IDs, there exists a small risk of financially-driven misidentifications since certain types of identified photos are considered to be more valuable [9].\r\nA.2.2 Current Workflow: The compare interface's \"Identify\" button (see Figure 2-C) is the only mechanism available on the platform for users to provide (a unary) input on photo identifications. CWPS users currently cannot provide fine-grained feedback on a given photo's identity, either at the time of identifying or afterwards on the photo page.\r\nA.2.3 Design Goal 2 : Users should be able to express how certain or uncertain they are about a photo identification. Interventions for allowing users to provide structured feedback can not only facilitate more accurate, fine-grained responses, but also encourage deliberation on the available evidence before making a decision, borrowing inspiration from other social computing systems that support users reflecting and deliberating on available information [28, 29].\r\nA.3 Limited validation opportunities for the community\r\nA.3.1 Challenges: For humans, deciding whether two photos show the same person is a highly subjective experience. A study by Mohanty et al. [41] showed that participants in a crowdsourcing study often disagree with each other and with facial recognition results in person identification tasks. The same study showed that people often seek a second opinion from peers for validating photo identifications. Without validation, original crowdsourced historical research may result in unresolved cases of conflicting IDs proposed for the same face, incorrect IDs remaining unchecked, or airtight IDs not being confirmed.\r\nA.3.2 Current Workflow: Once a photo has been identified, CWPS does not offer other users the option to express their opinions on whether two photos show the same person or not, or if the photo has been identified correctly or not (see Figure 8). The photo page does not indicate if (or why) the photos are linked to each other, how they are similar, or which user(s) considered them to be similar. Further, it does not indicate the role of facial recognition in linking them together.\r\nA.3.3 Design Goal 3 : To support accurate original historical research, the platform should encourage information stewardship from the community by allowing members to give their opinions on existing photo identifications [72]. Further, the platform should also be transparent about the role of community stewardship and facial recognition for a given photo identification, which can then act as evidence for aiding subsequent validation efforts by other users, thus supporting a form of stigmergic collaboration [13, 14].\r\nB DETAILS FOR LOG ANALYSIS\r\n \r\nTo understand the community's stewardship behaviors, we examined website logs for all user activities for one year since we launched the new features (December 2020 – December 2021). During this period, we observed 5843 voting instances on 5672 photos for 5355 unique IDs, where a user voting on whether Photo N can be identified as a Person M or not is considered to be one voting instance. For a given voting instance, we analyzed 1) any associated facial similarity comparisons to understand how they deliberated on their final voting decision, and 2) the user's confidence vote plus any justification notes left by the user to understand their decision rationale. Of the 5672 query photos, 4297 photos (4377 voting instances) did not have any facial similarity comparisons — only the user's confidence on the ID (plus any notes) was captured.\r\nFor the remaining voting instances which had at least one associated facial similarity comparison (i.e., users deliberate through the two-step validation process), we broke them down into pre-identified (i.e., the user knew the ID of the photo ahead of time and therefore, might be looking for a specific target on the search results page) and post-identified (i.e., the user did not know the ID of the photo and therefore, may not have a specific target while analyzing the search results) cases. As mentioned earlier in Section 4.2, users had the opportunity to validate IDs either on the search results page on the photo page. While a pre-identified voting instance almost certainly originated from the search results page, a post-identified case could be from either page. Since the logs did not give us the page origin of each vote directly, we triangulated from CWPS's timestamps to determine whether a given voting instance was for a pre-identified case or a post-identified one.\r\nAfter collating all the associated facial similarity comparisons for a given voting instance, we had 1064 pre-identified and 576 post-identified user deliberations to analyze. Each deliberation instance is a user's attempt to identify a query photo as a given target (person) ID, where they first compare facial similarity with all other photos that have been identified as the target ID, followed by the user's confidence on the query photo being the target ID. We analyzed the user responses to see whether the facial similarity comparisons had any impact on the user's confidence.\r\nUsers had provided comments in 682 (out of 5843, or 11.7%) voting instances. We coded these comments using an iterative, inductive approach, which resulted in five high-level themes, which can broken down into 13 sub-categories (see Appendix C).\r\nTo understand any stigmergic collaboration processes at play, we also analyzed the IDs which had multiple votes to check for agreements and disagreements between the voters. We further analyzed how the community's facial similarity comparisons compare against the facial recognition scores.\r\nC THEMES FOR NOTES ANALYSIS\r\nPhoto Comparisons\r\nHigh-Level Comparison: The note mentions \"replica\", \"facial similarity\", \"facial match\", \"identical\", \"visual comparison\", and other similar terms that describe comparisons with a prior identified photo.\r\nDescribing facial features: The note mention facial features like \"eyes\", \"hairline\", \"ears\", etc. to make comparisons\r\nWord-of-Mouth\r\nDescendant: The note either mentions that the user is a descendant of the person being identified, or they got the information from the family of the person.\r\nOwnership: The note either mentions that the user owns a printed version of the photo, compared with a photo in their collection, or they know the owner of the photo.\r\nFamiliarity/Self-Reported Research: The note mentions that the user has seen the photo somewhere, be it in a book, museum, etc.\r\nVisual Clues\r\nUniform: The note mentions visual clues that pertain to the uniform of the person (e.g., hat insignia, shoulder straps, etc.) The user may infer the possible service information (i.e., ranks, branches, regiments, etc.) from the uniform clues.\r\nInscription: The note mentions the presence of a period inscription on the photo (a highly trustworthy primary source for a person's ID), or an album case, or modern inscriptions such as books, which is generally the name of the person being identified. In some instances, the inscribed text may point to the person's service information.\r\nOther Visual Clues: The note mentions visual clues in the photo (e.g., backmarks, borders, etc.) beyond the person's face.\r\nExternal Information\r\nExternal URL: The note mentions an external URL, which supposedly has additional information about the photo's ID.\r\nOther sources (e.g., museum, website, book, etc.): The note mentions an external source (e.g., museum, book, etc.) that supposedly has evidence for the photo's ID, but no URLs are provided. Details about the source may or may not be available.\r\nLack of Information / Seeking Additional Evidence: The note mentions the lack of evidence or seeking additional evidence, be it about the source or the service information.\r\nProviding Additional Information\r\nBiographical Information: The notes mentions additional information about the person's service records, specific regiments, biographical information (name, year, location), etc.\r\nAdditional Context: The note mentions some additional context provided by the user to justify their decision, such as information about the photo collection, or pointing to someone else's research, or some historical context, or incorrect evidence, or if the person is prominent.\r\nD PARTICIPANT DETAILS\r\nUndergraduate and master's students concentrating in history who use Civil War photos for their coursework and research projects, but are not (yet) employed in a professional capacity as historians. We recruited five students via recommendations from our university's history department. None of the students had used CWPS before, or were known to the authors prior to the study. Three students were men and two were women, and all were in the \"18 to 30\" age group. We anonymize them with identifiers S1–S5.\r\nExperienced users of Civil War Photo Sleuth who have added over 50 photos each and have substantial knowledge of Civil War history, but are not professional historians. We recruited five amateur experts from the CWPS contact list. All five users were men, and they were distributed across different age groups (two in \"18 to 30\", two in \"31 to 40\", and one in \"51 to 60\"). We anonymize them with identifiers C1–C5. C1 and C3 are among the most active daily users on CWPS. Only two of the five had used Photo Steward before.\r\nExpert historians with a graduate degree in history, specializing in American Civil War history, but with little or no previous experience with CWPS. We recruited five historians via recommendations from our university's history department. Three historians were men and two were women. They were distributed across different age groups (two in \"18 to 30\", two in \"31 to 40\", and one in \"51 to 60\"). We anonymize them with identifiers H1–H5. None of them had used Photo Steward before.\r\nE LAB STUDY QUESTIONS\r\nE.1 Semi-Structured Questions\r\nIs there a way that you would like to capture your thoughts on this ID and share them with others, if possible? If so, what would that look like? If not, can you explain why not?\r\nWhat did you think about the community opinions?\r\nWhat do you think about the ID quality visualization?\r\nWhat did you think about the 2-step process while agreeing/disagreeing on an identity?\r\nWhat is your overall opinion of both the interfaces?\r\nWhich interface would you prefer for validating the information? And why?\r\nWhat would you change or improve?\r\nE.2 Usability Survey\r\nQ1. The community's opinions about an identity were clear and easy to understand in the 2nd system. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ2. The community's opinions about an identity were useful for assessing the information. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ3. The process of voting on an identity was clear and easy to understand. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ4. Comparing other photos first and then voting on an identity helped me deliberate and make more accurate decisions. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ5. I was able to validate the information better using the 2nd system. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nF SUMMARY OF CHANGES\r\nWe summarize in Table 6 about how Photo Steward differs from CWPS.\r\nTable 6: Summary of changes: Civil War Photo Sleuth (CWPS) with and without Photo Steward.\r\n\r\nCWPS\r\nCWPS + Photo Steward\r\nDecision-Making: How do users make identification decisions for the query photo when they see the similar-looking search results retrieved by facial recognition?\r\nA single-step comparison interface that allows the user to compare the query photo with one similar-looking photo and the associated biographical information of the target ID. The single-step workflow conflates the two tasks of facial similarity and person identification.\r\nA multi-step validation interface that allows the user to compare the query photo with all previously identified photos of a similar-looking target ID (search result). The two-step workflow decouples the two tasks of facial similarity and person identification, thus allowing the user to deliberate on the facial similarity comparison between the query photo and the target photo(s) before making a decision on the target identity.\r\nUsers make a unary input on the query photo's ID by clicking an \"Identify\" button in the comparison interface, which will link the target ID to the query photo. The comparison interface does not allow users to rule out candidates.\r\nUsers can provide fine-grained decisions for both facial similarity and person identification steps in the validation interface. In Step 1, the user compares the query photo and the target photo for facial similarity by selecting from the following options: No (Different Person), Not Sure, Yes (Facial Match), and Yes (Replica). In Step 2, users indicate how confident they are about the query photo's ID (as the proposed target ID) by selecting the following options: No (Highly Confident), No (Slightly Confident), Not Sure, Yes (Slightly Confident), and Yes (Highly Confident).\r\nAccess: What kind of validation opportunities are available for users?\r\nUsers can only access the comparison interface from the search results page if they search for similar-looking candidates using facial recognition. Once the photo has been identified, users cannot access the comparison interface on the photo page. The community cannot weigh in on an ID's reliability on the photo page.\r\nUsers can access the validation interface on both the search results page (while identifying the query photo) and the photo page (after the photo has been identified). The community can weigh in on the reliability of the proposed ID(s) and facial matches using the validation interface on the photo page, thus engaging in a form of stigmergic collaboration.\r\nEvidence: What kind of stewardship evidence is presented to the users?\r\nThe photo page displays the proposed ID(s) for the photo without any additional information about the ID's reliability. Similarly, other photos that have been matched to the query photo are also displayed without any reliability indicator. Since community opinions are not captured on CWPS, they are not displayed.\r\nThe photo page displays the proposed ID(s) for the photo along with stewardship visualizations of the a) community's confidence on the ID, and b) facial similarity comparison with other photos of the same ID by both the community and AI.\r\nFOOTNOTE\r\n1 www.civilwarphotosleuth.com\r\n\r\n\r\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs International 4.0 License.\r\nCI '23, November 06–09, 2023, Delft, Netherlands\r\n© 2023 Copyright held by the owner/author(s).\r\nACM ISBN 979-8-4007-0113-9/23/11.\r\nDOI: https://doi.org/10.1145/3582269.3615600",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== Kurt paper.txt ===\nPhoto Steward: A Deliberative Collective Intelligence Workflow for Validating Historical Archives\r\nVikram Mohanty, Computer Science, Virginia Tech, USA, vikrammohanty@vt.edu\r\nKurt Luther, Computer Science, Virginia Tech, USA, kluther@vt.edu\r\n\r\nDOI: https://doi.org/10.1145/3582269.3615600\r\nCI '23: Collective Intelligence Conference, Delft, Netherlands, November 2023\r\nHistorical photographs of people generate significant cultural and economic value, but correctly identifying the subjects of photos can be a difficult task, requiring careful attention to detail while synthesizing large amounts of data from diverse sources. When photos are misidentified, the negative consequences can include financial losses and inaccuracies in the historical record, and even the spread of mis- and disinformation. To address this challenge, we introduce Photo Steward, an information stewardship architecture that leverages a deliberative workflow for validating historical photo IDs. We explored Photo Steward in the context of Civil War Photo Sleuth (CWPS), a popular online community dedicated to identifying photos from the American Civil War era (1861–65) using facial recognition and crowdsourcing. While the platform has been successful in identifying hundreds of unknown photographs, there have been concerns about unverified identifications and misidentifications. Our exploratory evaluation of Photo Steward on CWPS showed that its validation workflow encouraged users to deliberate while making photo ID decisions. Further, its stewardship visualizations helped users to assess photo ID information accurately, while fostering diverse forms of stigmergic collaboration.\r\nCCS Concepts: • Human-centered computing → Collaborative and social computing design and evaluation methods; • Human-centered computing → Interactive systems and tools;\r\n\r\nKeywords: crowdsourcing, human-AI interaction, online deliberation, community stewardship, information assessability, online communities, history, person identification, facial recognition, stigmergic collaboration\r\n\r\nACM Reference Format:\r\nVikram Mohanty and Kurt Luther. 2023. Photo Steward: A Deliberative Collective Intelligence Workflow for Validating Historical Archives. In Collective Intelligence Conference (CI '23), November 06--09, 2023, Delft, Netherlands. ACM, New York, NY, USA 19 Pages. https://doi.org/10.1145/3582269.3615600\r\n\r\nFigure 1: Workflow of Photo Steward. (1) Decision-Making: The user compares photos for facial similarity and make decisions on the photo ID using Photo Steward's deliberative validation interface. (2) Access: The user accesses the validation interface from Civil War Photo Sleuth's search results and photo page. (3) Evidence: The community's responses from the validation interface feed into stewardship visualizations that are visible on the Photo Page, which subsequently foster a form of stigmergic collaboration among the users.\r\n1 INTRODUCTION\r\nThe task of correctly identifying individuals in historical photos holds great cultural and economic importance [3, 19, 39, 61]. This identification process is analogous to solving a complex mystery. It often involves corroboration of multiple research processes such as investigating visual clues in a photo, finding relevant reference resources, and comparing multiple low-resolution reference photos [31, 32, 37]. Historical photos pose many challenges, including low-resolution images, scattered reference materials, limited domain expertise, and lack of suitable verification tools. These hurdles often result in misidentifications which can have negative consequences, ranging from distorting historical narratives [60] and fueling conspiracy theories [12] to spreading disinformation [16] and unwarranted financial gains from inaccurate representations [21]. As online platforms such as Ancestry.com, Find-a-Grave, and FamilySearch democratize historical and genealogical research, the risk of misidentification is further amplified due to factors such as inadequate experience, confirmation bias, and automation bias introduced by imperfect automated tools [44, 75].\r\nTo address these challenges, we introduce Photo Steward, a deliberative workflow that leverages collective intelligence to validate historical photo identifications (IDs). Photo Steward's architecture builds upon the concept of information stewardship [18, 72], which involves community-driven validation of content, as seen in online communities like iNaturalist and Wikipedia. We designed and applied the Photo Steward architecture to Civil War Photo Sleuth (CWPS),1 an AI-infused online platform for identifying historical photos. CWPS has over 20,000 registered users and over 25,000 identified Civil War portraits, and faces the problem of historical photo misidentification [43]. Photo Steward provides a validation workflow that promotes careful deliberation during facial similarity comparison and photo ID verification, while enabling users to share opinions. It also visualizes community opinions on the reliability of photo IDs and facial similarity, fostering information stewardship at multiple levels.\r\nWe publicly released Photo Steward on CWPS and conducted an exploratory evaluation of twelve months of usage, including interviews with users of different expertise levels and log analysis of stewardship behaviors on the platform. We found that Photo Steward's stewardship visualizations helped users to find additional evidence (e.g., external sources, uniform clues matching service records, etc.) for assessing the reliability of photo IDs. Users validated hundreds of different IDs on the platform, and found the workflow to be useful for deliberating on facial similarity comparison and fine-grained photo ID decisions.\r\nWe also discuss the implications of community participation, deliberative human-AI interaction, and assessable designs for historical photo identification.\r\n2 RELATED WORK\r\n2.1 Misinformation in a Historical Context\r\nNumerous online communities, forums, and websites have emerged in recent years for archiving and documenting history [57], generating family histories [74, 75], identifying and sharing historical photos [43], trading antiques [1, 8], and facilitating discussions around history [20]. Much like popular social media platforms such as Facebook and Twitter, these history-based platforms are also prone to the problem of misinformation, albeit in a more specialized historical research context.\r\nPrior work has shown that erroneous family history trees were being disseminated across Find-a-Grave and Ancestry, two popular genealogy research communities, as a result of the platform's low bar to entry and inexperienced contributors' over-reliance on inaccurate automated features [75]. Mohanty et al. [43] showed that despite successful identifications on Civil War Photo Sleuth (CWPS), several photos were misidentified in the first month, particularly for photos without period inscriptions or duplicate views (12 misidentifications out of 37). Multiple factors — the correct candidate not present in the search pool, or the user incorrectly assessing facial similarity [53] and picking the wrong match — can lead to incorrect IDs. A follow-up benchmarking study of the underlying face recognition algorithm [44] highlighted its low precision (i.e., it retrieves over hundreds of search results), raising the possibility of errors due to automation bias, i.e., the tendency of users to over-rely on automation for making a decision [47, 50, 64].\r\nIdentifying historical photos is a complex investigative process, often involving the corroboration of multiple evidence pieces [31, 33] and can be seen analogous to \"finding a needle in a haystack\". As a result, historical photo IDs run a high risk of getting misidentified even with the best of intentions. Multiple Civil War photos have also been misidentified in the collections of professionally managed museums and archives, such as the US Library of Congress [36] and the Abraham Lincoln Presidential Library [34]. At the same time, historical photo IDs also have the potential to generate significant monetary value [1, 3, 8, 10], and such financial incentives might also lead to falsified identifications [21]. Validating these historical photo IDs, which are a result of complex, subjective original research, becomes tricky without the lack of domain expertise and access to investigative tools.\r\nWe addressed these challenges in Photo Steward by designing a stewardship architecture that allows users to share their expertise with others. To address the impact of automation, we introduced a two-step validation workflow for the users to deliberate on decisions while interacting with the AI's recommendations.\r\n2.2 Data Validation in Online Communities\r\nMultiple online platforms have leveraged the strengths of crowdsourced contributions for validating the quality of data generated on those sites. Elliott discusses how stigmergic collaboration, where indirect coordination within a community stimulates subsequent actions, plays a role in maintaining articles on Wikipedia [13]. This concept was observed by Wiggins et al. in their study of iNaturalist, an online platform for identifying species, wherein community stewardship behaviors were seen as users agreed on organism identifications to influence the platform's quality grade status [72]. Prior work has shown that stewardship visualizations on Wikipedia (i.e., article quality) [18] and iNaturalist (i.e., ID research grade status) [72] have a positive impact on users' assessment of the information.\r\nAlong these lines, we also built DoubleCheck [42], a quality assessment framework that builds upon the concepts of provenance and stewardship for verifying historical photo IDs. DoubleCheck focused on displaying quality indicator badges for historical photo IDs by capturing accurate provenance information and combining the source trustworthiness information with community opinions on the ID. In this work, we focus solely on the underlying stewardship architecture that helped facilitate the community opinions. Both DoubleCheck and Photo Steward were evaluated in the same lab study, but there is no overlapping data.\r\nVisualizations displaying (surrogate) quality metrics, such as popularity among expert users, social reputation, and content coverage, have been effective in helping users assess the credibility of websites and search results [62]. Prior work has also shown that visualizing the history of edits for a Wikipedia article can have a significant impact on users' perceived trustworthiness of the article [54, 67]. Similarly, Chevalier et al. [7] showed that visualizing the number of contributors, length of the article and discussion, and the history of edits helped users assess the quality of Wikipedia articles faster. On the other hand, Towne et al. [68] found that being exposed to editor conflicts in the discussion of a Wikipedia article lowered the perception of the article's quality, even though the users reported that the transparency raised their perceptions of the page and Wikipedia in general. Morris et al. [45] found that Twitter users relied on the author information for making assessments about the credibility of information in a tweet.\r\nDrawing from this prior work, we designed Photo Steward's stewardship visualizations to highlight the role of collective intelligence, while fostering stigmergic collaboration on CWPS to validate the quality of photo identifications.\r\n2.3 Background: Civil War Photo Sleuthing\r\nThe American Civil War (1861–65) was one of the first major conflicts to be extensively photographed. Over 3 million soldiers fought in the war, with many of them having been photographed at least once. Over 150 years, many of these photos have survived in museums, libraries, and personal collections, but only 10–20% are are identified [69, 77]. Civil War photography has garnered a lot of interest among historians, collectors, dealers, genealogists, archivists, and other experts, who often try to identify unknown photos for personal, cultural, and economic reasons. However, the identification process is complex and challenging, which often involves identifying visual clues in a photo and manually scanning through hundreds of low-resolution photos, military records, and reference books for corroborating evidence [31, 33, 38].\r\n3 ENHANCING CIVIL WAR PHOTO SLEUTH: DESIGN OPPORTUNITIES\r\n\r\nFigure 2: CWPS Haystack Model: Person Identification Pipeline [43]\r\nCivil War Photo Sleuth (CWPS) is a free, public website where users can identify unknown portraits from the American Civil War era using a person identification pipeline that combines crowdsourced human expertise and face recognition [43]. Drawing analogies to finding a needle in a haystack, Mohanty et al. propose a 'haystack model' to describe CWPS's person identification pipeline. In this pipeline, a user begins the identification process by first tagging a photo for uniform clues, which then generates search filters based on service records, and then facial recognition returns facially similar-looking results from a pool of potential candidates, ordered by similarity to the query photo, that satisfy the search filters (see Figure 2).\r\nThe CWPS haystack model is designed to prevent misidentifications by placing human decision-making at the forefront and treating AI as a supportive tool. It avoids automatically selecting the best match or displaying the algorithm's inconsistent confidence levels [41, 44]. Instead, the user carefully inspects search results for potential matches based on facial similarity and corresponding biographical details. Once a photo is identified, CWPS links the face and identity together and displays the ID on the photo page.\r\nDespite these measures, the open participation model of CWPS, which lacks verification, has raised concerns about the trustworthiness of proposed identities and the potential increase of \"false positives\" as the site grows [22]. To address these concerns, we enumerate three design goals which draw upon prior work on Civil War photo identifications and CWPS system designs, evaluations, and critiques [22, 30, 33, 35, 41, 43, 44], as well as our own observations and experiences using the publicly available version of the website. We provide further details in Appendix A.\r\nDesign Goal 1: Decouple facial similarity comparison from the overall task of person identification.. The current CWPS workflow conflates facial similarity and person identification into a single decision-making process (see Figure 2-C). Facial similarity, while important, can conflict with the identity suggested by personal details like biographical information and service records. The facial recognition algorithm's low precision [44] adds to the complexity, with the possibility of users interacting with false positives. In order to discourage over-reliance on facial similarity, we propose this design goal of separating both these tasks, allowing users to deliberate on the facial similarity and other person identification attributes separately, thereby minimizing inaccuracies.\r\nDesign Goal 2: Support fine-grained, deliberative decision-making.. With the current CWPS interface only permitting binary feedback during photo identification, there is a heightened risk of misrepresentation and misidentifications (see Figure 2-C). Historical photo identification is intricate, often demanding careful corroboration of numerous evidence pieces, with user confidence varying according to the evidence at hand [33]. To more effectively facilitate this complex process, we advocate for a design that enables users to express their level of certainty in their identification decisions. This design goal seeks to foster more accurate and nuanced user feedback, as well as reflective and deliberate decision-making [28, 29].\r\nDesign Goal 3: Encourage community contribution and transparency for validating photo IDs.. Identifying individuals in photos can be considered a subjective process and often benefits from multiple perspectives [41]. Currently, CWPS lacks mechanisms for community feedback and transparency about the process of photo identification, leaving potential misidentifications unchecked (see Figure 8 in Appendix). To support accurate original historical research, the platform should encourage community participation in validating identifications and promoting transparency around the roles of community contribution and facial recognition [18, 72]. This approach also encourages collective responsibility, facilitating stigmergic collaboration [13, 14], where user contributions guide future validation efforts.\r\n4 SYSTEM DESCRIPTION: PHOTO STEWARD\r\nWe developed Photo Steward, an information stewardship architecture that integrates a deliberative workflow for the community to validate historical photo identifications, which we then integrated into CWPS. Photo Steward's architecture has three main components (see Figure 1): 1) a deliberative decision-making interface for facial similarity comparison and photo identification, 2) new access points for validating photo identities, and 3) stewardship evidence for fostering stigmergic collaborations.\r\n4.1 DECISION-MAKING: Deliberating on facial similarity and photo identification\r\nAs part of Photo Steward, we introduce a multi-step \"Validation Interface\" (see Figures 3 and 4) to replace CWPS's single-step comparison interface. Photo Steward's validation interface allows users to deliberate while interacting with the facial recognition results. Meeting Design Goal 1, the validation workflow separates the task of facial similarity comparison from the overall goal of identifying the photo.\r\nTo inform our design, we draw on evidence-based decision-making [11], a model primarily used in healthcare, policymaking, and judicial sectors, which advocates for justifying decisions (photo IDs in this case) by gathering available evidence (facial similarity as visual evidence here). In the first step, the user compares the query photo to all other photos with the same identity for facial similarity. After deliberating on the facial similarity evidence, the user then votes on whether the query photo fits the target identity in the second step (which is the user's primary goal).\r\nThe validation interface is divided into four columns (from left to right): 1) the task description, 2) the query photo, 3) the evidence that is being weighed, and 4) the biographical information. The query photo and evidence are positioned in the two middle panels for easy side-by-side comparison. The task description panel displays the rating question for both the facial similarity comparison and the identification steps. Here, we used structured feedback to capture both the user's facial similarity comparison and their confidence on the photo ID, in an effort for encouraging users to exercise personal deliberation on all available evidence before making a decision on the ID. The interface updates the task description and evidence column depending on which task the user is performing.\r\nTo investigate the identity of a query photo, the user opens the validation interface which loads all the photos and biographical information available for the target identity.\r\n4.1.1 Validation Step 1: Facial Similarity Comparison. For the first step of the validation process, the interface displays the target photo in the evidence column next to the query photo for easy facial similarity comparison (see Figure 3).\r\n\r\nFigure 3: Step 1 of Photo Steward's Validation Interface. Users can compare two photos and answer whether they show the same person or not. They have the option of selecting whether the two photos are a facial match (i.e., same person, different views) or a replica (i.e., same person, same view). Here, the user is comparing whether Photo 41929 and Photo 1 show the same person or not. If multiple faces are available for the same ID, they appear one after the other in the order in which they were uploaded to CWPS.\r\n\r\nThe user's task is to determine whether both photos show the same person (regardless of whether the identity is known). Users can select from the following options: No (Different Person), Not Sure, Yes (Facial Match) and Yes (Replica). Mohanty et al. found that photos correctly identified on CWPS were either facial matches (i.e., same person, different view) or replicas (i.e., same person, same view) [43], which informed the design of this input scale. Since facial similarity does not have any standard scale and users may perceive the similarity or dissimilarity of two faces differently [40, 76], we chose not to capture any further granularity in their responses for facial match, replica or different person as this might lead to inconsistent data collection.\r\nCapturing these responses in a structured way allows users to deliberate on the task of facial similarity; this becomes more critical as users are also interacting with the results of a low-precision facial recognition algorithm [44]. In this step, the user compares facial similarity of the query photo with all available photos of the target identity, one photo at a time.\r\n4.1.2 Validation Step 2: Fine-Grained Photo Identification. In this step, the user analyzes the biographical information and incorporates the facial similarity evidence from the previous step to make a decision on the photo's identity.\r\nThe validation interface displays information in the same four-column layout (see Figure 4), with the evidence column now displaying a summary of the user's responses about facial similarity between the query photo and the target photo(s). The biography column shows the name and the service records for the user to analyze.\r\n\r\nFigure 4: Step 2 of Photo Steward's Validation Interface. Users vote on whether the query photo can be identified as the target identity by expressing their confidence. They can also add an optional note to justify their decision. The evidence panel displays a summary of the user's responses from the first step, where the faces are displayed next to the user's facial similarity comparison with the query photo. The faces are ordered in the way they appear for comparison, i.e., the order in which they were uploaded to CWPS.\r\n\r\nThe user now decides whether the query photo can be identified as the target identity (see Figure 4). The instruction above nudges the user to factor in the prior photo comparison evidence and the biography information. Meeting Design Goal 2, users indicate their confidence about the task question by selecting one of the five options displayed in radio buttons: No (Highly Confident), No (Slightly Confident), Not Sure, Yes (Slightly Confident) and Yes (Highly Confident). This scale, which offers more nuance than a binary decision, serves as a proxy measure for the ID's reliability given the complexities of quantifying accuracy in a historical photo ID investigation. These options reflect the varying degrees of confidence users have based on the quality and quantity of corroborating evidence, such as reputable sources, facial similarity to additional photos, and expert opinions. Users also have the option to elaborate their decision rationale in a free-text note.\r\n4.2 ACCESS: Expanding Validation Opportunities for Photo Identifications\r\nPhoto Steward provides stewardship capabilities for the CWPS user community by allowing them to access and use the validation interface at different stages of the photo identification process from multiple gateways (see Figure 1). On the \"Search Results\" page, it can be used for identifying a photo from a pool of potential similar-looking candidates, or ruling out some potential candidates. After a photo has been identified, users can also access it on the \"Photo Page\" to either validate an existing ID or dispute an incorrect one, and collaborate with other users in a stigmergic manner (Design Goal 3).\r\n4.2.1 Search Results Page: Matching and Ruling Out Candidates. While identifying a query photo, users can now inspect potential matches on the search results page with the help of the validation interface. The \"Compare\" button on a search result brings up the validation interface, loading all the target information for the corresponding search result. The target identities in the validation interface will update as the users check new search candidates for matches. The interface allows users to make two types of decisions, depending on their confidence response: 1) either of the \"Yes\" responses will match the photo with the target identity with varying degrees of confidence, and 2) either the \"No\" or \"Not Sure\" responses will rule out the search candidate as a potential match for the current user.\r\n4.2.2 Photo Page: Validating and Disputing Existing IDs. After a photo has been identified, Photo Steward allows users to review opinions from other users (described in Section 4.3) and contribute their own for a given photo ID on CWPS's photo page, fulfilling Design Goal 3. By clicking the \"Give Your Opinion\" button, users launch the validation interface featuring the query photo, the linked target identity, and target photos of the same ID, a new feature previously absent from CWPS. The two-step process mirrors that on the search results page, enabling community deliberation on the validity of an ID. Users can validate the facial match among photos linked to the same ID, express their agreement or disagreement on an ID with varying confidence levels, and optionally add a note explaining their decision. Thus, each vote contributes to a stigmergic collaboration, enhancing the reliability of photo IDs on CWPS. Consistent with CWPS's open participation model, Photo Steward allows any registered user to share their opinion on an identification.\r\n4.3 EVIDENCE: Visualizing Information Stewardship\r\nAs part of Photo Steward, we designed stewardship visualizations to help users assess the reliability of 1) facial matches (i.e., photos that were matched to each other by the user), and 2) photo IDs. The CWPS community's opinions on facial similarity comparison and photo IDs, captured through the validation interface, feeds into these reliability visualizations. These visualizations not only promote user accountability through social translucence [15], but also serve as deliberative evidence for subsequent stigmergic user collaborations (Design Goal 3).\r\n4.3.1 Reliability of Facial Similarity. For each photo pair that has been compared, the system aggregates the community's decisions for the visual match type and generates a distribution, which is displayed in the form of an interactive horizontal bar chart on the photo page. This chart appears next to the corresponding photo matched to the query photo (see Figure 5). Users can click the \"View Details\" button or an individual bar to see how each user voted. When multiple photos have been matched to the query photo, the matched photos appear one below the other, with each having its own visualization next to it. The bar charts are stacked vertically above each other to allow users to easily see and compare the reliability of every match.\r\nTo complement the community stewardship visualization, we also added an AI stewardship badge that indicates whether the particular match is supported by facial recognition (see Figure 5). On the search results page, CWPS retrieves those search results that have a facial similarity score greater than 0.50, so we use the same threshold here. However, the badge intentionally does not display the exact similarity scores (which have been found to be inconsistent [41, 44]) to avoid a false perception of precision, and cautions users to carefully analyze all the context and evidence, as there is a possibility of false positives with face recognition.\r\n\r\nFigure 5: Facial Similarity Reliability Visualization on the Photo Page. The photo matched to the query photo is displayed here, whereas the query photo is displayed on top of the page. Other photos matched to the query photo are displayed vertically one below the other. Users can bring up the query photo and the matched photo side-by-side by clicking the \"Compare\" button. Each matched photo has its own visualization next to it, and the responses are visible to everyone.\r\n\r\n4.3.2 Reliability of Photo Identifications. Similar to the facial similarity visualization, the system aggregates the community's confidence levels for an identification and generates a distribution, displayed in the form of an interactive horizontal bar chart. This visualization is displayed for every proposed identity on the photo page, below the biography subsection (see Figure 6a). If users propose multiple IDs for a given photo, they are displayed one below the other; each ID will have its own visualizations. The community's confidence votes are aggregated to order these IDs. Users can click the \"View Details\" button or the individual bars to view a modal dialog box with each user's confidence scores and optional text comments (see Figure 6b).\r\n\r\nFigure 6: Photo ID Reliability Visualization on the Photo Page for a given ID. If multiple IDs are present, they are displayed vertically one below the other, ordered in terms of aggregate votes. Each ID will have a separate visualization listed under the respective IDs.\r\n\r\nThis visualization reflects the community's expertise, and is intended for users to quickly assess the reliability of an identification. Prior work on crowdsourced person identification [41] suggests that airtight identifications are likely to show consensus from the community, whereas potential misidentifications are likely to reflect disagreement from the community. Further, the comments may reflect the voters' decision rationale and any external research they conducted before giving their decision on the photo's identification, allowing users to build on the work of others in making their own assessment [17].\r\n4.4 Summary\r\nPhoto Steward augments the CWPS platform with an information stewardship architecture to support community validation of historical photo IDs in a stigmergic manner. We summarize all the changes in the Appendix (see Table 6).\r\n5 EVALUATION\r\nWe obtained permission to publicly launch Photo Steward on CWPS in December 2020. We conducted a mixed-methods, exploratory evaluation study to understand how well users with different expertise levels could validate and assess Civil War photo IDs using CWPS with Photo Steward. Specifically, we wanted to understand 1) how users validated photo IDs using Photo Steward, 2) how the stewardship visualizations (i.e., ID and facial similarity reliability visualizations) impacted users' assessment of an ID. The study was approved by our university's IRB.\r\n5.1 Log Analysis\r\nTo understand the community's stewardship behaviors, we analyzed website logs of all user activities for a year after new features were launched, which included 5843 voting instances on 5672 photos for 5355 unique IDs. Our analysis included categorization of user deliberations as 'pre-identified' (i.e., the user knew the ID of the photo ahead of time and therefore, might be looking for a specific target on the search results page) or 'post-identified' (i.e., the user did not know the ID of the photo and therefore, may not have a specific target while analyzing the search results), coding of user comments (see Appendix C), and comparison of community's facial similarity comparisons against facial recognition scores. Details of these analysis methods are provided in Appendix B.\r\n5.2 Lab Study\r\nIn order to understand how well Photo Steward supports diverse users in validating the quality of photo identifications, we also conducted an exploratory lab study.\r\n5.2.1 Participants. We recruited 15 participants representing the three major expertise levels: 5 history students, 5 amateur experts (experienced users of CWPS), and 5 expert historians. Participant details can be found in Appendix D. We anonymize these groups with the following identifiers, respectively: S1–S5, C1—C5, and H1–H5.\r\n5.2.2 Dataset. For the study, we created a dataset of 10 different photos identified on CWPS. Three of these photos had an ID conflict, i.e., multiple identities were proposed. For two of these photos, one ID was correct and the other one was incorrect. The community had already researched both photos, voted on the correct ID, and left credible evidence in the comments. Both IDs were linked to additional photos as well. The third photo was one of the seeded photos on CWPS, but was originally misidentified. We added another false ID, making both IDs for the third photo incorrect. All photos had multiple photos matched to them; eight of them were linked via facial matches, while two of them had replicas.\r\n5.2.3 Procedure. The entire study was conducted online via recorded Zoom sessions, with at least one researcher attending each session. Each participant first completed a consent form and a pre-survey describing their demographics and Civil War photography experience.\r\nAs part of the study, participants reviewed three randomly assigned photos from the dataset one-by-one in the original CWPS system first, followed by the same photos on the Photo Steward version. Participants used a think-aloud protocol while using the two systems; after the completion of the task with each system, they were asked a few semi-structured questions about their experience. Finally, the participants completed a summative post-survey of standard usability questions (e.g., ease of use, usefulness of features, instruction clarity, preferred system, etc.) (see Appendix E)\r\nWe maintained this sequence (original CWPS first, CWPS with Photo Steward second) for all the participants, rather than using a randomized sequence, for two reasons. First, we did not want participants' assessments to be biased in favor of Photo Steward after seeing additional features in the new interface. This design allowed us to observe if the original interface misled the participants towards incorrect assessments, and if, subsequently, the Photo Steward interface helped correct them. Second, in a randomized sequence, Photo Steward would expose the participants to new information in the form of prior user votes and responses, and therefore, may confound how they assess the information on the original CWPS version.\r\n5.2.4 Data Analysis. The first author fully transcribed and analyzed the interviews and think-aloud recordings using an inductive qualitative thematic approach [4]. The transcript sections were first divided according to the interface in question (i.e., original CWPS or Photo Steward), followed by an open coding of the transcripts using MAXQDA 2020 [65]. The first author iterated and settled on a total of 28 codes (e.g., change in opinions, comparison interface, source trustworthiness, etc.) for 634 coded segments across all the transcripts. These codes were then organized into themes as described in Section 6 after discussing with the co-author.\r\n5.2.5 Limitations. We conducted a qualitative lab study to understand how users with different backgrounds and expertises validated photo IDs using Photo Steward and hit theoretical saturation. However, there are a couple of limitations with the study: 1) limited insights on the role of expertise, and 2) the task sequence could have order effects. Further, the large-scale analysis of Photo Steward logs provided us with insights of its usage amongst users. However, it lacked an expert-prepared gold standard dataset, which hindered our ability to conduct specific performance analyses as part of this study.\r\n6 FINDINGS\r\nUsing the methods above, we evaluated how well Photo Steward's stewardship architecture supported CWPS users in validating photo identifications, compared to the original version of CWPS.\r\n6.1 Validation Interface\r\nUsers found Photo Steward's validation interface to be useful for comparing different photos.. While assessing the IDs with the original interface, participants would go back and forth between different photos to compare whether they are the same person or not. Some participants opened the photos in two different browser windows and kept them side-by-side. While using the validation interface in the new system, participants appreciated being able to see the photos side-by-side at the same time.\r\nH1 said, \"As an historian using this, this is really great to see them both together. It just makes a comparison a lot easy for me to do. I mean, this is the same gentleman, he's got a little dark facial hair. It looks a little bit different there and the photo on the right, but the facial match is definitely there. \" This was also echoed by C3, who said, \"This, I really find extremely useful, especially when I'm trying to do facial recognition. I can zoom in and have them side by side here. [...] Where in the past, I would have to go back and forth between tabs or cut and paste them into a different document to look at them side by side.\"\r\nFrom our logs, we found that 223 users had compared 2319 unique photo pairs for facial similarity, with 156 pairs receiving comparisons from at least 2 different users. The facial similarity responses were distributed as follows: 763 replicas, 1232 facial matches, 283 unsure, and 280 different people.\r\nTable 1: Distribution of User Votes.\r\n\r\nVotes\r\n/ ID\r\n# of IDs\r\nNote\r\nPresent\r\nNegative\r\nVotes\r\n1\r\n5650\r\n511\r\n61\r\n2\r\n157\r\n(Agreement: 119)\r\n(Disagreement: 38)\r\n83\r\n12\r\n3+\r\n36\r\n(Agreement: 21)\r\n(Disagreement: 15)\r\n31\r\n4\r\nTable 2: Distribution of Confidence Levels.\r\n\r\nMean\r\nConfidence\r\n1\r\nVote / ID\r\n2\r\nVotes / ID\r\n3+\r\nVotes / ID\r\n-2 (No - Highly Confident)\r\nto\r\n-1 (No - Slightly Confident)\r\n45\r\n3\r\n2\r\n-1 (No - Slightly Confident)\r\nto\r\n0 (Not Sure)\r\n16\r\n3\r\n2\r\n0 (Not Sure)\r\n99\r\n8\r\n4\r\n0 (Not Sure)\r\nto\r\n1 (Yes - Slightly Confident)\r\n760\r\n15\r\n5\r\n1 (Yes - Slightly Confident)\r\nto\r\n2 (Yes - Highly Confident)\r\n4730\r\n128\r\n23\r\nUsers preferred the ability to provide granular feedback for photo IDs using Photo Steward's validation interface.. All participants expressed preference for the fine-grained confidence levels, including the ability to dispute an ID, in Photo Steward's validation interface, appreciating how it more accurately mirrored the inherent uncertainty present when assessing photo IDs. S3 said, \"I definitely like the five levels. I think it leaves more room for interpretation. Like sometimes it's kind of hard to just say yes or a hard no because so much goes into it. Especially because a lot of this stuff was so long ago, there's so many unanswered questions.\" H4 initially defended the original interface's binary vote, but changed her mind after experiencing Photo Steward's confidence levels: \"[W]hat I had said has this very black and white feel to it, you're wrong or you're right. I like these degrees of disagreement or agreement. I think that's way more helpful broadly.\"\r\nThe usage of the voting feature was reflected in our logs, which showed 5843 voting instances from 328 unique users (see Table 1). Table 2 shows that while users utilized the full range of confidence levels, including when they were unsure (mean confidence = 0) or slightly confident (mean confidence = 0 to 1) about the ID, the vast majority of the votes were highly confident ones (mean confidence = 1 to 2). A small proportion of votes (77) were cast for disputing an ID.\r\nUsers justified their voting decisions through notes covering an extensive range of topics..\r\nTable 3: Distribution of Note Topics. The table also displays how the notes are distributed for different user confidence levels.\r\n\r\nCategory\r\nSub-Category\r\nTotal\r\nNumber of\r\nNotes\r\nNo\r\n(Highly\r\nConfident)\r\nNo\r\n(Slightly\r\nConfident)\r\nNot\r\nSure\r\nYes\r\n(Slightly\r\nConfident)\r\nYes\r\n(Highly\r\nConfident)\r\nPhoto\r\nComparison\r\nHigh-Level\r\nComparison\r\n206\r\n9\r\n5\r\n9\r\n20\r\n163\r\n\r\nDescribing\r\nFacial Features\r\n37\r\n4\r\n3\r\n9\r\n8\r\n13\r\nWord-of-Mouth\r\nDescendant\r\n55\r\n1\r\n0\r\n0\r\n8\r\n46\r\n\r\nOwnership\r\n47\r\n1\r\n0\r\n0\r\n0\r\n46\r\n\r\nFamiliarity\r\n18\r\n0\r\n0\r\n0\r\n2\r\n16\r\nVisual\r\nEvidence\r\nUniform\r\n73\r\n7\r\n1\r\n8\r\n17\r\n40\r\n\r\nInscription\r\n145\r\n4\r\n3\r\n3\r\n6\r\n129\r\n\r\nOther\r\nVisual Clues\r\n17\r\n1\r\n0\r\n1\r\n2\r\n13\r\nExternal\r\nInformation\r\nExternal URL\r\n47\r\n1\r\n0\r\n1\r\n4\r\n41\r\n\r\nOther Sources\r\n172\r\n9\r\n1\r\n1\r\n21\r\n140\r\n\r\nLack of\r\nInformation\r\n21\r\n0\r\n2\r\n13\r\n6\r\n0\r\nProviding\r\nAdditional\r\nInformation\r\nBiographical\r\nInformation\r\n46\r\n11\r\n2\r\n3\r\n3\r\n27\r\n\r\nAdditional\r\nContext\r\n117\r\n5\r\n0\r\n2\r\n11\r\n99\r\nFrom our logs, we found that 155 users had left 682 notes for 600 different photos. However, as Table 1 shows, around 10% of the votes had a note. Table 3 shows the different topics covered by the notes. We observe that users' voting patterns are significantly influenced by the availability and quality of evidence, with clear facial similarity, period inscriptions, personal anecdotes, visible clues in the photograph, and added biographical context often leading to high confidence \"Yes\" votes, while lack of information typically results in \"Not Sure\" votes.\r\nUsers most frequently left a comment attributing facial similarity (after comparing it in the first step of the validation process) to be the reason for their decision (e.g., \"Identical to the other CDV\"). In some instances, they would expound on it by discussing facial features: \"The eyes, nose, cheek bones, shape of face, all look similar to George Pickett, although possibly reversed based on hair part\". In many instances, we found users inferring biographical information (service records, location, etc.) from visual evidence in the photo, be it uniform or backmarks (e.g., \"Initials MN on chinstrap (brass letters). Signature on verso is made out to Marlin's oldest sister. Style of insignia is consistent with other 1862 recruits for Co. B 1st USSS\").\r\nInterestingly, we also observed a large number of word-of-mouth evidence notes for justifying the user's decisions, such as claiming to be a descendant, or owning the original copy of the photo, or having seen the photo somewhere. For example, one user noted, \"He is my great-great grandfather and this photo has been passed down through the generations to me and was identified by his son John Albert Johnson, my father's grandfather.\" Users also left external URLs and source details in the notes as evidence. Sometimes, they provided additional context (e.g., \"This image came with a group of 7th Iowa images. The majority were of Company G., but there is only one person in the entire 7th Iowa Infantry that could be identified by the first or last name of 'Nelson.'\").\r\nThe validation workflow encouraged users to exercise careful deliberation while making photo ID decisions.. Users felt that the questions in each step of the validation process helped them to carefully weigh in all the evidence and deliberate while voting on the ID (Q4, mean = 4.60, SD = 0.49). C3 explained why the two steps were necessary: \"It's two separate things. One is asking, do you think that this face is the same face? Then the second is, do you think that this face matches this name? I think that that is a necessary question for both of those scenarios. I don't think it's redundant, I think it's necessary.\" H2 appreciated the thoughtfulness that the two-step process encouraged, saying, \"It could be the same guy, but it might be a different guy, but now that you know the other interface kind of forces me to slow down a little bit and think more carefully, because it's asking specific questions about things.\" A couple of participants, however, expressed initial confusion over the separation and order of these two tasks.\r\n\r\nFigure 7: Deliberation in case of post-identification voting. The table shows the distribution of confidence votes (second step of the validation process) against the user's facial similarity comparison (first step of the validation process). The comparison scores were computed by aggregating all the facial similarity comparisons in the first step (replica = 2, facial match = 1, not sure = 0, and different people = -1).\r\n\r\nOur logs affirm the deliberative aspect of the two-step validation process. We observed that when users compared one or more photos in the first step, they factored in the facial similarity evidence for their ID vote decision. Figure 7 shows that when the cumulative comparison scores are positive (i.e., majority of the comparisons were a facial match or a replica), the confidence scores are generally positive. Similarly, when the cumulative comparison scores are negative, the confidence votes are also largely negative. When the users are unsure about the facial similarity comparison, it reflects in their final ID vote. Notably, when there were mismatches between comparison scores and ID votes, users provided reasons in the notes, citing visual evidence and an unsure photo comparison.\r\nPhoto Steward's stewardship architecture fostered diverse forms of meaningful stigmergic collaboration amongst users.. Although most IDs received only a single vote, about 3% (193 IDs) received multiple votes (see Table 1). Among these, 140 IDs saw total consensus among voters on confidence scores, with half involving an explanatory note. When the initial vote lacked a note, subsequent voters often added information like external sources or context. Almost all IDs where voters agreed positively were supported by metadata such as period inscriptions, scholarly sources, or additional comparison photos, underscoring the credibility of these agreements.\r\nWhen users disagreed on an ID, they would almost always justify in notes why they differed from the previous voters' opinions (43 out of 53 disagreement instances). From analyzing the notes, we found that the subsequent voters provided additional information about external sources or service records to disagree (e.g., \"Please see Military Images magazine, MI, Volume XVI, Number 3, November - December 1994 for the correct identification of this image. The correct identification via the Michael McAfee collection is Richard Cramer, 4th Michigan Infantry\"). Among 18 instances of conflicting IDs for the same photo, seven saw resolution through a process of voting one ID over another, often accompanied by external evidence in notes or citing facial similarity.\r\nUser agreement was particularly strong in facial similarity comparisons. Of the 2200 unique photo pairs compared, 141 received multiple user reviews, with 114 seeing total agreement. The remaining cases typically involved user errors in distinguishing between 'facial match' and 'replica', or uncertainty about facial similarity.\r\n6.2 Stewardship Visualizations\r\nStewardship visualizations reflecting community insights enhanced the accuracy of photo ID assessments.. Participants found Photo Steward's bar chart visualization showing the five confidence levels to be simple and easy to understand (Q1, mean = 4.93, SD = 0.25). S1 said, \"I'm a visual person. Bar graphs or charts like statistical data helps me put things into a better perspective or gives me an idea of what I am working with versus just something more plain [like] the other interface.\"\r\nWhen participants were uncertain about an ID, they saw the additional evidence and justification that the community members had provided along with their vote as essential to taking the vote seriously. S5 said, \"If there's other users giving comments like 'I've used this source,' and you know you get information from a source where they found it, I'm probably gonna agree with them. If they're just voting yes without anything else, then I am probably more likely to go out and find sources for the photo myself and make my own decision.\"\r\nIn three instances, students (S1, S3, and S5) initially made incorrect assessments on the original interface but rectified these after using Photo Steward. The community's consensus and evidence in Photo Steward were pivotal in these corrections. S5, confronted with ID conflicts, stated after using Photo Steward: \"You know that helps a lot with what people are thinking and presumably these people have also gone to the Maine State Archives or something and verified and then given their opinion so that helps.\" Similarly, historian H2 and collector C3 initially made incorrect choices but amended their decisions after analyzing community-based evidence on Photo Steward. H2 remarked: \"So I believe it's the first person, of course, because it's based on the Maine State Archives. And then you've got the piece on there that said that you looked and found a different man. With that name in the actual regimental history. I say that's fairly accurate information.\" This highlights the critical role of community consensus in resolving photo ID conflicts.\r\nOn the other hand, when participants were confident about an ID, seeing the community's opinions affirmed their own assessments, for better or worse. In the words of H5: \"I really liked the very clear community consensus, and the ability to be able to see the identities of the people who were looking at these images. It was something that (gave) a boost of confidence in terms of my final decision.\" Overall, the participants found the community opinions to be useful for assessing the IDs (Q2, mean = 4.93, SD = 0.25).\r\nUsers gave additional weight to the opinions of members they were familiar with and desired more contextual information about all contributors.. Participants examined the bar chart visualization details and gave higher weight to the opinions of prominent names from the Civil War photography community while assessing the ID. C3 said, \"Here's <name redacted>, period inscription with valediction, the uniform matches his service record. <name redacted> said the same thing. These two uploaders I hold in very high regard to their opinions on this site. If they're saying that they think highly confident of this identification, that gives me a lot of confidence as well.\" H5 became further unsure about an ID after seeing a fellow historian's vote: \"And, you know, sort of knowing <name redacted>, someone who I know deals with primary source material a lot, and sort of being on the fence with it as well, leaves me in that unsure position.\"\r\nParticipants also sought additional details about community members, proposing indicators of professional status or active participation in the CWPS community. H1 emphasized his appreciation of amateur experts and opposition to gatekeeping, yet he suggested having some kind of credential indicator next to the username would be helpful: \"It will be great if, you know, somebody was an academic historian or a published Civil War author, if there was some way to just say some little tidbit next to <name redacted> 'Oh, saying, hey, I'm from <university redacted>, Professor or, you know, author of whatever.'\" Others preferred indicators of community activity levels or personal connections (i.e., descendants) to the identified individual in the photo.\r\nOur log analysis showed revealed the presence of certain active voters, with 9 out of 328 unique voters voting on more than 50 photos each, and 2 voting on over 2000 photos each (mostly ones that they had uploaded and identified). If we consider only votes on photos identified by someone else, we have 8 users who have voted on more than 15 IDs each, with 1 user voting on over 150 IDs.\r\nThe community opinions made the platform feel more engaging, but users had mixed thoughts about the ideal number of votes per photo.. Participants, in general, felt the community opinions made Photo Steward more engaging compared to the original system. H5 said, \"I thought it appeared more user-centric and user-friendly and it felt more participatory.\" C3 saw the benefits of increased engagement for making IDs more reliable: \"I think for this crowdsourcing project that we're building on this database [it] is very important to have those comments, those feedback, that we see in the voting system. It only makes this ID stronger and makes the project and the database a more trustworthy and reliable source.\"\r\nUsers wanted to see more community opinions and comments for IDs that had few votes. S1, on seeing only one vote for an ID, said, \"That doesn't make me feel as confident because that's not too many for me to give an answer.\" S3 said it would have been easier to assess some photos if \"there was more input from other people.\" Users had mixed opinions about how many votes they wanted to see for verifying an ID. In general, they wanted to see consensus among the community for an identity and at least three votes. C3 said, \"I usually shoot for like three to four [votes] as the lowest where I take some good quality out of those votes. [...] If there's only one or two, and especially if there's two that are split, that is not as reliable to me.\"\r\nParticipants' concerns about spreading voters too thin were borne out in the log data (see Table 1). We analyzed the logs to check how often CWPS users vote on the photos they are browsing. We found 1784 instances (out of 5843 voting instances) where the number of \"lurkers\" for a given photo page exceeded the number of voters on that photo.\r\nTable 4: Face Recognition Similarity Score Stats vs. User Comparisons (Facial Similarity).\r\n\r\nUser Comparison\r\nMean\r\nMedian\r\nSD\r\nCount\r\nReplica\r\n0.88\r\n0.91\r\n0.16\r\n659\r\nFacial Match\r\n0.52\r\n0.65\r\n0.31\r\n1015\r\nDifferent Person\r\n0.55\r\n0.59\r\n0.16\r\n265\r\nNot Sure\r\n0.51\r\n0.58\r\n0.23\r\n261\r\nUsers found the face recognition badge and community's opinions complementary for assessing the reliability of matched photos..\r\nTable 5: User Comparisons (Facial Similarity) vs. Face Recognition.\r\n\r\n\r\n# of Photo Pairs\r\n# of Photo Pairs\r\ncompared by 1 user\r\n# of Photo Pairs\r\ncompared by 2 users\r\n# of Photo Pairs\r\ncompared by 2+ users\r\nSupported by users\r\nand face recognition\r\n1408\r\n1281\r\n109\r\n18\r\nDisputed by users\r\nand face recognition\r\n26\r\n24\r\n0\r\n1\r\nUsers unsure,\r\nface recognition disputes\r\n42\r\n42\r\n0\r\n0\r\nUsers unsure,\r\nface recognition supports\r\n219\r\n217\r\n2\r\n0\r\nUsers dispute,\r\nface recognition supports\r\n239\r\n238\r\n1\r\n0\r\nUsers support,\r\nface recognition disputes\r\n266\r\n257\r\n8\r\n1\r\nFrom our logs, we found that 1408 photo pair comparisons were supported by both users and facial recognition, by far the most common outcome (see Table 5). This information would be visible to the larger user community in the form of stewardship visualizations (see Figure 5). Interestingly, we also see 266 cases where a comparison is supported by users, but disputed by facial recognition (similarity confidence score < 0.50). That outcome was approximately as common as when users disputed a comparison but face recognition supported it (239 pairs) or when a user was unsure but face recognition supported it (219 pairs). However, it was far less common for facial recognition to dispute a comparison when a user also disputed it (26 pairs) or was unsure (42 pairs).\r\nWhen we analyze the face recognition's confidence scores in more detail, Table 4 shows that there is a clear separation between replicas and other types of user comparison scores. Face recognition confidence scores for photo pairs that users labeled as replicas were much higher in terms of both mean and median (0.88 and 0.91, respectively, versus scores in the 0.50s and 0.60s for all others). While these fine-grained scores are not displayed to users — they intentionally see only the face recognition badge — the very close mean and median confidence scores for \"facial match\" versus \"different person\" illustrate the difficulty of automatically identifying non-replica matches and offer support for a hybrid human-AI approach (cf. Section 4.3.1).\r\nParticipants found the community's opinions to be helpful for assessing whether two photos were facial matches or not. S4 said, \"I mean, I think it's cool to see what the community is saying, because I do feel when it comes to saying 'Is this the same person in both these pictures?', that's really the best way to do it if you don't have any [other] information.\" C1 said that he found the community opinions for facial matches especially helpful because he is \"face blind\": \"I can't really identify the face-to-face, but the hair and the mustache and all the stuff that, in addition, it helps me with that for sure. It's good because other than having to find somebody close to me and be like, 'Do you think this is the same people?' [I] have that community right there.\"\r\nParticipants had mixed opinions about facial recognition technology, but most found the badge indicating whether it supported the two photos being a match to be a useful data point. S3 said, \"The facial recognition saying they are similar — I would go ahead and trust that but I don't know if I would trust it enough to make a verification on my own.\" In general, participants found the strengths of facial recognition and the community to be complementary in determining whether two photos showed the same person, and liked seeing both results together. H4 felt the community and the technology had separate roles:\r\nThis is facial recognition, and this is the historical background. I trust facial recognition, but it makes me feel better to have that historical background. I think the human eye can be tricked by different hairstyles and different beards. Just to have this outside historical verification to say like, 'Okay, maybe you or I was tricked, but the machine was not,' I think that's really helpful.\r\nS1 relied on both the community and facial recognition to make a decision on a facial match: \"I think both give me kind of an idea. Okay, there's this facial recognition technology being used, but also there's other users that are leaning towards that this is the same person.\" H5 got a similar boost of confidence: \"This is certainly reassuring seeing not only the AI match, but also in terms of the community — seeing that seven users have said that this is a facial match as well. I'd be quite convinced by this.\"\r\n7 DISCUSSION\r\n7.1 Leveraging Collective Intelligence for Validating Person Identification\r\nPrior work has raised concerns about misinformation in online history communities [43, 75]. To address these problems on CWPS, we built Photo Steward for supporting community-based validation of photo IDs. Users found Photo Steward's stewardship visualizations not only helpful for affirming their own assessment, but also for discovering new knowledge and correcting their decisions, if need be. These visualizations, combined with the validation workflow, exhibit a form of stigmergic collaboration, where users build on prior knowledge left by the community and leave their own assessment for other users [14, 17, 27, 56].\r\nPhoto Steward allowed users to express how confident they are about an ID in a fine-grained manner, in contrast to the binary agreements or disagreements observed on iNaturalist by Wiggins et al. [72]. The CWPS community preferred this nuanced form of stewardship as users are likely to have different degrees of confidence based on the evidence available for identifying a photo, thus demonstrating the effectiveness of Design Goal 2. As S3 pointed out, users often experience difficulty in making a binary decision about individuals who lived 150 years back due to the lack of surviving documentation.\r\nBeyond its basic usefulness, Photo Steward's full potential can best be realized through sustained community participation, but most IDs on CWPS only received one vote. To address this challenge, we can leverage different crowdsourcing and online community strategies. For example, we can draw the community's attention towards IDs that are \"more of a puzzle,\" as H5 suggested, similar to Twitter's Birdwatch promoting tweets for fact-checking [52]. Designing nudges to encourage lurkers to vote on the IDs they are viewing can further help in these efforts. Organizing community events can help foster interest and participation in collaboratively verifying IDs, drawing inspiration from crowdsourcing events like CrowdSolve, where experts and novices collaborate on solving missing persons cold cases [70]. Incentive mechanisms such as leaderboards and challenges [46] can drive extrinsic motivation within the community for verifying the IDs. Finding users who are more likely to vote on an ID, based on their skills and interests, can also be an effective collaboration strategy [71]. In future work, we plan to integrate these strategies and introduce explicit \"calls to action\" [51, 58] on the home feed, guiding the community's attention towards IDs that require validation and fostering more sustainable, collaborative participation in historical photo identification.\r\n7.2 Exercising Deliberation in Human-AI Teams\r\nWe found that Photo Steward's validation workflow was effective not only for voting on the IDs and comparing the photos side-by-side, but also encouraged users to deliberate on their decision, drawing parallels to other social computing systems that support reflection and deliberation (e.g., [28, 29]). This deliberative intervention was non-trivial as users on CWPS follow an identification pipeline which is powered by facial recognition, an AI algorithm that is far from perfect [44]. Users are trying to find the correct match, if present at all, from a pool of potential candidates, which are largely comprised of similar-looking false positives — akin to finding a needle in a haystack. Further, the task of comparing photos of people is by no means an easy task for humans, even in a modern context [53]. While Photo Steward can not completely curb automation bias, an issue that has been previously observed in multiple online history communities [44, 75], its multi-step, validation workflow with structured feedback interventions encouraged users to deliberate over AI suggestions before making a decision.\r\nPhoto Steward's workflow also compartmentalizes the tasks that AI is good at – such as quickly retrieving similar-looking candidates from a large search pool – from the tasks where the AI makes more errors – such as verifying whether two faces show the same person or not [5, 55]. Decoupling facial similarity comparison from the person identification task (Design Goal 1) allows the users to now focus on the face verification task. In doing so, Photo Steward's workflow supports effective human-AI teaming in the context of person identification by allowing the user to make a granular assessment for the face verification task instead of the AI, while also ensuring that an AI-retrieved, similar-looking potential candidate is being compared against.\r\nAs imperfect AI algorithms get deployed in high-stakes scenarios such as medical imaging, law enforcement, etc. [6], it becomes more critical to reduce automation bias and encourage more deliberative decision-making. Amershi et al. recommend granular user feedback while interacting with AI systems as part of their \"Guidelines for Human-AI Interaction\" [2]. Similarly, other forms of design interventions, such as counterfactual AI explanations [63], chatbots [26], and community opinions [59] can also be explored for encouraging deliberative decision-making with AI assistance.\r\nPrior work in human-face recognition teams has shown that algorithmic suggestions can have a significant biasing effect on a user's decision [23]. Our findings showed that Photo Steward was able to encourage users to exercise deliberation while interacting with results retrieved by facial recognition. At the same time, Table 4 also showed that users can differ from the algorithm's suggestions, thus necessitating a deeper dive analysis of this dissonance as part of future work.\r\n7.3 Assessing Quality in Crowdsourced Original Historical Research\r\nWe found that Photo Steward's stewardship visualizations helped users assess the reliability of photo IDs on CWPS, which were a result of Design Goal 3. However, crowdsourced identifications always run the risk of groupthink [24, 25], which can eventually mislead users into believing and amplifying misidentifications, a concern also raised by H4. Public deliberation of modern photo IDs on social media can have profound negative consequences for false targets, as exemplified by the Boston Marathon bombing [49, 66] and the recent US Capitol riot [48]. This raises the question: are Photo Steward's stewardship visualizations sufficient for assessing the quality of photo identifications made on CWPS?\r\nPrior work on crowdsourced scholarship suggests an answer. Rosenzweig [57] analyzed Wikipedia as a source of historical scholarship, noting its policy against original research, and advocating for it as a tool for teaching the limitations of information sources and critical analysis of primary and secondary sources. Motivated along similar lines, Forte et al [18] proposed the assessability framework for designing assessable participatory information systems, based on information provenance and stewardship. The concept of provenance, extensively used in history and archival studies, describes information that makes it possible to trace the ownership or origins of the content, while stewardship refers to the processes that were used for maintaining the content, including its authorship. In the case of Wikipedia, Forte et al. found that visualizing provenance (i.e., citation types) and stewardship (i.e., article quality) had a significant impact on assessments of articles and Wikipedia as an information source.\r\nWhile Photo Steward enables information stewardship on CWPS, there is an opportunity for incorporating provenance into the CWPS platform to make it a truly assessable online platform. A significant proportion of the notes left by users on Photo Steward qualified as provenance information, namely comments about period inscriptions, family trees, external sources and URLs. The challenges of assessing IDs on CWPS are, however, different from assessing information on Wikipedia, primarily because CWPS supports original research unlike Wikipedia's no original research policy [73]. This was also the reason why we designed Photo Steward to be a review system rather than a single editable output such as Wikipedia; original research such as historical photo identifications is often times an evolving investigation rather than a final decision. To assess the reliability of original photo IDs made on CWPS, users may want to factor in the provenance of the reference photos that were used in the identification process. In such cases, Photo Steward's stewardship visualizations (i.e., facial similarity reliability) can further help the user in assessing whether the reference photos can be used as reliable provenance or not.\r\n8 CONCLUSION\r\nPhoto Steward attempts to help users assess and validate photo IDs better on CWPS. We present an information stewardship architecture, and adapt it for the task of historical person identification. We demonstrate the effectiveness of Photo Steward on CWPS, an existing online platform, where users found the stewardship visualizations, which included the community opinions and the AI verdict, useful for making accurate assessments of photo IDs on the platform. Further, users found Photo Sleuth's multi-step, structured validation workflow to help them deliberate before making decisions about the photo's identity. This work opens doors for exploring new ways to leverage collective intelligence and AI in creating assessable online information systems for historical archives.\r\nACKNOWLEDGMENTS\r\nWe wish to thank Ron Coddington, Paul Quigley, Liling Yuan, and our study participants. This research was supported by NSF IIS-1651969 and a Virginia Tech ICTAS Junior Faculty Award.\r\nREFERENCES\r\n2021. Heritage Auctions: World's Largest Collectibles Auctioneer. https://www.ha.com/ Navigate tocitation 1citation 2\r\nSaleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. 2019. Guidelines for human-AI interaction. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–13. Navigate tocitation 1\r\nBrakkton Booker. 2015. $2 photo found at Junk Store has Billy the kid in it, could be worth $5M. https://www.npr.org/sections/thetwo-way/2015/10/15/448993361/-2-photo-found-at-junk-store-has-billy-the-kid-in-it-could-be-worth-5-million Navigate tocitation 1citation 2\r\nVirginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3, 2 (2006), 77–101. Navigate tocitation 1\r\nJoy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Conference on Fairness, Accountability and Transparency, FAT 2018, 23-24 February 2018, New York, NY, USA. 77–91. http://proceedings.mlr.press/v81/buolamwini18a.html Navigate tocitation 1\r\nCarrie J Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg S Corrado, Martin C Stumpe, et al. 2019. Human-centered tools for coping with imperfect algorithms during medical decision-making. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–14. Navigate tocitation 1\r\nFanny Chevalier, Stéphane Huot, and Jean-Daniel Fekete. 2010. Wikipediaviz: Conveying article quality for casual wikipedia readers. In 2010 IEEE Pacific Visualization Symposium (PacificVis). IEEE, 49–56. Navigate tocitation 1\r\nH Jason Combs. 2005. The Internet's Impact on the Antiques Trade. Material Culture (2005), 26–41. Navigate tocitation 1citation 2\r\nAnthony DeBartolo. 1975. Appraisers can tell when old photo really may be golden. https://www.chicagotribune.com/news/ct-xpm-1985-12-27-8503300067-story.html Navigate tocitation 1\r\nAnthony DeBartolo. 2021. Appraisers can tell when old photo really may be golden. https://www.chicagotribune.com/news/ct-xpm-1985-12-27-8503300067-story.html Navigate tocitation 1\r\nHarley D Dickinson. 1998. Evidence-based decision-making: an argumentative approach. International Journal of Medical Informatics 51, 2-3 (1998), 71–81. Navigate tocitation 1\r\nFor The Inquirer Edward Colimore. 2019. Did John Wilkes Booth get away with murdering President Abraham Lincoln?https://www.inquirer.com/news/john-wilkes-booth-lincoln-conspiracy-photo-recognition-20190415.html Navigate tocitation 1\r\nMark Elliott. 2006. Stigmergic Collaboration: The Evolution of Group Work: Introduction. m/c journal 9, 2 (2006). Navigate tocitation 1citation 2citation 3\r\nMark Elliott. 2016. Stigmergic collaboration: A framework for understanding and designing mass collaboration. In Mass collaboration and education. Springer, 65–84. Navigate tocitation 1citation 2citation 3\r\nThomas Erickson and Wendy A Kellogg. 2000. Social translucence: an approach to designing systems that support social processes. ACM transactions on computer-human interaction (TOCHI) 7, 1 (2000), 59–83. Navigate tocitation 1\r\nDan Evon. 2020. Did Joe Biden's Great-Grandfather Own Slaves?https://www.snopes.com/fact-check/joe-biden-slaves-great-grandfather/ Navigate tocitation 1\r\nKristie Fisher, Scott Counts, and Aniket Kittur. 2012. Distributed sensemaking: improving sensemaking by leveraging the efforts of previous users. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 247–256. Navigate tocitation 1citation 2\r\nAndrea Forte, Nazanin Andalibi, Thomas Park, and Heather Willever-Farr. 2014. Designing information savvy societies: an introduction to assessability. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2471–2480. Navigate tocitation 1citation 2citation 3citation 4\r\nJacey Fortin. 2018. She Was the Only Woman in a Photo of 38 Scientists, and Now She's Been Identified. The New York Times (Mar 2018). https://www.nytimes.com/2018/03/19/us/twitter-mystery-photo.html Navigate tocitation 1\r\nSarah A Gilbert. 2020. \" I run the world's largest historical outreach project and it's on a cesspool of a website.\" Moderating a Public Scholarship Site on Reddit: A Case Study of r/AskHistorians. Proceedings of the ACM on Human-Computer Interaction 4, CSCW1 (2020), 1–27. Navigate tocitation 1\r\nJerome S Handler and Michael L Tuite. 2007. Retouching History: The Modern Falsification of a Civil War Photograph. Navigate tocitation 1citation 2\r\nM. Keith Harris. 2019. Civil War Photo Sleuth. Journal of American History 106, 2 (2019), 544–546. https://doi.org/10.1093/jahist/jaz498 Navigate tocitation 1citation 2\r\nJohn J Howard, Laura R Rabbitt, and Yevgeniy B Sirotin. 2020. Human-algorithm teaming in face recognition: How algorithm outcomes cognitively bias human decision-making. Plos one 15, 8 (2020), e0237855. Navigate tocitation 1\r\nPan Hui and Sonja Buchegger. 2009. Groupthink and peer pressure: Social influence in online social network groups. In 2009 International Conference on Advances in Social Network Analysis and Mining. IEEE, 53–59. Navigate tocitation 1\r\nNassim JafariNaimi and Eric M Meyers. 2015. Collective intelligence or group think? Engaging participation patterns in World Without Oil. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing. 1872–1881. Navigate tocitation 1\r\nSoomin Kim, Jinsu Eun, Joseph Seering, and Joonhwan Lee. 2021. Moderator chatbot for deliberative discussion: Effects of discussion structure and discussant facilitation. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–26. Navigate tocitation 1\r\nAniket Kittur, Andrew M Peters, Abdigani Diriye, and Michael Bove. 2014. Standing on the schemas of giants: socially augmented information foraging. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. 999–1010. Navigate tocitation 1\r\nTravis Kriplean, Caitlin Bonnar, Alan Borning, Bo Kinney, and Brian Gill. 2014. Integrating On-demand Fact-checking with Public Dialogue. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing(CSCW '14). ACM, New York, NY, USA, 1188–1199. https://doi.org/10.1145/2531602.2531677 Navigate tocitation 1citation 2citation 3\r\nTravis Kriplean, Michael Toomim, Jonathan Morgan, Alan Borning, and Andrew Ko. 2012. Is This What You Meant?: Promoting Listening on the Web with Reflect. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems(CHI '12). ACM, New York, NY, USA, 1559–1568. https://doi.org/10.1145/2207676.2208621 Navigate tocitation 1citation 2citation 3\r\nKurt Luther. 2016. How Fellow Collectors, Field Photos and Muttonchops Identified an Unknown Officer. Military Images 34, 1 (2016), 29–31. Navigate tocitation 1\r\nKurt Luther. 2017. Merrill Carbine Leads to a Soldier's Identification. Military Images 35, 2 (2017), 64–65. Navigate tocitation 1citation 2citation 3\r\nKurt Luther. 2018. Non-Traditional Research Tools—and Serendipity. Military Images 36, 3 (2018), 12–13. Navigate tocitation 1\r\nKurt Luther. 2018. What are the odds? Photo sleuthing by the numbers. Military Images 36, 1 (2018), 12–15. Navigate tocitation 1citation 2citation 3citation 4citation 5\r\nKurt Luther. 2019. What to Do When Gold Standards Go Wrong?Military Images 37, 1 (2019), 8–9. https://www.jstor.org/stable/26532101 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: How to Trust the Worthiness of an Identification. Military Images 38, 3 (213) (2020), 8–11. https://www.jstor.org/stable/26914966 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: Lost and Found in the Library of Congress. Military Images 38, 2 (212) (2020), 10–13. https://www.jstor.org/stable/26890126 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: The Art of Photo Sleuthing. Military Images 38, 4 (214) (2020), 8–11. https://www.jstor.org/stable/26925454 Navigate tocitation 1\r\nRamona Martinez. 2012. Photo mystery solved, then doubted, then deciphered, thanks to readers. https://www.npr.org/sections/pictureshow/2012/04/17/150801239/photo-mystery-solved-then-doubted-then-resolved-thanks-to-readers Navigate tocitation 1\r\nRamona Martinez. 2012. Unknown No More: Identifying A Civil War Soldier. http://www.npr.org/2012/04/11/150288978/unknown-no-more-identifying-a-civil-war-soldier Navigate tocitation 1\r\nChristian A Meissner and John C Brigham. 2001. Thirty years of investigating the own-race bias in memory for faces: A meta-analytic review.Psychology, Public Policy, and Law 7, 1 (2001), 3. Navigate tocitation 1\r\nVikram Mohanty, Kareem Abdol-Hamid, Courtney Ebersohl, and Kurt Luther. 2019. Second opinion: Supporting last-mile person identification with crowdsourcing and face recognition. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 86–96. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6\r\nVikram Mohanty and Kurt Luther. 2023. DoubleCheck: Designing Community-based Assessability for Historical Person Identification. ACM Journal on Computing and Cultural Heritage (JOCCH) (to appear) (2023). Navigate tocitation 1\r\nVikram Mohanty, David Thames, Sneha Mehta, and Kurt Luther. 2019. Photo sleuth: Combining human expertise and face recognition to identify historical portraits. In Proceedings of the 24th International Conference on Intelligent User Interfaces. 547–557. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6citation 7citation 8\r\nVikram Mohanty, David Thames, Sneha Mehta, and Kurt Luther. 2020. Photo Sleuth: Identifying Historical Portraits with Face Recognition and Crowdsourced Human Expertise. ACM Transactions on Interactive Intelligent Systems (TiiS) 10, 4 (2020), 1–36. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6citation 7citation 8citation 9citation 10\r\nMeredith Ringel Morris, Scott Counts, Asta Roseway, Aaron Hoff, and Julia Schwarz. 2012. Tweeting is believing? Understanding microblog credibility perceptions. In Proceedings of the ACM 2012 conference on computer supported cooperative work. 441–450. Navigate tocitation 1\r\nBenedikt Morschheuser, Juho Hamari, and Jonna Koivisto. 2016. Gamification in crowdsourcing: a review. In 2016 49th Hawaii International Conference on System Sciences (HICSS). IEEE, 4375–4384. Navigate tocitation 1\r\nKathleen L Mosier and Linda J Skitka. 1999. Automation use and automation bias. In Proceedings of the human factors and ergonomics society annual meeting, Vol. 43. SAGE Publications Sage CA: Los Angeles, CA, 344–348. Navigate tocitation 1citation 2\r\nGreg Myre. 2021. How Online Sleuths Identified Rioters At The Capitol. https://www.npr.org/2021/01/11/955513539/how-online-sleuths-identified-rioters-at-the-capitol Navigate tocitation 1\r\nJohnny Nhan, Laura Huey, and Ryan Broll. 2017. Digilantism: An analysis of crowdsourcing and the Boston marathon bombings. The British journal of criminology 57, 2 (2017), 341–361. Navigate tocitation 1\r\nRaja Parasuraman and Dietrich H Manzey. 2010. Complacency and bias in human use of automation: An attentional integration. Human factors 52, 3 (2010), 381–410. Navigate tocitation 1\r\nJunwon Park, Ranjay Krishna, Pranav Khadpe, Li Fei-Fei, and Michael Bernstein. 2019. AI-based request augmentation to increase crowdsourcing participation. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 115–124. Navigate tocitation 1\r\nSarah Perez. 2022. Twitter to show 'Birdwatch' community fact-checks to more users, following criticism. https://techcrunch.com/2022/03/03/twitter-to-show-birdwatch-community-fact-checks-to-more-users-following-criticism/ Navigate tocitation 1\r\nP Jonathon Phillips, Amy N Yates, Ying Hu, Carina A Hahn, Eilidh Noyes, Kelsey Jackson, Jacqueline G Cavazos, Géraldine Jeckeln, Rajeev Ranjan, Swami Sankaranarayanan, et al. 2018. Face recognition accuracy of forensic examiners, superrecognizers, and face recognition algorithms. Proceedings of the National Academy of Sciences 115, 24 (2018), 6171–6176. Navigate tocitation 1citation 2\r\nPeter Pirolli, Evelin Wollny, and Bongwon Suh. 2009. So you know you're getting the best possible information: a tool that increases Wikipedia credibility. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1505–1508. Navigate tocitation 1\r\nInioluwa Deborah Raji and Joy Buolamwini. 2019. Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products. Navigate tocitation 1\r\nAmira Rezgui and Kevin Crowston. 2018. Stigmergic coordination in Wikipedia. In Proceedings of the 14th International Symposium on Open Collaboration. 1–12. Navigate tocitation 1\r\nRoy Rosenzweig. 2006. Can History Be Open Source? Wikipedia and the Future of the Past. Journal of American History 93, 1 (June 2006), 117–146. Navigate tocitation 1citation 2\r\nSaiph Savage, Andres Monroy-Hernandez, and Tobias Höllerer. 2016. Botivist: Calling volunteers to action using online bots. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing. 813–822. Navigate tocitation 1\r\nMike Schaekermann, Joslin Goh, Kate Larson, and Edith Law. 2018. Resolvable vs. irresolvable disagreement: A study on worker deliberation in crowd work. Proceedings of the ACM on Human-Computer Interaction 2, CSCW (2018), 1–19. Navigate tocitation 1\r\nMichael S. Schmidt. 2018. 'Flags of Our Fathers' Author Now Doubts His Father Was in Iwo Jima Photo. The New York Times (Jan 2018). https://www.nytimes.com/2016/05/04/us/iwo-jima-marines-bradley.html Navigate tocitation 1\r\nJennifer Schuessler. 2017. Found: Oldest Known Photo of a U.S. President (Socks and All). https://www.nytimes.com/2017/08/16/arts/design/john-quincy-adams-daguerreotype-sothebys-auction.html Navigate tocitation 1\r\nJulia Schwarz and Meredith Morris. 2011. Augmenting web pages and search results to support credibility assessment. In Proceedings of the SIGCHI conference on human factors in computing systems. 1245–1254. Navigate tocitation 1\r\nRuoxi Shang, KJ Kevin Feng, and Chirag Shah. 2022. Why Am I Not Seeing It? Understanding Users' Needs for Counterfactual Explanations in Everyday Recommendations. In 2022 ACM Conference on Fairness, Accountability, and Transparency. 1330–1340. Navigate tocitation 1\r\nLinda J Skitka, Kathleen L Mosier, and Mark Burdick. 1999. Does automation bias decision-making?International Journal of Human-Computer Studies 51, 5 (1999), 991–1006. Navigate tocitation 1\r\nVerbi Software. 2019. MAXQDA 2020 [computer software]. VERBI Software. Available from maxqda.com. Navigate tocitation 1\r\nNPR Staff. 2016. How Social Media Smeared A Missing Student As A Terrorism Suspect. https://www.npr.org/sections/codeswitch/2016/04/18/474671097/how-social-media-smeared-a-missing-student-as-a-terrorism-suspect Navigate tocitation 1\r\nBongwon Suh, Ed H Chi, Aniket Kittur, and Bryan A Pendleton. 2008. Lifting the veil: improving accountability and social transparency in Wikipedia with wikidashboard. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1037–1040. Navigate tocitation 1\r\nW Ben Towne, Aniket Kittur, Peter Kinnaird, and James Herbsleb. 2013. Your process is showing: controversy management and perceived quality in Wikipedia. In Proceedings of the 2013 conference on Computer supported cooperative work. 1059–1068. Navigate tocitation 1\r\nCivil War Trust. 2021. Military Images Magazine | Interview with Ron Coddington. https://www.battlefields.org/learn/articles/military-images-magazine Navigate tocitation 1\r\nSukrit Venkatagiri, Aakash Gautam, and Kurt Luther. 2021. CrowdSolve: Managing Tensions in an Expert-Led Crowdsourced Investigation. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–30. Navigate tocitation 1\r\nShaun Wallace, Lucy Van Kleunen, Marianne Aubin-Le Quere, Abraham Peterkin, Yirui Huang, and Jeff Huang. 2017. Drafty: Enlisting Users To Be Editors Who Maintain Structured Data. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 5. Navigate tocitation 1\r\nAndrea Wiggins and Yurong He. 2016. Community-based data validation practices in citizen science. In Proceedings of the 19th ACM Conference on computer-supported cooperative work & social computing. 1548–1559. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6\r\nFoundation Wikimedia. 2022. No original research. https://en.wikipedia.org/wiki/Wikipedia:No_original_research Navigate tocitation 1\r\nHeather Willever-Farr, Lisl Zach, and Andrea Forte. 2012. Tell me about my family: A study of cooperative research on Ancestry. com. In Proceedings of the 2012 iConference. ACM, 303–310. Navigate tocitation 1\r\nHeather L Willever-Farr and Andrea Forte. 2014. Family matters: Control and conflict in online family history production. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. ACM, 475–486. Navigate tocitation 1citation 2citation 3citation 4citation 5\r\nJeremy B Wilmer. 2017. Individual differences in face recognition: A decade of discovery. Current Directions in Psychological Science 26, 3 (2017), 225–230. Navigate tocitation 1\r\nBob Zeller. 2022. Searching for photos of Civil War Soldiers | David Wynn Vaughan. https://www.civilwarphotography.org/searching-for-photos-of-civil-war-soldiers/ Navigate tocitation 1\r\nA CIVIL WAR PHOTO SLEUTH: DESIGN CHALLENGES AND OPPORTUNITIES\r\n \r\nHere, we provide details of three key challenges posed by Civil War Photo Sleuth that might have contributed towards misidentification and subsequently, inaccurate assessments, and how they present design opportunities for Photo Steward.\r\nA.1 Conflating facial similarity with photo identification\r\nA.1.1 Challenges: Mohanty et al. conducted a benchmarking study of CWPS [44], and found the face recognition algorithm to be of low precision; i.e., it retrieved hundreds of search results which may look similar to the query photo but are actually different people (false positives). Low precision increases the chances that users will interact with a lot of false positives. In such cases, one may need to garner additional information (i.e., comparing biographical information) before making a decision. However, it is plausible that automation bias may play a role in non-expert users making a match solely based on facial similarity [47]. As a result, there is strong potential for misidentification (see Figure 8).\r\nA.1.2 Current Workflow: CWPS's compare interface allows users to closely inspect the search results for a potential match, but does not make any distinction between facial similarity comparison and photo identification. Both are conflated into a one-step process, with one \"Identify\" button for the users to make their decisions (see Figure 2-C). Yet, users may want to indicate agreement with just the facial similarity (i.e., query photo and the search result showing the same person) but not the identity (i.e., name and biographical information), or vice versa.\r\nA.1.3 Design Goal 1: To support accurate investigation of photo identifications, users should be able to deliberate on the different aspects of the decision-making process. Providing users with a decision-making workflow that decouples facial similarity comparison from the overall photo identification task would allow them to focus on these tasks separately, while discouraging them from making decisions solely on the basis of facial similarity.\r\n\r\nFigure 8: An example of a misidentified photo on CWPS. This photo was identified as John C Whiteside based on facial similarity with the top search result: an identified reference photo of John C Whiteside. However, other visual clues, such as the photographer studio, do not align with Whiteside's biographical information.\r\n\r\nA.2 Lacking support for fine-grained, deliberative decision making\r\nA.2.1 Challenges: Historical photo identification is a complex task, where experts often corroborate multiple pieces of evidence, including facial similarity comparison, before reaching a decision about the identity of the photo [33]. While confirming an identity, experts may be highly confident if the source, military records, uniform clues, and additional photos of the same person all line up, or slightly confident if they need additional evidence. Conflicting evidence pieces may also affect their confidence levels. Similarly, they may have different degrees of certainty while ruling out an identity for a photo. A lack of support for expressing and displaying granularity in these photo identification decisions can lead to varying degrees of uncertainty being captured and misinterpreted as a confirmation, and eventually propagating misidentifications. Further, while it is safe to assume the vast majority of the Civil War photography community care about the accuracy of the photo IDs, there exists a small risk of financially-driven misidentifications since certain types of identified photos are considered to be more valuable [9].\r\nA.2.2 Current Workflow: The compare interface's \"Identify\" button (see Figure 2-C) is the only mechanism available on the platform for users to provide (a unary) input on photo identifications. CWPS users currently cannot provide fine-grained feedback on a given photo's identity, either at the time of identifying or afterwards on the photo page.\r\nA.2.3 Design Goal 2 : Users should be able to express how certain or uncertain they are about a photo identification. Interventions for allowing users to provide structured feedback can not only facilitate more accurate, fine-grained responses, but also encourage deliberation on the available evidence before making a decision, borrowing inspiration from other social computing systems that support users reflecting and deliberating on available information [28, 29].\r\nA.3 Limited validation opportunities for the community\r\nA.3.1 Challenges: For humans, deciding whether two photos show the same person is a highly subjective experience. A study by Mohanty et al. [41] showed that participants in a crowdsourcing study often disagree with each other and with facial recognition results in person identification tasks. The same study showed that people often seek a second opinion from peers for validating photo identifications. Without validation, original crowdsourced historical research may result in unresolved cases of conflicting IDs proposed for the same face, incorrect IDs remaining unchecked, or airtight IDs not being confirmed.\r\nA.3.2 Current Workflow: Once a photo has been identified, CWPS does not offer other users the option to express their opinions on whether two photos show the same person or not, or if the photo has been identified correctly or not (see Figure 8). The photo page does not indicate if (or why) the photos are linked to each other, how they are similar, or which user(s) considered them to be similar. Further, it does not indicate the role of facial recognition in linking them together.\r\nA.3.3 Design Goal 3 : To support accurate original historical research, the platform should encourage information stewardship from the community by allowing members to give their opinions on existing photo identifications [72]. Further, the platform should also be transparent about the role of community stewardship and facial recognition for a given photo identification, which can then act as evidence for aiding subsequent validation efforts by other users, thus supporting a form of stigmergic collaboration [13, 14].\r\nB DETAILS FOR LOG ANALYSIS\r\n \r\nTo understand the community's stewardship behaviors, we examined website logs for all user activities for one year since we launched the new features (December 2020 – December 2021). During this period, we observed 5843 voting instances on 5672 photos for 5355 unique IDs, where a user voting on whether Photo N can be identified as a Person M or not is considered to be one voting instance. For a given voting instance, we analyzed 1) any associated facial similarity comparisons to understand how they deliberated on their final voting decision, and 2) the user's confidence vote plus any justification notes left by the user to understand their decision rationale. Of the 5672 query photos, 4297 photos (4377 voting instances) did not have any facial similarity comparisons — only the user's confidence on the ID (plus any notes) was captured.\r\nFor the remaining voting instances which had at least one associated facial similarity comparison (i.e., users deliberate through the two-step validation process), we broke them down into pre-identified (i.e., the user knew the ID of the photo ahead of time and therefore, might be looking for a specific target on the search results page) and post-identified (i.e., the user did not know the ID of the photo and therefore, may not have a specific target while analyzing the search results) cases. As mentioned earlier in Section 4.2, users had the opportunity to validate IDs either on the search results page on the photo page. While a pre-identified voting instance almost certainly originated from the search results page, a post-identified case could be from either page. Since the logs did not give us the page origin of each vote directly, we triangulated from CWPS's timestamps to determine whether a given voting instance was for a pre-identified case or a post-identified one.\r\nAfter collating all the associated facial similarity comparisons for a given voting instance, we had 1064 pre-identified and 576 post-identified user deliberations to analyze. Each deliberation instance is a user's attempt to identify a query photo as a given target (person) ID, where they first compare facial similarity with all other photos that have been identified as the target ID, followed by the user's confidence on the query photo being the target ID. We analyzed the user responses to see whether the facial similarity comparisons had any impact on the user's confidence.\r\nUsers had provided comments in 682 (out of 5843, or 11.7%) voting instances. We coded these comments using an iterative, inductive approach, which resulted in five high-level themes, which can broken down into 13 sub-categories (see Appendix C).\r\nTo understand any stigmergic collaboration processes at play, we also analyzed the IDs which had multiple votes to check for agreements and disagreements between the voters. We further analyzed how the community's facial similarity comparisons compare against the facial recognition scores.\r\nC THEMES FOR NOTES ANALYSIS\r\nPhoto Comparisons\r\nHigh-Level Comparison: The note mentions \"replica\", \"facial similarity\", \"facial match\", \"identical\", \"visual comparison\", and other similar terms that describe comparisons with a prior identified photo.\r\nDescribing facial features: The note mention facial features like \"eyes\", \"hairline\", \"ears\", etc. to make comparisons\r\nWord-of-Mouth\r\nDescendant: The note either mentions that the user is a descendant of the person being identified, or they got the information from the family of the person.\r\nOwnership: The note either mentions that the user owns a printed version of the photo, compared with a photo in their collection, or they know the owner of the photo.\r\nFamiliarity/Self-Reported Research: The note mentions that the user has seen the photo somewhere, be it in a book, museum, etc.\r\nVisual Clues\r\nUniform: The note mentions visual clues that pertain to the uniform of the person (e.g., hat insignia, shoulder straps, etc.) The user may infer the possible service information (i.e., ranks, branches, regiments, etc.) from the uniform clues.\r\nInscription: The note mentions the presence of a period inscription on the photo (a highly trustworthy primary source for a person's ID), or an album case, or modern inscriptions such as books, which is generally the name of the person being identified. In some instances, the inscribed text may point to the person's service information.\r\nOther Visual Clues: The note mentions visual clues in the photo (e.g., backmarks, borders, etc.) beyond the person's face.\r\nExternal Information\r\nExternal URL: The note mentions an external URL, which supposedly has additional information about the photo's ID.\r\nOther sources (e.g., museum, website, book, etc.): The note mentions an external source (e.g., museum, book, etc.) that supposedly has evidence for the photo's ID, but no URLs are provided. Details about the source may or may not be available.\r\nLack of Information / Seeking Additional Evidence: The note mentions the lack of evidence or seeking additional evidence, be it about the source or the service information.\r\nProviding Additional Information\r\nBiographical Information: The notes mentions additional information about the person's service records, specific regiments, biographical information (name, year, location), etc.\r\nAdditional Context: The note mentions some additional context provided by the user to justify their decision, such as information about the photo collection, or pointing to someone else's research, or some historical context, or incorrect evidence, or if the person is prominent.\r\nD PARTICIPANT DETAILS\r\nUndergraduate and master's students concentrating in history who use Civil War photos for their coursework and research projects, but are not (yet) employed in a professional capacity as historians. We recruited five students via recommendations from our university's history department. None of the students had used CWPS before, or were known to the authors prior to the study. Three students were men and two were women, and all were in the \"18 to 30\" age group. We anonymize them with identifiers S1–S5.\r\nExperienced users of Civil War Photo Sleuth who have added over 50 photos each and have substantial knowledge of Civil War history, but are not professional historians. We recruited five amateur experts from the CWPS contact list. All five users were men, and they were distributed across different age groups (two in \"18 to 30\", two in \"31 to 40\", and one in \"51 to 60\"). We anonymize them with identifiers C1–C5. C1 and C3 are among the most active daily users on CWPS. Only two of the five had used Photo Steward before.\r\nExpert historians with a graduate degree in history, specializing in American Civil War history, but with little or no previous experience with CWPS. We recruited five historians via recommendations from our university's history department. Three historians were men and two were women. They were distributed across different age groups (two in \"18 to 30\", two in \"31 to 40\", and one in \"51 to 60\"). We anonymize them with identifiers H1–H5. None of them had used Photo Steward before.\r\nE LAB STUDY QUESTIONS\r\nE.1 Semi-Structured Questions\r\nIs there a way that you would like to capture your thoughts on this ID and share them with others, if possible? If so, what would that look like? If not, can you explain why not?\r\nWhat did you think about the community opinions?\r\nWhat do you think about the ID quality visualization?\r\nWhat did you think about the 2-step process while agreeing/disagreeing on an identity?\r\nWhat is your overall opinion of both the interfaces?\r\nWhich interface would you prefer for validating the information? And why?\r\nWhat would you change or improve?\r\nE.2 Usability Survey\r\nQ1. The community's opinions about an identity were clear and easy to understand in the 2nd system. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ2. The community's opinions about an identity were useful for assessing the information. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ3. The process of voting on an identity was clear and easy to understand. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ4. Comparing other photos first and then voting on an identity helped me deliberate and make more accurate decisions. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ5. I was able to validate the information better using the 2nd system. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nF SUMMARY OF CHANGES\r\nWe summarize in Table 6 about how Photo Steward differs from CWPS.\r\nTable 6: Summary of changes: Civil War Photo Sleuth (CWPS) with and without Photo Steward.\r\n\r\nCWPS\r\nCWPS + Photo Steward\r\nDecision-Making: How do users make identification decisions for the query photo when they see the similar-looking search results retrieved by facial recognition?\r\nA single-step comparison interface that allows the user to compare the query photo with one similar-looking photo and the associated biographical information of the target ID. The single-step workflow conflates the two tasks of facial similarity and person identification.\r\nA multi-step validation interface that allows the user to compare the query photo with all previously identified photos of a similar-looking target ID (search result). The two-step workflow decouples the two tasks of facial similarity and person identification, thus allowing the user to deliberate on the facial similarity comparison between the query photo and the target photo(s) before making a decision on the target identity.\r\nUsers make a unary input on the query photo's ID by clicking an \"Identify\" button in the comparison interface, which will link the target ID to the query photo. The comparison interface does not allow users to rule out candidates.\r\nUsers can provide fine-grained decisions for both facial similarity and person identification steps in the validation interface. In Step 1, the user compares the query photo and the target photo for facial similarity by selecting from the following options: No (Different Person), Not Sure, Yes (Facial Match), and Yes (Replica). In Step 2, users indicate how confident they are about the query photo's ID (as the proposed target ID) by selecting the following options: No (Highly Confident), No (Slightly Confident), Not Sure, Yes (Slightly Confident), and Yes (Highly Confident).\r\nAccess: What kind of validation opportunities are available for users?\r\nUsers can only access the comparison interface from the search results page if they search for similar-looking candidates using facial recognition. Once the photo has been identified, users cannot access the comparison interface on the photo page. The community cannot weigh in on an ID's reliability on the photo page.\r\nUsers can access the validation interface on both the search results page (while identifying the query photo) and the photo page (after the photo has been identified). The community can weigh in on the reliability of the proposed ID(s) and facial matches using the validation interface on the photo page, thus engaging in a form of stigmergic collaboration.\r\nEvidence: What kind of stewardship evidence is presented to the users?\r\nThe photo page displays the proposed ID(s) for the photo without any additional information about the ID's reliability. Similarly, other photos that have been matched to the query photo are also displayed without any reliability indicator. Since community opinions are not captured on CWPS, they are not displayed.\r\nThe photo page displays the proposed ID(s) for the photo along with stewardship visualizations of the a) community's confidence on the ID, and b) facial similarity comparison with other photos of the same ID by both the community and AI.\r\nFOOTNOTE\r\n1 www.civilwarphotosleuth.com\r\n\r\n\r\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs International 4.0 License.\r\nCI '23, November 06–09, 2023, Delft, Netherlands\r\n© 2023 Copyright held by the owner/author(s).\r\nACM ISBN 979-8-4007-0113-9/23/11.\r\nDOI: https://doi.org/10.1145/3582269.3615600\n",
    "totalFiles": 1
  },
  {
    "id": "extraction_1754350334056_tiashwtcm",
    "timestamp": "2025-08-04T23:32:14.056Z",
    "files": [
      {
        "name": "story.txt",
        "size": 2786,
        "type": "text/plain",
        "extractedText": "Ron: Great. I want to start with a detail about you that I’ve always appreciated, which is that you started out as a collector and someone who was deeply interested in the Civil War. Talk a little about how you became interested and got started.\r\n\r\nKurt: Sure thing. I am a bit of a Civil War collector. I grew up in a family of Civil War and history collectors and appreciators. I spent many childhood hours at antique shops and flea markets, and our home was full of interesting Civil War collectibles—books, artifacts, and all kinds of interesting stuff. That was my world as a kid. As I grew older, I realized that I could actually have my own Civil War collection. What was a little different about me is that I gravitated toward Civil War photographs—a category my family didn’t have too much of. I just felt drawn to the imagery of people from that era, their incredible stories, learning about the names behind the faces, and ultimately building my own collection of images while loving the process of researching those individuals.\r\n\r\nRon: One of my favorite stories is your family connection—how there’s a photograph connected to it all. I’d love to hear you tell that again.\r\n\r\nKurt: Absolutely. One of the reasons my family had an interest in the Civil War is that we have some Civil War ancestors. Three of them fought with the Union Army in Pennsylvania regiments, and I grew up learning about their adventures. We researched all we could about them through military records, but we didn’t have any images—we couldn’t see them or know what they looked like. I got really lucky during the 150th-anniversary commemorations of the Civil War. In Pittsburgh, where I grew up and where my Civil War ancestors lived, there was an exhibit at the Heinz History Center. I noticed a really cool photo album of Civil War soldiers. The label said that the company in the album was one that my ancestor had fought in. Out of thousands of possibilities, that single regiment appeared there. I asked the museum staff if they could have the owner, Ken Turner, look inside to see if my ancestor’s photo was in there. And sure enough, inside the album was a photograph of my Civil War ancestor, Corporal Oliver Croxton in uniform. I was looking into the eyes of my great-great-great grand uncle.\r\n\r\nRon: How did you feel when you saw that for the first time?\r\n\r\nKurt: It felt amazing. Here was someone I had thought about my whole life, trying to learn as much as I could about him. And once you start studying Civil War portraits, you learn that the odds of finding one specific photo of a soldier—especially a lower-ranked one—are really low. So having that opportunity, seeing his face, and getting a much clearer picture of who he was… it was really powerful.",
        "extractionStatus": "success"
      },
      {
        "name": "Kurt paper.txt",
        "size": 103748,
        "type": "text/plain",
        "extractedText": "Photo Steward: A Deliberative Collective Intelligence Workflow for Validating Historical Archives\r\nVikram Mohanty, Computer Science, Virginia Tech, USA, vikrammohanty@vt.edu\r\nKurt Luther, Computer Science, Virginia Tech, USA, kluther@vt.edu\r\n\r\nDOI: https://doi.org/10.1145/3582269.3615600\r\nCI '23: Collective Intelligence Conference, Delft, Netherlands, November 2023\r\nHistorical photographs of people generate significant cultural and economic value, but correctly identifying the subjects of photos can be a difficult task, requiring careful attention to detail while synthesizing large amounts of data from diverse sources. When photos are misidentified, the negative consequences can include financial losses and inaccuracies in the historical record, and even the spread of mis- and disinformation. To address this challenge, we introduce Photo Steward, an information stewardship architecture that leverages a deliberative workflow for validating historical photo IDs. We explored Photo Steward in the context of Civil War Photo Sleuth (CWPS), a popular online community dedicated to identifying photos from the American Civil War era (1861–65) using facial recognition and crowdsourcing. While the platform has been successful in identifying hundreds of unknown photographs, there have been concerns about unverified identifications and misidentifications. Our exploratory evaluation of Photo Steward on CWPS showed that its validation workflow encouraged users to deliberate while making photo ID decisions. Further, its stewardship visualizations helped users to assess photo ID information accurately, while fostering diverse forms of stigmergic collaboration.\r\nCCS Concepts: • Human-centered computing → Collaborative and social computing design and evaluation methods; • Human-centered computing → Interactive systems and tools;\r\n\r\nKeywords: crowdsourcing, human-AI interaction, online deliberation, community stewardship, information assessability, online communities, history, person identification, facial recognition, stigmergic collaboration\r\n\r\nACM Reference Format:\r\nVikram Mohanty and Kurt Luther. 2023. Photo Steward: A Deliberative Collective Intelligence Workflow for Validating Historical Archives. In Collective Intelligence Conference (CI '23), November 06--09, 2023, Delft, Netherlands. ACM, New York, NY, USA 19 Pages. https://doi.org/10.1145/3582269.3615600\r\n\r\nFigure 1: Workflow of Photo Steward. (1) Decision-Making: The user compares photos for facial similarity and make decisions on the photo ID using Photo Steward's deliberative validation interface. (2) Access: The user accesses the validation interface from Civil War Photo Sleuth's search results and photo page. (3) Evidence: The community's responses from the validation interface feed into stewardship visualizations that are visible on the Photo Page, which subsequently foster a form of stigmergic collaboration among the users.\r\n1 INTRODUCTION\r\nThe task of correctly identifying individuals in historical photos holds great cultural and economic importance [3, 19, 39, 61]. This identification process is analogous to solving a complex mystery. It often involves corroboration of multiple research processes such as investigating visual clues in a photo, finding relevant reference resources, and comparing multiple low-resolution reference photos [31, 32, 37]. Historical photos pose many challenges, including low-resolution images, scattered reference materials, limited domain expertise, and lack of suitable verification tools. These hurdles often result in misidentifications which can have negative consequences, ranging from distorting historical narratives [60] and fueling conspiracy theories [12] to spreading disinformation [16] and unwarranted financial gains from inaccurate representations [21]. As online platforms such as Ancestry.com, Find-a-Grave, and FamilySearch democratize historical and genealogical research, the risk of misidentification is further amplified due to factors such as inadequate experience, confirmation bias, and automation bias introduced by imperfect automated tools [44, 75].\r\nTo address these challenges, we introduce Photo Steward, a deliberative workflow that leverages collective intelligence to validate historical photo identifications (IDs). Photo Steward's architecture builds upon the concept of information stewardship [18, 72], which involves community-driven validation of content, as seen in online communities like iNaturalist and Wikipedia. We designed and applied the Photo Steward architecture to Civil War Photo Sleuth (CWPS),1 an AI-infused online platform for identifying historical photos. CWPS has over 20,000 registered users and over 25,000 identified Civil War portraits, and faces the problem of historical photo misidentification [43]. Photo Steward provides a validation workflow that promotes careful deliberation during facial similarity comparison and photo ID verification, while enabling users to share opinions. It also visualizes community opinions on the reliability of photo IDs and facial similarity, fostering information stewardship at multiple levels.\r\nWe publicly released Photo Steward on CWPS and conducted an exploratory evaluation of twelve months of usage, including interviews with users of different expertise levels and log analysis of stewardship behaviors on the platform. We found that Photo Steward's stewardship visualizations helped users to find additional evidence (e.g., external sources, uniform clues matching service records, etc.) for assessing the reliability of photo IDs. Users validated hundreds of different IDs on the platform, and found the workflow to be useful for deliberating on facial similarity comparison and fine-grained photo ID decisions.\r\nWe also discuss the implications of community participation, deliberative human-AI interaction, and assessable designs for historical photo identification.\r\n2 RELATED WORK\r\n2.1 Misinformation in a Historical Context\r\nNumerous online communities, forums, and websites have emerged in recent years for archiving and documenting history [57], generating family histories [74, 75], identifying and sharing historical photos [43], trading antiques [1, 8], and facilitating discussions around history [20]. Much like popular social media platforms such as Facebook and Twitter, these history-based platforms are also prone to the problem of misinformation, albeit in a more specialized historical research context.\r\nPrior work has shown that erroneous family history trees were being disseminated across Find-a-Grave and Ancestry, two popular genealogy research communities, as a result of the platform's low bar to entry and inexperienced contributors' over-reliance on inaccurate automated features [75]. Mohanty et al. [43] showed that despite successful identifications on Civil War Photo Sleuth (CWPS), several photos were misidentified in the first month, particularly for photos without period inscriptions or duplicate views (12 misidentifications out of 37). Multiple factors — the correct candidate not present in the search pool, or the user incorrectly assessing facial similarity [53] and picking the wrong match — can lead to incorrect IDs. A follow-up benchmarking study of the underlying face recognition algorithm [44] highlighted its low precision (i.e., it retrieves over hundreds of search results), raising the possibility of errors due to automation bias, i.e., the tendency of users to over-rely on automation for making a decision [47, 50, 64].\r\nIdentifying historical photos is a complex investigative process, often involving the corroboration of multiple evidence pieces [31, 33] and can be seen analogous to \"finding a needle in a haystack\". As a result, historical photo IDs run a high risk of getting misidentified even with the best of intentions. Multiple Civil War photos have also been misidentified in the collections of professionally managed museums and archives, such as the US Library of Congress [36] and the Abraham Lincoln Presidential Library [34]. At the same time, historical photo IDs also have the potential to generate significant monetary value [1, 3, 8, 10], and such financial incentives might also lead to falsified identifications [21]. Validating these historical photo IDs, which are a result of complex, subjective original research, becomes tricky without the lack of domain expertise and access to investigative tools.\r\nWe addressed these challenges in Photo Steward by designing a stewardship architecture that allows users to share their expertise with others. To address the impact of automation, we introduced a two-step validation workflow for the users to deliberate on decisions while interacting with the AI's recommendations.\r\n2.2 Data Validation in Online Communities\r\nMultiple online platforms have leveraged the strengths of crowdsourced contributions for validating the quality of data generated on those sites. Elliott discusses how stigmergic collaboration, where indirect coordination within a community stimulates subsequent actions, plays a role in maintaining articles on Wikipedia [13]. This concept was observed by Wiggins et al. in their study of iNaturalist, an online platform for identifying species, wherein community stewardship behaviors were seen as users agreed on organism identifications to influence the platform's quality grade status [72]. Prior work has shown that stewardship visualizations on Wikipedia (i.e., article quality) [18] and iNaturalist (i.e., ID research grade status) [72] have a positive impact on users' assessment of the information.\r\nAlong these lines, we also built DoubleCheck [42], a quality assessment framework that builds upon the concepts of provenance and stewardship for verifying historical photo IDs. DoubleCheck focused on displaying quality indicator badges for historical photo IDs by capturing accurate provenance information and combining the source trustworthiness information with community opinions on the ID. In this work, we focus solely on the underlying stewardship architecture that helped facilitate the community opinions. Both DoubleCheck and Photo Steward were evaluated in the same lab study, but there is no overlapping data.\r\nVisualizations displaying (surrogate) quality metrics, such as popularity among expert users, social reputation, and content coverage, have been effective in helping users assess the credibility of websites and search results [62]. Prior work has also shown that visualizing the history of edits for a Wikipedia article can have a significant impact on users' perceived trustworthiness of the article [54, 67]. Similarly, Chevalier et al. [7] showed that visualizing the number of contributors, length of the article and discussion, and the history of edits helped users assess the quality of Wikipedia articles faster. On the other hand, Towne et al. [68] found that being exposed to editor conflicts in the discussion of a Wikipedia article lowered the perception of the article's quality, even though the users reported that the transparency raised their perceptions of the page and Wikipedia in general. Morris et al. [45] found that Twitter users relied on the author information for making assessments about the credibility of information in a tweet.\r\nDrawing from this prior work, we designed Photo Steward's stewardship visualizations to highlight the role of collective intelligence, while fostering stigmergic collaboration on CWPS to validate the quality of photo identifications.\r\n2.3 Background: Civil War Photo Sleuthing\r\nThe American Civil War (1861–65) was one of the first major conflicts to be extensively photographed. Over 3 million soldiers fought in the war, with many of them having been photographed at least once. Over 150 years, many of these photos have survived in museums, libraries, and personal collections, but only 10–20% are are identified [69, 77]. Civil War photography has garnered a lot of interest among historians, collectors, dealers, genealogists, archivists, and other experts, who often try to identify unknown photos for personal, cultural, and economic reasons. However, the identification process is complex and challenging, which often involves identifying visual clues in a photo and manually scanning through hundreds of low-resolution photos, military records, and reference books for corroborating evidence [31, 33, 38].\r\n3 ENHANCING CIVIL WAR PHOTO SLEUTH: DESIGN OPPORTUNITIES\r\n\r\nFigure 2: CWPS Haystack Model: Person Identification Pipeline [43]\r\nCivil War Photo Sleuth (CWPS) is a free, public website where users can identify unknown portraits from the American Civil War era using a person identification pipeline that combines crowdsourced human expertise and face recognition [43]. Drawing analogies to finding a needle in a haystack, Mohanty et al. propose a 'haystack model' to describe CWPS's person identification pipeline. In this pipeline, a user begins the identification process by first tagging a photo for uniform clues, which then generates search filters based on service records, and then facial recognition returns facially similar-looking results from a pool of potential candidates, ordered by similarity to the query photo, that satisfy the search filters (see Figure 2).\r\nThe CWPS haystack model is designed to prevent misidentifications by placing human decision-making at the forefront and treating AI as a supportive tool. It avoids automatically selecting the best match or displaying the algorithm's inconsistent confidence levels [41, 44]. Instead, the user carefully inspects search results for potential matches based on facial similarity and corresponding biographical details. Once a photo is identified, CWPS links the face and identity together and displays the ID on the photo page.\r\nDespite these measures, the open participation model of CWPS, which lacks verification, has raised concerns about the trustworthiness of proposed identities and the potential increase of \"false positives\" as the site grows [22]. To address these concerns, we enumerate three design goals which draw upon prior work on Civil War photo identifications and CWPS system designs, evaluations, and critiques [22, 30, 33, 35, 41, 43, 44], as well as our own observations and experiences using the publicly available version of the website. We provide further details in Appendix A.\r\nDesign Goal 1: Decouple facial similarity comparison from the overall task of person identification.. The current CWPS workflow conflates facial similarity and person identification into a single decision-making process (see Figure 2-C). Facial similarity, while important, can conflict with the identity suggested by personal details like biographical information and service records. The facial recognition algorithm's low precision [44] adds to the complexity, with the possibility of users interacting with false positives. In order to discourage over-reliance on facial similarity, we propose this design goal of separating both these tasks, allowing users to deliberate on the facial similarity and other person identification attributes separately, thereby minimizing inaccuracies.\r\nDesign Goal 2: Support fine-grained, deliberative decision-making.. With the current CWPS interface only permitting binary feedback during photo identification, there is a heightened risk of misrepresentation and misidentifications (see Figure 2-C). Historical photo identification is intricate, often demanding careful corroboration of numerous evidence pieces, with user confidence varying according to the evidence at hand [33]. To more effectively facilitate this complex process, we advocate for a design that enables users to express their level of certainty in their identification decisions. This design goal seeks to foster more accurate and nuanced user feedback, as well as reflective and deliberate decision-making [28, 29].\r\nDesign Goal 3: Encourage community contribution and transparency for validating photo IDs.. Identifying individuals in photos can be considered a subjective process and often benefits from multiple perspectives [41]. Currently, CWPS lacks mechanisms for community feedback and transparency about the process of photo identification, leaving potential misidentifications unchecked (see Figure 8 in Appendix). To support accurate original historical research, the platform should encourage community participation in validating identifications and promoting transparency around the roles of community contribution and facial recognition [18, 72]. This approach also encourages collective responsibility, facilitating stigmergic collaboration [13, 14], where user contributions guide future validation efforts.\r\n4 SYSTEM DESCRIPTION: PHOTO STEWARD\r\nWe developed Photo Steward, an information stewardship architecture that integrates a deliberative workflow for the community to validate historical photo identifications, which we then integrated into CWPS. Photo Steward's architecture has three main components (see Figure 1): 1) a deliberative decision-making interface for facial similarity comparison and photo identification, 2) new access points for validating photo identities, and 3) stewardship evidence for fostering stigmergic collaborations.\r\n4.1 DECISION-MAKING: Deliberating on facial similarity and photo identification\r\nAs part of Photo Steward, we introduce a multi-step \"Validation Interface\" (see Figures 3 and 4) to replace CWPS's single-step comparison interface. Photo Steward's validation interface allows users to deliberate while interacting with the facial recognition results. Meeting Design Goal 1, the validation workflow separates the task of facial similarity comparison from the overall goal of identifying the photo.\r\nTo inform our design, we draw on evidence-based decision-making [11], a model primarily used in healthcare, policymaking, and judicial sectors, which advocates for justifying decisions (photo IDs in this case) by gathering available evidence (facial similarity as visual evidence here). In the first step, the user compares the query photo to all other photos with the same identity for facial similarity. After deliberating on the facial similarity evidence, the user then votes on whether the query photo fits the target identity in the second step (which is the user's primary goal).\r\nThe validation interface is divided into four columns (from left to right): 1) the task description, 2) the query photo, 3) the evidence that is being weighed, and 4) the biographical information. The query photo and evidence are positioned in the two middle panels for easy side-by-side comparison. The task description panel displays the rating question for both the facial similarity comparison and the identification steps. Here, we used structured feedback to capture both the user's facial similarity comparison and their confidence on the photo ID, in an effort for encouraging users to exercise personal deliberation on all available evidence before making a decision on the ID. The interface updates the task description and evidence column depending on which task the user is performing.\r\nTo investigate the identity of a query photo, the user opens the validation interface which loads all the photos and biographical information available for the target identity.\r\n4.1.1 Validation Step 1: Facial Similarity Comparison. For the first step of the validation process, the interface displays the target photo in the evidence column next to the query photo for easy facial similarity comparison (see Figure 3).\r\n\r\nFigure 3: Step 1 of Photo Steward's Validation Interface. Users can compare two photos and answer whether they show the same person or not. They have the option of selecting whether the two photos are a facial match (i.e., same person, different views) or a replica (i.e., same person, same view). Here, the user is comparing whether Photo 41929 and Photo 1 show the same person or not. If multiple faces are available for the same ID, they appear one after the other in the order in which they were uploaded to CWPS.\r\n\r\nThe user's task is to determine whether both photos show the same person (regardless of whether the identity is known). Users can select from the following options: No (Different Person), Not Sure, Yes (Facial Match) and Yes (Replica). Mohanty et al. found that photos correctly identified on CWPS were either facial matches (i.e., same person, different view) or replicas (i.e., same person, same view) [43], which informed the design of this input scale. Since facial similarity does not have any standard scale and users may perceive the similarity or dissimilarity of two faces differently [40, 76], we chose not to capture any further granularity in their responses for facial match, replica or different person as this might lead to inconsistent data collection.\r\nCapturing these responses in a structured way allows users to deliberate on the task of facial similarity; this becomes more critical as users are also interacting with the results of a low-precision facial recognition algorithm [44]. In this step, the user compares facial similarity of the query photo with all available photos of the target identity, one photo at a time.\r\n4.1.2 Validation Step 2: Fine-Grained Photo Identification. In this step, the user analyzes the biographical information and incorporates the facial similarity evidence from the previous step to make a decision on the photo's identity.\r\nThe validation interface displays information in the same four-column layout (see Figure 4), with the evidence column now displaying a summary of the user's responses about facial similarity between the query photo and the target photo(s). The biography column shows the name and the service records for the user to analyze.\r\n\r\nFigure 4: Step 2 of Photo Steward's Validation Interface. Users vote on whether the query photo can be identified as the target identity by expressing their confidence. They can also add an optional note to justify their decision. The evidence panel displays a summary of the user's responses from the first step, where the faces are displayed next to the user's facial similarity comparison with the query photo. The faces are ordered in the way they appear for comparison, i.e., the order in which they were uploaded to CWPS.\r\n\r\nThe user now decides whether the query photo can be identified as the target identity (see Figure 4). The instruction above nudges the user to factor in the prior photo comparison evidence and the biography information. Meeting Design Goal 2, users indicate their confidence about the task question by selecting one of the five options displayed in radio buttons: No (Highly Confident), No (Slightly Confident), Not Sure, Yes (Slightly Confident) and Yes (Highly Confident). This scale, which offers more nuance than a binary decision, serves as a proxy measure for the ID's reliability given the complexities of quantifying accuracy in a historical photo ID investigation. These options reflect the varying degrees of confidence users have based on the quality and quantity of corroborating evidence, such as reputable sources, facial similarity to additional photos, and expert opinions. Users also have the option to elaborate their decision rationale in a free-text note.\r\n4.2 ACCESS: Expanding Validation Opportunities for Photo Identifications\r\nPhoto Steward provides stewardship capabilities for the CWPS user community by allowing them to access and use the validation interface at different stages of the photo identification process from multiple gateways (see Figure 1). On the \"Search Results\" page, it can be used for identifying a photo from a pool of potential similar-looking candidates, or ruling out some potential candidates. After a photo has been identified, users can also access it on the \"Photo Page\" to either validate an existing ID or dispute an incorrect one, and collaborate with other users in a stigmergic manner (Design Goal 3).\r\n4.2.1 Search Results Page: Matching and Ruling Out Candidates. While identifying a query photo, users can now inspect potential matches on the search results page with the help of the validation interface. The \"Compare\" button on a search result brings up the validation interface, loading all the target information for the corresponding search result. The target identities in the validation interface will update as the users check new search candidates for matches. The interface allows users to make two types of decisions, depending on their confidence response: 1) either of the \"Yes\" responses will match the photo with the target identity with varying degrees of confidence, and 2) either the \"No\" or \"Not Sure\" responses will rule out the search candidate as a potential match for the current user.\r\n4.2.2 Photo Page: Validating and Disputing Existing IDs. After a photo has been identified, Photo Steward allows users to review opinions from other users (described in Section 4.3) and contribute their own for a given photo ID on CWPS's photo page, fulfilling Design Goal 3. By clicking the \"Give Your Opinion\" button, users launch the validation interface featuring the query photo, the linked target identity, and target photos of the same ID, a new feature previously absent from CWPS. The two-step process mirrors that on the search results page, enabling community deliberation on the validity of an ID. Users can validate the facial match among photos linked to the same ID, express their agreement or disagreement on an ID with varying confidence levels, and optionally add a note explaining their decision. Thus, each vote contributes to a stigmergic collaboration, enhancing the reliability of photo IDs on CWPS. Consistent with CWPS's open participation model, Photo Steward allows any registered user to share their opinion on an identification.\r\n4.3 EVIDENCE: Visualizing Information Stewardship\r\nAs part of Photo Steward, we designed stewardship visualizations to help users assess the reliability of 1) facial matches (i.e., photos that were matched to each other by the user), and 2) photo IDs. The CWPS community's opinions on facial similarity comparison and photo IDs, captured through the validation interface, feeds into these reliability visualizations. These visualizations not only promote user accountability through social translucence [15], but also serve as deliberative evidence for subsequent stigmergic user collaborations (Design Goal 3).\r\n4.3.1 Reliability of Facial Similarity. For each photo pair that has been compared, the system aggregates the community's decisions for the visual match type and generates a distribution, which is displayed in the form of an interactive horizontal bar chart on the photo page. This chart appears next to the corresponding photo matched to the query photo (see Figure 5). Users can click the \"View Details\" button or an individual bar to see how each user voted. When multiple photos have been matched to the query photo, the matched photos appear one below the other, with each having its own visualization next to it. The bar charts are stacked vertically above each other to allow users to easily see and compare the reliability of every match.\r\nTo complement the community stewardship visualization, we also added an AI stewardship badge that indicates whether the particular match is supported by facial recognition (see Figure 5). On the search results page, CWPS retrieves those search results that have a facial similarity score greater than 0.50, so we use the same threshold here. However, the badge intentionally does not display the exact similarity scores (which have been found to be inconsistent [41, 44]) to avoid a false perception of precision, and cautions users to carefully analyze all the context and evidence, as there is a possibility of false positives with face recognition.\r\n\r\nFigure 5: Facial Similarity Reliability Visualization on the Photo Page. The photo matched to the query photo is displayed here, whereas the query photo is displayed on top of the page. Other photos matched to the query photo are displayed vertically one below the other. Users can bring up the query photo and the matched photo side-by-side by clicking the \"Compare\" button. Each matched photo has its own visualization next to it, and the responses are visible to everyone.\r\n\r\n4.3.2 Reliability of Photo Identifications. Similar to the facial similarity visualization, the system aggregates the community's confidence levels for an identification and generates a distribution, displayed in the form of an interactive horizontal bar chart. This visualization is displayed for every proposed identity on the photo page, below the biography subsection (see Figure 6a). If users propose multiple IDs for a given photo, they are displayed one below the other; each ID will have its own visualizations. The community's confidence votes are aggregated to order these IDs. Users can click the \"View Details\" button or the individual bars to view a modal dialog box with each user's confidence scores and optional text comments (see Figure 6b).\r\n\r\nFigure 6: Photo ID Reliability Visualization on the Photo Page for a given ID. If multiple IDs are present, they are displayed vertically one below the other, ordered in terms of aggregate votes. Each ID will have a separate visualization listed under the respective IDs.\r\n\r\nThis visualization reflects the community's expertise, and is intended for users to quickly assess the reliability of an identification. Prior work on crowdsourced person identification [41] suggests that airtight identifications are likely to show consensus from the community, whereas potential misidentifications are likely to reflect disagreement from the community. Further, the comments may reflect the voters' decision rationale and any external research they conducted before giving their decision on the photo's identification, allowing users to build on the work of others in making their own assessment [17].\r\n4.4 Summary\r\nPhoto Steward augments the CWPS platform with an information stewardship architecture to support community validation of historical photo IDs in a stigmergic manner. We summarize all the changes in the Appendix (see Table 6).\r\n5 EVALUATION\r\nWe obtained permission to publicly launch Photo Steward on CWPS in December 2020. We conducted a mixed-methods, exploratory evaluation study to understand how well users with different expertise levels could validate and assess Civil War photo IDs using CWPS with Photo Steward. Specifically, we wanted to understand 1) how users validated photo IDs using Photo Steward, 2) how the stewardship visualizations (i.e., ID and facial similarity reliability visualizations) impacted users' assessment of an ID. The study was approved by our university's IRB.\r\n5.1 Log Analysis\r\nTo understand the community's stewardship behaviors, we analyzed website logs of all user activities for a year after new features were launched, which included 5843 voting instances on 5672 photos for 5355 unique IDs. Our analysis included categorization of user deliberations as 'pre-identified' (i.e., the user knew the ID of the photo ahead of time and therefore, might be looking for a specific target on the search results page) or 'post-identified' (i.e., the user did not know the ID of the photo and therefore, may not have a specific target while analyzing the search results), coding of user comments (see Appendix C), and comparison of community's facial similarity comparisons against facial recognition scores. Details of these analysis methods are provided in Appendix B.\r\n5.2 Lab Study\r\nIn order to understand how well Photo Steward supports diverse users in validating the quality of photo identifications, we also conducted an exploratory lab study.\r\n5.2.1 Participants. We recruited 15 participants representing the three major expertise levels: 5 history students, 5 amateur experts (experienced users of CWPS), and 5 expert historians. Participant details can be found in Appendix D. We anonymize these groups with the following identifiers, respectively: S1–S5, C1—C5, and H1–H5.\r\n5.2.2 Dataset. For the study, we created a dataset of 10 different photos identified on CWPS. Three of these photos had an ID conflict, i.e., multiple identities were proposed. For two of these photos, one ID was correct and the other one was incorrect. The community had already researched both photos, voted on the correct ID, and left credible evidence in the comments. Both IDs were linked to additional photos as well. The third photo was one of the seeded photos on CWPS, but was originally misidentified. We added another false ID, making both IDs for the third photo incorrect. All photos had multiple photos matched to them; eight of them were linked via facial matches, while two of them had replicas.\r\n5.2.3 Procedure. The entire study was conducted online via recorded Zoom sessions, with at least one researcher attending each session. Each participant first completed a consent form and a pre-survey describing their demographics and Civil War photography experience.\r\nAs part of the study, participants reviewed three randomly assigned photos from the dataset one-by-one in the original CWPS system first, followed by the same photos on the Photo Steward version. Participants used a think-aloud protocol while using the two systems; after the completion of the task with each system, they were asked a few semi-structured questions about their experience. Finally, the participants completed a summative post-survey of standard usability questions (e.g., ease of use, usefulness of features, instruction clarity, preferred system, etc.) (see Appendix E)\r\nWe maintained this sequence (original CWPS first, CWPS with Photo Steward second) for all the participants, rather than using a randomized sequence, for two reasons. First, we did not want participants' assessments to be biased in favor of Photo Steward after seeing additional features in the new interface. This design allowed us to observe if the original interface misled the participants towards incorrect assessments, and if, subsequently, the Photo Steward interface helped correct them. Second, in a randomized sequence, Photo Steward would expose the participants to new information in the form of prior user votes and responses, and therefore, may confound how they assess the information on the original CWPS version.\r\n5.2.4 Data Analysis. The first author fully transcribed and analyzed the interviews and think-aloud recordings using an inductive qualitative thematic approach [4]. The transcript sections were first divided according to the interface in question (i.e., original CWPS or Photo Steward), followed by an open coding of the transcripts using MAXQDA 2020 [65]. The first author iterated and settled on a total of 28 codes (e.g., change in opinions, comparison interface, source trustworthiness, etc.) for 634 coded segments across all the transcripts. These codes were then organized into themes as described in Section 6 after discussing with the co-author.\r\n5.2.5 Limitations. We conducted a qualitative lab study to understand how users with different backgrounds and expertises validated photo IDs using Photo Steward and hit theoretical saturation. However, there are a couple of limitations with the study: 1) limited insights on the role of expertise, and 2) the task sequence could have order effects. Further, the large-scale analysis of Photo Steward logs provided us with insights of its usage amongst users. However, it lacked an expert-prepared gold standard dataset, which hindered our ability to conduct specific performance analyses as part of this study.\r\n6 FINDINGS\r\nUsing the methods above, we evaluated how well Photo Steward's stewardship architecture supported CWPS users in validating photo identifications, compared to the original version of CWPS.\r\n6.1 Validation Interface\r\nUsers found Photo Steward's validation interface to be useful for comparing different photos.. While assessing the IDs with the original interface, participants would go back and forth between different photos to compare whether they are the same person or not. Some participants opened the photos in two different browser windows and kept them side-by-side. While using the validation interface in the new system, participants appreciated being able to see the photos side-by-side at the same time.\r\nH1 said, \"As an historian using this, this is really great to see them both together. It just makes a comparison a lot easy for me to do. I mean, this is the same gentleman, he's got a little dark facial hair. It looks a little bit different there and the photo on the right, but the facial match is definitely there. \" This was also echoed by C3, who said, \"This, I really find extremely useful, especially when I'm trying to do facial recognition. I can zoom in and have them side by side here. [...] Where in the past, I would have to go back and forth between tabs or cut and paste them into a different document to look at them side by side.\"\r\nFrom our logs, we found that 223 users had compared 2319 unique photo pairs for facial similarity, with 156 pairs receiving comparisons from at least 2 different users. The facial similarity responses were distributed as follows: 763 replicas, 1232 facial matches, 283 unsure, and 280 different people.\r\nTable 1: Distribution of User Votes.\r\n\r\nVotes\r\n/ ID\r\n# of IDs\r\nNote\r\nPresent\r\nNegative\r\nVotes\r\n1\r\n5650\r\n511\r\n61\r\n2\r\n157\r\n(Agreement: 119)\r\n(Disagreement: 38)\r\n83\r\n12\r\n3+\r\n36\r\n(Agreement: 21)\r\n(Disagreement: 15)\r\n31\r\n4\r\nTable 2: Distribution of Confidence Levels.\r\n\r\nMean\r\nConfidence\r\n1\r\nVote / ID\r\n2\r\nVotes / ID\r\n3+\r\nVotes / ID\r\n-2 (No - Highly Confident)\r\nto\r\n-1 (No - Slightly Confident)\r\n45\r\n3\r\n2\r\n-1 (No - Slightly Confident)\r\nto\r\n0 (Not Sure)\r\n16\r\n3\r\n2\r\n0 (Not Sure)\r\n99\r\n8\r\n4\r\n0 (Not Sure)\r\nto\r\n1 (Yes - Slightly Confident)\r\n760\r\n15\r\n5\r\n1 (Yes - Slightly Confident)\r\nto\r\n2 (Yes - Highly Confident)\r\n4730\r\n128\r\n23\r\nUsers preferred the ability to provide granular feedback for photo IDs using Photo Steward's validation interface.. All participants expressed preference for the fine-grained confidence levels, including the ability to dispute an ID, in Photo Steward's validation interface, appreciating how it more accurately mirrored the inherent uncertainty present when assessing photo IDs. S3 said, \"I definitely like the five levels. I think it leaves more room for interpretation. Like sometimes it's kind of hard to just say yes or a hard no because so much goes into it. Especially because a lot of this stuff was so long ago, there's so many unanswered questions.\" H4 initially defended the original interface's binary vote, but changed her mind after experiencing Photo Steward's confidence levels: \"[W]hat I had said has this very black and white feel to it, you're wrong or you're right. I like these degrees of disagreement or agreement. I think that's way more helpful broadly.\"\r\nThe usage of the voting feature was reflected in our logs, which showed 5843 voting instances from 328 unique users (see Table 1). Table 2 shows that while users utilized the full range of confidence levels, including when they were unsure (mean confidence = 0) or slightly confident (mean confidence = 0 to 1) about the ID, the vast majority of the votes were highly confident ones (mean confidence = 1 to 2). A small proportion of votes (77) were cast for disputing an ID.\r\nUsers justified their voting decisions through notes covering an extensive range of topics..\r\nTable 3: Distribution of Note Topics. The table also displays how the notes are distributed for different user confidence levels.\r\n\r\nCategory\r\nSub-Category\r\nTotal\r\nNumber of\r\nNotes\r\nNo\r\n(Highly\r\nConfident)\r\nNo\r\n(Slightly\r\nConfident)\r\nNot\r\nSure\r\nYes\r\n(Slightly\r\nConfident)\r\nYes\r\n(Highly\r\nConfident)\r\nPhoto\r\nComparison\r\nHigh-Level\r\nComparison\r\n206\r\n9\r\n5\r\n9\r\n20\r\n163\r\n\r\nDescribing\r\nFacial Features\r\n37\r\n4\r\n3\r\n9\r\n8\r\n13\r\nWord-of-Mouth\r\nDescendant\r\n55\r\n1\r\n0\r\n0\r\n8\r\n46\r\n\r\nOwnership\r\n47\r\n1\r\n0\r\n0\r\n0\r\n46\r\n\r\nFamiliarity\r\n18\r\n0\r\n0\r\n0\r\n2\r\n16\r\nVisual\r\nEvidence\r\nUniform\r\n73\r\n7\r\n1\r\n8\r\n17\r\n40\r\n\r\nInscription\r\n145\r\n4\r\n3\r\n3\r\n6\r\n129\r\n\r\nOther\r\nVisual Clues\r\n17\r\n1\r\n0\r\n1\r\n2\r\n13\r\nExternal\r\nInformation\r\nExternal URL\r\n47\r\n1\r\n0\r\n1\r\n4\r\n41\r\n\r\nOther Sources\r\n172\r\n9\r\n1\r\n1\r\n21\r\n140\r\n\r\nLack of\r\nInformation\r\n21\r\n0\r\n2\r\n13\r\n6\r\n0\r\nProviding\r\nAdditional\r\nInformation\r\nBiographical\r\nInformation\r\n46\r\n11\r\n2\r\n3\r\n3\r\n27\r\n\r\nAdditional\r\nContext\r\n117\r\n5\r\n0\r\n2\r\n11\r\n99\r\nFrom our logs, we found that 155 users had left 682 notes for 600 different photos. However, as Table 1 shows, around 10% of the votes had a note. Table 3 shows the different topics covered by the notes. We observe that users' voting patterns are significantly influenced by the availability and quality of evidence, with clear facial similarity, period inscriptions, personal anecdotes, visible clues in the photograph, and added biographical context often leading to high confidence \"Yes\" votes, while lack of information typically results in \"Not Sure\" votes.\r\nUsers most frequently left a comment attributing facial similarity (after comparing it in the first step of the validation process) to be the reason for their decision (e.g., \"Identical to the other CDV\"). In some instances, they would expound on it by discussing facial features: \"The eyes, nose, cheek bones, shape of face, all look similar to George Pickett, although possibly reversed based on hair part\". In many instances, we found users inferring biographical information (service records, location, etc.) from visual evidence in the photo, be it uniform or backmarks (e.g., \"Initials MN on chinstrap (brass letters). Signature on verso is made out to Marlin's oldest sister. Style of insignia is consistent with other 1862 recruits for Co. B 1st USSS\").\r\nInterestingly, we also observed a large number of word-of-mouth evidence notes for justifying the user's decisions, such as claiming to be a descendant, or owning the original copy of the photo, or having seen the photo somewhere. For example, one user noted, \"He is my great-great grandfather and this photo has been passed down through the generations to me and was identified by his son John Albert Johnson, my father's grandfather.\" Users also left external URLs and source details in the notes as evidence. Sometimes, they provided additional context (e.g., \"This image came with a group of 7th Iowa images. The majority were of Company G., but there is only one person in the entire 7th Iowa Infantry that could be identified by the first or last name of 'Nelson.'\").\r\nThe validation workflow encouraged users to exercise careful deliberation while making photo ID decisions.. Users felt that the questions in each step of the validation process helped them to carefully weigh in all the evidence and deliberate while voting on the ID (Q4, mean = 4.60, SD = 0.49). C3 explained why the two steps were necessary: \"It's two separate things. One is asking, do you think that this face is the same face? Then the second is, do you think that this face matches this name? I think that that is a necessary question for both of those scenarios. I don't think it's redundant, I think it's necessary.\" H2 appreciated the thoughtfulness that the two-step process encouraged, saying, \"It could be the same guy, but it might be a different guy, but now that you know the other interface kind of forces me to slow down a little bit and think more carefully, because it's asking specific questions about things.\" A couple of participants, however, expressed initial confusion over the separation and order of these two tasks.\r\n\r\nFigure 7: Deliberation in case of post-identification voting. The table shows the distribution of confidence votes (second step of the validation process) against the user's facial similarity comparison (first step of the validation process). The comparison scores were computed by aggregating all the facial similarity comparisons in the first step (replica = 2, facial match = 1, not sure = 0, and different people = -1).\r\n\r\nOur logs affirm the deliberative aspect of the two-step validation process. We observed that when users compared one or more photos in the first step, they factored in the facial similarity evidence for their ID vote decision. Figure 7 shows that when the cumulative comparison scores are positive (i.e., majority of the comparisons were a facial match or a replica), the confidence scores are generally positive. Similarly, when the cumulative comparison scores are negative, the confidence votes are also largely negative. When the users are unsure about the facial similarity comparison, it reflects in their final ID vote. Notably, when there were mismatches between comparison scores and ID votes, users provided reasons in the notes, citing visual evidence and an unsure photo comparison.\r\nPhoto Steward's stewardship architecture fostered diverse forms of meaningful stigmergic collaboration amongst users.. Although most IDs received only a single vote, about 3% (193 IDs) received multiple votes (see Table 1). Among these, 140 IDs saw total consensus among voters on confidence scores, with half involving an explanatory note. When the initial vote lacked a note, subsequent voters often added information like external sources or context. Almost all IDs where voters agreed positively were supported by metadata such as period inscriptions, scholarly sources, or additional comparison photos, underscoring the credibility of these agreements.\r\nWhen users disagreed on an ID, they would almost always justify in notes why they differed from the previous voters' opinions (43 out of 53 disagreement instances). From analyzing the notes, we found that the subsequent voters provided additional information about external sources or service records to disagree (e.g., \"Please see Military Images magazine, MI, Volume XVI, Number 3, November - December 1994 for the correct identification of this image. The correct identification via the Michael McAfee collection is Richard Cramer, 4th Michigan Infantry\"). Among 18 instances of conflicting IDs for the same photo, seven saw resolution through a process of voting one ID over another, often accompanied by external evidence in notes or citing facial similarity.\r\nUser agreement was particularly strong in facial similarity comparisons. Of the 2200 unique photo pairs compared, 141 received multiple user reviews, with 114 seeing total agreement. The remaining cases typically involved user errors in distinguishing between 'facial match' and 'replica', or uncertainty about facial similarity.\r\n6.2 Stewardship Visualizations\r\nStewardship visualizations reflecting community insights enhanced the accuracy of photo ID assessments.. Participants found Photo Steward's bar chart visualization showing the five confidence levels to be simple and easy to understand (Q1, mean = 4.93, SD = 0.25). S1 said, \"I'm a visual person. Bar graphs or charts like statistical data helps me put things into a better perspective or gives me an idea of what I am working with versus just something more plain [like] the other interface.\"\r\nWhen participants were uncertain about an ID, they saw the additional evidence and justification that the community members had provided along with their vote as essential to taking the vote seriously. S5 said, \"If there's other users giving comments like 'I've used this source,' and you know you get information from a source where they found it, I'm probably gonna agree with them. If they're just voting yes without anything else, then I am probably more likely to go out and find sources for the photo myself and make my own decision.\"\r\nIn three instances, students (S1, S3, and S5) initially made incorrect assessments on the original interface but rectified these after using Photo Steward. The community's consensus and evidence in Photo Steward were pivotal in these corrections. S5, confronted with ID conflicts, stated after using Photo Steward: \"You know that helps a lot with what people are thinking and presumably these people have also gone to the Maine State Archives or something and verified and then given their opinion so that helps.\" Similarly, historian H2 and collector C3 initially made incorrect choices but amended their decisions after analyzing community-based evidence on Photo Steward. H2 remarked: \"So I believe it's the first person, of course, because it's based on the Maine State Archives. And then you've got the piece on there that said that you looked and found a different man. With that name in the actual regimental history. I say that's fairly accurate information.\" This highlights the critical role of community consensus in resolving photo ID conflicts.\r\nOn the other hand, when participants were confident about an ID, seeing the community's opinions affirmed their own assessments, for better or worse. In the words of H5: \"I really liked the very clear community consensus, and the ability to be able to see the identities of the people who were looking at these images. It was something that (gave) a boost of confidence in terms of my final decision.\" Overall, the participants found the community opinions to be useful for assessing the IDs (Q2, mean = 4.93, SD = 0.25).\r\nUsers gave additional weight to the opinions of members they were familiar with and desired more contextual information about all contributors.. Participants examined the bar chart visualization details and gave higher weight to the opinions of prominent names from the Civil War photography community while assessing the ID. C3 said, \"Here's <name redacted>, period inscription with valediction, the uniform matches his service record. <name redacted> said the same thing. These two uploaders I hold in very high regard to their opinions on this site. If they're saying that they think highly confident of this identification, that gives me a lot of confidence as well.\" H5 became further unsure about an ID after seeing a fellow historian's vote: \"And, you know, sort of knowing <name redacted>, someone who I know deals with primary source material a lot, and sort of being on the fence with it as well, leaves me in that unsure position.\"\r\nParticipants also sought additional details about community members, proposing indicators of professional status or active participation in the CWPS community. H1 emphasized his appreciation of amateur experts and opposition to gatekeeping, yet he suggested having some kind of credential indicator next to the username would be helpful: \"It will be great if, you know, somebody was an academic historian or a published Civil War author, if there was some way to just say some little tidbit next to <name redacted> 'Oh, saying, hey, I'm from <university redacted>, Professor or, you know, author of whatever.'\" Others preferred indicators of community activity levels or personal connections (i.e., descendants) to the identified individual in the photo.\r\nOur log analysis showed revealed the presence of certain active voters, with 9 out of 328 unique voters voting on more than 50 photos each, and 2 voting on over 2000 photos each (mostly ones that they had uploaded and identified). If we consider only votes on photos identified by someone else, we have 8 users who have voted on more than 15 IDs each, with 1 user voting on over 150 IDs.\r\nThe community opinions made the platform feel more engaging, but users had mixed thoughts about the ideal number of votes per photo.. Participants, in general, felt the community opinions made Photo Steward more engaging compared to the original system. H5 said, \"I thought it appeared more user-centric and user-friendly and it felt more participatory.\" C3 saw the benefits of increased engagement for making IDs more reliable: \"I think for this crowdsourcing project that we're building on this database [it] is very important to have those comments, those feedback, that we see in the voting system. It only makes this ID stronger and makes the project and the database a more trustworthy and reliable source.\"\r\nUsers wanted to see more community opinions and comments for IDs that had few votes. S1, on seeing only one vote for an ID, said, \"That doesn't make me feel as confident because that's not too many for me to give an answer.\" S3 said it would have been easier to assess some photos if \"there was more input from other people.\" Users had mixed opinions about how many votes they wanted to see for verifying an ID. In general, they wanted to see consensus among the community for an identity and at least three votes. C3 said, \"I usually shoot for like three to four [votes] as the lowest where I take some good quality out of those votes. [...] If there's only one or two, and especially if there's two that are split, that is not as reliable to me.\"\r\nParticipants' concerns about spreading voters too thin were borne out in the log data (see Table 1). We analyzed the logs to check how often CWPS users vote on the photos they are browsing. We found 1784 instances (out of 5843 voting instances) where the number of \"lurkers\" for a given photo page exceeded the number of voters on that photo.\r\nTable 4: Face Recognition Similarity Score Stats vs. User Comparisons (Facial Similarity).\r\n\r\nUser Comparison\r\nMean\r\nMedian\r\nSD\r\nCount\r\nReplica\r\n0.88\r\n0.91\r\n0.16\r\n659\r\nFacial Match\r\n0.52\r\n0.65\r\n0.31\r\n1015\r\nDifferent Person\r\n0.55\r\n0.59\r\n0.16\r\n265\r\nNot Sure\r\n0.51\r\n0.58\r\n0.23\r\n261\r\nUsers found the face recognition badge and community's opinions complementary for assessing the reliability of matched photos..\r\nTable 5: User Comparisons (Facial Similarity) vs. Face Recognition.\r\n\r\n\r\n# of Photo Pairs\r\n# of Photo Pairs\r\ncompared by 1 user\r\n# of Photo Pairs\r\ncompared by 2 users\r\n# of Photo Pairs\r\ncompared by 2+ users\r\nSupported by users\r\nand face recognition\r\n1408\r\n1281\r\n109\r\n18\r\nDisputed by users\r\nand face recognition\r\n26\r\n24\r\n0\r\n1\r\nUsers unsure,\r\nface recognition disputes\r\n42\r\n42\r\n0\r\n0\r\nUsers unsure,\r\nface recognition supports\r\n219\r\n217\r\n2\r\n0\r\nUsers dispute,\r\nface recognition supports\r\n239\r\n238\r\n1\r\n0\r\nUsers support,\r\nface recognition disputes\r\n266\r\n257\r\n8\r\n1\r\nFrom our logs, we found that 1408 photo pair comparisons were supported by both users and facial recognition, by far the most common outcome (see Table 5). This information would be visible to the larger user community in the form of stewardship visualizations (see Figure 5). Interestingly, we also see 266 cases where a comparison is supported by users, but disputed by facial recognition (similarity confidence score < 0.50). That outcome was approximately as common as when users disputed a comparison but face recognition supported it (239 pairs) or when a user was unsure but face recognition supported it (219 pairs). However, it was far less common for facial recognition to dispute a comparison when a user also disputed it (26 pairs) or was unsure (42 pairs).\r\nWhen we analyze the face recognition's confidence scores in more detail, Table 4 shows that there is a clear separation between replicas and other types of user comparison scores. Face recognition confidence scores for photo pairs that users labeled as replicas were much higher in terms of both mean and median (0.88 and 0.91, respectively, versus scores in the 0.50s and 0.60s for all others). While these fine-grained scores are not displayed to users — they intentionally see only the face recognition badge — the very close mean and median confidence scores for \"facial match\" versus \"different person\" illustrate the difficulty of automatically identifying non-replica matches and offer support for a hybrid human-AI approach (cf. Section 4.3.1).\r\nParticipants found the community's opinions to be helpful for assessing whether two photos were facial matches or not. S4 said, \"I mean, I think it's cool to see what the community is saying, because I do feel when it comes to saying 'Is this the same person in both these pictures?', that's really the best way to do it if you don't have any [other] information.\" C1 said that he found the community opinions for facial matches especially helpful because he is \"face blind\": \"I can't really identify the face-to-face, but the hair and the mustache and all the stuff that, in addition, it helps me with that for sure. It's good because other than having to find somebody close to me and be like, 'Do you think this is the same people?' [I] have that community right there.\"\r\nParticipants had mixed opinions about facial recognition technology, but most found the badge indicating whether it supported the two photos being a match to be a useful data point. S3 said, \"The facial recognition saying they are similar — I would go ahead and trust that but I don't know if I would trust it enough to make a verification on my own.\" In general, participants found the strengths of facial recognition and the community to be complementary in determining whether two photos showed the same person, and liked seeing both results together. H4 felt the community and the technology had separate roles:\r\nThis is facial recognition, and this is the historical background. I trust facial recognition, but it makes me feel better to have that historical background. I think the human eye can be tricked by different hairstyles and different beards. Just to have this outside historical verification to say like, 'Okay, maybe you or I was tricked, but the machine was not,' I think that's really helpful.\r\nS1 relied on both the community and facial recognition to make a decision on a facial match: \"I think both give me kind of an idea. Okay, there's this facial recognition technology being used, but also there's other users that are leaning towards that this is the same person.\" H5 got a similar boost of confidence: \"This is certainly reassuring seeing not only the AI match, but also in terms of the community — seeing that seven users have said that this is a facial match as well. I'd be quite convinced by this.\"\r\n7 DISCUSSION\r\n7.1 Leveraging Collective Intelligence for Validating Person Identification\r\nPrior work has raised concerns about misinformation in online history communities [43, 75]. To address these problems on CWPS, we built Photo Steward for supporting community-based validation of photo IDs. Users found Photo Steward's stewardship visualizations not only helpful for affirming their own assessment, but also for discovering new knowledge and correcting their decisions, if need be. These visualizations, combined with the validation workflow, exhibit a form of stigmergic collaboration, where users build on prior knowledge left by the community and leave their own assessment for other users [14, 17, 27, 56].\r\nPhoto Steward allowed users to express how confident they are about an ID in a fine-grained manner, in contrast to the binary agreements or disagreements observed on iNaturalist by Wiggins et al. [72]. The CWPS community preferred this nuanced form of stewardship as users are likely to have different degrees of confidence based on the evidence available for identifying a photo, thus demonstrating the effectiveness of Design Goal 2. As S3 pointed out, users often experience difficulty in making a binary decision about individuals who lived 150 years back due to the lack of surviving documentation.\r\nBeyond its basic usefulness, Photo Steward's full potential can best be realized through sustained community participation, but most IDs on CWPS only received one vote. To address this challenge, we can leverage different crowdsourcing and online community strategies. For example, we can draw the community's attention towards IDs that are \"more of a puzzle,\" as H5 suggested, similar to Twitter's Birdwatch promoting tweets for fact-checking [52]. Designing nudges to encourage lurkers to vote on the IDs they are viewing can further help in these efforts. Organizing community events can help foster interest and participation in collaboratively verifying IDs, drawing inspiration from crowdsourcing events like CrowdSolve, where experts and novices collaborate on solving missing persons cold cases [70]. Incentive mechanisms such as leaderboards and challenges [46] can drive extrinsic motivation within the community for verifying the IDs. Finding users who are more likely to vote on an ID, based on their skills and interests, can also be an effective collaboration strategy [71]. In future work, we plan to integrate these strategies and introduce explicit \"calls to action\" [51, 58] on the home feed, guiding the community's attention towards IDs that require validation and fostering more sustainable, collaborative participation in historical photo identification.\r\n7.2 Exercising Deliberation in Human-AI Teams\r\nWe found that Photo Steward's validation workflow was effective not only for voting on the IDs and comparing the photos side-by-side, but also encouraged users to deliberate on their decision, drawing parallels to other social computing systems that support reflection and deliberation (e.g., [28, 29]). This deliberative intervention was non-trivial as users on CWPS follow an identification pipeline which is powered by facial recognition, an AI algorithm that is far from perfect [44]. Users are trying to find the correct match, if present at all, from a pool of potential candidates, which are largely comprised of similar-looking false positives — akin to finding a needle in a haystack. Further, the task of comparing photos of people is by no means an easy task for humans, even in a modern context [53]. While Photo Steward can not completely curb automation bias, an issue that has been previously observed in multiple online history communities [44, 75], its multi-step, validation workflow with structured feedback interventions encouraged users to deliberate over AI suggestions before making a decision.\r\nPhoto Steward's workflow also compartmentalizes the tasks that AI is good at – such as quickly retrieving similar-looking candidates from a large search pool – from the tasks where the AI makes more errors – such as verifying whether two faces show the same person or not [5, 55]. Decoupling facial similarity comparison from the person identification task (Design Goal 1) allows the users to now focus on the face verification task. In doing so, Photo Steward's workflow supports effective human-AI teaming in the context of person identification by allowing the user to make a granular assessment for the face verification task instead of the AI, while also ensuring that an AI-retrieved, similar-looking potential candidate is being compared against.\r\nAs imperfect AI algorithms get deployed in high-stakes scenarios such as medical imaging, law enforcement, etc. [6], it becomes more critical to reduce automation bias and encourage more deliberative decision-making. Amershi et al. recommend granular user feedback while interacting with AI systems as part of their \"Guidelines for Human-AI Interaction\" [2]. Similarly, other forms of design interventions, such as counterfactual AI explanations [63], chatbots [26], and community opinions [59] can also be explored for encouraging deliberative decision-making with AI assistance.\r\nPrior work in human-face recognition teams has shown that algorithmic suggestions can have a significant biasing effect on a user's decision [23]. Our findings showed that Photo Steward was able to encourage users to exercise deliberation while interacting with results retrieved by facial recognition. At the same time, Table 4 also showed that users can differ from the algorithm's suggestions, thus necessitating a deeper dive analysis of this dissonance as part of future work.\r\n7.3 Assessing Quality in Crowdsourced Original Historical Research\r\nWe found that Photo Steward's stewardship visualizations helped users assess the reliability of photo IDs on CWPS, which were a result of Design Goal 3. However, crowdsourced identifications always run the risk of groupthink [24, 25], which can eventually mislead users into believing and amplifying misidentifications, a concern also raised by H4. Public deliberation of modern photo IDs on social media can have profound negative consequences for false targets, as exemplified by the Boston Marathon bombing [49, 66] and the recent US Capitol riot [48]. This raises the question: are Photo Steward's stewardship visualizations sufficient for assessing the quality of photo identifications made on CWPS?\r\nPrior work on crowdsourced scholarship suggests an answer. Rosenzweig [57] analyzed Wikipedia as a source of historical scholarship, noting its policy against original research, and advocating for it as a tool for teaching the limitations of information sources and critical analysis of primary and secondary sources. Motivated along similar lines, Forte et al [18] proposed the assessability framework for designing assessable participatory information systems, based on information provenance and stewardship. The concept of provenance, extensively used in history and archival studies, describes information that makes it possible to trace the ownership or origins of the content, while stewardship refers to the processes that were used for maintaining the content, including its authorship. In the case of Wikipedia, Forte et al. found that visualizing provenance (i.e., citation types) and stewardship (i.e., article quality) had a significant impact on assessments of articles and Wikipedia as an information source.\r\nWhile Photo Steward enables information stewardship on CWPS, there is an opportunity for incorporating provenance into the CWPS platform to make it a truly assessable online platform. A significant proportion of the notes left by users on Photo Steward qualified as provenance information, namely comments about period inscriptions, family trees, external sources and URLs. The challenges of assessing IDs on CWPS are, however, different from assessing information on Wikipedia, primarily because CWPS supports original research unlike Wikipedia's no original research policy [73]. This was also the reason why we designed Photo Steward to be a review system rather than a single editable output such as Wikipedia; original research such as historical photo identifications is often times an evolving investigation rather than a final decision. To assess the reliability of original photo IDs made on CWPS, users may want to factor in the provenance of the reference photos that were used in the identification process. In such cases, Photo Steward's stewardship visualizations (i.e., facial similarity reliability) can further help the user in assessing whether the reference photos can be used as reliable provenance or not.\r\n8 CONCLUSION\r\nPhoto Steward attempts to help users assess and validate photo IDs better on CWPS. We present an information stewardship architecture, and adapt it for the task of historical person identification. We demonstrate the effectiveness of Photo Steward on CWPS, an existing online platform, where users found the stewardship visualizations, which included the community opinions and the AI verdict, useful for making accurate assessments of photo IDs on the platform. Further, users found Photo Sleuth's multi-step, structured validation workflow to help them deliberate before making decisions about the photo's identity. This work opens doors for exploring new ways to leverage collective intelligence and AI in creating assessable online information systems for historical archives.\r\nACKNOWLEDGMENTS\r\nWe wish to thank Ron Coddington, Paul Quigley, Liling Yuan, and our study participants. This research was supported by NSF IIS-1651969 and a Virginia Tech ICTAS Junior Faculty Award.\r\nREFERENCES\r\n2021. Heritage Auctions: World's Largest Collectibles Auctioneer. https://www.ha.com/ Navigate tocitation 1citation 2\r\nSaleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. 2019. Guidelines for human-AI interaction. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–13. Navigate tocitation 1\r\nBrakkton Booker. 2015. $2 photo found at Junk Store has Billy the kid in it, could be worth $5M. https://www.npr.org/sections/thetwo-way/2015/10/15/448993361/-2-photo-found-at-junk-store-has-billy-the-kid-in-it-could-be-worth-5-million Navigate tocitation 1citation 2\r\nVirginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3, 2 (2006), 77–101. Navigate tocitation 1\r\nJoy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Conference on Fairness, Accountability and Transparency, FAT 2018, 23-24 February 2018, New York, NY, USA. 77–91. http://proceedings.mlr.press/v81/buolamwini18a.html Navigate tocitation 1\r\nCarrie J Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg S Corrado, Martin C Stumpe, et al. 2019. Human-centered tools for coping with imperfect algorithms during medical decision-making. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–14. Navigate tocitation 1\r\nFanny Chevalier, Stéphane Huot, and Jean-Daniel Fekete. 2010. Wikipediaviz: Conveying article quality for casual wikipedia readers. In 2010 IEEE Pacific Visualization Symposium (PacificVis). IEEE, 49–56. Navigate tocitation 1\r\nH Jason Combs. 2005. The Internet's Impact on the Antiques Trade. Material Culture (2005), 26–41. Navigate tocitation 1citation 2\r\nAnthony DeBartolo. 1975. Appraisers can tell when old photo really may be golden. https://www.chicagotribune.com/news/ct-xpm-1985-12-27-8503300067-story.html Navigate tocitation 1\r\nAnthony DeBartolo. 2021. Appraisers can tell when old photo really may be golden. https://www.chicagotribune.com/news/ct-xpm-1985-12-27-8503300067-story.html Navigate tocitation 1\r\nHarley D Dickinson. 1998. Evidence-based decision-making: an argumentative approach. International Journal of Medical Informatics 51, 2-3 (1998), 71–81. Navigate tocitation 1\r\nFor The Inquirer Edward Colimore. 2019. Did John Wilkes Booth get away with murdering President Abraham Lincoln?https://www.inquirer.com/news/john-wilkes-booth-lincoln-conspiracy-photo-recognition-20190415.html Navigate tocitation 1\r\nMark Elliott. 2006. Stigmergic Collaboration: The Evolution of Group Work: Introduction. m/c journal 9, 2 (2006). Navigate tocitation 1citation 2citation 3\r\nMark Elliott. 2016. Stigmergic collaboration: A framework for understanding and designing mass collaboration. In Mass collaboration and education. Springer, 65–84. Navigate tocitation 1citation 2citation 3\r\nThomas Erickson and Wendy A Kellogg. 2000. Social translucence: an approach to designing systems that support social processes. ACM transactions on computer-human interaction (TOCHI) 7, 1 (2000), 59–83. Navigate tocitation 1\r\nDan Evon. 2020. Did Joe Biden's Great-Grandfather Own Slaves?https://www.snopes.com/fact-check/joe-biden-slaves-great-grandfather/ Navigate tocitation 1\r\nKristie Fisher, Scott Counts, and Aniket Kittur. 2012. Distributed sensemaking: improving sensemaking by leveraging the efforts of previous users. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 247–256. Navigate tocitation 1citation 2\r\nAndrea Forte, Nazanin Andalibi, Thomas Park, and Heather Willever-Farr. 2014. Designing information savvy societies: an introduction to assessability. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2471–2480. Navigate tocitation 1citation 2citation 3citation 4\r\nJacey Fortin. 2018. She Was the Only Woman in a Photo of 38 Scientists, and Now She's Been Identified. The New York Times (Mar 2018). https://www.nytimes.com/2018/03/19/us/twitter-mystery-photo.html Navigate tocitation 1\r\nSarah A Gilbert. 2020. \" I run the world's largest historical outreach project and it's on a cesspool of a website.\" Moderating a Public Scholarship Site on Reddit: A Case Study of r/AskHistorians. Proceedings of the ACM on Human-Computer Interaction 4, CSCW1 (2020), 1–27. Navigate tocitation 1\r\nJerome S Handler and Michael L Tuite. 2007. Retouching History: The Modern Falsification of a Civil War Photograph. Navigate tocitation 1citation 2\r\nM. Keith Harris. 2019. Civil War Photo Sleuth. Journal of American History 106, 2 (2019), 544–546. https://doi.org/10.1093/jahist/jaz498 Navigate tocitation 1citation 2\r\nJohn J Howard, Laura R Rabbitt, and Yevgeniy B Sirotin. 2020. Human-algorithm teaming in face recognition: How algorithm outcomes cognitively bias human decision-making. Plos one 15, 8 (2020), e0237855. Navigate tocitation 1\r\nPan Hui and Sonja Buchegger. 2009. Groupthink and peer pressure: Social influence in online social network groups. In 2009 International Conference on Advances in Social Network Analysis and Mining. IEEE, 53–59. Navigate tocitation 1\r\nNassim JafariNaimi and Eric M Meyers. 2015. Collective intelligence or group think? Engaging participation patterns in World Without Oil. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing. 1872–1881. Navigate tocitation 1\r\nSoomin Kim, Jinsu Eun, Joseph Seering, and Joonhwan Lee. 2021. Moderator chatbot for deliberative discussion: Effects of discussion structure and discussant facilitation. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–26. Navigate tocitation 1\r\nAniket Kittur, Andrew M Peters, Abdigani Diriye, and Michael Bove. 2014. Standing on the schemas of giants: socially augmented information foraging. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. 999–1010. Navigate tocitation 1\r\nTravis Kriplean, Caitlin Bonnar, Alan Borning, Bo Kinney, and Brian Gill. 2014. Integrating On-demand Fact-checking with Public Dialogue. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing(CSCW '14). ACM, New York, NY, USA, 1188–1199. https://doi.org/10.1145/2531602.2531677 Navigate tocitation 1citation 2citation 3\r\nTravis Kriplean, Michael Toomim, Jonathan Morgan, Alan Borning, and Andrew Ko. 2012. Is This What You Meant?: Promoting Listening on the Web with Reflect. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems(CHI '12). ACM, New York, NY, USA, 1559–1568. https://doi.org/10.1145/2207676.2208621 Navigate tocitation 1citation 2citation 3\r\nKurt Luther. 2016. How Fellow Collectors, Field Photos and Muttonchops Identified an Unknown Officer. Military Images 34, 1 (2016), 29–31. Navigate tocitation 1\r\nKurt Luther. 2017. Merrill Carbine Leads to a Soldier's Identification. Military Images 35, 2 (2017), 64–65. Navigate tocitation 1citation 2citation 3\r\nKurt Luther. 2018. Non-Traditional Research Tools—and Serendipity. Military Images 36, 3 (2018), 12–13. Navigate tocitation 1\r\nKurt Luther. 2018. What are the odds? Photo sleuthing by the numbers. Military Images 36, 1 (2018), 12–15. Navigate tocitation 1citation 2citation 3citation 4citation 5\r\nKurt Luther. 2019. What to Do When Gold Standards Go Wrong?Military Images 37, 1 (2019), 8–9. https://www.jstor.org/stable/26532101 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: How to Trust the Worthiness of an Identification. Military Images 38, 3 (213) (2020), 8–11. https://www.jstor.org/stable/26914966 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: Lost and Found in the Library of Congress. Military Images 38, 2 (212) (2020), 10–13. https://www.jstor.org/stable/26890126 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: The Art of Photo Sleuthing. Military Images 38, 4 (214) (2020), 8–11. https://www.jstor.org/stable/26925454 Navigate tocitation 1\r\nRamona Martinez. 2012. Photo mystery solved, then doubted, then deciphered, thanks to readers. https://www.npr.org/sections/pictureshow/2012/04/17/150801239/photo-mystery-solved-then-doubted-then-resolved-thanks-to-readers Navigate tocitation 1\r\nRamona Martinez. 2012. Unknown No More: Identifying A Civil War Soldier. http://www.npr.org/2012/04/11/150288978/unknown-no-more-identifying-a-civil-war-soldier Navigate tocitation 1\r\nChristian A Meissner and John C Brigham. 2001. Thirty years of investigating the own-race bias in memory for faces: A meta-analytic review.Psychology, Public Policy, and Law 7, 1 (2001), 3. Navigate tocitation 1\r\nVikram Mohanty, Kareem Abdol-Hamid, Courtney Ebersohl, and Kurt Luther. 2019. Second opinion: Supporting last-mile person identification with crowdsourcing and face recognition. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 86–96. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6\r\nVikram Mohanty and Kurt Luther. 2023. DoubleCheck: Designing Community-based Assessability for Historical Person Identification. ACM Journal on Computing and Cultural Heritage (JOCCH) (to appear) (2023). Navigate tocitation 1\r\nVikram Mohanty, David Thames, Sneha Mehta, and Kurt Luther. 2019. Photo sleuth: Combining human expertise and face recognition to identify historical portraits. In Proceedings of the 24th International Conference on Intelligent User Interfaces. 547–557. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6citation 7citation 8\r\nVikram Mohanty, David Thames, Sneha Mehta, and Kurt Luther. 2020. Photo Sleuth: Identifying Historical Portraits with Face Recognition and Crowdsourced Human Expertise. ACM Transactions on Interactive Intelligent Systems (TiiS) 10, 4 (2020), 1–36. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6citation 7citation 8citation 9citation 10\r\nMeredith Ringel Morris, Scott Counts, Asta Roseway, Aaron Hoff, and Julia Schwarz. 2012. Tweeting is believing? Understanding microblog credibility perceptions. In Proceedings of the ACM 2012 conference on computer supported cooperative work. 441–450. Navigate tocitation 1\r\nBenedikt Morschheuser, Juho Hamari, and Jonna Koivisto. 2016. Gamification in crowdsourcing: a review. In 2016 49th Hawaii International Conference on System Sciences (HICSS). IEEE, 4375–4384. Navigate tocitation 1\r\nKathleen L Mosier and Linda J Skitka. 1999. Automation use and automation bias. In Proceedings of the human factors and ergonomics society annual meeting, Vol. 43. SAGE Publications Sage CA: Los Angeles, CA, 344–348. Navigate tocitation 1citation 2\r\nGreg Myre. 2021. How Online Sleuths Identified Rioters At The Capitol. https://www.npr.org/2021/01/11/955513539/how-online-sleuths-identified-rioters-at-the-capitol Navigate tocitation 1\r\nJohnny Nhan, Laura Huey, and Ryan Broll. 2017. Digilantism: An analysis of crowdsourcing and the Boston marathon bombings. The British journal of criminology 57, 2 (2017), 341–361. Navigate tocitation 1\r\nRaja Parasuraman and Dietrich H Manzey. 2010. Complacency and bias in human use of automation: An attentional integration. Human factors 52, 3 (2010), 381–410. Navigate tocitation 1\r\nJunwon Park, Ranjay Krishna, Pranav Khadpe, Li Fei-Fei, and Michael Bernstein. 2019. AI-based request augmentation to increase crowdsourcing participation. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 115–124. Navigate tocitation 1\r\nSarah Perez. 2022. Twitter to show 'Birdwatch' community fact-checks to more users, following criticism. https://techcrunch.com/2022/03/03/twitter-to-show-birdwatch-community-fact-checks-to-more-users-following-criticism/ Navigate tocitation 1\r\nP Jonathon Phillips, Amy N Yates, Ying Hu, Carina A Hahn, Eilidh Noyes, Kelsey Jackson, Jacqueline G Cavazos, Géraldine Jeckeln, Rajeev Ranjan, Swami Sankaranarayanan, et al. 2018. Face recognition accuracy of forensic examiners, superrecognizers, and face recognition algorithms. Proceedings of the National Academy of Sciences 115, 24 (2018), 6171–6176. Navigate tocitation 1citation 2\r\nPeter Pirolli, Evelin Wollny, and Bongwon Suh. 2009. So you know you're getting the best possible information: a tool that increases Wikipedia credibility. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1505–1508. Navigate tocitation 1\r\nInioluwa Deborah Raji and Joy Buolamwini. 2019. Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products. Navigate tocitation 1\r\nAmira Rezgui and Kevin Crowston. 2018. Stigmergic coordination in Wikipedia. In Proceedings of the 14th International Symposium on Open Collaboration. 1–12. Navigate tocitation 1\r\nRoy Rosenzweig. 2006. Can History Be Open Source? Wikipedia and the Future of the Past. Journal of American History 93, 1 (June 2006), 117–146. Navigate tocitation 1citation 2\r\nSaiph Savage, Andres Monroy-Hernandez, and Tobias Höllerer. 2016. Botivist: Calling volunteers to action using online bots. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing. 813–822. Navigate tocitation 1\r\nMike Schaekermann, Joslin Goh, Kate Larson, and Edith Law. 2018. Resolvable vs. irresolvable disagreement: A study on worker deliberation in crowd work. Proceedings of the ACM on Human-Computer Interaction 2, CSCW (2018), 1–19. Navigate tocitation 1\r\nMichael S. Schmidt. 2018. 'Flags of Our Fathers' Author Now Doubts His Father Was in Iwo Jima Photo. The New York Times (Jan 2018). https://www.nytimes.com/2016/05/04/us/iwo-jima-marines-bradley.html Navigate tocitation 1\r\nJennifer Schuessler. 2017. Found: Oldest Known Photo of a U.S. President (Socks and All). https://www.nytimes.com/2017/08/16/arts/design/john-quincy-adams-daguerreotype-sothebys-auction.html Navigate tocitation 1\r\nJulia Schwarz and Meredith Morris. 2011. Augmenting web pages and search results to support credibility assessment. In Proceedings of the SIGCHI conference on human factors in computing systems. 1245–1254. Navigate tocitation 1\r\nRuoxi Shang, KJ Kevin Feng, and Chirag Shah. 2022. Why Am I Not Seeing It? Understanding Users' Needs for Counterfactual Explanations in Everyday Recommendations. In 2022 ACM Conference on Fairness, Accountability, and Transparency. 1330–1340. Navigate tocitation 1\r\nLinda J Skitka, Kathleen L Mosier, and Mark Burdick. 1999. Does automation bias decision-making?International Journal of Human-Computer Studies 51, 5 (1999), 991–1006. Navigate tocitation 1\r\nVerbi Software. 2019. MAXQDA 2020 [computer software]. VERBI Software. Available from maxqda.com. Navigate tocitation 1\r\nNPR Staff. 2016. How Social Media Smeared A Missing Student As A Terrorism Suspect. https://www.npr.org/sections/codeswitch/2016/04/18/474671097/how-social-media-smeared-a-missing-student-as-a-terrorism-suspect Navigate tocitation 1\r\nBongwon Suh, Ed H Chi, Aniket Kittur, and Bryan A Pendleton. 2008. Lifting the veil: improving accountability and social transparency in Wikipedia with wikidashboard. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1037–1040. Navigate tocitation 1\r\nW Ben Towne, Aniket Kittur, Peter Kinnaird, and James Herbsleb. 2013. Your process is showing: controversy management and perceived quality in Wikipedia. In Proceedings of the 2013 conference on Computer supported cooperative work. 1059–1068. Navigate tocitation 1\r\nCivil War Trust. 2021. Military Images Magazine | Interview with Ron Coddington. https://www.battlefields.org/learn/articles/military-images-magazine Navigate tocitation 1\r\nSukrit Venkatagiri, Aakash Gautam, and Kurt Luther. 2021. CrowdSolve: Managing Tensions in an Expert-Led Crowdsourced Investigation. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–30. Navigate tocitation 1\r\nShaun Wallace, Lucy Van Kleunen, Marianne Aubin-Le Quere, Abraham Peterkin, Yirui Huang, and Jeff Huang. 2017. Drafty: Enlisting Users To Be Editors Who Maintain Structured Data. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 5. Navigate tocitation 1\r\nAndrea Wiggins and Yurong He. 2016. Community-based data validation practices in citizen science. In Proceedings of the 19th ACM Conference on computer-supported cooperative work & social computing. 1548–1559. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6\r\nFoundation Wikimedia. 2022. No original research. https://en.wikipedia.org/wiki/Wikipedia:No_original_research Navigate tocitation 1\r\nHeather Willever-Farr, Lisl Zach, and Andrea Forte. 2012. Tell me about my family: A study of cooperative research on Ancestry. com. In Proceedings of the 2012 iConference. ACM, 303–310. Navigate tocitation 1\r\nHeather L Willever-Farr and Andrea Forte. 2014. Family matters: Control and conflict in online family history production. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. ACM, 475–486. Navigate tocitation 1citation 2citation 3citation 4citation 5\r\nJeremy B Wilmer. 2017. Individual differences in face recognition: A decade of discovery. Current Directions in Psychological Science 26, 3 (2017), 225–230. Navigate tocitation 1\r\nBob Zeller. 2022. Searching for photos of Civil War Soldiers | David Wynn Vaughan. https://www.civilwarphotography.org/searching-for-photos-of-civil-war-soldiers/ Navigate tocitation 1\r\nA CIVIL WAR PHOTO SLEUTH: DESIGN CHALLENGES AND OPPORTUNITIES\r\n \r\nHere, we provide details of three key challenges posed by Civil War Photo Sleuth that might have contributed towards misidentification and subsequently, inaccurate assessments, and how they present design opportunities for Photo Steward.\r\nA.1 Conflating facial similarity with photo identification\r\nA.1.1 Challenges: Mohanty et al. conducted a benchmarking study of CWPS [44], and found the face recognition algorithm to be of low precision; i.e., it retrieved hundreds of search results which may look similar to the query photo but are actually different people (false positives). Low precision increases the chances that users will interact with a lot of false positives. In such cases, one may need to garner additional information (i.e., comparing biographical information) before making a decision. However, it is plausible that automation bias may play a role in non-expert users making a match solely based on facial similarity [47]. As a result, there is strong potential for misidentification (see Figure 8).\r\nA.1.2 Current Workflow: CWPS's compare interface allows users to closely inspect the search results for a potential match, but does not make any distinction between facial similarity comparison and photo identification. Both are conflated into a one-step process, with one \"Identify\" button for the users to make their decisions (see Figure 2-C). Yet, users may want to indicate agreement with just the facial similarity (i.e., query photo and the search result showing the same person) but not the identity (i.e., name and biographical information), or vice versa.\r\nA.1.3 Design Goal 1: To support accurate investigation of photo identifications, users should be able to deliberate on the different aspects of the decision-making process. Providing users with a decision-making workflow that decouples facial similarity comparison from the overall photo identification task would allow them to focus on these tasks separately, while discouraging them from making decisions solely on the basis of facial similarity.\r\n\r\nFigure 8: An example of a misidentified photo on CWPS. This photo was identified as John C Whiteside based on facial similarity with the top search result: an identified reference photo of John C Whiteside. However, other visual clues, such as the photographer studio, do not align with Whiteside's biographical information.\r\n\r\nA.2 Lacking support for fine-grained, deliberative decision making\r\nA.2.1 Challenges: Historical photo identification is a complex task, where experts often corroborate multiple pieces of evidence, including facial similarity comparison, before reaching a decision about the identity of the photo [33]. While confirming an identity, experts may be highly confident if the source, military records, uniform clues, and additional photos of the same person all line up, or slightly confident if they need additional evidence. Conflicting evidence pieces may also affect their confidence levels. Similarly, they may have different degrees of certainty while ruling out an identity for a photo. A lack of support for expressing and displaying granularity in these photo identification decisions can lead to varying degrees of uncertainty being captured and misinterpreted as a confirmation, and eventually propagating misidentifications. Further, while it is safe to assume the vast majority of the Civil War photography community care about the accuracy of the photo IDs, there exists a small risk of financially-driven misidentifications since certain types of identified photos are considered to be more valuable [9].\r\nA.2.2 Current Workflow: The compare interface's \"Identify\" button (see Figure 2-C) is the only mechanism available on the platform for users to provide (a unary) input on photo identifications. CWPS users currently cannot provide fine-grained feedback on a given photo's identity, either at the time of identifying or afterwards on the photo page.\r\nA.2.3 Design Goal 2 : Users should be able to express how certain or uncertain they are about a photo identification. Interventions for allowing users to provide structured feedback can not only facilitate more accurate, fine-grained responses, but also encourage deliberation on the available evidence before making a decision, borrowing inspiration from other social computing systems that support users reflecting and deliberating on available information [28, 29].\r\nA.3 Limited validation opportunities for the community\r\nA.3.1 Challenges: For humans, deciding whether two photos show the same person is a highly subjective experience. A study by Mohanty et al. [41] showed that participants in a crowdsourcing study often disagree with each other and with facial recognition results in person identification tasks. The same study showed that people often seek a second opinion from peers for validating photo identifications. Without validation, original crowdsourced historical research may result in unresolved cases of conflicting IDs proposed for the same face, incorrect IDs remaining unchecked, or airtight IDs not being confirmed.\r\nA.3.2 Current Workflow: Once a photo has been identified, CWPS does not offer other users the option to express their opinions on whether two photos show the same person or not, or if the photo has been identified correctly or not (see Figure 8). The photo page does not indicate if (or why) the photos are linked to each other, how they are similar, or which user(s) considered them to be similar. Further, it does not indicate the role of facial recognition in linking them together.\r\nA.3.3 Design Goal 3 : To support accurate original historical research, the platform should encourage information stewardship from the community by allowing members to give their opinions on existing photo identifications [72]. Further, the platform should also be transparent about the role of community stewardship and facial recognition for a given photo identification, which can then act as evidence for aiding subsequent validation efforts by other users, thus supporting a form of stigmergic collaboration [13, 14].\r\nB DETAILS FOR LOG ANALYSIS\r\n \r\nTo understand the community's stewardship behaviors, we examined website logs for all user activities for one year since we launched the new features (December 2020 – December 2021). During this period, we observed 5843 voting instances on 5672 photos for 5355 unique IDs, where a user voting on whether Photo N can be identified as a Person M or not is considered to be one voting instance. For a given voting instance, we analyzed 1) any associated facial similarity comparisons to understand how they deliberated on their final voting decision, and 2) the user's confidence vote plus any justification notes left by the user to understand their decision rationale. Of the 5672 query photos, 4297 photos (4377 voting instances) did not have any facial similarity comparisons — only the user's confidence on the ID (plus any notes) was captured.\r\nFor the remaining voting instances which had at least one associated facial similarity comparison (i.e., users deliberate through the two-step validation process), we broke them down into pre-identified (i.e., the user knew the ID of the photo ahead of time and therefore, might be looking for a specific target on the search results page) and post-identified (i.e., the user did not know the ID of the photo and therefore, may not have a specific target while analyzing the search results) cases. As mentioned earlier in Section 4.2, users had the opportunity to validate IDs either on the search results page on the photo page. While a pre-identified voting instance almost certainly originated from the search results page, a post-identified case could be from either page. Since the logs did not give us the page origin of each vote directly, we triangulated from CWPS's timestamps to determine whether a given voting instance was for a pre-identified case or a post-identified one.\r\nAfter collating all the associated facial similarity comparisons for a given voting instance, we had 1064 pre-identified and 576 post-identified user deliberations to analyze. Each deliberation instance is a user's attempt to identify a query photo as a given target (person) ID, where they first compare facial similarity with all other photos that have been identified as the target ID, followed by the user's confidence on the query photo being the target ID. We analyzed the user responses to see whether the facial similarity comparisons had any impact on the user's confidence.\r\nUsers had provided comments in 682 (out of 5843, or 11.7%) voting instances. We coded these comments using an iterative, inductive approach, which resulted in five high-level themes, which can broken down into 13 sub-categories (see Appendix C).\r\nTo understand any stigmergic collaboration processes at play, we also analyzed the IDs which had multiple votes to check for agreements and disagreements between the voters. We further analyzed how the community's facial similarity comparisons compare against the facial recognition scores.\r\nC THEMES FOR NOTES ANALYSIS\r\nPhoto Comparisons\r\nHigh-Level Comparison: The note mentions \"replica\", \"facial similarity\", \"facial match\", \"identical\", \"visual comparison\", and other similar terms that describe comparisons with a prior identified photo.\r\nDescribing facial features: The note mention facial features like \"eyes\", \"hairline\", \"ears\", etc. to make comparisons\r\nWord-of-Mouth\r\nDescendant: The note either mentions that the user is a descendant of the person being identified, or they got the information from the family of the person.\r\nOwnership: The note either mentions that the user owns a printed version of the photo, compared with a photo in their collection, or they know the owner of the photo.\r\nFamiliarity/Self-Reported Research: The note mentions that the user has seen the photo somewhere, be it in a book, museum, etc.\r\nVisual Clues\r\nUniform: The note mentions visual clues that pertain to the uniform of the person (e.g., hat insignia, shoulder straps, etc.) The user may infer the possible service information (i.e., ranks, branches, regiments, etc.) from the uniform clues.\r\nInscription: The note mentions the presence of a period inscription on the photo (a highly trustworthy primary source for a person's ID), or an album case, or modern inscriptions such as books, which is generally the name of the person being identified. In some instances, the inscribed text may point to the person's service information.\r\nOther Visual Clues: The note mentions visual clues in the photo (e.g., backmarks, borders, etc.) beyond the person's face.\r\nExternal Information\r\nExternal URL: The note mentions an external URL, which supposedly has additional information about the photo's ID.\r\nOther sources (e.g., museum, website, book, etc.): The note mentions an external source (e.g., museum, book, etc.) that supposedly has evidence for the photo's ID, but no URLs are provided. Details about the source may or may not be available.\r\nLack of Information / Seeking Additional Evidence: The note mentions the lack of evidence or seeking additional evidence, be it about the source or the service information.\r\nProviding Additional Information\r\nBiographical Information: The notes mentions additional information about the person's service records, specific regiments, biographical information (name, year, location), etc.\r\nAdditional Context: The note mentions some additional context provided by the user to justify their decision, such as information about the photo collection, or pointing to someone else's research, or some historical context, or incorrect evidence, or if the person is prominent.\r\nD PARTICIPANT DETAILS\r\nUndergraduate and master's students concentrating in history who use Civil War photos for their coursework and research projects, but are not (yet) employed in a professional capacity as historians. We recruited five students via recommendations from our university's history department. None of the students had used CWPS before, or were known to the authors prior to the study. Three students were men and two were women, and all were in the \"18 to 30\" age group. We anonymize them with identifiers S1–S5.\r\nExperienced users of Civil War Photo Sleuth who have added over 50 photos each and have substantial knowledge of Civil War history, but are not professional historians. We recruited five amateur experts from the CWPS contact list. All five users were men, and they were distributed across different age groups (two in \"18 to 30\", two in \"31 to 40\", and one in \"51 to 60\"). We anonymize them with identifiers C1–C5. C1 and C3 are among the most active daily users on CWPS. Only two of the five had used Photo Steward before.\r\nExpert historians with a graduate degree in history, specializing in American Civil War history, but with little or no previous experience with CWPS. We recruited five historians via recommendations from our university's history department. Three historians were men and two were women. They were distributed across different age groups (two in \"18 to 30\", two in \"31 to 40\", and one in \"51 to 60\"). We anonymize them with identifiers H1–H5. None of them had used Photo Steward before.\r\nE LAB STUDY QUESTIONS\r\nE.1 Semi-Structured Questions\r\nIs there a way that you would like to capture your thoughts on this ID and share them with others, if possible? If so, what would that look like? If not, can you explain why not?\r\nWhat did you think about the community opinions?\r\nWhat do you think about the ID quality visualization?\r\nWhat did you think about the 2-step process while agreeing/disagreeing on an identity?\r\nWhat is your overall opinion of both the interfaces?\r\nWhich interface would you prefer for validating the information? And why?\r\nWhat would you change or improve?\r\nE.2 Usability Survey\r\nQ1. The community's opinions about an identity were clear and easy to understand in the 2nd system. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ2. The community's opinions about an identity were useful for assessing the information. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ3. The process of voting on an identity was clear and easy to understand. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ4. Comparing other photos first and then voting on an identity helped me deliberate and make more accurate decisions. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ5. I was able to validate the information better using the 2nd system. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nF SUMMARY OF CHANGES\r\nWe summarize in Table 6 about how Photo Steward differs from CWPS.\r\nTable 6: Summary of changes: Civil War Photo Sleuth (CWPS) with and without Photo Steward.\r\n\r\nCWPS\r\nCWPS + Photo Steward\r\nDecision-Making: How do users make identification decisions for the query photo when they see the similar-looking search results retrieved by facial recognition?\r\nA single-step comparison interface that allows the user to compare the query photo with one similar-looking photo and the associated biographical information of the target ID. The single-step workflow conflates the two tasks of facial similarity and person identification.\r\nA multi-step validation interface that allows the user to compare the query photo with all previously identified photos of a similar-looking target ID (search result). The two-step workflow decouples the two tasks of facial similarity and person identification, thus allowing the user to deliberate on the facial similarity comparison between the query photo and the target photo(s) before making a decision on the target identity.\r\nUsers make a unary input on the query photo's ID by clicking an \"Identify\" button in the comparison interface, which will link the target ID to the query photo. The comparison interface does not allow users to rule out candidates.\r\nUsers can provide fine-grained decisions for both facial similarity and person identification steps in the validation interface. In Step 1, the user compares the query photo and the target photo for facial similarity by selecting from the following options: No (Different Person), Not Sure, Yes (Facial Match), and Yes (Replica). In Step 2, users indicate how confident they are about the query photo's ID (as the proposed target ID) by selecting the following options: No (Highly Confident), No (Slightly Confident), Not Sure, Yes (Slightly Confident), and Yes (Highly Confident).\r\nAccess: What kind of validation opportunities are available for users?\r\nUsers can only access the comparison interface from the search results page if they search for similar-looking candidates using facial recognition. Once the photo has been identified, users cannot access the comparison interface on the photo page. The community cannot weigh in on an ID's reliability on the photo page.\r\nUsers can access the validation interface on both the search results page (while identifying the query photo) and the photo page (after the photo has been identified). The community can weigh in on the reliability of the proposed ID(s) and facial matches using the validation interface on the photo page, thus engaging in a form of stigmergic collaboration.\r\nEvidence: What kind of stewardship evidence is presented to the users?\r\nThe photo page displays the proposed ID(s) for the photo without any additional information about the ID's reliability. Similarly, other photos that have been matched to the query photo are also displayed without any reliability indicator. Since community opinions are not captured on CWPS, they are not displayed.\r\nThe photo page displays the proposed ID(s) for the photo along with stewardship visualizations of the a) community's confidence on the ID, and b) facial similarity comparison with other photos of the same ID by both the community and AI.\r\nFOOTNOTE\r\n1 www.civilwarphotosleuth.com\r\n\r\n\r\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs International 4.0 License.\r\nCI '23, November 06–09, 2023, Delft, Netherlands\r\n© 2023 Copyright held by the owner/author(s).\r\nACM ISBN 979-8-4007-0113-9/23/11.\r\nDOI: https://doi.org/10.1145/3582269.3615600",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== story.txt ===\nRon: Great. I want to start with a detail about you that I’ve always appreciated, which is that you started out as a collector and someone who was deeply interested in the Civil War. Talk a little about how you became interested and got started.\r\n\r\nKurt: Sure thing. I am a bit of a Civil War collector. I grew up in a family of Civil War and history collectors and appreciators. I spent many childhood hours at antique shops and flea markets, and our home was full of interesting Civil War collectibles—books, artifacts, and all kinds of interesting stuff. That was my world as a kid. As I grew older, I realized that I could actually have my own Civil War collection. What was a little different about me is that I gravitated toward Civil War photographs—a category my family didn’t have too much of. I just felt drawn to the imagery of people from that era, their incredible stories, learning about the names behind the faces, and ultimately building my own collection of images while loving the process of researching those individuals.\r\n\r\nRon: One of my favorite stories is your family connection—how there’s a photograph connected to it all. I’d love to hear you tell that again.\r\n\r\nKurt: Absolutely. One of the reasons my family had an interest in the Civil War is that we have some Civil War ancestors. Three of them fought with the Union Army in Pennsylvania regiments, and I grew up learning about their adventures. We researched all we could about them through military records, but we didn’t have any images—we couldn’t see them or know what they looked like. I got really lucky during the 150th-anniversary commemorations of the Civil War. In Pittsburgh, where I grew up and where my Civil War ancestors lived, there was an exhibit at the Heinz History Center. I noticed a really cool photo album of Civil War soldiers. The label said that the company in the album was one that my ancestor had fought in. Out of thousands of possibilities, that single regiment appeared there. I asked the museum staff if they could have the owner, Ken Turner, look inside to see if my ancestor’s photo was in there. And sure enough, inside the album was a photograph of my Civil War ancestor, Corporal Oliver Croxton in uniform. I was looking into the eyes of my great-great-great grand uncle.\r\n\r\nRon: How did you feel when you saw that for the first time?\r\n\r\nKurt: It felt amazing. Here was someone I had thought about my whole life, trying to learn as much as I could about him. And once you start studying Civil War portraits, you learn that the odds of finding one specific photo of a soldier—especially a lower-ranked one—are really low. So having that opportunity, seeing his face, and getting a much clearer picture of who he was… it was really powerful.\n\n==================================================\n\n=== Kurt paper.txt ===\nPhoto Steward: A Deliberative Collective Intelligence Workflow for Validating Historical Archives\r\nVikram Mohanty, Computer Science, Virginia Tech, USA, vikrammohanty@vt.edu\r\nKurt Luther, Computer Science, Virginia Tech, USA, kluther@vt.edu\r\n\r\nDOI: https://doi.org/10.1145/3582269.3615600\r\nCI '23: Collective Intelligence Conference, Delft, Netherlands, November 2023\r\nHistorical photographs of people generate significant cultural and economic value, but correctly identifying the subjects of photos can be a difficult task, requiring careful attention to detail while synthesizing large amounts of data from diverse sources. When photos are misidentified, the negative consequences can include financial losses and inaccuracies in the historical record, and even the spread of mis- and disinformation. To address this challenge, we introduce Photo Steward, an information stewardship architecture that leverages a deliberative workflow for validating historical photo IDs. We explored Photo Steward in the context of Civil War Photo Sleuth (CWPS), a popular online community dedicated to identifying photos from the American Civil War era (1861–65) using facial recognition and crowdsourcing. While the platform has been successful in identifying hundreds of unknown photographs, there have been concerns about unverified identifications and misidentifications. Our exploratory evaluation of Photo Steward on CWPS showed that its validation workflow encouraged users to deliberate while making photo ID decisions. Further, its stewardship visualizations helped users to assess photo ID information accurately, while fostering diverse forms of stigmergic collaboration.\r\nCCS Concepts: • Human-centered computing → Collaborative and social computing design and evaluation methods; • Human-centered computing → Interactive systems and tools;\r\n\r\nKeywords: crowdsourcing, human-AI interaction, online deliberation, community stewardship, information assessability, online communities, history, person identification, facial recognition, stigmergic collaboration\r\n\r\nACM Reference Format:\r\nVikram Mohanty and Kurt Luther. 2023. Photo Steward: A Deliberative Collective Intelligence Workflow for Validating Historical Archives. In Collective Intelligence Conference (CI '23), November 06--09, 2023, Delft, Netherlands. ACM, New York, NY, USA 19 Pages. https://doi.org/10.1145/3582269.3615600\r\n\r\nFigure 1: Workflow of Photo Steward. (1) Decision-Making: The user compares photos for facial similarity and make decisions on the photo ID using Photo Steward's deliberative validation interface. (2) Access: The user accesses the validation interface from Civil War Photo Sleuth's search results and photo page. (3) Evidence: The community's responses from the validation interface feed into stewardship visualizations that are visible on the Photo Page, which subsequently foster a form of stigmergic collaboration among the users.\r\n1 INTRODUCTION\r\nThe task of correctly identifying individuals in historical photos holds great cultural and economic importance [3, 19, 39, 61]. This identification process is analogous to solving a complex mystery. It often involves corroboration of multiple research processes such as investigating visual clues in a photo, finding relevant reference resources, and comparing multiple low-resolution reference photos [31, 32, 37]. Historical photos pose many challenges, including low-resolution images, scattered reference materials, limited domain expertise, and lack of suitable verification tools. These hurdles often result in misidentifications which can have negative consequences, ranging from distorting historical narratives [60] and fueling conspiracy theories [12] to spreading disinformation [16] and unwarranted financial gains from inaccurate representations [21]. As online platforms such as Ancestry.com, Find-a-Grave, and FamilySearch democratize historical and genealogical research, the risk of misidentification is further amplified due to factors such as inadequate experience, confirmation bias, and automation bias introduced by imperfect automated tools [44, 75].\r\nTo address these challenges, we introduce Photo Steward, a deliberative workflow that leverages collective intelligence to validate historical photo identifications (IDs). Photo Steward's architecture builds upon the concept of information stewardship [18, 72], which involves community-driven validation of content, as seen in online communities like iNaturalist and Wikipedia. We designed and applied the Photo Steward architecture to Civil War Photo Sleuth (CWPS),1 an AI-infused online platform for identifying historical photos. CWPS has over 20,000 registered users and over 25,000 identified Civil War portraits, and faces the problem of historical photo misidentification [43]. Photo Steward provides a validation workflow that promotes careful deliberation during facial similarity comparison and photo ID verification, while enabling users to share opinions. It also visualizes community opinions on the reliability of photo IDs and facial similarity, fostering information stewardship at multiple levels.\r\nWe publicly released Photo Steward on CWPS and conducted an exploratory evaluation of twelve months of usage, including interviews with users of different expertise levels and log analysis of stewardship behaviors on the platform. We found that Photo Steward's stewardship visualizations helped users to find additional evidence (e.g., external sources, uniform clues matching service records, etc.) for assessing the reliability of photo IDs. Users validated hundreds of different IDs on the platform, and found the workflow to be useful for deliberating on facial similarity comparison and fine-grained photo ID decisions.\r\nWe also discuss the implications of community participation, deliberative human-AI interaction, and assessable designs for historical photo identification.\r\n2 RELATED WORK\r\n2.1 Misinformation in a Historical Context\r\nNumerous online communities, forums, and websites have emerged in recent years for archiving and documenting history [57], generating family histories [74, 75], identifying and sharing historical photos [43], trading antiques [1, 8], and facilitating discussions around history [20]. Much like popular social media platforms such as Facebook and Twitter, these history-based platforms are also prone to the problem of misinformation, albeit in a more specialized historical research context.\r\nPrior work has shown that erroneous family history trees were being disseminated across Find-a-Grave and Ancestry, two popular genealogy research communities, as a result of the platform's low bar to entry and inexperienced contributors' over-reliance on inaccurate automated features [75]. Mohanty et al. [43] showed that despite successful identifications on Civil War Photo Sleuth (CWPS), several photos were misidentified in the first month, particularly for photos without period inscriptions or duplicate views (12 misidentifications out of 37). Multiple factors — the correct candidate not present in the search pool, or the user incorrectly assessing facial similarity [53] and picking the wrong match — can lead to incorrect IDs. A follow-up benchmarking study of the underlying face recognition algorithm [44] highlighted its low precision (i.e., it retrieves over hundreds of search results), raising the possibility of errors due to automation bias, i.e., the tendency of users to over-rely on automation for making a decision [47, 50, 64].\r\nIdentifying historical photos is a complex investigative process, often involving the corroboration of multiple evidence pieces [31, 33] and can be seen analogous to \"finding a needle in a haystack\". As a result, historical photo IDs run a high risk of getting misidentified even with the best of intentions. Multiple Civil War photos have also been misidentified in the collections of professionally managed museums and archives, such as the US Library of Congress [36] and the Abraham Lincoln Presidential Library [34]. At the same time, historical photo IDs also have the potential to generate significant monetary value [1, 3, 8, 10], and such financial incentives might also lead to falsified identifications [21]. Validating these historical photo IDs, which are a result of complex, subjective original research, becomes tricky without the lack of domain expertise and access to investigative tools.\r\nWe addressed these challenges in Photo Steward by designing a stewardship architecture that allows users to share their expertise with others. To address the impact of automation, we introduced a two-step validation workflow for the users to deliberate on decisions while interacting with the AI's recommendations.\r\n2.2 Data Validation in Online Communities\r\nMultiple online platforms have leveraged the strengths of crowdsourced contributions for validating the quality of data generated on those sites. Elliott discusses how stigmergic collaboration, where indirect coordination within a community stimulates subsequent actions, plays a role in maintaining articles on Wikipedia [13]. This concept was observed by Wiggins et al. in their study of iNaturalist, an online platform for identifying species, wherein community stewardship behaviors were seen as users agreed on organism identifications to influence the platform's quality grade status [72]. Prior work has shown that stewardship visualizations on Wikipedia (i.e., article quality) [18] and iNaturalist (i.e., ID research grade status) [72] have a positive impact on users' assessment of the information.\r\nAlong these lines, we also built DoubleCheck [42], a quality assessment framework that builds upon the concepts of provenance and stewardship for verifying historical photo IDs. DoubleCheck focused on displaying quality indicator badges for historical photo IDs by capturing accurate provenance information and combining the source trustworthiness information with community opinions on the ID. In this work, we focus solely on the underlying stewardship architecture that helped facilitate the community opinions. Both DoubleCheck and Photo Steward were evaluated in the same lab study, but there is no overlapping data.\r\nVisualizations displaying (surrogate) quality metrics, such as popularity among expert users, social reputation, and content coverage, have been effective in helping users assess the credibility of websites and search results [62]. Prior work has also shown that visualizing the history of edits for a Wikipedia article can have a significant impact on users' perceived trustworthiness of the article [54, 67]. Similarly, Chevalier et al. [7] showed that visualizing the number of contributors, length of the article and discussion, and the history of edits helped users assess the quality of Wikipedia articles faster. On the other hand, Towne et al. [68] found that being exposed to editor conflicts in the discussion of a Wikipedia article lowered the perception of the article's quality, even though the users reported that the transparency raised their perceptions of the page and Wikipedia in general. Morris et al. [45] found that Twitter users relied on the author information for making assessments about the credibility of information in a tweet.\r\nDrawing from this prior work, we designed Photo Steward's stewardship visualizations to highlight the role of collective intelligence, while fostering stigmergic collaboration on CWPS to validate the quality of photo identifications.\r\n2.3 Background: Civil War Photo Sleuthing\r\nThe American Civil War (1861–65) was one of the first major conflicts to be extensively photographed. Over 3 million soldiers fought in the war, with many of them having been photographed at least once. Over 150 years, many of these photos have survived in museums, libraries, and personal collections, but only 10–20% are are identified [69, 77]. Civil War photography has garnered a lot of interest among historians, collectors, dealers, genealogists, archivists, and other experts, who often try to identify unknown photos for personal, cultural, and economic reasons. However, the identification process is complex and challenging, which often involves identifying visual clues in a photo and manually scanning through hundreds of low-resolution photos, military records, and reference books for corroborating evidence [31, 33, 38].\r\n3 ENHANCING CIVIL WAR PHOTO SLEUTH: DESIGN OPPORTUNITIES\r\n\r\nFigure 2: CWPS Haystack Model: Person Identification Pipeline [43]\r\nCivil War Photo Sleuth (CWPS) is a free, public website where users can identify unknown portraits from the American Civil War era using a person identification pipeline that combines crowdsourced human expertise and face recognition [43]. Drawing analogies to finding a needle in a haystack, Mohanty et al. propose a 'haystack model' to describe CWPS's person identification pipeline. In this pipeline, a user begins the identification process by first tagging a photo for uniform clues, which then generates search filters based on service records, and then facial recognition returns facially similar-looking results from a pool of potential candidates, ordered by similarity to the query photo, that satisfy the search filters (see Figure 2).\r\nThe CWPS haystack model is designed to prevent misidentifications by placing human decision-making at the forefront and treating AI as a supportive tool. It avoids automatically selecting the best match or displaying the algorithm's inconsistent confidence levels [41, 44]. Instead, the user carefully inspects search results for potential matches based on facial similarity and corresponding biographical details. Once a photo is identified, CWPS links the face and identity together and displays the ID on the photo page.\r\nDespite these measures, the open participation model of CWPS, which lacks verification, has raised concerns about the trustworthiness of proposed identities and the potential increase of \"false positives\" as the site grows [22]. To address these concerns, we enumerate three design goals which draw upon prior work on Civil War photo identifications and CWPS system designs, evaluations, and critiques [22, 30, 33, 35, 41, 43, 44], as well as our own observations and experiences using the publicly available version of the website. We provide further details in Appendix A.\r\nDesign Goal 1: Decouple facial similarity comparison from the overall task of person identification.. The current CWPS workflow conflates facial similarity and person identification into a single decision-making process (see Figure 2-C). Facial similarity, while important, can conflict with the identity suggested by personal details like biographical information and service records. The facial recognition algorithm's low precision [44] adds to the complexity, with the possibility of users interacting with false positives. In order to discourage over-reliance on facial similarity, we propose this design goal of separating both these tasks, allowing users to deliberate on the facial similarity and other person identification attributes separately, thereby minimizing inaccuracies.\r\nDesign Goal 2: Support fine-grained, deliberative decision-making.. With the current CWPS interface only permitting binary feedback during photo identification, there is a heightened risk of misrepresentation and misidentifications (see Figure 2-C). Historical photo identification is intricate, often demanding careful corroboration of numerous evidence pieces, with user confidence varying according to the evidence at hand [33]. To more effectively facilitate this complex process, we advocate for a design that enables users to express their level of certainty in their identification decisions. This design goal seeks to foster more accurate and nuanced user feedback, as well as reflective and deliberate decision-making [28, 29].\r\nDesign Goal 3: Encourage community contribution and transparency for validating photo IDs.. Identifying individuals in photos can be considered a subjective process and often benefits from multiple perspectives [41]. Currently, CWPS lacks mechanisms for community feedback and transparency about the process of photo identification, leaving potential misidentifications unchecked (see Figure 8 in Appendix). To support accurate original historical research, the platform should encourage community participation in validating identifications and promoting transparency around the roles of community contribution and facial recognition [18, 72]. This approach also encourages collective responsibility, facilitating stigmergic collaboration [13, 14], where user contributions guide future validation efforts.\r\n4 SYSTEM DESCRIPTION: PHOTO STEWARD\r\nWe developed Photo Steward, an information stewardship architecture that integrates a deliberative workflow for the community to validate historical photo identifications, which we then integrated into CWPS. Photo Steward's architecture has three main components (see Figure 1): 1) a deliberative decision-making interface for facial similarity comparison and photo identification, 2) new access points for validating photo identities, and 3) stewardship evidence for fostering stigmergic collaborations.\r\n4.1 DECISION-MAKING: Deliberating on facial similarity and photo identification\r\nAs part of Photo Steward, we introduce a multi-step \"Validation Interface\" (see Figures 3 and 4) to replace CWPS's single-step comparison interface. Photo Steward's validation interface allows users to deliberate while interacting with the facial recognition results. Meeting Design Goal 1, the validation workflow separates the task of facial similarity comparison from the overall goal of identifying the photo.\r\nTo inform our design, we draw on evidence-based decision-making [11], a model primarily used in healthcare, policymaking, and judicial sectors, which advocates for justifying decisions (photo IDs in this case) by gathering available evidence (facial similarity as visual evidence here). In the first step, the user compares the query photo to all other photos with the same identity for facial similarity. After deliberating on the facial similarity evidence, the user then votes on whether the query photo fits the target identity in the second step (which is the user's primary goal).\r\nThe validation interface is divided into four columns (from left to right): 1) the task description, 2) the query photo, 3) the evidence that is being weighed, and 4) the biographical information. The query photo and evidence are positioned in the two middle panels for easy side-by-side comparison. The task description panel displays the rating question for both the facial similarity comparison and the identification steps. Here, we used structured feedback to capture both the user's facial similarity comparison and their confidence on the photo ID, in an effort for encouraging users to exercise personal deliberation on all available evidence before making a decision on the ID. The interface updates the task description and evidence column depending on which task the user is performing.\r\nTo investigate the identity of a query photo, the user opens the validation interface which loads all the photos and biographical information available for the target identity.\r\n4.1.1 Validation Step 1: Facial Similarity Comparison. For the first step of the validation process, the interface displays the target photo in the evidence column next to the query photo for easy facial similarity comparison (see Figure 3).\r\n\r\nFigure 3: Step 1 of Photo Steward's Validation Interface. Users can compare two photos and answer whether they show the same person or not. They have the option of selecting whether the two photos are a facial match (i.e., same person, different views) or a replica (i.e., same person, same view). Here, the user is comparing whether Photo 41929 and Photo 1 show the same person or not. If multiple faces are available for the same ID, they appear one after the other in the order in which they were uploaded to CWPS.\r\n\r\nThe user's task is to determine whether both photos show the same person (regardless of whether the identity is known). Users can select from the following options: No (Different Person), Not Sure, Yes (Facial Match) and Yes (Replica). Mohanty et al. found that photos correctly identified on CWPS were either facial matches (i.e., same person, different view) or replicas (i.e., same person, same view) [43], which informed the design of this input scale. Since facial similarity does not have any standard scale and users may perceive the similarity or dissimilarity of two faces differently [40, 76], we chose not to capture any further granularity in their responses for facial match, replica or different person as this might lead to inconsistent data collection.\r\nCapturing these responses in a structured way allows users to deliberate on the task of facial similarity; this becomes more critical as users are also interacting with the results of a low-precision facial recognition algorithm [44]. In this step, the user compares facial similarity of the query photo with all available photos of the target identity, one photo at a time.\r\n4.1.2 Validation Step 2: Fine-Grained Photo Identification. In this step, the user analyzes the biographical information and incorporates the facial similarity evidence from the previous step to make a decision on the photo's identity.\r\nThe validation interface displays information in the same four-column layout (see Figure 4), with the evidence column now displaying a summary of the user's responses about facial similarity between the query photo and the target photo(s). The biography column shows the name and the service records for the user to analyze.\r\n\r\nFigure 4: Step 2 of Photo Steward's Validation Interface. Users vote on whether the query photo can be identified as the target identity by expressing their confidence. They can also add an optional note to justify their decision. The evidence panel displays a summary of the user's responses from the first step, where the faces are displayed next to the user's facial similarity comparison with the query photo. The faces are ordered in the way they appear for comparison, i.e., the order in which they were uploaded to CWPS.\r\n\r\nThe user now decides whether the query photo can be identified as the target identity (see Figure 4). The instruction above nudges the user to factor in the prior photo comparison evidence and the biography information. Meeting Design Goal 2, users indicate their confidence about the task question by selecting one of the five options displayed in radio buttons: No (Highly Confident), No (Slightly Confident), Not Sure, Yes (Slightly Confident) and Yes (Highly Confident). This scale, which offers more nuance than a binary decision, serves as a proxy measure for the ID's reliability given the complexities of quantifying accuracy in a historical photo ID investigation. These options reflect the varying degrees of confidence users have based on the quality and quantity of corroborating evidence, such as reputable sources, facial similarity to additional photos, and expert opinions. Users also have the option to elaborate their decision rationale in a free-text note.\r\n4.2 ACCESS: Expanding Validation Opportunities for Photo Identifications\r\nPhoto Steward provides stewardship capabilities for the CWPS user community by allowing them to access and use the validation interface at different stages of the photo identification process from multiple gateways (see Figure 1). On the \"Search Results\" page, it can be used for identifying a photo from a pool of potential similar-looking candidates, or ruling out some potential candidates. After a photo has been identified, users can also access it on the \"Photo Page\" to either validate an existing ID or dispute an incorrect one, and collaborate with other users in a stigmergic manner (Design Goal 3).\r\n4.2.1 Search Results Page: Matching and Ruling Out Candidates. While identifying a query photo, users can now inspect potential matches on the search results page with the help of the validation interface. The \"Compare\" button on a search result brings up the validation interface, loading all the target information for the corresponding search result. The target identities in the validation interface will update as the users check new search candidates for matches. The interface allows users to make two types of decisions, depending on their confidence response: 1) either of the \"Yes\" responses will match the photo with the target identity with varying degrees of confidence, and 2) either the \"No\" or \"Not Sure\" responses will rule out the search candidate as a potential match for the current user.\r\n4.2.2 Photo Page: Validating and Disputing Existing IDs. After a photo has been identified, Photo Steward allows users to review opinions from other users (described in Section 4.3) and contribute their own for a given photo ID on CWPS's photo page, fulfilling Design Goal 3. By clicking the \"Give Your Opinion\" button, users launch the validation interface featuring the query photo, the linked target identity, and target photos of the same ID, a new feature previously absent from CWPS. The two-step process mirrors that on the search results page, enabling community deliberation on the validity of an ID. Users can validate the facial match among photos linked to the same ID, express their agreement or disagreement on an ID with varying confidence levels, and optionally add a note explaining their decision. Thus, each vote contributes to a stigmergic collaboration, enhancing the reliability of photo IDs on CWPS. Consistent with CWPS's open participation model, Photo Steward allows any registered user to share their opinion on an identification.\r\n4.3 EVIDENCE: Visualizing Information Stewardship\r\nAs part of Photo Steward, we designed stewardship visualizations to help users assess the reliability of 1) facial matches (i.e., photos that were matched to each other by the user), and 2) photo IDs. The CWPS community's opinions on facial similarity comparison and photo IDs, captured through the validation interface, feeds into these reliability visualizations. These visualizations not only promote user accountability through social translucence [15], but also serve as deliberative evidence for subsequent stigmergic user collaborations (Design Goal 3).\r\n4.3.1 Reliability of Facial Similarity. For each photo pair that has been compared, the system aggregates the community's decisions for the visual match type and generates a distribution, which is displayed in the form of an interactive horizontal bar chart on the photo page. This chart appears next to the corresponding photo matched to the query photo (see Figure 5). Users can click the \"View Details\" button or an individual bar to see how each user voted. When multiple photos have been matched to the query photo, the matched photos appear one below the other, with each having its own visualization next to it. The bar charts are stacked vertically above each other to allow users to easily see and compare the reliability of every match.\r\nTo complement the community stewardship visualization, we also added an AI stewardship badge that indicates whether the particular match is supported by facial recognition (see Figure 5). On the search results page, CWPS retrieves those search results that have a facial similarity score greater than 0.50, so we use the same threshold here. However, the badge intentionally does not display the exact similarity scores (which have been found to be inconsistent [41, 44]) to avoid a false perception of precision, and cautions users to carefully analyze all the context and evidence, as there is a possibility of false positives with face recognition.\r\n\r\nFigure 5: Facial Similarity Reliability Visualization on the Photo Page. The photo matched to the query photo is displayed here, whereas the query photo is displayed on top of the page. Other photos matched to the query photo are displayed vertically one below the other. Users can bring up the query photo and the matched photo side-by-side by clicking the \"Compare\" button. Each matched photo has its own visualization next to it, and the responses are visible to everyone.\r\n\r\n4.3.2 Reliability of Photo Identifications. Similar to the facial similarity visualization, the system aggregates the community's confidence levels for an identification and generates a distribution, displayed in the form of an interactive horizontal bar chart. This visualization is displayed for every proposed identity on the photo page, below the biography subsection (see Figure 6a). If users propose multiple IDs for a given photo, they are displayed one below the other; each ID will have its own visualizations. The community's confidence votes are aggregated to order these IDs. Users can click the \"View Details\" button or the individual bars to view a modal dialog box with each user's confidence scores and optional text comments (see Figure 6b).\r\n\r\nFigure 6: Photo ID Reliability Visualization on the Photo Page for a given ID. If multiple IDs are present, they are displayed vertically one below the other, ordered in terms of aggregate votes. Each ID will have a separate visualization listed under the respective IDs.\r\n\r\nThis visualization reflects the community's expertise, and is intended for users to quickly assess the reliability of an identification. Prior work on crowdsourced person identification [41] suggests that airtight identifications are likely to show consensus from the community, whereas potential misidentifications are likely to reflect disagreement from the community. Further, the comments may reflect the voters' decision rationale and any external research they conducted before giving their decision on the photo's identification, allowing users to build on the work of others in making their own assessment [17].\r\n4.4 Summary\r\nPhoto Steward augments the CWPS platform with an information stewardship architecture to support community validation of historical photo IDs in a stigmergic manner. We summarize all the changes in the Appendix (see Table 6).\r\n5 EVALUATION\r\nWe obtained permission to publicly launch Photo Steward on CWPS in December 2020. We conducted a mixed-methods, exploratory evaluation study to understand how well users with different expertise levels could validate and assess Civil War photo IDs using CWPS with Photo Steward. Specifically, we wanted to understand 1) how users validated photo IDs using Photo Steward, 2) how the stewardship visualizations (i.e., ID and facial similarity reliability visualizations) impacted users' assessment of an ID. The study was approved by our university's IRB.\r\n5.1 Log Analysis\r\nTo understand the community's stewardship behaviors, we analyzed website logs of all user activities for a year after new features were launched, which included 5843 voting instances on 5672 photos for 5355 unique IDs. Our analysis included categorization of user deliberations as 'pre-identified' (i.e., the user knew the ID of the photo ahead of time and therefore, might be looking for a specific target on the search results page) or 'post-identified' (i.e., the user did not know the ID of the photo and therefore, may not have a specific target while analyzing the search results), coding of user comments (see Appendix C), and comparison of community's facial similarity comparisons against facial recognition scores. Details of these analysis methods are provided in Appendix B.\r\n5.2 Lab Study\r\nIn order to understand how well Photo Steward supports diverse users in validating the quality of photo identifications, we also conducted an exploratory lab study.\r\n5.2.1 Participants. We recruited 15 participants representing the three major expertise levels: 5 history students, 5 amateur experts (experienced users of CWPS), and 5 expert historians. Participant details can be found in Appendix D. We anonymize these groups with the following identifiers, respectively: S1–S5, C1—C5, and H1–H5.\r\n5.2.2 Dataset. For the study, we created a dataset of 10 different photos identified on CWPS. Three of these photos had an ID conflict, i.e., multiple identities were proposed. For two of these photos, one ID was correct and the other one was incorrect. The community had already researched both photos, voted on the correct ID, and left credible evidence in the comments. Both IDs were linked to additional photos as well. The third photo was one of the seeded photos on CWPS, but was originally misidentified. We added another false ID, making both IDs for the third photo incorrect. All photos had multiple photos matched to them; eight of them were linked via facial matches, while two of them had replicas.\r\n5.2.3 Procedure. The entire study was conducted online via recorded Zoom sessions, with at least one researcher attending each session. Each participant first completed a consent form and a pre-survey describing their demographics and Civil War photography experience.\r\nAs part of the study, participants reviewed three randomly assigned photos from the dataset one-by-one in the original CWPS system first, followed by the same photos on the Photo Steward version. Participants used a think-aloud protocol while using the two systems; after the completion of the task with each system, they were asked a few semi-structured questions about their experience. Finally, the participants completed a summative post-survey of standard usability questions (e.g., ease of use, usefulness of features, instruction clarity, preferred system, etc.) (see Appendix E)\r\nWe maintained this sequence (original CWPS first, CWPS with Photo Steward second) for all the participants, rather than using a randomized sequence, for two reasons. First, we did not want participants' assessments to be biased in favor of Photo Steward after seeing additional features in the new interface. This design allowed us to observe if the original interface misled the participants towards incorrect assessments, and if, subsequently, the Photo Steward interface helped correct them. Second, in a randomized sequence, Photo Steward would expose the participants to new information in the form of prior user votes and responses, and therefore, may confound how they assess the information on the original CWPS version.\r\n5.2.4 Data Analysis. The first author fully transcribed and analyzed the interviews and think-aloud recordings using an inductive qualitative thematic approach [4]. The transcript sections were first divided according to the interface in question (i.e., original CWPS or Photo Steward), followed by an open coding of the transcripts using MAXQDA 2020 [65]. The first author iterated and settled on a total of 28 codes (e.g., change in opinions, comparison interface, source trustworthiness, etc.) for 634 coded segments across all the transcripts. These codes were then organized into themes as described in Section 6 after discussing with the co-author.\r\n5.2.5 Limitations. We conducted a qualitative lab study to understand how users with different backgrounds and expertises validated photo IDs using Photo Steward and hit theoretical saturation. However, there are a couple of limitations with the study: 1) limited insights on the role of expertise, and 2) the task sequence could have order effects. Further, the large-scale analysis of Photo Steward logs provided us with insights of its usage amongst users. However, it lacked an expert-prepared gold standard dataset, which hindered our ability to conduct specific performance analyses as part of this study.\r\n6 FINDINGS\r\nUsing the methods above, we evaluated how well Photo Steward's stewardship architecture supported CWPS users in validating photo identifications, compared to the original version of CWPS.\r\n6.1 Validation Interface\r\nUsers found Photo Steward's validation interface to be useful for comparing different photos.. While assessing the IDs with the original interface, participants would go back and forth between different photos to compare whether they are the same person or not. Some participants opened the photos in two different browser windows and kept them side-by-side. While using the validation interface in the new system, participants appreciated being able to see the photos side-by-side at the same time.\r\nH1 said, \"As an historian using this, this is really great to see them both together. It just makes a comparison a lot easy for me to do. I mean, this is the same gentleman, he's got a little dark facial hair. It looks a little bit different there and the photo on the right, but the facial match is definitely there. \" This was also echoed by C3, who said, \"This, I really find extremely useful, especially when I'm trying to do facial recognition. I can zoom in and have them side by side here. [...] Where in the past, I would have to go back and forth between tabs or cut and paste them into a different document to look at them side by side.\"\r\nFrom our logs, we found that 223 users had compared 2319 unique photo pairs for facial similarity, with 156 pairs receiving comparisons from at least 2 different users. The facial similarity responses were distributed as follows: 763 replicas, 1232 facial matches, 283 unsure, and 280 different people.\r\nTable 1: Distribution of User Votes.\r\n\r\nVotes\r\n/ ID\r\n# of IDs\r\nNote\r\nPresent\r\nNegative\r\nVotes\r\n1\r\n5650\r\n511\r\n61\r\n2\r\n157\r\n(Agreement: 119)\r\n(Disagreement: 38)\r\n83\r\n12\r\n3+\r\n36\r\n(Agreement: 21)\r\n(Disagreement: 15)\r\n31\r\n4\r\nTable 2: Distribution of Confidence Levels.\r\n\r\nMean\r\nConfidence\r\n1\r\nVote / ID\r\n2\r\nVotes / ID\r\n3+\r\nVotes / ID\r\n-2 (No - Highly Confident)\r\nto\r\n-1 (No - Slightly Confident)\r\n45\r\n3\r\n2\r\n-1 (No - Slightly Confident)\r\nto\r\n0 (Not Sure)\r\n16\r\n3\r\n2\r\n0 (Not Sure)\r\n99\r\n8\r\n4\r\n0 (Not Sure)\r\nto\r\n1 (Yes - Slightly Confident)\r\n760\r\n15\r\n5\r\n1 (Yes - Slightly Confident)\r\nto\r\n2 (Yes - Highly Confident)\r\n4730\r\n128\r\n23\r\nUsers preferred the ability to provide granular feedback for photo IDs using Photo Steward's validation interface.. All participants expressed preference for the fine-grained confidence levels, including the ability to dispute an ID, in Photo Steward's validation interface, appreciating how it more accurately mirrored the inherent uncertainty present when assessing photo IDs. S3 said, \"I definitely like the five levels. I think it leaves more room for interpretation. Like sometimes it's kind of hard to just say yes or a hard no because so much goes into it. Especially because a lot of this stuff was so long ago, there's so many unanswered questions.\" H4 initially defended the original interface's binary vote, but changed her mind after experiencing Photo Steward's confidence levels: \"[W]hat I had said has this very black and white feel to it, you're wrong or you're right. I like these degrees of disagreement or agreement. I think that's way more helpful broadly.\"\r\nThe usage of the voting feature was reflected in our logs, which showed 5843 voting instances from 328 unique users (see Table 1). Table 2 shows that while users utilized the full range of confidence levels, including when they were unsure (mean confidence = 0) or slightly confident (mean confidence = 0 to 1) about the ID, the vast majority of the votes were highly confident ones (mean confidence = 1 to 2). A small proportion of votes (77) were cast for disputing an ID.\r\nUsers justified their voting decisions through notes covering an extensive range of topics..\r\nTable 3: Distribution of Note Topics. The table also displays how the notes are distributed for different user confidence levels.\r\n\r\nCategory\r\nSub-Category\r\nTotal\r\nNumber of\r\nNotes\r\nNo\r\n(Highly\r\nConfident)\r\nNo\r\n(Slightly\r\nConfident)\r\nNot\r\nSure\r\nYes\r\n(Slightly\r\nConfident)\r\nYes\r\n(Highly\r\nConfident)\r\nPhoto\r\nComparison\r\nHigh-Level\r\nComparison\r\n206\r\n9\r\n5\r\n9\r\n20\r\n163\r\n\r\nDescribing\r\nFacial Features\r\n37\r\n4\r\n3\r\n9\r\n8\r\n13\r\nWord-of-Mouth\r\nDescendant\r\n55\r\n1\r\n0\r\n0\r\n8\r\n46\r\n\r\nOwnership\r\n47\r\n1\r\n0\r\n0\r\n0\r\n46\r\n\r\nFamiliarity\r\n18\r\n0\r\n0\r\n0\r\n2\r\n16\r\nVisual\r\nEvidence\r\nUniform\r\n73\r\n7\r\n1\r\n8\r\n17\r\n40\r\n\r\nInscription\r\n145\r\n4\r\n3\r\n3\r\n6\r\n129\r\n\r\nOther\r\nVisual Clues\r\n17\r\n1\r\n0\r\n1\r\n2\r\n13\r\nExternal\r\nInformation\r\nExternal URL\r\n47\r\n1\r\n0\r\n1\r\n4\r\n41\r\n\r\nOther Sources\r\n172\r\n9\r\n1\r\n1\r\n21\r\n140\r\n\r\nLack of\r\nInformation\r\n21\r\n0\r\n2\r\n13\r\n6\r\n0\r\nProviding\r\nAdditional\r\nInformation\r\nBiographical\r\nInformation\r\n46\r\n11\r\n2\r\n3\r\n3\r\n27\r\n\r\nAdditional\r\nContext\r\n117\r\n5\r\n0\r\n2\r\n11\r\n99\r\nFrom our logs, we found that 155 users had left 682 notes for 600 different photos. However, as Table 1 shows, around 10% of the votes had a note. Table 3 shows the different topics covered by the notes. We observe that users' voting patterns are significantly influenced by the availability and quality of evidence, with clear facial similarity, period inscriptions, personal anecdotes, visible clues in the photograph, and added biographical context often leading to high confidence \"Yes\" votes, while lack of information typically results in \"Not Sure\" votes.\r\nUsers most frequently left a comment attributing facial similarity (after comparing it in the first step of the validation process) to be the reason for their decision (e.g., \"Identical to the other CDV\"). In some instances, they would expound on it by discussing facial features: \"The eyes, nose, cheek bones, shape of face, all look similar to George Pickett, although possibly reversed based on hair part\". In many instances, we found users inferring biographical information (service records, location, etc.) from visual evidence in the photo, be it uniform or backmarks (e.g., \"Initials MN on chinstrap (brass letters). Signature on verso is made out to Marlin's oldest sister. Style of insignia is consistent with other 1862 recruits for Co. B 1st USSS\").\r\nInterestingly, we also observed a large number of word-of-mouth evidence notes for justifying the user's decisions, such as claiming to be a descendant, or owning the original copy of the photo, or having seen the photo somewhere. For example, one user noted, \"He is my great-great grandfather and this photo has been passed down through the generations to me and was identified by his son John Albert Johnson, my father's grandfather.\" Users also left external URLs and source details in the notes as evidence. Sometimes, they provided additional context (e.g., \"This image came with a group of 7th Iowa images. The majority were of Company G., but there is only one person in the entire 7th Iowa Infantry that could be identified by the first or last name of 'Nelson.'\").\r\nThe validation workflow encouraged users to exercise careful deliberation while making photo ID decisions.. Users felt that the questions in each step of the validation process helped them to carefully weigh in all the evidence and deliberate while voting on the ID (Q4, mean = 4.60, SD = 0.49). C3 explained why the two steps were necessary: \"It's two separate things. One is asking, do you think that this face is the same face? Then the second is, do you think that this face matches this name? I think that that is a necessary question for both of those scenarios. I don't think it's redundant, I think it's necessary.\" H2 appreciated the thoughtfulness that the two-step process encouraged, saying, \"It could be the same guy, but it might be a different guy, but now that you know the other interface kind of forces me to slow down a little bit and think more carefully, because it's asking specific questions about things.\" A couple of participants, however, expressed initial confusion over the separation and order of these two tasks.\r\n\r\nFigure 7: Deliberation in case of post-identification voting. The table shows the distribution of confidence votes (second step of the validation process) against the user's facial similarity comparison (first step of the validation process). The comparison scores were computed by aggregating all the facial similarity comparisons in the first step (replica = 2, facial match = 1, not sure = 0, and different people = -1).\r\n\r\nOur logs affirm the deliberative aspect of the two-step validation process. We observed that when users compared one or more photos in the first step, they factored in the facial similarity evidence for their ID vote decision. Figure 7 shows that when the cumulative comparison scores are positive (i.e., majority of the comparisons were a facial match or a replica), the confidence scores are generally positive. Similarly, when the cumulative comparison scores are negative, the confidence votes are also largely negative. When the users are unsure about the facial similarity comparison, it reflects in their final ID vote. Notably, when there were mismatches between comparison scores and ID votes, users provided reasons in the notes, citing visual evidence and an unsure photo comparison.\r\nPhoto Steward's stewardship architecture fostered diverse forms of meaningful stigmergic collaboration amongst users.. Although most IDs received only a single vote, about 3% (193 IDs) received multiple votes (see Table 1). Among these, 140 IDs saw total consensus among voters on confidence scores, with half involving an explanatory note. When the initial vote lacked a note, subsequent voters often added information like external sources or context. Almost all IDs where voters agreed positively were supported by metadata such as period inscriptions, scholarly sources, or additional comparison photos, underscoring the credibility of these agreements.\r\nWhen users disagreed on an ID, they would almost always justify in notes why they differed from the previous voters' opinions (43 out of 53 disagreement instances). From analyzing the notes, we found that the subsequent voters provided additional information about external sources or service records to disagree (e.g., \"Please see Military Images magazine, MI, Volume XVI, Number 3, November - December 1994 for the correct identification of this image. The correct identification via the Michael McAfee collection is Richard Cramer, 4th Michigan Infantry\"). Among 18 instances of conflicting IDs for the same photo, seven saw resolution through a process of voting one ID over another, often accompanied by external evidence in notes or citing facial similarity.\r\nUser agreement was particularly strong in facial similarity comparisons. Of the 2200 unique photo pairs compared, 141 received multiple user reviews, with 114 seeing total agreement. The remaining cases typically involved user errors in distinguishing between 'facial match' and 'replica', or uncertainty about facial similarity.\r\n6.2 Stewardship Visualizations\r\nStewardship visualizations reflecting community insights enhanced the accuracy of photo ID assessments.. Participants found Photo Steward's bar chart visualization showing the five confidence levels to be simple and easy to understand (Q1, mean = 4.93, SD = 0.25). S1 said, \"I'm a visual person. Bar graphs or charts like statistical data helps me put things into a better perspective or gives me an idea of what I am working with versus just something more plain [like] the other interface.\"\r\nWhen participants were uncertain about an ID, they saw the additional evidence and justification that the community members had provided along with their vote as essential to taking the vote seriously. S5 said, \"If there's other users giving comments like 'I've used this source,' and you know you get information from a source where they found it, I'm probably gonna agree with them. If they're just voting yes without anything else, then I am probably more likely to go out and find sources for the photo myself and make my own decision.\"\r\nIn three instances, students (S1, S3, and S5) initially made incorrect assessments on the original interface but rectified these after using Photo Steward. The community's consensus and evidence in Photo Steward were pivotal in these corrections. S5, confronted with ID conflicts, stated after using Photo Steward: \"You know that helps a lot with what people are thinking and presumably these people have also gone to the Maine State Archives or something and verified and then given their opinion so that helps.\" Similarly, historian H2 and collector C3 initially made incorrect choices but amended their decisions after analyzing community-based evidence on Photo Steward. H2 remarked: \"So I believe it's the first person, of course, because it's based on the Maine State Archives. And then you've got the piece on there that said that you looked and found a different man. With that name in the actual regimental history. I say that's fairly accurate information.\" This highlights the critical role of community consensus in resolving photo ID conflicts.\r\nOn the other hand, when participants were confident about an ID, seeing the community's opinions affirmed their own assessments, for better or worse. In the words of H5: \"I really liked the very clear community consensus, and the ability to be able to see the identities of the people who were looking at these images. It was something that (gave) a boost of confidence in terms of my final decision.\" Overall, the participants found the community opinions to be useful for assessing the IDs (Q2, mean = 4.93, SD = 0.25).\r\nUsers gave additional weight to the opinions of members they were familiar with and desired more contextual information about all contributors.. Participants examined the bar chart visualization details and gave higher weight to the opinions of prominent names from the Civil War photography community while assessing the ID. C3 said, \"Here's <name redacted>, period inscription with valediction, the uniform matches his service record. <name redacted> said the same thing. These two uploaders I hold in very high regard to their opinions on this site. If they're saying that they think highly confident of this identification, that gives me a lot of confidence as well.\" H5 became further unsure about an ID after seeing a fellow historian's vote: \"And, you know, sort of knowing <name redacted>, someone who I know deals with primary source material a lot, and sort of being on the fence with it as well, leaves me in that unsure position.\"\r\nParticipants also sought additional details about community members, proposing indicators of professional status or active participation in the CWPS community. H1 emphasized his appreciation of amateur experts and opposition to gatekeeping, yet he suggested having some kind of credential indicator next to the username would be helpful: \"It will be great if, you know, somebody was an academic historian or a published Civil War author, if there was some way to just say some little tidbit next to <name redacted> 'Oh, saying, hey, I'm from <university redacted>, Professor or, you know, author of whatever.'\" Others preferred indicators of community activity levels or personal connections (i.e., descendants) to the identified individual in the photo.\r\nOur log analysis showed revealed the presence of certain active voters, with 9 out of 328 unique voters voting on more than 50 photos each, and 2 voting on over 2000 photos each (mostly ones that they had uploaded and identified). If we consider only votes on photos identified by someone else, we have 8 users who have voted on more than 15 IDs each, with 1 user voting on over 150 IDs.\r\nThe community opinions made the platform feel more engaging, but users had mixed thoughts about the ideal number of votes per photo.. Participants, in general, felt the community opinions made Photo Steward more engaging compared to the original system. H5 said, \"I thought it appeared more user-centric and user-friendly and it felt more participatory.\" C3 saw the benefits of increased engagement for making IDs more reliable: \"I think for this crowdsourcing project that we're building on this database [it] is very important to have those comments, those feedback, that we see in the voting system. It only makes this ID stronger and makes the project and the database a more trustworthy and reliable source.\"\r\nUsers wanted to see more community opinions and comments for IDs that had few votes. S1, on seeing only one vote for an ID, said, \"That doesn't make me feel as confident because that's not too many for me to give an answer.\" S3 said it would have been easier to assess some photos if \"there was more input from other people.\" Users had mixed opinions about how many votes they wanted to see for verifying an ID. In general, they wanted to see consensus among the community for an identity and at least three votes. C3 said, \"I usually shoot for like three to four [votes] as the lowest where I take some good quality out of those votes. [...] If there's only one or two, and especially if there's two that are split, that is not as reliable to me.\"\r\nParticipants' concerns about spreading voters too thin were borne out in the log data (see Table 1). We analyzed the logs to check how often CWPS users vote on the photos they are browsing. We found 1784 instances (out of 5843 voting instances) where the number of \"lurkers\" for a given photo page exceeded the number of voters on that photo.\r\nTable 4: Face Recognition Similarity Score Stats vs. User Comparisons (Facial Similarity).\r\n\r\nUser Comparison\r\nMean\r\nMedian\r\nSD\r\nCount\r\nReplica\r\n0.88\r\n0.91\r\n0.16\r\n659\r\nFacial Match\r\n0.52\r\n0.65\r\n0.31\r\n1015\r\nDifferent Person\r\n0.55\r\n0.59\r\n0.16\r\n265\r\nNot Sure\r\n0.51\r\n0.58\r\n0.23\r\n261\r\nUsers found the face recognition badge and community's opinions complementary for assessing the reliability of matched photos..\r\nTable 5: User Comparisons (Facial Similarity) vs. Face Recognition.\r\n\r\n\r\n# of Photo Pairs\r\n# of Photo Pairs\r\ncompared by 1 user\r\n# of Photo Pairs\r\ncompared by 2 users\r\n# of Photo Pairs\r\ncompared by 2+ users\r\nSupported by users\r\nand face recognition\r\n1408\r\n1281\r\n109\r\n18\r\nDisputed by users\r\nand face recognition\r\n26\r\n24\r\n0\r\n1\r\nUsers unsure,\r\nface recognition disputes\r\n42\r\n42\r\n0\r\n0\r\nUsers unsure,\r\nface recognition supports\r\n219\r\n217\r\n2\r\n0\r\nUsers dispute,\r\nface recognition supports\r\n239\r\n238\r\n1\r\n0\r\nUsers support,\r\nface recognition disputes\r\n266\r\n257\r\n8\r\n1\r\nFrom our logs, we found that 1408 photo pair comparisons were supported by both users and facial recognition, by far the most common outcome (see Table 5). This information would be visible to the larger user community in the form of stewardship visualizations (see Figure 5). Interestingly, we also see 266 cases where a comparison is supported by users, but disputed by facial recognition (similarity confidence score < 0.50). That outcome was approximately as common as when users disputed a comparison but face recognition supported it (239 pairs) or when a user was unsure but face recognition supported it (219 pairs). However, it was far less common for facial recognition to dispute a comparison when a user also disputed it (26 pairs) or was unsure (42 pairs).\r\nWhen we analyze the face recognition's confidence scores in more detail, Table 4 shows that there is a clear separation between replicas and other types of user comparison scores. Face recognition confidence scores for photo pairs that users labeled as replicas were much higher in terms of both mean and median (0.88 and 0.91, respectively, versus scores in the 0.50s and 0.60s for all others). While these fine-grained scores are not displayed to users — they intentionally see only the face recognition badge — the very close mean and median confidence scores for \"facial match\" versus \"different person\" illustrate the difficulty of automatically identifying non-replica matches and offer support for a hybrid human-AI approach (cf. Section 4.3.1).\r\nParticipants found the community's opinions to be helpful for assessing whether two photos were facial matches or not. S4 said, \"I mean, I think it's cool to see what the community is saying, because I do feel when it comes to saying 'Is this the same person in both these pictures?', that's really the best way to do it if you don't have any [other] information.\" C1 said that he found the community opinions for facial matches especially helpful because he is \"face blind\": \"I can't really identify the face-to-face, but the hair and the mustache and all the stuff that, in addition, it helps me with that for sure. It's good because other than having to find somebody close to me and be like, 'Do you think this is the same people?' [I] have that community right there.\"\r\nParticipants had mixed opinions about facial recognition technology, but most found the badge indicating whether it supported the two photos being a match to be a useful data point. S3 said, \"The facial recognition saying they are similar — I would go ahead and trust that but I don't know if I would trust it enough to make a verification on my own.\" In general, participants found the strengths of facial recognition and the community to be complementary in determining whether two photos showed the same person, and liked seeing both results together. H4 felt the community and the technology had separate roles:\r\nThis is facial recognition, and this is the historical background. I trust facial recognition, but it makes me feel better to have that historical background. I think the human eye can be tricked by different hairstyles and different beards. Just to have this outside historical verification to say like, 'Okay, maybe you or I was tricked, but the machine was not,' I think that's really helpful.\r\nS1 relied on both the community and facial recognition to make a decision on a facial match: \"I think both give me kind of an idea. Okay, there's this facial recognition technology being used, but also there's other users that are leaning towards that this is the same person.\" H5 got a similar boost of confidence: \"This is certainly reassuring seeing not only the AI match, but also in terms of the community — seeing that seven users have said that this is a facial match as well. I'd be quite convinced by this.\"\r\n7 DISCUSSION\r\n7.1 Leveraging Collective Intelligence for Validating Person Identification\r\nPrior work has raised concerns about misinformation in online history communities [43, 75]. To address these problems on CWPS, we built Photo Steward for supporting community-based validation of photo IDs. Users found Photo Steward's stewardship visualizations not only helpful for affirming their own assessment, but also for discovering new knowledge and correcting their decisions, if need be. These visualizations, combined with the validation workflow, exhibit a form of stigmergic collaboration, where users build on prior knowledge left by the community and leave their own assessment for other users [14, 17, 27, 56].\r\nPhoto Steward allowed users to express how confident they are about an ID in a fine-grained manner, in contrast to the binary agreements or disagreements observed on iNaturalist by Wiggins et al. [72]. The CWPS community preferred this nuanced form of stewardship as users are likely to have different degrees of confidence based on the evidence available for identifying a photo, thus demonstrating the effectiveness of Design Goal 2. As S3 pointed out, users often experience difficulty in making a binary decision about individuals who lived 150 years back due to the lack of surviving documentation.\r\nBeyond its basic usefulness, Photo Steward's full potential can best be realized through sustained community participation, but most IDs on CWPS only received one vote. To address this challenge, we can leverage different crowdsourcing and online community strategies. For example, we can draw the community's attention towards IDs that are \"more of a puzzle,\" as H5 suggested, similar to Twitter's Birdwatch promoting tweets for fact-checking [52]. Designing nudges to encourage lurkers to vote on the IDs they are viewing can further help in these efforts. Organizing community events can help foster interest and participation in collaboratively verifying IDs, drawing inspiration from crowdsourcing events like CrowdSolve, where experts and novices collaborate on solving missing persons cold cases [70]. Incentive mechanisms such as leaderboards and challenges [46] can drive extrinsic motivation within the community for verifying the IDs. Finding users who are more likely to vote on an ID, based on their skills and interests, can also be an effective collaboration strategy [71]. In future work, we plan to integrate these strategies and introduce explicit \"calls to action\" [51, 58] on the home feed, guiding the community's attention towards IDs that require validation and fostering more sustainable, collaborative participation in historical photo identification.\r\n7.2 Exercising Deliberation in Human-AI Teams\r\nWe found that Photo Steward's validation workflow was effective not only for voting on the IDs and comparing the photos side-by-side, but also encouraged users to deliberate on their decision, drawing parallels to other social computing systems that support reflection and deliberation (e.g., [28, 29]). This deliberative intervention was non-trivial as users on CWPS follow an identification pipeline which is powered by facial recognition, an AI algorithm that is far from perfect [44]. Users are trying to find the correct match, if present at all, from a pool of potential candidates, which are largely comprised of similar-looking false positives — akin to finding a needle in a haystack. Further, the task of comparing photos of people is by no means an easy task for humans, even in a modern context [53]. While Photo Steward can not completely curb automation bias, an issue that has been previously observed in multiple online history communities [44, 75], its multi-step, validation workflow with structured feedback interventions encouraged users to deliberate over AI suggestions before making a decision.\r\nPhoto Steward's workflow also compartmentalizes the tasks that AI is good at – such as quickly retrieving similar-looking candidates from a large search pool – from the tasks where the AI makes more errors – such as verifying whether two faces show the same person or not [5, 55]. Decoupling facial similarity comparison from the person identification task (Design Goal 1) allows the users to now focus on the face verification task. In doing so, Photo Steward's workflow supports effective human-AI teaming in the context of person identification by allowing the user to make a granular assessment for the face verification task instead of the AI, while also ensuring that an AI-retrieved, similar-looking potential candidate is being compared against.\r\nAs imperfect AI algorithms get deployed in high-stakes scenarios such as medical imaging, law enforcement, etc. [6], it becomes more critical to reduce automation bias and encourage more deliberative decision-making. Amershi et al. recommend granular user feedback while interacting with AI systems as part of their \"Guidelines for Human-AI Interaction\" [2]. Similarly, other forms of design interventions, such as counterfactual AI explanations [63], chatbots [26], and community opinions [59] can also be explored for encouraging deliberative decision-making with AI assistance.\r\nPrior work in human-face recognition teams has shown that algorithmic suggestions can have a significant biasing effect on a user's decision [23]. Our findings showed that Photo Steward was able to encourage users to exercise deliberation while interacting with results retrieved by facial recognition. At the same time, Table 4 also showed that users can differ from the algorithm's suggestions, thus necessitating a deeper dive analysis of this dissonance as part of future work.\r\n7.3 Assessing Quality in Crowdsourced Original Historical Research\r\nWe found that Photo Steward's stewardship visualizations helped users assess the reliability of photo IDs on CWPS, which were a result of Design Goal 3. However, crowdsourced identifications always run the risk of groupthink [24, 25], which can eventually mislead users into believing and amplifying misidentifications, a concern also raised by H4. Public deliberation of modern photo IDs on social media can have profound negative consequences for false targets, as exemplified by the Boston Marathon bombing [49, 66] and the recent US Capitol riot [48]. This raises the question: are Photo Steward's stewardship visualizations sufficient for assessing the quality of photo identifications made on CWPS?\r\nPrior work on crowdsourced scholarship suggests an answer. Rosenzweig [57] analyzed Wikipedia as a source of historical scholarship, noting its policy against original research, and advocating for it as a tool for teaching the limitations of information sources and critical analysis of primary and secondary sources. Motivated along similar lines, Forte et al [18] proposed the assessability framework for designing assessable participatory information systems, based on information provenance and stewardship. The concept of provenance, extensively used in history and archival studies, describes information that makes it possible to trace the ownership or origins of the content, while stewardship refers to the processes that were used for maintaining the content, including its authorship. In the case of Wikipedia, Forte et al. found that visualizing provenance (i.e., citation types) and stewardship (i.e., article quality) had a significant impact on assessments of articles and Wikipedia as an information source.\r\nWhile Photo Steward enables information stewardship on CWPS, there is an opportunity for incorporating provenance into the CWPS platform to make it a truly assessable online platform. A significant proportion of the notes left by users on Photo Steward qualified as provenance information, namely comments about period inscriptions, family trees, external sources and URLs. The challenges of assessing IDs on CWPS are, however, different from assessing information on Wikipedia, primarily because CWPS supports original research unlike Wikipedia's no original research policy [73]. This was also the reason why we designed Photo Steward to be a review system rather than a single editable output such as Wikipedia; original research such as historical photo identifications is often times an evolving investigation rather than a final decision. To assess the reliability of original photo IDs made on CWPS, users may want to factor in the provenance of the reference photos that were used in the identification process. In such cases, Photo Steward's stewardship visualizations (i.e., facial similarity reliability) can further help the user in assessing whether the reference photos can be used as reliable provenance or not.\r\n8 CONCLUSION\r\nPhoto Steward attempts to help users assess and validate photo IDs better on CWPS. We present an information stewardship architecture, and adapt it for the task of historical person identification. We demonstrate the effectiveness of Photo Steward on CWPS, an existing online platform, where users found the stewardship visualizations, which included the community opinions and the AI verdict, useful for making accurate assessments of photo IDs on the platform. Further, users found Photo Sleuth's multi-step, structured validation workflow to help them deliberate before making decisions about the photo's identity. This work opens doors for exploring new ways to leverage collective intelligence and AI in creating assessable online information systems for historical archives.\r\nACKNOWLEDGMENTS\r\nWe wish to thank Ron Coddington, Paul Quigley, Liling Yuan, and our study participants. This research was supported by NSF IIS-1651969 and a Virginia Tech ICTAS Junior Faculty Award.\r\nREFERENCES\r\n2021. Heritage Auctions: World's Largest Collectibles Auctioneer. https://www.ha.com/ Navigate tocitation 1citation 2\r\nSaleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. 2019. Guidelines for human-AI interaction. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–13. Navigate tocitation 1\r\nBrakkton Booker. 2015. $2 photo found at Junk Store has Billy the kid in it, could be worth $5M. https://www.npr.org/sections/thetwo-way/2015/10/15/448993361/-2-photo-found-at-junk-store-has-billy-the-kid-in-it-could-be-worth-5-million Navigate tocitation 1citation 2\r\nVirginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3, 2 (2006), 77–101. Navigate tocitation 1\r\nJoy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Conference on Fairness, Accountability and Transparency, FAT 2018, 23-24 February 2018, New York, NY, USA. 77–91. http://proceedings.mlr.press/v81/buolamwini18a.html Navigate tocitation 1\r\nCarrie J Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg S Corrado, Martin C Stumpe, et al. 2019. Human-centered tools for coping with imperfect algorithms during medical decision-making. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–14. Navigate tocitation 1\r\nFanny Chevalier, Stéphane Huot, and Jean-Daniel Fekete. 2010. Wikipediaviz: Conveying article quality for casual wikipedia readers. In 2010 IEEE Pacific Visualization Symposium (PacificVis). IEEE, 49–56. Navigate tocitation 1\r\nH Jason Combs. 2005. The Internet's Impact on the Antiques Trade. Material Culture (2005), 26–41. Navigate tocitation 1citation 2\r\nAnthony DeBartolo. 1975. Appraisers can tell when old photo really may be golden. https://www.chicagotribune.com/news/ct-xpm-1985-12-27-8503300067-story.html Navigate tocitation 1\r\nAnthony DeBartolo. 2021. Appraisers can tell when old photo really may be golden. https://www.chicagotribune.com/news/ct-xpm-1985-12-27-8503300067-story.html Navigate tocitation 1\r\nHarley D Dickinson. 1998. Evidence-based decision-making: an argumentative approach. International Journal of Medical Informatics 51, 2-3 (1998), 71–81. Navigate tocitation 1\r\nFor The Inquirer Edward Colimore. 2019. Did John Wilkes Booth get away with murdering President Abraham Lincoln?https://www.inquirer.com/news/john-wilkes-booth-lincoln-conspiracy-photo-recognition-20190415.html Navigate tocitation 1\r\nMark Elliott. 2006. Stigmergic Collaboration: The Evolution of Group Work: Introduction. m/c journal 9, 2 (2006). Navigate tocitation 1citation 2citation 3\r\nMark Elliott. 2016. Stigmergic collaboration: A framework for understanding and designing mass collaboration. In Mass collaboration and education. Springer, 65–84. Navigate tocitation 1citation 2citation 3\r\nThomas Erickson and Wendy A Kellogg. 2000. Social translucence: an approach to designing systems that support social processes. ACM transactions on computer-human interaction (TOCHI) 7, 1 (2000), 59–83. Navigate tocitation 1\r\nDan Evon. 2020. Did Joe Biden's Great-Grandfather Own Slaves?https://www.snopes.com/fact-check/joe-biden-slaves-great-grandfather/ Navigate tocitation 1\r\nKristie Fisher, Scott Counts, and Aniket Kittur. 2012. Distributed sensemaking: improving sensemaking by leveraging the efforts of previous users. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 247–256. Navigate tocitation 1citation 2\r\nAndrea Forte, Nazanin Andalibi, Thomas Park, and Heather Willever-Farr. 2014. Designing information savvy societies: an introduction to assessability. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2471–2480. Navigate tocitation 1citation 2citation 3citation 4\r\nJacey Fortin. 2018. She Was the Only Woman in a Photo of 38 Scientists, and Now She's Been Identified. The New York Times (Mar 2018). https://www.nytimes.com/2018/03/19/us/twitter-mystery-photo.html Navigate tocitation 1\r\nSarah A Gilbert. 2020. \" I run the world's largest historical outreach project and it's on a cesspool of a website.\" Moderating a Public Scholarship Site on Reddit: A Case Study of r/AskHistorians. Proceedings of the ACM on Human-Computer Interaction 4, CSCW1 (2020), 1–27. Navigate tocitation 1\r\nJerome S Handler and Michael L Tuite. 2007. Retouching History: The Modern Falsification of a Civil War Photograph. Navigate tocitation 1citation 2\r\nM. Keith Harris. 2019. Civil War Photo Sleuth. Journal of American History 106, 2 (2019), 544–546. https://doi.org/10.1093/jahist/jaz498 Navigate tocitation 1citation 2\r\nJohn J Howard, Laura R Rabbitt, and Yevgeniy B Sirotin. 2020. Human-algorithm teaming in face recognition: How algorithm outcomes cognitively bias human decision-making. Plos one 15, 8 (2020), e0237855. Navigate tocitation 1\r\nPan Hui and Sonja Buchegger. 2009. Groupthink and peer pressure: Social influence in online social network groups. In 2009 International Conference on Advances in Social Network Analysis and Mining. IEEE, 53–59. Navigate tocitation 1\r\nNassim JafariNaimi and Eric M Meyers. 2015. Collective intelligence or group think? Engaging participation patterns in World Without Oil. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing. 1872–1881. Navigate tocitation 1\r\nSoomin Kim, Jinsu Eun, Joseph Seering, and Joonhwan Lee. 2021. Moderator chatbot for deliberative discussion: Effects of discussion structure and discussant facilitation. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–26. Navigate tocitation 1\r\nAniket Kittur, Andrew M Peters, Abdigani Diriye, and Michael Bove. 2014. Standing on the schemas of giants: socially augmented information foraging. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. 999–1010. Navigate tocitation 1\r\nTravis Kriplean, Caitlin Bonnar, Alan Borning, Bo Kinney, and Brian Gill. 2014. Integrating On-demand Fact-checking with Public Dialogue. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing(CSCW '14). ACM, New York, NY, USA, 1188–1199. https://doi.org/10.1145/2531602.2531677 Navigate tocitation 1citation 2citation 3\r\nTravis Kriplean, Michael Toomim, Jonathan Morgan, Alan Borning, and Andrew Ko. 2012. Is This What You Meant?: Promoting Listening on the Web with Reflect. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems(CHI '12). ACM, New York, NY, USA, 1559–1568. https://doi.org/10.1145/2207676.2208621 Navigate tocitation 1citation 2citation 3\r\nKurt Luther. 2016. How Fellow Collectors, Field Photos and Muttonchops Identified an Unknown Officer. Military Images 34, 1 (2016), 29–31. Navigate tocitation 1\r\nKurt Luther. 2017. Merrill Carbine Leads to a Soldier's Identification. Military Images 35, 2 (2017), 64–65. Navigate tocitation 1citation 2citation 3\r\nKurt Luther. 2018. Non-Traditional Research Tools—and Serendipity. Military Images 36, 3 (2018), 12–13. Navigate tocitation 1\r\nKurt Luther. 2018. What are the odds? Photo sleuthing by the numbers. Military Images 36, 1 (2018), 12–15. Navigate tocitation 1citation 2citation 3citation 4citation 5\r\nKurt Luther. 2019. What to Do When Gold Standards Go Wrong?Military Images 37, 1 (2019), 8–9. https://www.jstor.org/stable/26532101 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: How to Trust the Worthiness of an Identification. Military Images 38, 3 (213) (2020), 8–11. https://www.jstor.org/stable/26914966 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: Lost and Found in the Library of Congress. Military Images 38, 2 (212) (2020), 10–13. https://www.jstor.org/stable/26890126 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: The Art of Photo Sleuthing. Military Images 38, 4 (214) (2020), 8–11. https://www.jstor.org/stable/26925454 Navigate tocitation 1\r\nRamona Martinez. 2012. Photo mystery solved, then doubted, then deciphered, thanks to readers. https://www.npr.org/sections/pictureshow/2012/04/17/150801239/photo-mystery-solved-then-doubted-then-resolved-thanks-to-readers Navigate tocitation 1\r\nRamona Martinez. 2012. Unknown No More: Identifying A Civil War Soldier. http://www.npr.org/2012/04/11/150288978/unknown-no-more-identifying-a-civil-war-soldier Navigate tocitation 1\r\nChristian A Meissner and John C Brigham. 2001. Thirty years of investigating the own-race bias in memory for faces: A meta-analytic review.Psychology, Public Policy, and Law 7, 1 (2001), 3. Navigate tocitation 1\r\nVikram Mohanty, Kareem Abdol-Hamid, Courtney Ebersohl, and Kurt Luther. 2019. Second opinion: Supporting last-mile person identification with crowdsourcing and face recognition. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 86–96. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6\r\nVikram Mohanty and Kurt Luther. 2023. DoubleCheck: Designing Community-based Assessability for Historical Person Identification. ACM Journal on Computing and Cultural Heritage (JOCCH) (to appear) (2023). Navigate tocitation 1\r\nVikram Mohanty, David Thames, Sneha Mehta, and Kurt Luther. 2019. Photo sleuth: Combining human expertise and face recognition to identify historical portraits. In Proceedings of the 24th International Conference on Intelligent User Interfaces. 547–557. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6citation 7citation 8\r\nVikram Mohanty, David Thames, Sneha Mehta, and Kurt Luther. 2020. Photo Sleuth: Identifying Historical Portraits with Face Recognition and Crowdsourced Human Expertise. ACM Transactions on Interactive Intelligent Systems (TiiS) 10, 4 (2020), 1–36. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6citation 7citation 8citation 9citation 10\r\nMeredith Ringel Morris, Scott Counts, Asta Roseway, Aaron Hoff, and Julia Schwarz. 2012. Tweeting is believing? Understanding microblog credibility perceptions. In Proceedings of the ACM 2012 conference on computer supported cooperative work. 441–450. Navigate tocitation 1\r\nBenedikt Morschheuser, Juho Hamari, and Jonna Koivisto. 2016. Gamification in crowdsourcing: a review. In 2016 49th Hawaii International Conference on System Sciences (HICSS). IEEE, 4375–4384. Navigate tocitation 1\r\nKathleen L Mosier and Linda J Skitka. 1999. Automation use and automation bias. In Proceedings of the human factors and ergonomics society annual meeting, Vol. 43. SAGE Publications Sage CA: Los Angeles, CA, 344–348. Navigate tocitation 1citation 2\r\nGreg Myre. 2021. How Online Sleuths Identified Rioters At The Capitol. https://www.npr.org/2021/01/11/955513539/how-online-sleuths-identified-rioters-at-the-capitol Navigate tocitation 1\r\nJohnny Nhan, Laura Huey, and Ryan Broll. 2017. Digilantism: An analysis of crowdsourcing and the Boston marathon bombings. The British journal of criminology 57, 2 (2017), 341–361. Navigate tocitation 1\r\nRaja Parasuraman and Dietrich H Manzey. 2010. Complacency and bias in human use of automation: An attentional integration. Human factors 52, 3 (2010), 381–410. Navigate tocitation 1\r\nJunwon Park, Ranjay Krishna, Pranav Khadpe, Li Fei-Fei, and Michael Bernstein. 2019. AI-based request augmentation to increase crowdsourcing participation. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 115–124. Navigate tocitation 1\r\nSarah Perez. 2022. Twitter to show 'Birdwatch' community fact-checks to more users, following criticism. https://techcrunch.com/2022/03/03/twitter-to-show-birdwatch-community-fact-checks-to-more-users-following-criticism/ Navigate tocitation 1\r\nP Jonathon Phillips, Amy N Yates, Ying Hu, Carina A Hahn, Eilidh Noyes, Kelsey Jackson, Jacqueline G Cavazos, Géraldine Jeckeln, Rajeev Ranjan, Swami Sankaranarayanan, et al. 2018. Face recognition accuracy of forensic examiners, superrecognizers, and face recognition algorithms. Proceedings of the National Academy of Sciences 115, 24 (2018), 6171–6176. Navigate tocitation 1citation 2\r\nPeter Pirolli, Evelin Wollny, and Bongwon Suh. 2009. So you know you're getting the best possible information: a tool that increases Wikipedia credibility. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1505–1508. Navigate tocitation 1\r\nInioluwa Deborah Raji and Joy Buolamwini. 2019. Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products. Navigate tocitation 1\r\nAmira Rezgui and Kevin Crowston. 2018. Stigmergic coordination in Wikipedia. In Proceedings of the 14th International Symposium on Open Collaboration. 1–12. Navigate tocitation 1\r\nRoy Rosenzweig. 2006. Can History Be Open Source? Wikipedia and the Future of the Past. Journal of American History 93, 1 (June 2006), 117–146. Navigate tocitation 1citation 2\r\nSaiph Savage, Andres Monroy-Hernandez, and Tobias Höllerer. 2016. Botivist: Calling volunteers to action using online bots. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing. 813–822. Navigate tocitation 1\r\nMike Schaekermann, Joslin Goh, Kate Larson, and Edith Law. 2018. Resolvable vs. irresolvable disagreement: A study on worker deliberation in crowd work. Proceedings of the ACM on Human-Computer Interaction 2, CSCW (2018), 1–19. Navigate tocitation 1\r\nMichael S. Schmidt. 2018. 'Flags of Our Fathers' Author Now Doubts His Father Was in Iwo Jima Photo. The New York Times (Jan 2018). https://www.nytimes.com/2016/05/04/us/iwo-jima-marines-bradley.html Navigate tocitation 1\r\nJennifer Schuessler. 2017. Found: Oldest Known Photo of a U.S. President (Socks and All). https://www.nytimes.com/2017/08/16/arts/design/john-quincy-adams-daguerreotype-sothebys-auction.html Navigate tocitation 1\r\nJulia Schwarz and Meredith Morris. 2011. Augmenting web pages and search results to support credibility assessment. In Proceedings of the SIGCHI conference on human factors in computing systems. 1245–1254. Navigate tocitation 1\r\nRuoxi Shang, KJ Kevin Feng, and Chirag Shah. 2022. Why Am I Not Seeing It? Understanding Users' Needs for Counterfactual Explanations in Everyday Recommendations. In 2022 ACM Conference on Fairness, Accountability, and Transparency. 1330–1340. Navigate tocitation 1\r\nLinda J Skitka, Kathleen L Mosier, and Mark Burdick. 1999. Does automation bias decision-making?International Journal of Human-Computer Studies 51, 5 (1999), 991–1006. Navigate tocitation 1\r\nVerbi Software. 2019. MAXQDA 2020 [computer software]. VERBI Software. Available from maxqda.com. Navigate tocitation 1\r\nNPR Staff. 2016. How Social Media Smeared A Missing Student As A Terrorism Suspect. https://www.npr.org/sections/codeswitch/2016/04/18/474671097/how-social-media-smeared-a-missing-student-as-a-terrorism-suspect Navigate tocitation 1\r\nBongwon Suh, Ed H Chi, Aniket Kittur, and Bryan A Pendleton. 2008. Lifting the veil: improving accountability and social transparency in Wikipedia with wikidashboard. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1037–1040. Navigate tocitation 1\r\nW Ben Towne, Aniket Kittur, Peter Kinnaird, and James Herbsleb. 2013. Your process is showing: controversy management and perceived quality in Wikipedia. In Proceedings of the 2013 conference on Computer supported cooperative work. 1059–1068. Navigate tocitation 1\r\nCivil War Trust. 2021. Military Images Magazine | Interview with Ron Coddington. https://www.battlefields.org/learn/articles/military-images-magazine Navigate tocitation 1\r\nSukrit Venkatagiri, Aakash Gautam, and Kurt Luther. 2021. CrowdSolve: Managing Tensions in an Expert-Led Crowdsourced Investigation. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–30. Navigate tocitation 1\r\nShaun Wallace, Lucy Van Kleunen, Marianne Aubin-Le Quere, Abraham Peterkin, Yirui Huang, and Jeff Huang. 2017. Drafty: Enlisting Users To Be Editors Who Maintain Structured Data. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 5. Navigate tocitation 1\r\nAndrea Wiggins and Yurong He. 2016. Community-based data validation practices in citizen science. In Proceedings of the 19th ACM Conference on computer-supported cooperative work & social computing. 1548–1559. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6\r\nFoundation Wikimedia. 2022. No original research. https://en.wikipedia.org/wiki/Wikipedia:No_original_research Navigate tocitation 1\r\nHeather Willever-Farr, Lisl Zach, and Andrea Forte. 2012. Tell me about my family: A study of cooperative research on Ancestry. com. In Proceedings of the 2012 iConference. ACM, 303–310. Navigate tocitation 1\r\nHeather L Willever-Farr and Andrea Forte. 2014. Family matters: Control and conflict in online family history production. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. ACM, 475–486. Navigate tocitation 1citation 2citation 3citation 4citation 5\r\nJeremy B Wilmer. 2017. Individual differences in face recognition: A decade of discovery. Current Directions in Psychological Science 26, 3 (2017), 225–230. Navigate tocitation 1\r\nBob Zeller. 2022. Searching for photos of Civil War Soldiers | David Wynn Vaughan. https://www.civilwarphotography.org/searching-for-photos-of-civil-war-soldiers/ Navigate tocitation 1\r\nA CIVIL WAR PHOTO SLEUTH: DESIGN CHALLENGES AND OPPORTUNITIES\r\n \r\nHere, we provide details of three key challenges posed by Civil War Photo Sleuth that might have contributed towards misidentification and subsequently, inaccurate assessments, and how they present design opportunities for Photo Steward.\r\nA.1 Conflating facial similarity with photo identification\r\nA.1.1 Challenges: Mohanty et al. conducted a benchmarking study of CWPS [44], and found the face recognition algorithm to be of low precision; i.e., it retrieved hundreds of search results which may look similar to the query photo but are actually different people (false positives). Low precision increases the chances that users will interact with a lot of false positives. In such cases, one may need to garner additional information (i.e., comparing biographical information) before making a decision. However, it is plausible that automation bias may play a role in non-expert users making a match solely based on facial similarity [47]. As a result, there is strong potential for misidentification (see Figure 8).\r\nA.1.2 Current Workflow: CWPS's compare interface allows users to closely inspect the search results for a potential match, but does not make any distinction between facial similarity comparison and photo identification. Both are conflated into a one-step process, with one \"Identify\" button for the users to make their decisions (see Figure 2-C). Yet, users may want to indicate agreement with just the facial similarity (i.e., query photo and the search result showing the same person) but not the identity (i.e., name and biographical information), or vice versa.\r\nA.1.3 Design Goal 1: To support accurate investigation of photo identifications, users should be able to deliberate on the different aspects of the decision-making process. Providing users with a decision-making workflow that decouples facial similarity comparison from the overall photo identification task would allow them to focus on these tasks separately, while discouraging them from making decisions solely on the basis of facial similarity.\r\n\r\nFigure 8: An example of a misidentified photo on CWPS. This photo was identified as John C Whiteside based on facial similarity with the top search result: an identified reference photo of John C Whiteside. However, other visual clues, such as the photographer studio, do not align with Whiteside's biographical information.\r\n\r\nA.2 Lacking support for fine-grained, deliberative decision making\r\nA.2.1 Challenges: Historical photo identification is a complex task, where experts often corroborate multiple pieces of evidence, including facial similarity comparison, before reaching a decision about the identity of the photo [33]. While confirming an identity, experts may be highly confident if the source, military records, uniform clues, and additional photos of the same person all line up, or slightly confident if they need additional evidence. Conflicting evidence pieces may also affect their confidence levels. Similarly, they may have different degrees of certainty while ruling out an identity for a photo. A lack of support for expressing and displaying granularity in these photo identification decisions can lead to varying degrees of uncertainty being captured and misinterpreted as a confirmation, and eventually propagating misidentifications. Further, while it is safe to assume the vast majority of the Civil War photography community care about the accuracy of the photo IDs, there exists a small risk of financially-driven misidentifications since certain types of identified photos are considered to be more valuable [9].\r\nA.2.2 Current Workflow: The compare interface's \"Identify\" button (see Figure 2-C) is the only mechanism available on the platform for users to provide (a unary) input on photo identifications. CWPS users currently cannot provide fine-grained feedback on a given photo's identity, either at the time of identifying or afterwards on the photo page.\r\nA.2.3 Design Goal 2 : Users should be able to express how certain or uncertain they are about a photo identification. Interventions for allowing users to provide structured feedback can not only facilitate more accurate, fine-grained responses, but also encourage deliberation on the available evidence before making a decision, borrowing inspiration from other social computing systems that support users reflecting and deliberating on available information [28, 29].\r\nA.3 Limited validation opportunities for the community\r\nA.3.1 Challenges: For humans, deciding whether two photos show the same person is a highly subjective experience. A study by Mohanty et al. [41] showed that participants in a crowdsourcing study often disagree with each other and with facial recognition results in person identification tasks. The same study showed that people often seek a second opinion from peers for validating photo identifications. Without validation, original crowdsourced historical research may result in unresolved cases of conflicting IDs proposed for the same face, incorrect IDs remaining unchecked, or airtight IDs not being confirmed.\r\nA.3.2 Current Workflow: Once a photo has been identified, CWPS does not offer other users the option to express their opinions on whether two photos show the same person or not, or if the photo has been identified correctly or not (see Figure 8). The photo page does not indicate if (or why) the photos are linked to each other, how they are similar, or which user(s) considered them to be similar. Further, it does not indicate the role of facial recognition in linking them together.\r\nA.3.3 Design Goal 3 : To support accurate original historical research, the platform should encourage information stewardship from the community by allowing members to give their opinions on existing photo identifications [72]. Further, the platform should also be transparent about the role of community stewardship and facial recognition for a given photo identification, which can then act as evidence for aiding subsequent validation efforts by other users, thus supporting a form of stigmergic collaboration [13, 14].\r\nB DETAILS FOR LOG ANALYSIS\r\n \r\nTo understand the community's stewardship behaviors, we examined website logs for all user activities for one year since we launched the new features (December 2020 – December 2021). During this period, we observed 5843 voting instances on 5672 photos for 5355 unique IDs, where a user voting on whether Photo N can be identified as a Person M or not is considered to be one voting instance. For a given voting instance, we analyzed 1) any associated facial similarity comparisons to understand how they deliberated on their final voting decision, and 2) the user's confidence vote plus any justification notes left by the user to understand their decision rationale. Of the 5672 query photos, 4297 photos (4377 voting instances) did not have any facial similarity comparisons — only the user's confidence on the ID (plus any notes) was captured.\r\nFor the remaining voting instances which had at least one associated facial similarity comparison (i.e., users deliberate through the two-step validation process), we broke them down into pre-identified (i.e., the user knew the ID of the photo ahead of time and therefore, might be looking for a specific target on the search results page) and post-identified (i.e., the user did not know the ID of the photo and therefore, may not have a specific target while analyzing the search results) cases. As mentioned earlier in Section 4.2, users had the opportunity to validate IDs either on the search results page on the photo page. While a pre-identified voting instance almost certainly originated from the search results page, a post-identified case could be from either page. Since the logs did not give us the page origin of each vote directly, we triangulated from CWPS's timestamps to determine whether a given voting instance was for a pre-identified case or a post-identified one.\r\nAfter collating all the associated facial similarity comparisons for a given voting instance, we had 1064 pre-identified and 576 post-identified user deliberations to analyze. Each deliberation instance is a user's attempt to identify a query photo as a given target (person) ID, where they first compare facial similarity with all other photos that have been identified as the target ID, followed by the user's confidence on the query photo being the target ID. We analyzed the user responses to see whether the facial similarity comparisons had any impact on the user's confidence.\r\nUsers had provided comments in 682 (out of 5843, or 11.7%) voting instances. We coded these comments using an iterative, inductive approach, which resulted in five high-level themes, which can broken down into 13 sub-categories (see Appendix C).\r\nTo understand any stigmergic collaboration processes at play, we also analyzed the IDs which had multiple votes to check for agreements and disagreements between the voters. We further analyzed how the community's facial similarity comparisons compare against the facial recognition scores.\r\nC THEMES FOR NOTES ANALYSIS\r\nPhoto Comparisons\r\nHigh-Level Comparison: The note mentions \"replica\", \"facial similarity\", \"facial match\", \"identical\", \"visual comparison\", and other similar terms that describe comparisons with a prior identified photo.\r\nDescribing facial features: The note mention facial features like \"eyes\", \"hairline\", \"ears\", etc. to make comparisons\r\nWord-of-Mouth\r\nDescendant: The note either mentions that the user is a descendant of the person being identified, or they got the information from the family of the person.\r\nOwnership: The note either mentions that the user owns a printed version of the photo, compared with a photo in their collection, or they know the owner of the photo.\r\nFamiliarity/Self-Reported Research: The note mentions that the user has seen the photo somewhere, be it in a book, museum, etc.\r\nVisual Clues\r\nUniform: The note mentions visual clues that pertain to the uniform of the person (e.g., hat insignia, shoulder straps, etc.) The user may infer the possible service information (i.e., ranks, branches, regiments, etc.) from the uniform clues.\r\nInscription: The note mentions the presence of a period inscription on the photo (a highly trustworthy primary source for a person's ID), or an album case, or modern inscriptions such as books, which is generally the name of the person being identified. In some instances, the inscribed text may point to the person's service information.\r\nOther Visual Clues: The note mentions visual clues in the photo (e.g., backmarks, borders, etc.) beyond the person's face.\r\nExternal Information\r\nExternal URL: The note mentions an external URL, which supposedly has additional information about the photo's ID.\r\nOther sources (e.g., museum, website, book, etc.): The note mentions an external source (e.g., museum, book, etc.) that supposedly has evidence for the photo's ID, but no URLs are provided. Details about the source may or may not be available.\r\nLack of Information / Seeking Additional Evidence: The note mentions the lack of evidence or seeking additional evidence, be it about the source or the service information.\r\nProviding Additional Information\r\nBiographical Information: The notes mentions additional information about the person's service records, specific regiments, biographical information (name, year, location), etc.\r\nAdditional Context: The note mentions some additional context provided by the user to justify their decision, such as information about the photo collection, or pointing to someone else's research, or some historical context, or incorrect evidence, or if the person is prominent.\r\nD PARTICIPANT DETAILS\r\nUndergraduate and master's students concentrating in history who use Civil War photos for their coursework and research projects, but are not (yet) employed in a professional capacity as historians. We recruited five students via recommendations from our university's history department. None of the students had used CWPS before, or were known to the authors prior to the study. Three students were men and two were women, and all were in the \"18 to 30\" age group. We anonymize them with identifiers S1–S5.\r\nExperienced users of Civil War Photo Sleuth who have added over 50 photos each and have substantial knowledge of Civil War history, but are not professional historians. We recruited five amateur experts from the CWPS contact list. All five users were men, and they were distributed across different age groups (two in \"18 to 30\", two in \"31 to 40\", and one in \"51 to 60\"). We anonymize them with identifiers C1–C5. C1 and C3 are among the most active daily users on CWPS. Only two of the five had used Photo Steward before.\r\nExpert historians with a graduate degree in history, specializing in American Civil War history, but with little or no previous experience with CWPS. We recruited five historians via recommendations from our university's history department. Three historians were men and two were women. They were distributed across different age groups (two in \"18 to 30\", two in \"31 to 40\", and one in \"51 to 60\"). We anonymize them with identifiers H1–H5. None of them had used Photo Steward before.\r\nE LAB STUDY QUESTIONS\r\nE.1 Semi-Structured Questions\r\nIs there a way that you would like to capture your thoughts on this ID and share them with others, if possible? If so, what would that look like? If not, can you explain why not?\r\nWhat did you think about the community opinions?\r\nWhat do you think about the ID quality visualization?\r\nWhat did you think about the 2-step process while agreeing/disagreeing on an identity?\r\nWhat is your overall opinion of both the interfaces?\r\nWhich interface would you prefer for validating the information? And why?\r\nWhat would you change or improve?\r\nE.2 Usability Survey\r\nQ1. The community's opinions about an identity were clear and easy to understand in the 2nd system. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ2. The community's opinions about an identity were useful for assessing the information. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ3. The process of voting on an identity was clear and easy to understand. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ4. Comparing other photos first and then voting on an identity helped me deliberate and make more accurate decisions. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ5. I was able to validate the information better using the 2nd system. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nF SUMMARY OF CHANGES\r\nWe summarize in Table 6 about how Photo Steward differs from CWPS.\r\nTable 6: Summary of changes: Civil War Photo Sleuth (CWPS) with and without Photo Steward.\r\n\r\nCWPS\r\nCWPS + Photo Steward\r\nDecision-Making: How do users make identification decisions for the query photo when they see the similar-looking search results retrieved by facial recognition?\r\nA single-step comparison interface that allows the user to compare the query photo with one similar-looking photo and the associated biographical information of the target ID. The single-step workflow conflates the two tasks of facial similarity and person identification.\r\nA multi-step validation interface that allows the user to compare the query photo with all previously identified photos of a similar-looking target ID (search result). The two-step workflow decouples the two tasks of facial similarity and person identification, thus allowing the user to deliberate on the facial similarity comparison between the query photo and the target photo(s) before making a decision on the target identity.\r\nUsers make a unary input on the query photo's ID by clicking an \"Identify\" button in the comparison interface, which will link the target ID to the query photo. The comparison interface does not allow users to rule out candidates.\r\nUsers can provide fine-grained decisions for both facial similarity and person identification steps in the validation interface. In Step 1, the user compares the query photo and the target photo for facial similarity by selecting from the following options: No (Different Person), Not Sure, Yes (Facial Match), and Yes (Replica). In Step 2, users indicate how confident they are about the query photo's ID (as the proposed target ID) by selecting the following options: No (Highly Confident), No (Slightly Confident), Not Sure, Yes (Slightly Confident), and Yes (Highly Confident).\r\nAccess: What kind of validation opportunities are available for users?\r\nUsers can only access the comparison interface from the search results page if they search for similar-looking candidates using facial recognition. Once the photo has been identified, users cannot access the comparison interface on the photo page. The community cannot weigh in on an ID's reliability on the photo page.\r\nUsers can access the validation interface on both the search results page (while identifying the query photo) and the photo page (after the photo has been identified). The community can weigh in on the reliability of the proposed ID(s) and facial matches using the validation interface on the photo page, thus engaging in a form of stigmergic collaboration.\r\nEvidence: What kind of stewardship evidence is presented to the users?\r\nThe photo page displays the proposed ID(s) for the photo without any additional information about the ID's reliability. Similarly, other photos that have been matched to the query photo are also displayed without any reliability indicator. Since community opinions are not captured on CWPS, they are not displayed.\r\nThe photo page displays the proposed ID(s) for the photo along with stewardship visualizations of the a) community's confidence on the ID, and b) facial similarity comparison with other photos of the same ID by both the community and AI.\r\nFOOTNOTE\r\n1 www.civilwarphotosleuth.com\r\n\r\n\r\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs International 4.0 License.\r\nCI '23, November 06–09, 2023, Delft, Netherlands\r\n© 2023 Copyright held by the owner/author(s).\r\nACM ISBN 979-8-4007-0113-9/23/11.\r\nDOI: https://doi.org/10.1145/3582269.3615600\n",
    "totalFiles": 2
  },
  {
    "id": "extraction_1754351178929_l9duuhxge",
    "timestamp": "2025-08-04T23:46:18.929Z",
    "files": [
      {
        "name": "story.txt",
        "size": 2786,
        "type": "text/plain",
        "extractedText": "Ron: Great. I want to start with a detail about you that I’ve always appreciated, which is that you started out as a collector and someone who was deeply interested in the Civil War. Talk a little about how you became interested and got started.\r\n\r\nKurt: Sure thing. I am a bit of a Civil War collector. I grew up in a family of Civil War and history collectors and appreciators. I spent many childhood hours at antique shops and flea markets, and our home was full of interesting Civil War collectibles—books, artifacts, and all kinds of interesting stuff. That was my world as a kid. As I grew older, I realized that I could actually have my own Civil War collection. What was a little different about me is that I gravitated toward Civil War photographs—a category my family didn’t have too much of. I just felt drawn to the imagery of people from that era, their incredible stories, learning about the names behind the faces, and ultimately building my own collection of images while loving the process of researching those individuals.\r\n\r\nRon: One of my favorite stories is your family connection—how there’s a photograph connected to it all. I’d love to hear you tell that again.\r\n\r\nKurt: Absolutely. One of the reasons my family had an interest in the Civil War is that we have some Civil War ancestors. Three of them fought with the Union Army in Pennsylvania regiments, and I grew up learning about their adventures. We researched all we could about them through military records, but we didn’t have any images—we couldn’t see them or know what they looked like. I got really lucky during the 150th-anniversary commemorations of the Civil War. In Pittsburgh, where I grew up and where my Civil War ancestors lived, there was an exhibit at the Heinz History Center. I noticed a really cool photo album of Civil War soldiers. The label said that the company in the album was one that my ancestor had fought in. Out of thousands of possibilities, that single regiment appeared there. I asked the museum staff if they could have the owner, Ken Turner, look inside to see if my ancestor’s photo was in there. And sure enough, inside the album was a photograph of my Civil War ancestor, Corporal Oliver Croxton in uniform. I was looking into the eyes of my great-great-great grand uncle.\r\n\r\nRon: How did you feel when you saw that for the first time?\r\n\r\nKurt: It felt amazing. Here was someone I had thought about my whole life, trying to learn as much as I could about him. And once you start studying Civil War portraits, you learn that the odds of finding one specific photo of a soldier—especially a lower-ranked one—are really low. So having that opportunity, seeing his face, and getting a much clearer picture of who he was… it was really powerful.",
        "extractionStatus": "success"
      },
      {
        "name": "Kurt paper.txt",
        "size": 103748,
        "type": "text/plain",
        "extractedText": "Photo Steward: A Deliberative Collective Intelligence Workflow for Validating Historical Archives\r\nVikram Mohanty, Computer Science, Virginia Tech, USA, vikrammohanty@vt.edu\r\nKurt Luther, Computer Science, Virginia Tech, USA, kluther@vt.edu\r\n\r\nDOI: https://doi.org/10.1145/3582269.3615600\r\nCI '23: Collective Intelligence Conference, Delft, Netherlands, November 2023\r\nHistorical photographs of people generate significant cultural and economic value, but correctly identifying the subjects of photos can be a difficult task, requiring careful attention to detail while synthesizing large amounts of data from diverse sources. When photos are misidentified, the negative consequences can include financial losses and inaccuracies in the historical record, and even the spread of mis- and disinformation. To address this challenge, we introduce Photo Steward, an information stewardship architecture that leverages a deliberative workflow for validating historical photo IDs. We explored Photo Steward in the context of Civil War Photo Sleuth (CWPS), a popular online community dedicated to identifying photos from the American Civil War era (1861–65) using facial recognition and crowdsourcing. While the platform has been successful in identifying hundreds of unknown photographs, there have been concerns about unverified identifications and misidentifications. Our exploratory evaluation of Photo Steward on CWPS showed that its validation workflow encouraged users to deliberate while making photo ID decisions. Further, its stewardship visualizations helped users to assess photo ID information accurately, while fostering diverse forms of stigmergic collaboration.\r\nCCS Concepts: • Human-centered computing → Collaborative and social computing design and evaluation methods; • Human-centered computing → Interactive systems and tools;\r\n\r\nKeywords: crowdsourcing, human-AI interaction, online deliberation, community stewardship, information assessability, online communities, history, person identification, facial recognition, stigmergic collaboration\r\n\r\nACM Reference Format:\r\nVikram Mohanty and Kurt Luther. 2023. Photo Steward: A Deliberative Collective Intelligence Workflow for Validating Historical Archives. In Collective Intelligence Conference (CI '23), November 06--09, 2023, Delft, Netherlands. ACM, New York, NY, USA 19 Pages. https://doi.org/10.1145/3582269.3615600\r\n\r\nFigure 1: Workflow of Photo Steward. (1) Decision-Making: The user compares photos for facial similarity and make decisions on the photo ID using Photo Steward's deliberative validation interface. (2) Access: The user accesses the validation interface from Civil War Photo Sleuth's search results and photo page. (3) Evidence: The community's responses from the validation interface feed into stewardship visualizations that are visible on the Photo Page, which subsequently foster a form of stigmergic collaboration among the users.\r\n1 INTRODUCTION\r\nThe task of correctly identifying individuals in historical photos holds great cultural and economic importance [3, 19, 39, 61]. This identification process is analogous to solving a complex mystery. It often involves corroboration of multiple research processes such as investigating visual clues in a photo, finding relevant reference resources, and comparing multiple low-resolution reference photos [31, 32, 37]. Historical photos pose many challenges, including low-resolution images, scattered reference materials, limited domain expertise, and lack of suitable verification tools. These hurdles often result in misidentifications which can have negative consequences, ranging from distorting historical narratives [60] and fueling conspiracy theories [12] to spreading disinformation [16] and unwarranted financial gains from inaccurate representations [21]. As online platforms such as Ancestry.com, Find-a-Grave, and FamilySearch democratize historical and genealogical research, the risk of misidentification is further amplified due to factors such as inadequate experience, confirmation bias, and automation bias introduced by imperfect automated tools [44, 75].\r\nTo address these challenges, we introduce Photo Steward, a deliberative workflow that leverages collective intelligence to validate historical photo identifications (IDs). Photo Steward's architecture builds upon the concept of information stewardship [18, 72], which involves community-driven validation of content, as seen in online communities like iNaturalist and Wikipedia. We designed and applied the Photo Steward architecture to Civil War Photo Sleuth (CWPS),1 an AI-infused online platform for identifying historical photos. CWPS has over 20,000 registered users and over 25,000 identified Civil War portraits, and faces the problem of historical photo misidentification [43]. Photo Steward provides a validation workflow that promotes careful deliberation during facial similarity comparison and photo ID verification, while enabling users to share opinions. It also visualizes community opinions on the reliability of photo IDs and facial similarity, fostering information stewardship at multiple levels.\r\nWe publicly released Photo Steward on CWPS and conducted an exploratory evaluation of twelve months of usage, including interviews with users of different expertise levels and log analysis of stewardship behaviors on the platform. We found that Photo Steward's stewardship visualizations helped users to find additional evidence (e.g., external sources, uniform clues matching service records, etc.) for assessing the reliability of photo IDs. Users validated hundreds of different IDs on the platform, and found the workflow to be useful for deliberating on facial similarity comparison and fine-grained photo ID decisions.\r\nWe also discuss the implications of community participation, deliberative human-AI interaction, and assessable designs for historical photo identification.\r\n2 RELATED WORK\r\n2.1 Misinformation in a Historical Context\r\nNumerous online communities, forums, and websites have emerged in recent years for archiving and documenting history [57], generating family histories [74, 75], identifying and sharing historical photos [43], trading antiques [1, 8], and facilitating discussions around history [20]. Much like popular social media platforms such as Facebook and Twitter, these history-based platforms are also prone to the problem of misinformation, albeit in a more specialized historical research context.\r\nPrior work has shown that erroneous family history trees were being disseminated across Find-a-Grave and Ancestry, two popular genealogy research communities, as a result of the platform's low bar to entry and inexperienced contributors' over-reliance on inaccurate automated features [75]. Mohanty et al. [43] showed that despite successful identifications on Civil War Photo Sleuth (CWPS), several photos were misidentified in the first month, particularly for photos without period inscriptions or duplicate views (12 misidentifications out of 37). Multiple factors — the correct candidate not present in the search pool, or the user incorrectly assessing facial similarity [53] and picking the wrong match — can lead to incorrect IDs. A follow-up benchmarking study of the underlying face recognition algorithm [44] highlighted its low precision (i.e., it retrieves over hundreds of search results), raising the possibility of errors due to automation bias, i.e., the tendency of users to over-rely on automation for making a decision [47, 50, 64].\r\nIdentifying historical photos is a complex investigative process, often involving the corroboration of multiple evidence pieces [31, 33] and can be seen analogous to \"finding a needle in a haystack\". As a result, historical photo IDs run a high risk of getting misidentified even with the best of intentions. Multiple Civil War photos have also been misidentified in the collections of professionally managed museums and archives, such as the US Library of Congress [36] and the Abraham Lincoln Presidential Library [34]. At the same time, historical photo IDs also have the potential to generate significant monetary value [1, 3, 8, 10], and such financial incentives might also lead to falsified identifications [21]. Validating these historical photo IDs, which are a result of complex, subjective original research, becomes tricky without the lack of domain expertise and access to investigative tools.\r\nWe addressed these challenges in Photo Steward by designing a stewardship architecture that allows users to share their expertise with others. To address the impact of automation, we introduced a two-step validation workflow for the users to deliberate on decisions while interacting with the AI's recommendations.\r\n2.2 Data Validation in Online Communities\r\nMultiple online platforms have leveraged the strengths of crowdsourced contributions for validating the quality of data generated on those sites. Elliott discusses how stigmergic collaboration, where indirect coordination within a community stimulates subsequent actions, plays a role in maintaining articles on Wikipedia [13]. This concept was observed by Wiggins et al. in their study of iNaturalist, an online platform for identifying species, wherein community stewardship behaviors were seen as users agreed on organism identifications to influence the platform's quality grade status [72]. Prior work has shown that stewardship visualizations on Wikipedia (i.e., article quality) [18] and iNaturalist (i.e., ID research grade status) [72] have a positive impact on users' assessment of the information.\r\nAlong these lines, we also built DoubleCheck [42], a quality assessment framework that builds upon the concepts of provenance and stewardship for verifying historical photo IDs. DoubleCheck focused on displaying quality indicator badges for historical photo IDs by capturing accurate provenance information and combining the source trustworthiness information with community opinions on the ID. In this work, we focus solely on the underlying stewardship architecture that helped facilitate the community opinions. Both DoubleCheck and Photo Steward were evaluated in the same lab study, but there is no overlapping data.\r\nVisualizations displaying (surrogate) quality metrics, such as popularity among expert users, social reputation, and content coverage, have been effective in helping users assess the credibility of websites and search results [62]. Prior work has also shown that visualizing the history of edits for a Wikipedia article can have a significant impact on users' perceived trustworthiness of the article [54, 67]. Similarly, Chevalier et al. [7] showed that visualizing the number of contributors, length of the article and discussion, and the history of edits helped users assess the quality of Wikipedia articles faster. On the other hand, Towne et al. [68] found that being exposed to editor conflicts in the discussion of a Wikipedia article lowered the perception of the article's quality, even though the users reported that the transparency raised their perceptions of the page and Wikipedia in general. Morris et al. [45] found that Twitter users relied on the author information for making assessments about the credibility of information in a tweet.\r\nDrawing from this prior work, we designed Photo Steward's stewardship visualizations to highlight the role of collective intelligence, while fostering stigmergic collaboration on CWPS to validate the quality of photo identifications.\r\n2.3 Background: Civil War Photo Sleuthing\r\nThe American Civil War (1861–65) was one of the first major conflicts to be extensively photographed. Over 3 million soldiers fought in the war, with many of them having been photographed at least once. Over 150 years, many of these photos have survived in museums, libraries, and personal collections, but only 10–20% are are identified [69, 77]. Civil War photography has garnered a lot of interest among historians, collectors, dealers, genealogists, archivists, and other experts, who often try to identify unknown photos for personal, cultural, and economic reasons. However, the identification process is complex and challenging, which often involves identifying visual clues in a photo and manually scanning through hundreds of low-resolution photos, military records, and reference books for corroborating evidence [31, 33, 38].\r\n3 ENHANCING CIVIL WAR PHOTO SLEUTH: DESIGN OPPORTUNITIES\r\n\r\nFigure 2: CWPS Haystack Model: Person Identification Pipeline [43]\r\nCivil War Photo Sleuth (CWPS) is a free, public website where users can identify unknown portraits from the American Civil War era using a person identification pipeline that combines crowdsourced human expertise and face recognition [43]. Drawing analogies to finding a needle in a haystack, Mohanty et al. propose a 'haystack model' to describe CWPS's person identification pipeline. In this pipeline, a user begins the identification process by first tagging a photo for uniform clues, which then generates search filters based on service records, and then facial recognition returns facially similar-looking results from a pool of potential candidates, ordered by similarity to the query photo, that satisfy the search filters (see Figure 2).\r\nThe CWPS haystack model is designed to prevent misidentifications by placing human decision-making at the forefront and treating AI as a supportive tool. It avoids automatically selecting the best match or displaying the algorithm's inconsistent confidence levels [41, 44]. Instead, the user carefully inspects search results for potential matches based on facial similarity and corresponding biographical details. Once a photo is identified, CWPS links the face and identity together and displays the ID on the photo page.\r\nDespite these measures, the open participation model of CWPS, which lacks verification, has raised concerns about the trustworthiness of proposed identities and the potential increase of \"false positives\" as the site grows [22]. To address these concerns, we enumerate three design goals which draw upon prior work on Civil War photo identifications and CWPS system designs, evaluations, and critiques [22, 30, 33, 35, 41, 43, 44], as well as our own observations and experiences using the publicly available version of the website. We provide further details in Appendix A.\r\nDesign Goal 1: Decouple facial similarity comparison from the overall task of person identification.. The current CWPS workflow conflates facial similarity and person identification into a single decision-making process (see Figure 2-C). Facial similarity, while important, can conflict with the identity suggested by personal details like biographical information and service records. The facial recognition algorithm's low precision [44] adds to the complexity, with the possibility of users interacting with false positives. In order to discourage over-reliance on facial similarity, we propose this design goal of separating both these tasks, allowing users to deliberate on the facial similarity and other person identification attributes separately, thereby minimizing inaccuracies.\r\nDesign Goal 2: Support fine-grained, deliberative decision-making.. With the current CWPS interface only permitting binary feedback during photo identification, there is a heightened risk of misrepresentation and misidentifications (see Figure 2-C). Historical photo identification is intricate, often demanding careful corroboration of numerous evidence pieces, with user confidence varying according to the evidence at hand [33]. To more effectively facilitate this complex process, we advocate for a design that enables users to express their level of certainty in their identification decisions. This design goal seeks to foster more accurate and nuanced user feedback, as well as reflective and deliberate decision-making [28, 29].\r\nDesign Goal 3: Encourage community contribution and transparency for validating photo IDs.. Identifying individuals in photos can be considered a subjective process and often benefits from multiple perspectives [41]. Currently, CWPS lacks mechanisms for community feedback and transparency about the process of photo identification, leaving potential misidentifications unchecked (see Figure 8 in Appendix). To support accurate original historical research, the platform should encourage community participation in validating identifications and promoting transparency around the roles of community contribution and facial recognition [18, 72]. This approach also encourages collective responsibility, facilitating stigmergic collaboration [13, 14], where user contributions guide future validation efforts.\r\n4 SYSTEM DESCRIPTION: PHOTO STEWARD\r\nWe developed Photo Steward, an information stewardship architecture that integrates a deliberative workflow for the community to validate historical photo identifications, which we then integrated into CWPS. Photo Steward's architecture has three main components (see Figure 1): 1) a deliberative decision-making interface for facial similarity comparison and photo identification, 2) new access points for validating photo identities, and 3) stewardship evidence for fostering stigmergic collaborations.\r\n4.1 DECISION-MAKING: Deliberating on facial similarity and photo identification\r\nAs part of Photo Steward, we introduce a multi-step \"Validation Interface\" (see Figures 3 and 4) to replace CWPS's single-step comparison interface. Photo Steward's validation interface allows users to deliberate while interacting with the facial recognition results. Meeting Design Goal 1, the validation workflow separates the task of facial similarity comparison from the overall goal of identifying the photo.\r\nTo inform our design, we draw on evidence-based decision-making [11], a model primarily used in healthcare, policymaking, and judicial sectors, which advocates for justifying decisions (photo IDs in this case) by gathering available evidence (facial similarity as visual evidence here). In the first step, the user compares the query photo to all other photos with the same identity for facial similarity. After deliberating on the facial similarity evidence, the user then votes on whether the query photo fits the target identity in the second step (which is the user's primary goal).\r\nThe validation interface is divided into four columns (from left to right): 1) the task description, 2) the query photo, 3) the evidence that is being weighed, and 4) the biographical information. The query photo and evidence are positioned in the two middle panels for easy side-by-side comparison. The task description panel displays the rating question for both the facial similarity comparison and the identification steps. Here, we used structured feedback to capture both the user's facial similarity comparison and their confidence on the photo ID, in an effort for encouraging users to exercise personal deliberation on all available evidence before making a decision on the ID. The interface updates the task description and evidence column depending on which task the user is performing.\r\nTo investigate the identity of a query photo, the user opens the validation interface which loads all the photos and biographical information available for the target identity.\r\n4.1.1 Validation Step 1: Facial Similarity Comparison. For the first step of the validation process, the interface displays the target photo in the evidence column next to the query photo for easy facial similarity comparison (see Figure 3).\r\n\r\nFigure 3: Step 1 of Photo Steward's Validation Interface. Users can compare two photos and answer whether they show the same person or not. They have the option of selecting whether the two photos are a facial match (i.e., same person, different views) or a replica (i.e., same person, same view). Here, the user is comparing whether Photo 41929 and Photo 1 show the same person or not. If multiple faces are available for the same ID, they appear one after the other in the order in which they were uploaded to CWPS.\r\n\r\nThe user's task is to determine whether both photos show the same person (regardless of whether the identity is known). Users can select from the following options: No (Different Person), Not Sure, Yes (Facial Match) and Yes (Replica). Mohanty et al. found that photos correctly identified on CWPS were either facial matches (i.e., same person, different view) or replicas (i.e., same person, same view) [43], which informed the design of this input scale. Since facial similarity does not have any standard scale and users may perceive the similarity or dissimilarity of two faces differently [40, 76], we chose not to capture any further granularity in their responses for facial match, replica or different person as this might lead to inconsistent data collection.\r\nCapturing these responses in a structured way allows users to deliberate on the task of facial similarity; this becomes more critical as users are also interacting with the results of a low-precision facial recognition algorithm [44]. In this step, the user compares facial similarity of the query photo with all available photos of the target identity, one photo at a time.\r\n4.1.2 Validation Step 2: Fine-Grained Photo Identification. In this step, the user analyzes the biographical information and incorporates the facial similarity evidence from the previous step to make a decision on the photo's identity.\r\nThe validation interface displays information in the same four-column layout (see Figure 4), with the evidence column now displaying a summary of the user's responses about facial similarity between the query photo and the target photo(s). The biography column shows the name and the service records for the user to analyze.\r\n\r\nFigure 4: Step 2 of Photo Steward's Validation Interface. Users vote on whether the query photo can be identified as the target identity by expressing their confidence. They can also add an optional note to justify their decision. The evidence panel displays a summary of the user's responses from the first step, where the faces are displayed next to the user's facial similarity comparison with the query photo. The faces are ordered in the way they appear for comparison, i.e., the order in which they were uploaded to CWPS.\r\n\r\nThe user now decides whether the query photo can be identified as the target identity (see Figure 4). The instruction above nudges the user to factor in the prior photo comparison evidence and the biography information. Meeting Design Goal 2, users indicate their confidence about the task question by selecting one of the five options displayed in radio buttons: No (Highly Confident), No (Slightly Confident), Not Sure, Yes (Slightly Confident) and Yes (Highly Confident). This scale, which offers more nuance than a binary decision, serves as a proxy measure for the ID's reliability given the complexities of quantifying accuracy in a historical photo ID investigation. These options reflect the varying degrees of confidence users have based on the quality and quantity of corroborating evidence, such as reputable sources, facial similarity to additional photos, and expert opinions. Users also have the option to elaborate their decision rationale in a free-text note.\r\n4.2 ACCESS: Expanding Validation Opportunities for Photo Identifications\r\nPhoto Steward provides stewardship capabilities for the CWPS user community by allowing them to access and use the validation interface at different stages of the photo identification process from multiple gateways (see Figure 1). On the \"Search Results\" page, it can be used for identifying a photo from a pool of potential similar-looking candidates, or ruling out some potential candidates. After a photo has been identified, users can also access it on the \"Photo Page\" to either validate an existing ID or dispute an incorrect one, and collaborate with other users in a stigmergic manner (Design Goal 3).\r\n4.2.1 Search Results Page: Matching and Ruling Out Candidates. While identifying a query photo, users can now inspect potential matches on the search results page with the help of the validation interface. The \"Compare\" button on a search result brings up the validation interface, loading all the target information for the corresponding search result. The target identities in the validation interface will update as the users check new search candidates for matches. The interface allows users to make two types of decisions, depending on their confidence response: 1) either of the \"Yes\" responses will match the photo with the target identity with varying degrees of confidence, and 2) either the \"No\" or \"Not Sure\" responses will rule out the search candidate as a potential match for the current user.\r\n4.2.2 Photo Page: Validating and Disputing Existing IDs. After a photo has been identified, Photo Steward allows users to review opinions from other users (described in Section 4.3) and contribute their own for a given photo ID on CWPS's photo page, fulfilling Design Goal 3. By clicking the \"Give Your Opinion\" button, users launch the validation interface featuring the query photo, the linked target identity, and target photos of the same ID, a new feature previously absent from CWPS. The two-step process mirrors that on the search results page, enabling community deliberation on the validity of an ID. Users can validate the facial match among photos linked to the same ID, express their agreement or disagreement on an ID with varying confidence levels, and optionally add a note explaining their decision. Thus, each vote contributes to a stigmergic collaboration, enhancing the reliability of photo IDs on CWPS. Consistent with CWPS's open participation model, Photo Steward allows any registered user to share their opinion on an identification.\r\n4.3 EVIDENCE: Visualizing Information Stewardship\r\nAs part of Photo Steward, we designed stewardship visualizations to help users assess the reliability of 1) facial matches (i.e., photos that were matched to each other by the user), and 2) photo IDs. The CWPS community's opinions on facial similarity comparison and photo IDs, captured through the validation interface, feeds into these reliability visualizations. These visualizations not only promote user accountability through social translucence [15], but also serve as deliberative evidence for subsequent stigmergic user collaborations (Design Goal 3).\r\n4.3.1 Reliability of Facial Similarity. For each photo pair that has been compared, the system aggregates the community's decisions for the visual match type and generates a distribution, which is displayed in the form of an interactive horizontal bar chart on the photo page. This chart appears next to the corresponding photo matched to the query photo (see Figure 5). Users can click the \"View Details\" button or an individual bar to see how each user voted. When multiple photos have been matched to the query photo, the matched photos appear one below the other, with each having its own visualization next to it. The bar charts are stacked vertically above each other to allow users to easily see and compare the reliability of every match.\r\nTo complement the community stewardship visualization, we also added an AI stewardship badge that indicates whether the particular match is supported by facial recognition (see Figure 5). On the search results page, CWPS retrieves those search results that have a facial similarity score greater than 0.50, so we use the same threshold here. However, the badge intentionally does not display the exact similarity scores (which have been found to be inconsistent [41, 44]) to avoid a false perception of precision, and cautions users to carefully analyze all the context and evidence, as there is a possibility of false positives with face recognition.\r\n\r\nFigure 5: Facial Similarity Reliability Visualization on the Photo Page. The photo matched to the query photo is displayed here, whereas the query photo is displayed on top of the page. Other photos matched to the query photo are displayed vertically one below the other. Users can bring up the query photo and the matched photo side-by-side by clicking the \"Compare\" button. Each matched photo has its own visualization next to it, and the responses are visible to everyone.\r\n\r\n4.3.2 Reliability of Photo Identifications. Similar to the facial similarity visualization, the system aggregates the community's confidence levels for an identification and generates a distribution, displayed in the form of an interactive horizontal bar chart. This visualization is displayed for every proposed identity on the photo page, below the biography subsection (see Figure 6a). If users propose multiple IDs for a given photo, they are displayed one below the other; each ID will have its own visualizations. The community's confidence votes are aggregated to order these IDs. Users can click the \"View Details\" button or the individual bars to view a modal dialog box with each user's confidence scores and optional text comments (see Figure 6b).\r\n\r\nFigure 6: Photo ID Reliability Visualization on the Photo Page for a given ID. If multiple IDs are present, they are displayed vertically one below the other, ordered in terms of aggregate votes. Each ID will have a separate visualization listed under the respective IDs.\r\n\r\nThis visualization reflects the community's expertise, and is intended for users to quickly assess the reliability of an identification. Prior work on crowdsourced person identification [41] suggests that airtight identifications are likely to show consensus from the community, whereas potential misidentifications are likely to reflect disagreement from the community. Further, the comments may reflect the voters' decision rationale and any external research they conducted before giving their decision on the photo's identification, allowing users to build on the work of others in making their own assessment [17].\r\n4.4 Summary\r\nPhoto Steward augments the CWPS platform with an information stewardship architecture to support community validation of historical photo IDs in a stigmergic manner. We summarize all the changes in the Appendix (see Table 6).\r\n5 EVALUATION\r\nWe obtained permission to publicly launch Photo Steward on CWPS in December 2020. We conducted a mixed-methods, exploratory evaluation study to understand how well users with different expertise levels could validate and assess Civil War photo IDs using CWPS with Photo Steward. Specifically, we wanted to understand 1) how users validated photo IDs using Photo Steward, 2) how the stewardship visualizations (i.e., ID and facial similarity reliability visualizations) impacted users' assessment of an ID. The study was approved by our university's IRB.\r\n5.1 Log Analysis\r\nTo understand the community's stewardship behaviors, we analyzed website logs of all user activities for a year after new features were launched, which included 5843 voting instances on 5672 photos for 5355 unique IDs. Our analysis included categorization of user deliberations as 'pre-identified' (i.e., the user knew the ID of the photo ahead of time and therefore, might be looking for a specific target on the search results page) or 'post-identified' (i.e., the user did not know the ID of the photo and therefore, may not have a specific target while analyzing the search results), coding of user comments (see Appendix C), and comparison of community's facial similarity comparisons against facial recognition scores. Details of these analysis methods are provided in Appendix B.\r\n5.2 Lab Study\r\nIn order to understand how well Photo Steward supports diverse users in validating the quality of photo identifications, we also conducted an exploratory lab study.\r\n5.2.1 Participants. We recruited 15 participants representing the three major expertise levels: 5 history students, 5 amateur experts (experienced users of CWPS), and 5 expert historians. Participant details can be found in Appendix D. We anonymize these groups with the following identifiers, respectively: S1–S5, C1—C5, and H1–H5.\r\n5.2.2 Dataset. For the study, we created a dataset of 10 different photos identified on CWPS. Three of these photos had an ID conflict, i.e., multiple identities were proposed. For two of these photos, one ID was correct and the other one was incorrect. The community had already researched both photos, voted on the correct ID, and left credible evidence in the comments. Both IDs were linked to additional photos as well. The third photo was one of the seeded photos on CWPS, but was originally misidentified. We added another false ID, making both IDs for the third photo incorrect. All photos had multiple photos matched to them; eight of them were linked via facial matches, while two of them had replicas.\r\n5.2.3 Procedure. The entire study was conducted online via recorded Zoom sessions, with at least one researcher attending each session. Each participant first completed a consent form and a pre-survey describing their demographics and Civil War photography experience.\r\nAs part of the study, participants reviewed three randomly assigned photos from the dataset one-by-one in the original CWPS system first, followed by the same photos on the Photo Steward version. Participants used a think-aloud protocol while using the two systems; after the completion of the task with each system, they were asked a few semi-structured questions about their experience. Finally, the participants completed a summative post-survey of standard usability questions (e.g., ease of use, usefulness of features, instruction clarity, preferred system, etc.) (see Appendix E)\r\nWe maintained this sequence (original CWPS first, CWPS with Photo Steward second) for all the participants, rather than using a randomized sequence, for two reasons. First, we did not want participants' assessments to be biased in favor of Photo Steward after seeing additional features in the new interface. This design allowed us to observe if the original interface misled the participants towards incorrect assessments, and if, subsequently, the Photo Steward interface helped correct them. Second, in a randomized sequence, Photo Steward would expose the participants to new information in the form of prior user votes and responses, and therefore, may confound how they assess the information on the original CWPS version.\r\n5.2.4 Data Analysis. The first author fully transcribed and analyzed the interviews and think-aloud recordings using an inductive qualitative thematic approach [4]. The transcript sections were first divided according to the interface in question (i.e., original CWPS or Photo Steward), followed by an open coding of the transcripts using MAXQDA 2020 [65]. The first author iterated and settled on a total of 28 codes (e.g., change in opinions, comparison interface, source trustworthiness, etc.) for 634 coded segments across all the transcripts. These codes were then organized into themes as described in Section 6 after discussing with the co-author.\r\n5.2.5 Limitations. We conducted a qualitative lab study to understand how users with different backgrounds and expertises validated photo IDs using Photo Steward and hit theoretical saturation. However, there are a couple of limitations with the study: 1) limited insights on the role of expertise, and 2) the task sequence could have order effects. Further, the large-scale analysis of Photo Steward logs provided us with insights of its usage amongst users. However, it lacked an expert-prepared gold standard dataset, which hindered our ability to conduct specific performance analyses as part of this study.\r\n6 FINDINGS\r\nUsing the methods above, we evaluated how well Photo Steward's stewardship architecture supported CWPS users in validating photo identifications, compared to the original version of CWPS.\r\n6.1 Validation Interface\r\nUsers found Photo Steward's validation interface to be useful for comparing different photos.. While assessing the IDs with the original interface, participants would go back and forth between different photos to compare whether they are the same person or not. Some participants opened the photos in two different browser windows and kept them side-by-side. While using the validation interface in the new system, participants appreciated being able to see the photos side-by-side at the same time.\r\nH1 said, \"As an historian using this, this is really great to see them both together. It just makes a comparison a lot easy for me to do. I mean, this is the same gentleman, he's got a little dark facial hair. It looks a little bit different there and the photo on the right, but the facial match is definitely there. \" This was also echoed by C3, who said, \"This, I really find extremely useful, especially when I'm trying to do facial recognition. I can zoom in and have them side by side here. [...] Where in the past, I would have to go back and forth between tabs or cut and paste them into a different document to look at them side by side.\"\r\nFrom our logs, we found that 223 users had compared 2319 unique photo pairs for facial similarity, with 156 pairs receiving comparisons from at least 2 different users. The facial similarity responses were distributed as follows: 763 replicas, 1232 facial matches, 283 unsure, and 280 different people.\r\nTable 1: Distribution of User Votes.\r\n\r\nVotes\r\n/ ID\r\n# of IDs\r\nNote\r\nPresent\r\nNegative\r\nVotes\r\n1\r\n5650\r\n511\r\n61\r\n2\r\n157\r\n(Agreement: 119)\r\n(Disagreement: 38)\r\n83\r\n12\r\n3+\r\n36\r\n(Agreement: 21)\r\n(Disagreement: 15)\r\n31\r\n4\r\nTable 2: Distribution of Confidence Levels.\r\n\r\nMean\r\nConfidence\r\n1\r\nVote / ID\r\n2\r\nVotes / ID\r\n3+\r\nVotes / ID\r\n-2 (No - Highly Confident)\r\nto\r\n-1 (No - Slightly Confident)\r\n45\r\n3\r\n2\r\n-1 (No - Slightly Confident)\r\nto\r\n0 (Not Sure)\r\n16\r\n3\r\n2\r\n0 (Not Sure)\r\n99\r\n8\r\n4\r\n0 (Not Sure)\r\nto\r\n1 (Yes - Slightly Confident)\r\n760\r\n15\r\n5\r\n1 (Yes - Slightly Confident)\r\nto\r\n2 (Yes - Highly Confident)\r\n4730\r\n128\r\n23\r\nUsers preferred the ability to provide granular feedback for photo IDs using Photo Steward's validation interface.. All participants expressed preference for the fine-grained confidence levels, including the ability to dispute an ID, in Photo Steward's validation interface, appreciating how it more accurately mirrored the inherent uncertainty present when assessing photo IDs. S3 said, \"I definitely like the five levels. I think it leaves more room for interpretation. Like sometimes it's kind of hard to just say yes or a hard no because so much goes into it. Especially because a lot of this stuff was so long ago, there's so many unanswered questions.\" H4 initially defended the original interface's binary vote, but changed her mind after experiencing Photo Steward's confidence levels: \"[W]hat I had said has this very black and white feel to it, you're wrong or you're right. I like these degrees of disagreement or agreement. I think that's way more helpful broadly.\"\r\nThe usage of the voting feature was reflected in our logs, which showed 5843 voting instances from 328 unique users (see Table 1). Table 2 shows that while users utilized the full range of confidence levels, including when they were unsure (mean confidence = 0) or slightly confident (mean confidence = 0 to 1) about the ID, the vast majority of the votes were highly confident ones (mean confidence = 1 to 2). A small proportion of votes (77) were cast for disputing an ID.\r\nUsers justified their voting decisions through notes covering an extensive range of topics..\r\nTable 3: Distribution of Note Topics. The table also displays how the notes are distributed for different user confidence levels.\r\n\r\nCategory\r\nSub-Category\r\nTotal\r\nNumber of\r\nNotes\r\nNo\r\n(Highly\r\nConfident)\r\nNo\r\n(Slightly\r\nConfident)\r\nNot\r\nSure\r\nYes\r\n(Slightly\r\nConfident)\r\nYes\r\n(Highly\r\nConfident)\r\nPhoto\r\nComparison\r\nHigh-Level\r\nComparison\r\n206\r\n9\r\n5\r\n9\r\n20\r\n163\r\n\r\nDescribing\r\nFacial Features\r\n37\r\n4\r\n3\r\n9\r\n8\r\n13\r\nWord-of-Mouth\r\nDescendant\r\n55\r\n1\r\n0\r\n0\r\n8\r\n46\r\n\r\nOwnership\r\n47\r\n1\r\n0\r\n0\r\n0\r\n46\r\n\r\nFamiliarity\r\n18\r\n0\r\n0\r\n0\r\n2\r\n16\r\nVisual\r\nEvidence\r\nUniform\r\n73\r\n7\r\n1\r\n8\r\n17\r\n40\r\n\r\nInscription\r\n145\r\n4\r\n3\r\n3\r\n6\r\n129\r\n\r\nOther\r\nVisual Clues\r\n17\r\n1\r\n0\r\n1\r\n2\r\n13\r\nExternal\r\nInformation\r\nExternal URL\r\n47\r\n1\r\n0\r\n1\r\n4\r\n41\r\n\r\nOther Sources\r\n172\r\n9\r\n1\r\n1\r\n21\r\n140\r\n\r\nLack of\r\nInformation\r\n21\r\n0\r\n2\r\n13\r\n6\r\n0\r\nProviding\r\nAdditional\r\nInformation\r\nBiographical\r\nInformation\r\n46\r\n11\r\n2\r\n3\r\n3\r\n27\r\n\r\nAdditional\r\nContext\r\n117\r\n5\r\n0\r\n2\r\n11\r\n99\r\nFrom our logs, we found that 155 users had left 682 notes for 600 different photos. However, as Table 1 shows, around 10% of the votes had a note. Table 3 shows the different topics covered by the notes. We observe that users' voting patterns are significantly influenced by the availability and quality of evidence, with clear facial similarity, period inscriptions, personal anecdotes, visible clues in the photograph, and added biographical context often leading to high confidence \"Yes\" votes, while lack of information typically results in \"Not Sure\" votes.\r\nUsers most frequently left a comment attributing facial similarity (after comparing it in the first step of the validation process) to be the reason for their decision (e.g., \"Identical to the other CDV\"). In some instances, they would expound on it by discussing facial features: \"The eyes, nose, cheek bones, shape of face, all look similar to George Pickett, although possibly reversed based on hair part\". In many instances, we found users inferring biographical information (service records, location, etc.) from visual evidence in the photo, be it uniform or backmarks (e.g., \"Initials MN on chinstrap (brass letters). Signature on verso is made out to Marlin's oldest sister. Style of insignia is consistent with other 1862 recruits for Co. B 1st USSS\").\r\nInterestingly, we also observed a large number of word-of-mouth evidence notes for justifying the user's decisions, such as claiming to be a descendant, or owning the original copy of the photo, or having seen the photo somewhere. For example, one user noted, \"He is my great-great grandfather and this photo has been passed down through the generations to me and was identified by his son John Albert Johnson, my father's grandfather.\" Users also left external URLs and source details in the notes as evidence. Sometimes, they provided additional context (e.g., \"This image came with a group of 7th Iowa images. The majority were of Company G., but there is only one person in the entire 7th Iowa Infantry that could be identified by the first or last name of 'Nelson.'\").\r\nThe validation workflow encouraged users to exercise careful deliberation while making photo ID decisions.. Users felt that the questions in each step of the validation process helped them to carefully weigh in all the evidence and deliberate while voting on the ID (Q4, mean = 4.60, SD = 0.49). C3 explained why the two steps were necessary: \"It's two separate things. One is asking, do you think that this face is the same face? Then the second is, do you think that this face matches this name? I think that that is a necessary question for both of those scenarios. I don't think it's redundant, I think it's necessary.\" H2 appreciated the thoughtfulness that the two-step process encouraged, saying, \"It could be the same guy, but it might be a different guy, but now that you know the other interface kind of forces me to slow down a little bit and think more carefully, because it's asking specific questions about things.\" A couple of participants, however, expressed initial confusion over the separation and order of these two tasks.\r\n\r\nFigure 7: Deliberation in case of post-identification voting. The table shows the distribution of confidence votes (second step of the validation process) against the user's facial similarity comparison (first step of the validation process). The comparison scores were computed by aggregating all the facial similarity comparisons in the first step (replica = 2, facial match = 1, not sure = 0, and different people = -1).\r\n\r\nOur logs affirm the deliberative aspect of the two-step validation process. We observed that when users compared one or more photos in the first step, they factored in the facial similarity evidence for their ID vote decision. Figure 7 shows that when the cumulative comparison scores are positive (i.e., majority of the comparisons were a facial match or a replica), the confidence scores are generally positive. Similarly, when the cumulative comparison scores are negative, the confidence votes are also largely negative. When the users are unsure about the facial similarity comparison, it reflects in their final ID vote. Notably, when there were mismatches between comparison scores and ID votes, users provided reasons in the notes, citing visual evidence and an unsure photo comparison.\r\nPhoto Steward's stewardship architecture fostered diverse forms of meaningful stigmergic collaboration amongst users.. Although most IDs received only a single vote, about 3% (193 IDs) received multiple votes (see Table 1). Among these, 140 IDs saw total consensus among voters on confidence scores, with half involving an explanatory note. When the initial vote lacked a note, subsequent voters often added information like external sources or context. Almost all IDs where voters agreed positively were supported by metadata such as period inscriptions, scholarly sources, or additional comparison photos, underscoring the credibility of these agreements.\r\nWhen users disagreed on an ID, they would almost always justify in notes why they differed from the previous voters' opinions (43 out of 53 disagreement instances). From analyzing the notes, we found that the subsequent voters provided additional information about external sources or service records to disagree (e.g., \"Please see Military Images magazine, MI, Volume XVI, Number 3, November - December 1994 for the correct identification of this image. The correct identification via the Michael McAfee collection is Richard Cramer, 4th Michigan Infantry\"). Among 18 instances of conflicting IDs for the same photo, seven saw resolution through a process of voting one ID over another, often accompanied by external evidence in notes or citing facial similarity.\r\nUser agreement was particularly strong in facial similarity comparisons. Of the 2200 unique photo pairs compared, 141 received multiple user reviews, with 114 seeing total agreement. The remaining cases typically involved user errors in distinguishing between 'facial match' and 'replica', or uncertainty about facial similarity.\r\n6.2 Stewardship Visualizations\r\nStewardship visualizations reflecting community insights enhanced the accuracy of photo ID assessments.. Participants found Photo Steward's bar chart visualization showing the five confidence levels to be simple and easy to understand (Q1, mean = 4.93, SD = 0.25). S1 said, \"I'm a visual person. Bar graphs or charts like statistical data helps me put things into a better perspective or gives me an idea of what I am working with versus just something more plain [like] the other interface.\"\r\nWhen participants were uncertain about an ID, they saw the additional evidence and justification that the community members had provided along with their vote as essential to taking the vote seriously. S5 said, \"If there's other users giving comments like 'I've used this source,' and you know you get information from a source where they found it, I'm probably gonna agree with them. If they're just voting yes without anything else, then I am probably more likely to go out and find sources for the photo myself and make my own decision.\"\r\nIn three instances, students (S1, S3, and S5) initially made incorrect assessments on the original interface but rectified these after using Photo Steward. The community's consensus and evidence in Photo Steward were pivotal in these corrections. S5, confronted with ID conflicts, stated after using Photo Steward: \"You know that helps a lot with what people are thinking and presumably these people have also gone to the Maine State Archives or something and verified and then given their opinion so that helps.\" Similarly, historian H2 and collector C3 initially made incorrect choices but amended their decisions after analyzing community-based evidence on Photo Steward. H2 remarked: \"So I believe it's the first person, of course, because it's based on the Maine State Archives. And then you've got the piece on there that said that you looked and found a different man. With that name in the actual regimental history. I say that's fairly accurate information.\" This highlights the critical role of community consensus in resolving photo ID conflicts.\r\nOn the other hand, when participants were confident about an ID, seeing the community's opinions affirmed their own assessments, for better or worse. In the words of H5: \"I really liked the very clear community consensus, and the ability to be able to see the identities of the people who were looking at these images. It was something that (gave) a boost of confidence in terms of my final decision.\" Overall, the participants found the community opinions to be useful for assessing the IDs (Q2, mean = 4.93, SD = 0.25).\r\nUsers gave additional weight to the opinions of members they were familiar with and desired more contextual information about all contributors.. Participants examined the bar chart visualization details and gave higher weight to the opinions of prominent names from the Civil War photography community while assessing the ID. C3 said, \"Here's <name redacted>, period inscription with valediction, the uniform matches his service record. <name redacted> said the same thing. These two uploaders I hold in very high regard to their opinions on this site. If they're saying that they think highly confident of this identification, that gives me a lot of confidence as well.\" H5 became further unsure about an ID after seeing a fellow historian's vote: \"And, you know, sort of knowing <name redacted>, someone who I know deals with primary source material a lot, and sort of being on the fence with it as well, leaves me in that unsure position.\"\r\nParticipants also sought additional details about community members, proposing indicators of professional status or active participation in the CWPS community. H1 emphasized his appreciation of amateur experts and opposition to gatekeeping, yet he suggested having some kind of credential indicator next to the username would be helpful: \"It will be great if, you know, somebody was an academic historian or a published Civil War author, if there was some way to just say some little tidbit next to <name redacted> 'Oh, saying, hey, I'm from <university redacted>, Professor or, you know, author of whatever.'\" Others preferred indicators of community activity levels or personal connections (i.e., descendants) to the identified individual in the photo.\r\nOur log analysis showed revealed the presence of certain active voters, with 9 out of 328 unique voters voting on more than 50 photos each, and 2 voting on over 2000 photos each (mostly ones that they had uploaded and identified). If we consider only votes on photos identified by someone else, we have 8 users who have voted on more than 15 IDs each, with 1 user voting on over 150 IDs.\r\nThe community opinions made the platform feel more engaging, but users had mixed thoughts about the ideal number of votes per photo.. Participants, in general, felt the community opinions made Photo Steward more engaging compared to the original system. H5 said, \"I thought it appeared more user-centric and user-friendly and it felt more participatory.\" C3 saw the benefits of increased engagement for making IDs more reliable: \"I think for this crowdsourcing project that we're building on this database [it] is very important to have those comments, those feedback, that we see in the voting system. It only makes this ID stronger and makes the project and the database a more trustworthy and reliable source.\"\r\nUsers wanted to see more community opinions and comments for IDs that had few votes. S1, on seeing only one vote for an ID, said, \"That doesn't make me feel as confident because that's not too many for me to give an answer.\" S3 said it would have been easier to assess some photos if \"there was more input from other people.\" Users had mixed opinions about how many votes they wanted to see for verifying an ID. In general, they wanted to see consensus among the community for an identity and at least three votes. C3 said, \"I usually shoot for like three to four [votes] as the lowest where I take some good quality out of those votes. [...] If there's only one or two, and especially if there's two that are split, that is not as reliable to me.\"\r\nParticipants' concerns about spreading voters too thin were borne out in the log data (see Table 1). We analyzed the logs to check how often CWPS users vote on the photos they are browsing. We found 1784 instances (out of 5843 voting instances) where the number of \"lurkers\" for a given photo page exceeded the number of voters on that photo.\r\nTable 4: Face Recognition Similarity Score Stats vs. User Comparisons (Facial Similarity).\r\n\r\nUser Comparison\r\nMean\r\nMedian\r\nSD\r\nCount\r\nReplica\r\n0.88\r\n0.91\r\n0.16\r\n659\r\nFacial Match\r\n0.52\r\n0.65\r\n0.31\r\n1015\r\nDifferent Person\r\n0.55\r\n0.59\r\n0.16\r\n265\r\nNot Sure\r\n0.51\r\n0.58\r\n0.23\r\n261\r\nUsers found the face recognition badge and community's opinions complementary for assessing the reliability of matched photos..\r\nTable 5: User Comparisons (Facial Similarity) vs. Face Recognition.\r\n\r\n\r\n# of Photo Pairs\r\n# of Photo Pairs\r\ncompared by 1 user\r\n# of Photo Pairs\r\ncompared by 2 users\r\n# of Photo Pairs\r\ncompared by 2+ users\r\nSupported by users\r\nand face recognition\r\n1408\r\n1281\r\n109\r\n18\r\nDisputed by users\r\nand face recognition\r\n26\r\n24\r\n0\r\n1\r\nUsers unsure,\r\nface recognition disputes\r\n42\r\n42\r\n0\r\n0\r\nUsers unsure,\r\nface recognition supports\r\n219\r\n217\r\n2\r\n0\r\nUsers dispute,\r\nface recognition supports\r\n239\r\n238\r\n1\r\n0\r\nUsers support,\r\nface recognition disputes\r\n266\r\n257\r\n8\r\n1\r\nFrom our logs, we found that 1408 photo pair comparisons were supported by both users and facial recognition, by far the most common outcome (see Table 5). This information would be visible to the larger user community in the form of stewardship visualizations (see Figure 5). Interestingly, we also see 266 cases where a comparison is supported by users, but disputed by facial recognition (similarity confidence score < 0.50). That outcome was approximately as common as when users disputed a comparison but face recognition supported it (239 pairs) or when a user was unsure but face recognition supported it (219 pairs). However, it was far less common for facial recognition to dispute a comparison when a user also disputed it (26 pairs) or was unsure (42 pairs).\r\nWhen we analyze the face recognition's confidence scores in more detail, Table 4 shows that there is a clear separation between replicas and other types of user comparison scores. Face recognition confidence scores for photo pairs that users labeled as replicas were much higher in terms of both mean and median (0.88 and 0.91, respectively, versus scores in the 0.50s and 0.60s for all others). While these fine-grained scores are not displayed to users — they intentionally see only the face recognition badge — the very close mean and median confidence scores for \"facial match\" versus \"different person\" illustrate the difficulty of automatically identifying non-replica matches and offer support for a hybrid human-AI approach (cf. Section 4.3.1).\r\nParticipants found the community's opinions to be helpful for assessing whether two photos were facial matches or not. S4 said, \"I mean, I think it's cool to see what the community is saying, because I do feel when it comes to saying 'Is this the same person in both these pictures?', that's really the best way to do it if you don't have any [other] information.\" C1 said that he found the community opinions for facial matches especially helpful because he is \"face blind\": \"I can't really identify the face-to-face, but the hair and the mustache and all the stuff that, in addition, it helps me with that for sure. It's good because other than having to find somebody close to me and be like, 'Do you think this is the same people?' [I] have that community right there.\"\r\nParticipants had mixed opinions about facial recognition technology, but most found the badge indicating whether it supported the two photos being a match to be a useful data point. S3 said, \"The facial recognition saying they are similar — I would go ahead and trust that but I don't know if I would trust it enough to make a verification on my own.\" In general, participants found the strengths of facial recognition and the community to be complementary in determining whether two photos showed the same person, and liked seeing both results together. H4 felt the community and the technology had separate roles:\r\nThis is facial recognition, and this is the historical background. I trust facial recognition, but it makes me feel better to have that historical background. I think the human eye can be tricked by different hairstyles and different beards. Just to have this outside historical verification to say like, 'Okay, maybe you or I was tricked, but the machine was not,' I think that's really helpful.\r\nS1 relied on both the community and facial recognition to make a decision on a facial match: \"I think both give me kind of an idea. Okay, there's this facial recognition technology being used, but also there's other users that are leaning towards that this is the same person.\" H5 got a similar boost of confidence: \"This is certainly reassuring seeing not only the AI match, but also in terms of the community — seeing that seven users have said that this is a facial match as well. I'd be quite convinced by this.\"\r\n7 DISCUSSION\r\n7.1 Leveraging Collective Intelligence for Validating Person Identification\r\nPrior work has raised concerns about misinformation in online history communities [43, 75]. To address these problems on CWPS, we built Photo Steward for supporting community-based validation of photo IDs. Users found Photo Steward's stewardship visualizations not only helpful for affirming their own assessment, but also for discovering new knowledge and correcting their decisions, if need be. These visualizations, combined with the validation workflow, exhibit a form of stigmergic collaboration, where users build on prior knowledge left by the community and leave their own assessment for other users [14, 17, 27, 56].\r\nPhoto Steward allowed users to express how confident they are about an ID in a fine-grained manner, in contrast to the binary agreements or disagreements observed on iNaturalist by Wiggins et al. [72]. The CWPS community preferred this nuanced form of stewardship as users are likely to have different degrees of confidence based on the evidence available for identifying a photo, thus demonstrating the effectiveness of Design Goal 2. As S3 pointed out, users often experience difficulty in making a binary decision about individuals who lived 150 years back due to the lack of surviving documentation.\r\nBeyond its basic usefulness, Photo Steward's full potential can best be realized through sustained community participation, but most IDs on CWPS only received one vote. To address this challenge, we can leverage different crowdsourcing and online community strategies. For example, we can draw the community's attention towards IDs that are \"more of a puzzle,\" as H5 suggested, similar to Twitter's Birdwatch promoting tweets for fact-checking [52]. Designing nudges to encourage lurkers to vote on the IDs they are viewing can further help in these efforts. Organizing community events can help foster interest and participation in collaboratively verifying IDs, drawing inspiration from crowdsourcing events like CrowdSolve, where experts and novices collaborate on solving missing persons cold cases [70]. Incentive mechanisms such as leaderboards and challenges [46] can drive extrinsic motivation within the community for verifying the IDs. Finding users who are more likely to vote on an ID, based on their skills and interests, can also be an effective collaboration strategy [71]. In future work, we plan to integrate these strategies and introduce explicit \"calls to action\" [51, 58] on the home feed, guiding the community's attention towards IDs that require validation and fostering more sustainable, collaborative participation in historical photo identification.\r\n7.2 Exercising Deliberation in Human-AI Teams\r\nWe found that Photo Steward's validation workflow was effective not only for voting on the IDs and comparing the photos side-by-side, but also encouraged users to deliberate on their decision, drawing parallels to other social computing systems that support reflection and deliberation (e.g., [28, 29]). This deliberative intervention was non-trivial as users on CWPS follow an identification pipeline which is powered by facial recognition, an AI algorithm that is far from perfect [44]. Users are trying to find the correct match, if present at all, from a pool of potential candidates, which are largely comprised of similar-looking false positives — akin to finding a needle in a haystack. Further, the task of comparing photos of people is by no means an easy task for humans, even in a modern context [53]. While Photo Steward can not completely curb automation bias, an issue that has been previously observed in multiple online history communities [44, 75], its multi-step, validation workflow with structured feedback interventions encouraged users to deliberate over AI suggestions before making a decision.\r\nPhoto Steward's workflow also compartmentalizes the tasks that AI is good at – such as quickly retrieving similar-looking candidates from a large search pool – from the tasks where the AI makes more errors – such as verifying whether two faces show the same person or not [5, 55]. Decoupling facial similarity comparison from the person identification task (Design Goal 1) allows the users to now focus on the face verification task. In doing so, Photo Steward's workflow supports effective human-AI teaming in the context of person identification by allowing the user to make a granular assessment for the face verification task instead of the AI, while also ensuring that an AI-retrieved, similar-looking potential candidate is being compared against.\r\nAs imperfect AI algorithms get deployed in high-stakes scenarios such as medical imaging, law enforcement, etc. [6], it becomes more critical to reduce automation bias and encourage more deliberative decision-making. Amershi et al. recommend granular user feedback while interacting with AI systems as part of their \"Guidelines for Human-AI Interaction\" [2]. Similarly, other forms of design interventions, such as counterfactual AI explanations [63], chatbots [26], and community opinions [59] can also be explored for encouraging deliberative decision-making with AI assistance.\r\nPrior work in human-face recognition teams has shown that algorithmic suggestions can have a significant biasing effect on a user's decision [23]. Our findings showed that Photo Steward was able to encourage users to exercise deliberation while interacting with results retrieved by facial recognition. At the same time, Table 4 also showed that users can differ from the algorithm's suggestions, thus necessitating a deeper dive analysis of this dissonance as part of future work.\r\n7.3 Assessing Quality in Crowdsourced Original Historical Research\r\nWe found that Photo Steward's stewardship visualizations helped users assess the reliability of photo IDs on CWPS, which were a result of Design Goal 3. However, crowdsourced identifications always run the risk of groupthink [24, 25], which can eventually mislead users into believing and amplifying misidentifications, a concern also raised by H4. Public deliberation of modern photo IDs on social media can have profound negative consequences for false targets, as exemplified by the Boston Marathon bombing [49, 66] and the recent US Capitol riot [48]. This raises the question: are Photo Steward's stewardship visualizations sufficient for assessing the quality of photo identifications made on CWPS?\r\nPrior work on crowdsourced scholarship suggests an answer. Rosenzweig [57] analyzed Wikipedia as a source of historical scholarship, noting its policy against original research, and advocating for it as a tool for teaching the limitations of information sources and critical analysis of primary and secondary sources. Motivated along similar lines, Forte et al [18] proposed the assessability framework for designing assessable participatory information systems, based on information provenance and stewardship. The concept of provenance, extensively used in history and archival studies, describes information that makes it possible to trace the ownership or origins of the content, while stewardship refers to the processes that were used for maintaining the content, including its authorship. In the case of Wikipedia, Forte et al. found that visualizing provenance (i.e., citation types) and stewardship (i.e., article quality) had a significant impact on assessments of articles and Wikipedia as an information source.\r\nWhile Photo Steward enables information stewardship on CWPS, there is an opportunity for incorporating provenance into the CWPS platform to make it a truly assessable online platform. A significant proportion of the notes left by users on Photo Steward qualified as provenance information, namely comments about period inscriptions, family trees, external sources and URLs. The challenges of assessing IDs on CWPS are, however, different from assessing information on Wikipedia, primarily because CWPS supports original research unlike Wikipedia's no original research policy [73]. This was also the reason why we designed Photo Steward to be a review system rather than a single editable output such as Wikipedia; original research such as historical photo identifications is often times an evolving investigation rather than a final decision. To assess the reliability of original photo IDs made on CWPS, users may want to factor in the provenance of the reference photos that were used in the identification process. In such cases, Photo Steward's stewardship visualizations (i.e., facial similarity reliability) can further help the user in assessing whether the reference photos can be used as reliable provenance or not.\r\n8 CONCLUSION\r\nPhoto Steward attempts to help users assess and validate photo IDs better on CWPS. We present an information stewardship architecture, and adapt it for the task of historical person identification. We demonstrate the effectiveness of Photo Steward on CWPS, an existing online platform, where users found the stewardship visualizations, which included the community opinions and the AI verdict, useful for making accurate assessments of photo IDs on the platform. Further, users found Photo Sleuth's multi-step, structured validation workflow to help them deliberate before making decisions about the photo's identity. This work opens doors for exploring new ways to leverage collective intelligence and AI in creating assessable online information systems for historical archives.\r\nACKNOWLEDGMENTS\r\nWe wish to thank Ron Coddington, Paul Quigley, Liling Yuan, and our study participants. This research was supported by NSF IIS-1651969 and a Virginia Tech ICTAS Junior Faculty Award.\r\nREFERENCES\r\n2021. Heritage Auctions: World's Largest Collectibles Auctioneer. https://www.ha.com/ Navigate tocitation 1citation 2\r\nSaleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. 2019. Guidelines for human-AI interaction. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–13. Navigate tocitation 1\r\nBrakkton Booker. 2015. $2 photo found at Junk Store has Billy the kid in it, could be worth $5M. https://www.npr.org/sections/thetwo-way/2015/10/15/448993361/-2-photo-found-at-junk-store-has-billy-the-kid-in-it-could-be-worth-5-million Navigate tocitation 1citation 2\r\nVirginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3, 2 (2006), 77–101. Navigate tocitation 1\r\nJoy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Conference on Fairness, Accountability and Transparency, FAT 2018, 23-24 February 2018, New York, NY, USA. 77–91. http://proceedings.mlr.press/v81/buolamwini18a.html Navigate tocitation 1\r\nCarrie J Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg S Corrado, Martin C Stumpe, et al. 2019. Human-centered tools for coping with imperfect algorithms during medical decision-making. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–14. Navigate tocitation 1\r\nFanny Chevalier, Stéphane Huot, and Jean-Daniel Fekete. 2010. Wikipediaviz: Conveying article quality for casual wikipedia readers. In 2010 IEEE Pacific Visualization Symposium (PacificVis). IEEE, 49–56. Navigate tocitation 1\r\nH Jason Combs. 2005. The Internet's Impact on the Antiques Trade. Material Culture (2005), 26–41. Navigate tocitation 1citation 2\r\nAnthony DeBartolo. 1975. Appraisers can tell when old photo really may be golden. https://www.chicagotribune.com/news/ct-xpm-1985-12-27-8503300067-story.html Navigate tocitation 1\r\nAnthony DeBartolo. 2021. Appraisers can tell when old photo really may be golden. https://www.chicagotribune.com/news/ct-xpm-1985-12-27-8503300067-story.html Navigate tocitation 1\r\nHarley D Dickinson. 1998. Evidence-based decision-making: an argumentative approach. International Journal of Medical Informatics 51, 2-3 (1998), 71–81. Navigate tocitation 1\r\nFor The Inquirer Edward Colimore. 2019. Did John Wilkes Booth get away with murdering President Abraham Lincoln?https://www.inquirer.com/news/john-wilkes-booth-lincoln-conspiracy-photo-recognition-20190415.html Navigate tocitation 1\r\nMark Elliott. 2006. Stigmergic Collaboration: The Evolution of Group Work: Introduction. m/c journal 9, 2 (2006). Navigate tocitation 1citation 2citation 3\r\nMark Elliott. 2016. Stigmergic collaboration: A framework for understanding and designing mass collaboration. In Mass collaboration and education. Springer, 65–84. Navigate tocitation 1citation 2citation 3\r\nThomas Erickson and Wendy A Kellogg. 2000. Social translucence: an approach to designing systems that support social processes. ACM transactions on computer-human interaction (TOCHI) 7, 1 (2000), 59–83. Navigate tocitation 1\r\nDan Evon. 2020. Did Joe Biden's Great-Grandfather Own Slaves?https://www.snopes.com/fact-check/joe-biden-slaves-great-grandfather/ Navigate tocitation 1\r\nKristie Fisher, Scott Counts, and Aniket Kittur. 2012. Distributed sensemaking: improving sensemaking by leveraging the efforts of previous users. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 247–256. Navigate tocitation 1citation 2\r\nAndrea Forte, Nazanin Andalibi, Thomas Park, and Heather Willever-Farr. 2014. Designing information savvy societies: an introduction to assessability. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2471–2480. Navigate tocitation 1citation 2citation 3citation 4\r\nJacey Fortin. 2018. She Was the Only Woman in a Photo of 38 Scientists, and Now She's Been Identified. The New York Times (Mar 2018). https://www.nytimes.com/2018/03/19/us/twitter-mystery-photo.html Navigate tocitation 1\r\nSarah A Gilbert. 2020. \" I run the world's largest historical outreach project and it's on a cesspool of a website.\" Moderating a Public Scholarship Site on Reddit: A Case Study of r/AskHistorians. Proceedings of the ACM on Human-Computer Interaction 4, CSCW1 (2020), 1–27. Navigate tocitation 1\r\nJerome S Handler and Michael L Tuite. 2007. Retouching History: The Modern Falsification of a Civil War Photograph. Navigate tocitation 1citation 2\r\nM. Keith Harris. 2019. Civil War Photo Sleuth. Journal of American History 106, 2 (2019), 544–546. https://doi.org/10.1093/jahist/jaz498 Navigate tocitation 1citation 2\r\nJohn J Howard, Laura R Rabbitt, and Yevgeniy B Sirotin. 2020. Human-algorithm teaming in face recognition: How algorithm outcomes cognitively bias human decision-making. Plos one 15, 8 (2020), e0237855. Navigate tocitation 1\r\nPan Hui and Sonja Buchegger. 2009. Groupthink and peer pressure: Social influence in online social network groups. In 2009 International Conference on Advances in Social Network Analysis and Mining. IEEE, 53–59. Navigate tocitation 1\r\nNassim JafariNaimi and Eric M Meyers. 2015. Collective intelligence or group think? Engaging participation patterns in World Without Oil. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing. 1872–1881. Navigate tocitation 1\r\nSoomin Kim, Jinsu Eun, Joseph Seering, and Joonhwan Lee. 2021. Moderator chatbot for deliberative discussion: Effects of discussion structure and discussant facilitation. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–26. Navigate tocitation 1\r\nAniket Kittur, Andrew M Peters, Abdigani Diriye, and Michael Bove. 2014. Standing on the schemas of giants: socially augmented information foraging. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. 999–1010. Navigate tocitation 1\r\nTravis Kriplean, Caitlin Bonnar, Alan Borning, Bo Kinney, and Brian Gill. 2014. Integrating On-demand Fact-checking with Public Dialogue. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing(CSCW '14). ACM, New York, NY, USA, 1188–1199. https://doi.org/10.1145/2531602.2531677 Navigate tocitation 1citation 2citation 3\r\nTravis Kriplean, Michael Toomim, Jonathan Morgan, Alan Borning, and Andrew Ko. 2012. Is This What You Meant?: Promoting Listening on the Web with Reflect. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems(CHI '12). ACM, New York, NY, USA, 1559–1568. https://doi.org/10.1145/2207676.2208621 Navigate tocitation 1citation 2citation 3\r\nKurt Luther. 2016. How Fellow Collectors, Field Photos and Muttonchops Identified an Unknown Officer. Military Images 34, 1 (2016), 29–31. Navigate tocitation 1\r\nKurt Luther. 2017. Merrill Carbine Leads to a Soldier's Identification. Military Images 35, 2 (2017), 64–65. Navigate tocitation 1citation 2citation 3\r\nKurt Luther. 2018. Non-Traditional Research Tools—and Serendipity. Military Images 36, 3 (2018), 12–13. Navigate tocitation 1\r\nKurt Luther. 2018. What are the odds? Photo sleuthing by the numbers. Military Images 36, 1 (2018), 12–15. Navigate tocitation 1citation 2citation 3citation 4citation 5\r\nKurt Luther. 2019. What to Do When Gold Standards Go Wrong?Military Images 37, 1 (2019), 8–9. https://www.jstor.org/stable/26532101 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: How to Trust the Worthiness of an Identification. Military Images 38, 3 (213) (2020), 8–11. https://www.jstor.org/stable/26914966 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: Lost and Found in the Library of Congress. Military Images 38, 2 (212) (2020), 10–13. https://www.jstor.org/stable/26890126 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: The Art of Photo Sleuthing. Military Images 38, 4 (214) (2020), 8–11. https://www.jstor.org/stable/26925454 Navigate tocitation 1\r\nRamona Martinez. 2012. Photo mystery solved, then doubted, then deciphered, thanks to readers. https://www.npr.org/sections/pictureshow/2012/04/17/150801239/photo-mystery-solved-then-doubted-then-resolved-thanks-to-readers Navigate tocitation 1\r\nRamona Martinez. 2012. Unknown No More: Identifying A Civil War Soldier. http://www.npr.org/2012/04/11/150288978/unknown-no-more-identifying-a-civil-war-soldier Navigate tocitation 1\r\nChristian A Meissner and John C Brigham. 2001. Thirty years of investigating the own-race bias in memory for faces: A meta-analytic review.Psychology, Public Policy, and Law 7, 1 (2001), 3. Navigate tocitation 1\r\nVikram Mohanty, Kareem Abdol-Hamid, Courtney Ebersohl, and Kurt Luther. 2019. Second opinion: Supporting last-mile person identification with crowdsourcing and face recognition. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 86–96. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6\r\nVikram Mohanty and Kurt Luther. 2023. DoubleCheck: Designing Community-based Assessability for Historical Person Identification. ACM Journal on Computing and Cultural Heritage (JOCCH) (to appear) (2023). Navigate tocitation 1\r\nVikram Mohanty, David Thames, Sneha Mehta, and Kurt Luther. 2019. Photo sleuth: Combining human expertise and face recognition to identify historical portraits. In Proceedings of the 24th International Conference on Intelligent User Interfaces. 547–557. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6citation 7citation 8\r\nVikram Mohanty, David Thames, Sneha Mehta, and Kurt Luther. 2020. Photo Sleuth: Identifying Historical Portraits with Face Recognition and Crowdsourced Human Expertise. ACM Transactions on Interactive Intelligent Systems (TiiS) 10, 4 (2020), 1–36. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6citation 7citation 8citation 9citation 10\r\nMeredith Ringel Morris, Scott Counts, Asta Roseway, Aaron Hoff, and Julia Schwarz. 2012. Tweeting is believing? Understanding microblog credibility perceptions. In Proceedings of the ACM 2012 conference on computer supported cooperative work. 441–450. Navigate tocitation 1\r\nBenedikt Morschheuser, Juho Hamari, and Jonna Koivisto. 2016. Gamification in crowdsourcing: a review. In 2016 49th Hawaii International Conference on System Sciences (HICSS). IEEE, 4375–4384. Navigate tocitation 1\r\nKathleen L Mosier and Linda J Skitka. 1999. Automation use and automation bias. In Proceedings of the human factors and ergonomics society annual meeting, Vol. 43. SAGE Publications Sage CA: Los Angeles, CA, 344–348. Navigate tocitation 1citation 2\r\nGreg Myre. 2021. How Online Sleuths Identified Rioters At The Capitol. https://www.npr.org/2021/01/11/955513539/how-online-sleuths-identified-rioters-at-the-capitol Navigate tocitation 1\r\nJohnny Nhan, Laura Huey, and Ryan Broll. 2017. Digilantism: An analysis of crowdsourcing and the Boston marathon bombings. The British journal of criminology 57, 2 (2017), 341–361. Navigate tocitation 1\r\nRaja Parasuraman and Dietrich H Manzey. 2010. Complacency and bias in human use of automation: An attentional integration. Human factors 52, 3 (2010), 381–410. Navigate tocitation 1\r\nJunwon Park, Ranjay Krishna, Pranav Khadpe, Li Fei-Fei, and Michael Bernstein. 2019. AI-based request augmentation to increase crowdsourcing participation. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 115–124. Navigate tocitation 1\r\nSarah Perez. 2022. Twitter to show 'Birdwatch' community fact-checks to more users, following criticism. https://techcrunch.com/2022/03/03/twitter-to-show-birdwatch-community-fact-checks-to-more-users-following-criticism/ Navigate tocitation 1\r\nP Jonathon Phillips, Amy N Yates, Ying Hu, Carina A Hahn, Eilidh Noyes, Kelsey Jackson, Jacqueline G Cavazos, Géraldine Jeckeln, Rajeev Ranjan, Swami Sankaranarayanan, et al. 2018. Face recognition accuracy of forensic examiners, superrecognizers, and face recognition algorithms. Proceedings of the National Academy of Sciences 115, 24 (2018), 6171–6176. Navigate tocitation 1citation 2\r\nPeter Pirolli, Evelin Wollny, and Bongwon Suh. 2009. So you know you're getting the best possible information: a tool that increases Wikipedia credibility. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1505–1508. Navigate tocitation 1\r\nInioluwa Deborah Raji and Joy Buolamwini. 2019. Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products. Navigate tocitation 1\r\nAmira Rezgui and Kevin Crowston. 2018. Stigmergic coordination in Wikipedia. In Proceedings of the 14th International Symposium on Open Collaboration. 1–12. Navigate tocitation 1\r\nRoy Rosenzweig. 2006. Can History Be Open Source? Wikipedia and the Future of the Past. Journal of American History 93, 1 (June 2006), 117–146. Navigate tocitation 1citation 2\r\nSaiph Savage, Andres Monroy-Hernandez, and Tobias Höllerer. 2016. Botivist: Calling volunteers to action using online bots. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing. 813–822. Navigate tocitation 1\r\nMike Schaekermann, Joslin Goh, Kate Larson, and Edith Law. 2018. Resolvable vs. irresolvable disagreement: A study on worker deliberation in crowd work. Proceedings of the ACM on Human-Computer Interaction 2, CSCW (2018), 1–19. Navigate tocitation 1\r\nMichael S. Schmidt. 2018. 'Flags of Our Fathers' Author Now Doubts His Father Was in Iwo Jima Photo. The New York Times (Jan 2018). https://www.nytimes.com/2016/05/04/us/iwo-jima-marines-bradley.html Navigate tocitation 1\r\nJennifer Schuessler. 2017. Found: Oldest Known Photo of a U.S. President (Socks and All). https://www.nytimes.com/2017/08/16/arts/design/john-quincy-adams-daguerreotype-sothebys-auction.html Navigate tocitation 1\r\nJulia Schwarz and Meredith Morris. 2011. Augmenting web pages and search results to support credibility assessment. In Proceedings of the SIGCHI conference on human factors in computing systems. 1245–1254. Navigate tocitation 1\r\nRuoxi Shang, KJ Kevin Feng, and Chirag Shah. 2022. Why Am I Not Seeing It? Understanding Users' Needs for Counterfactual Explanations in Everyday Recommendations. In 2022 ACM Conference on Fairness, Accountability, and Transparency. 1330–1340. Navigate tocitation 1\r\nLinda J Skitka, Kathleen L Mosier, and Mark Burdick. 1999. Does automation bias decision-making?International Journal of Human-Computer Studies 51, 5 (1999), 991–1006. Navigate tocitation 1\r\nVerbi Software. 2019. MAXQDA 2020 [computer software]. VERBI Software. Available from maxqda.com. Navigate tocitation 1\r\nNPR Staff. 2016. How Social Media Smeared A Missing Student As A Terrorism Suspect. https://www.npr.org/sections/codeswitch/2016/04/18/474671097/how-social-media-smeared-a-missing-student-as-a-terrorism-suspect Navigate tocitation 1\r\nBongwon Suh, Ed H Chi, Aniket Kittur, and Bryan A Pendleton. 2008. Lifting the veil: improving accountability and social transparency in Wikipedia with wikidashboard. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1037–1040. Navigate tocitation 1\r\nW Ben Towne, Aniket Kittur, Peter Kinnaird, and James Herbsleb. 2013. Your process is showing: controversy management and perceived quality in Wikipedia. In Proceedings of the 2013 conference on Computer supported cooperative work. 1059–1068. Navigate tocitation 1\r\nCivil War Trust. 2021. Military Images Magazine | Interview with Ron Coddington. https://www.battlefields.org/learn/articles/military-images-magazine Navigate tocitation 1\r\nSukrit Venkatagiri, Aakash Gautam, and Kurt Luther. 2021. CrowdSolve: Managing Tensions in an Expert-Led Crowdsourced Investigation. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–30. Navigate tocitation 1\r\nShaun Wallace, Lucy Van Kleunen, Marianne Aubin-Le Quere, Abraham Peterkin, Yirui Huang, and Jeff Huang. 2017. Drafty: Enlisting Users To Be Editors Who Maintain Structured Data. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 5. Navigate tocitation 1\r\nAndrea Wiggins and Yurong He. 2016. Community-based data validation practices in citizen science. In Proceedings of the 19th ACM Conference on computer-supported cooperative work & social computing. 1548–1559. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6\r\nFoundation Wikimedia. 2022. No original research. https://en.wikipedia.org/wiki/Wikipedia:No_original_research Navigate tocitation 1\r\nHeather Willever-Farr, Lisl Zach, and Andrea Forte. 2012. Tell me about my family: A study of cooperative research on Ancestry. com. In Proceedings of the 2012 iConference. ACM, 303–310. Navigate tocitation 1\r\nHeather L Willever-Farr and Andrea Forte. 2014. Family matters: Control and conflict in online family history production. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. ACM, 475–486. Navigate tocitation 1citation 2citation 3citation 4citation 5\r\nJeremy B Wilmer. 2017. Individual differences in face recognition: A decade of discovery. Current Directions in Psychological Science 26, 3 (2017), 225–230. Navigate tocitation 1\r\nBob Zeller. 2022. Searching for photos of Civil War Soldiers | David Wynn Vaughan. https://www.civilwarphotography.org/searching-for-photos-of-civil-war-soldiers/ Navigate tocitation 1\r\nA CIVIL WAR PHOTO SLEUTH: DESIGN CHALLENGES AND OPPORTUNITIES\r\n \r\nHere, we provide details of three key challenges posed by Civil War Photo Sleuth that might have contributed towards misidentification and subsequently, inaccurate assessments, and how they present design opportunities for Photo Steward.\r\nA.1 Conflating facial similarity with photo identification\r\nA.1.1 Challenges: Mohanty et al. conducted a benchmarking study of CWPS [44], and found the face recognition algorithm to be of low precision; i.e., it retrieved hundreds of search results which may look similar to the query photo but are actually different people (false positives). Low precision increases the chances that users will interact with a lot of false positives. In such cases, one may need to garner additional information (i.e., comparing biographical information) before making a decision. However, it is plausible that automation bias may play a role in non-expert users making a match solely based on facial similarity [47]. As a result, there is strong potential for misidentification (see Figure 8).\r\nA.1.2 Current Workflow: CWPS's compare interface allows users to closely inspect the search results for a potential match, but does not make any distinction between facial similarity comparison and photo identification. Both are conflated into a one-step process, with one \"Identify\" button for the users to make their decisions (see Figure 2-C). Yet, users may want to indicate agreement with just the facial similarity (i.e., query photo and the search result showing the same person) but not the identity (i.e., name and biographical information), or vice versa.\r\nA.1.3 Design Goal 1: To support accurate investigation of photo identifications, users should be able to deliberate on the different aspects of the decision-making process. Providing users with a decision-making workflow that decouples facial similarity comparison from the overall photo identification task would allow them to focus on these tasks separately, while discouraging them from making decisions solely on the basis of facial similarity.\r\n\r\nFigure 8: An example of a misidentified photo on CWPS. This photo was identified as John C Whiteside based on facial similarity with the top search result: an identified reference photo of John C Whiteside. However, other visual clues, such as the photographer studio, do not align with Whiteside's biographical information.\r\n\r\nA.2 Lacking support for fine-grained, deliberative decision making\r\nA.2.1 Challenges: Historical photo identification is a complex task, where experts often corroborate multiple pieces of evidence, including facial similarity comparison, before reaching a decision about the identity of the photo [33]. While confirming an identity, experts may be highly confident if the source, military records, uniform clues, and additional photos of the same person all line up, or slightly confident if they need additional evidence. Conflicting evidence pieces may also affect their confidence levels. Similarly, they may have different degrees of certainty while ruling out an identity for a photo. A lack of support for expressing and displaying granularity in these photo identification decisions can lead to varying degrees of uncertainty being captured and misinterpreted as a confirmation, and eventually propagating misidentifications. Further, while it is safe to assume the vast majority of the Civil War photography community care about the accuracy of the photo IDs, there exists a small risk of financially-driven misidentifications since certain types of identified photos are considered to be more valuable [9].\r\nA.2.2 Current Workflow: The compare interface's \"Identify\" button (see Figure 2-C) is the only mechanism available on the platform for users to provide (a unary) input on photo identifications. CWPS users currently cannot provide fine-grained feedback on a given photo's identity, either at the time of identifying or afterwards on the photo page.\r\nA.2.3 Design Goal 2 : Users should be able to express how certain or uncertain they are about a photo identification. Interventions for allowing users to provide structured feedback can not only facilitate more accurate, fine-grained responses, but also encourage deliberation on the available evidence before making a decision, borrowing inspiration from other social computing systems that support users reflecting and deliberating on available information [28, 29].\r\nA.3 Limited validation opportunities for the community\r\nA.3.1 Challenges: For humans, deciding whether two photos show the same person is a highly subjective experience. A study by Mohanty et al. [41] showed that participants in a crowdsourcing study often disagree with each other and with facial recognition results in person identification tasks. The same study showed that people often seek a second opinion from peers for validating photo identifications. Without validation, original crowdsourced historical research may result in unresolved cases of conflicting IDs proposed for the same face, incorrect IDs remaining unchecked, or airtight IDs not being confirmed.\r\nA.3.2 Current Workflow: Once a photo has been identified, CWPS does not offer other users the option to express their opinions on whether two photos show the same person or not, or if the photo has been identified correctly or not (see Figure 8). The photo page does not indicate if (or why) the photos are linked to each other, how they are similar, or which user(s) considered them to be similar. Further, it does not indicate the role of facial recognition in linking them together.\r\nA.3.3 Design Goal 3 : To support accurate original historical research, the platform should encourage information stewardship from the community by allowing members to give their opinions on existing photo identifications [72]. Further, the platform should also be transparent about the role of community stewardship and facial recognition for a given photo identification, which can then act as evidence for aiding subsequent validation efforts by other users, thus supporting a form of stigmergic collaboration [13, 14].\r\nB DETAILS FOR LOG ANALYSIS\r\n \r\nTo understand the community's stewardship behaviors, we examined website logs for all user activities for one year since we launched the new features (December 2020 – December 2021). During this period, we observed 5843 voting instances on 5672 photos for 5355 unique IDs, where a user voting on whether Photo N can be identified as a Person M or not is considered to be one voting instance. For a given voting instance, we analyzed 1) any associated facial similarity comparisons to understand how they deliberated on their final voting decision, and 2) the user's confidence vote plus any justification notes left by the user to understand their decision rationale. Of the 5672 query photos, 4297 photos (4377 voting instances) did not have any facial similarity comparisons — only the user's confidence on the ID (plus any notes) was captured.\r\nFor the remaining voting instances which had at least one associated facial similarity comparison (i.e., users deliberate through the two-step validation process), we broke them down into pre-identified (i.e., the user knew the ID of the photo ahead of time and therefore, might be looking for a specific target on the search results page) and post-identified (i.e., the user did not know the ID of the photo and therefore, may not have a specific target while analyzing the search results) cases. As mentioned earlier in Section 4.2, users had the opportunity to validate IDs either on the search results page on the photo page. While a pre-identified voting instance almost certainly originated from the search results page, a post-identified case could be from either page. Since the logs did not give us the page origin of each vote directly, we triangulated from CWPS's timestamps to determine whether a given voting instance was for a pre-identified case or a post-identified one.\r\nAfter collating all the associated facial similarity comparisons for a given voting instance, we had 1064 pre-identified and 576 post-identified user deliberations to analyze. Each deliberation instance is a user's attempt to identify a query photo as a given target (person) ID, where they first compare facial similarity with all other photos that have been identified as the target ID, followed by the user's confidence on the query photo being the target ID. We analyzed the user responses to see whether the facial similarity comparisons had any impact on the user's confidence.\r\nUsers had provided comments in 682 (out of 5843, or 11.7%) voting instances. We coded these comments using an iterative, inductive approach, which resulted in five high-level themes, which can broken down into 13 sub-categories (see Appendix C).\r\nTo understand any stigmergic collaboration processes at play, we also analyzed the IDs which had multiple votes to check for agreements and disagreements between the voters. We further analyzed how the community's facial similarity comparisons compare against the facial recognition scores.\r\nC THEMES FOR NOTES ANALYSIS\r\nPhoto Comparisons\r\nHigh-Level Comparison: The note mentions \"replica\", \"facial similarity\", \"facial match\", \"identical\", \"visual comparison\", and other similar terms that describe comparisons with a prior identified photo.\r\nDescribing facial features: The note mention facial features like \"eyes\", \"hairline\", \"ears\", etc. to make comparisons\r\nWord-of-Mouth\r\nDescendant: The note either mentions that the user is a descendant of the person being identified, or they got the information from the family of the person.\r\nOwnership: The note either mentions that the user owns a printed version of the photo, compared with a photo in their collection, or they know the owner of the photo.\r\nFamiliarity/Self-Reported Research: The note mentions that the user has seen the photo somewhere, be it in a book, museum, etc.\r\nVisual Clues\r\nUniform: The note mentions visual clues that pertain to the uniform of the person (e.g., hat insignia, shoulder straps, etc.) The user may infer the possible service information (i.e., ranks, branches, regiments, etc.) from the uniform clues.\r\nInscription: The note mentions the presence of a period inscription on the photo (a highly trustworthy primary source for a person's ID), or an album case, or modern inscriptions such as books, which is generally the name of the person being identified. In some instances, the inscribed text may point to the person's service information.\r\nOther Visual Clues: The note mentions visual clues in the photo (e.g., backmarks, borders, etc.) beyond the person's face.\r\nExternal Information\r\nExternal URL: The note mentions an external URL, which supposedly has additional information about the photo's ID.\r\nOther sources (e.g., museum, website, book, etc.): The note mentions an external source (e.g., museum, book, etc.) that supposedly has evidence for the photo's ID, but no URLs are provided. Details about the source may or may not be available.\r\nLack of Information / Seeking Additional Evidence: The note mentions the lack of evidence or seeking additional evidence, be it about the source or the service information.\r\nProviding Additional Information\r\nBiographical Information: The notes mentions additional information about the person's service records, specific regiments, biographical information (name, year, location), etc.\r\nAdditional Context: The note mentions some additional context provided by the user to justify their decision, such as information about the photo collection, or pointing to someone else's research, or some historical context, or incorrect evidence, or if the person is prominent.\r\nD PARTICIPANT DETAILS\r\nUndergraduate and master's students concentrating in history who use Civil War photos for their coursework and research projects, but are not (yet) employed in a professional capacity as historians. We recruited five students via recommendations from our university's history department. None of the students had used CWPS before, or were known to the authors prior to the study. Three students were men and two were women, and all were in the \"18 to 30\" age group. We anonymize them with identifiers S1–S5.\r\nExperienced users of Civil War Photo Sleuth who have added over 50 photos each and have substantial knowledge of Civil War history, but are not professional historians. We recruited five amateur experts from the CWPS contact list. All five users were men, and they were distributed across different age groups (two in \"18 to 30\", two in \"31 to 40\", and one in \"51 to 60\"). We anonymize them with identifiers C1–C5. C1 and C3 are among the most active daily users on CWPS. Only two of the five had used Photo Steward before.\r\nExpert historians with a graduate degree in history, specializing in American Civil War history, but with little or no previous experience with CWPS. We recruited five historians via recommendations from our university's history department. Three historians were men and two were women. They were distributed across different age groups (two in \"18 to 30\", two in \"31 to 40\", and one in \"51 to 60\"). We anonymize them with identifiers H1–H5. None of them had used Photo Steward before.\r\nE LAB STUDY QUESTIONS\r\nE.1 Semi-Structured Questions\r\nIs there a way that you would like to capture your thoughts on this ID and share them with others, if possible? If so, what would that look like? If not, can you explain why not?\r\nWhat did you think about the community opinions?\r\nWhat do you think about the ID quality visualization?\r\nWhat did you think about the 2-step process while agreeing/disagreeing on an identity?\r\nWhat is your overall opinion of both the interfaces?\r\nWhich interface would you prefer for validating the information? And why?\r\nWhat would you change or improve?\r\nE.2 Usability Survey\r\nQ1. The community's opinions about an identity were clear and easy to understand in the 2nd system. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ2. The community's opinions about an identity were useful for assessing the information. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ3. The process of voting on an identity was clear and easy to understand. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ4. Comparing other photos first and then voting on an identity helped me deliberate and make more accurate decisions. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ5. I was able to validate the information better using the 2nd system. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nF SUMMARY OF CHANGES\r\nWe summarize in Table 6 about how Photo Steward differs from CWPS.\r\nTable 6: Summary of changes: Civil War Photo Sleuth (CWPS) with and without Photo Steward.\r\n\r\nCWPS\r\nCWPS + Photo Steward\r\nDecision-Making: How do users make identification decisions for the query photo when they see the similar-looking search results retrieved by facial recognition?\r\nA single-step comparison interface that allows the user to compare the query photo with one similar-looking photo and the associated biographical information of the target ID. The single-step workflow conflates the two tasks of facial similarity and person identification.\r\nA multi-step validation interface that allows the user to compare the query photo with all previously identified photos of a similar-looking target ID (search result). The two-step workflow decouples the two tasks of facial similarity and person identification, thus allowing the user to deliberate on the facial similarity comparison between the query photo and the target photo(s) before making a decision on the target identity.\r\nUsers make a unary input on the query photo's ID by clicking an \"Identify\" button in the comparison interface, which will link the target ID to the query photo. The comparison interface does not allow users to rule out candidates.\r\nUsers can provide fine-grained decisions for both facial similarity and person identification steps in the validation interface. In Step 1, the user compares the query photo and the target photo for facial similarity by selecting from the following options: No (Different Person), Not Sure, Yes (Facial Match), and Yes (Replica). In Step 2, users indicate how confident they are about the query photo's ID (as the proposed target ID) by selecting the following options: No (Highly Confident), No (Slightly Confident), Not Sure, Yes (Slightly Confident), and Yes (Highly Confident).\r\nAccess: What kind of validation opportunities are available for users?\r\nUsers can only access the comparison interface from the search results page if they search for similar-looking candidates using facial recognition. Once the photo has been identified, users cannot access the comparison interface on the photo page. The community cannot weigh in on an ID's reliability on the photo page.\r\nUsers can access the validation interface on both the search results page (while identifying the query photo) and the photo page (after the photo has been identified). The community can weigh in on the reliability of the proposed ID(s) and facial matches using the validation interface on the photo page, thus engaging in a form of stigmergic collaboration.\r\nEvidence: What kind of stewardship evidence is presented to the users?\r\nThe photo page displays the proposed ID(s) for the photo without any additional information about the ID's reliability. Similarly, other photos that have been matched to the query photo are also displayed without any reliability indicator. Since community opinions are not captured on CWPS, they are not displayed.\r\nThe photo page displays the proposed ID(s) for the photo along with stewardship visualizations of the a) community's confidence on the ID, and b) facial similarity comparison with other photos of the same ID by both the community and AI.\r\nFOOTNOTE\r\n1 www.civilwarphotosleuth.com\r\n\r\n\r\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs International 4.0 License.\r\nCI '23, November 06–09, 2023, Delft, Netherlands\r\n© 2023 Copyright held by the owner/author(s).\r\nACM ISBN 979-8-4007-0113-9/23/11.\r\nDOI: https://doi.org/10.1145/3582269.3615600",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== story.txt ===\nRon: Great. I want to start with a detail about you that I’ve always appreciated, which is that you started out as a collector and someone who was deeply interested in the Civil War. Talk a little about how you became interested and got started.\r\n\r\nKurt: Sure thing. I am a bit of a Civil War collector. I grew up in a family of Civil War and history collectors and appreciators. I spent many childhood hours at antique shops and flea markets, and our home was full of interesting Civil War collectibles—books, artifacts, and all kinds of interesting stuff. That was my world as a kid. As I grew older, I realized that I could actually have my own Civil War collection. What was a little different about me is that I gravitated toward Civil War photographs—a category my family didn’t have too much of. I just felt drawn to the imagery of people from that era, their incredible stories, learning about the names behind the faces, and ultimately building my own collection of images while loving the process of researching those individuals.\r\n\r\nRon: One of my favorite stories is your family connection—how there’s a photograph connected to it all. I’d love to hear you tell that again.\r\n\r\nKurt: Absolutely. One of the reasons my family had an interest in the Civil War is that we have some Civil War ancestors. Three of them fought with the Union Army in Pennsylvania regiments, and I grew up learning about their adventures. We researched all we could about them through military records, but we didn’t have any images—we couldn’t see them or know what they looked like. I got really lucky during the 150th-anniversary commemorations of the Civil War. In Pittsburgh, where I grew up and where my Civil War ancestors lived, there was an exhibit at the Heinz History Center. I noticed a really cool photo album of Civil War soldiers. The label said that the company in the album was one that my ancestor had fought in. Out of thousands of possibilities, that single regiment appeared there. I asked the museum staff if they could have the owner, Ken Turner, look inside to see if my ancestor’s photo was in there. And sure enough, inside the album was a photograph of my Civil War ancestor, Corporal Oliver Croxton in uniform. I was looking into the eyes of my great-great-great grand uncle.\r\n\r\nRon: How did you feel when you saw that for the first time?\r\n\r\nKurt: It felt amazing. Here was someone I had thought about my whole life, trying to learn as much as I could about him. And once you start studying Civil War portraits, you learn that the odds of finding one specific photo of a soldier—especially a lower-ranked one—are really low. So having that opportunity, seeing his face, and getting a much clearer picture of who he was… it was really powerful.\n\n==================================================\n\n=== Kurt paper.txt ===\nPhoto Steward: A Deliberative Collective Intelligence Workflow for Validating Historical Archives\r\nVikram Mohanty, Computer Science, Virginia Tech, USA, vikrammohanty@vt.edu\r\nKurt Luther, Computer Science, Virginia Tech, USA, kluther@vt.edu\r\n\r\nDOI: https://doi.org/10.1145/3582269.3615600\r\nCI '23: Collective Intelligence Conference, Delft, Netherlands, November 2023\r\nHistorical photographs of people generate significant cultural and economic value, but correctly identifying the subjects of photos can be a difficult task, requiring careful attention to detail while synthesizing large amounts of data from diverse sources. When photos are misidentified, the negative consequences can include financial losses and inaccuracies in the historical record, and even the spread of mis- and disinformation. To address this challenge, we introduce Photo Steward, an information stewardship architecture that leverages a deliberative workflow for validating historical photo IDs. We explored Photo Steward in the context of Civil War Photo Sleuth (CWPS), a popular online community dedicated to identifying photos from the American Civil War era (1861–65) using facial recognition and crowdsourcing. While the platform has been successful in identifying hundreds of unknown photographs, there have been concerns about unverified identifications and misidentifications. Our exploratory evaluation of Photo Steward on CWPS showed that its validation workflow encouraged users to deliberate while making photo ID decisions. Further, its stewardship visualizations helped users to assess photo ID information accurately, while fostering diverse forms of stigmergic collaboration.\r\nCCS Concepts: • Human-centered computing → Collaborative and social computing design and evaluation methods; • Human-centered computing → Interactive systems and tools;\r\n\r\nKeywords: crowdsourcing, human-AI interaction, online deliberation, community stewardship, information assessability, online communities, history, person identification, facial recognition, stigmergic collaboration\r\n\r\nACM Reference Format:\r\nVikram Mohanty and Kurt Luther. 2023. Photo Steward: A Deliberative Collective Intelligence Workflow for Validating Historical Archives. In Collective Intelligence Conference (CI '23), November 06--09, 2023, Delft, Netherlands. ACM, New York, NY, USA 19 Pages. https://doi.org/10.1145/3582269.3615600\r\n\r\nFigure 1: Workflow of Photo Steward. (1) Decision-Making: The user compares photos for facial similarity and make decisions on the photo ID using Photo Steward's deliberative validation interface. (2) Access: The user accesses the validation interface from Civil War Photo Sleuth's search results and photo page. (3) Evidence: The community's responses from the validation interface feed into stewardship visualizations that are visible on the Photo Page, which subsequently foster a form of stigmergic collaboration among the users.\r\n1 INTRODUCTION\r\nThe task of correctly identifying individuals in historical photos holds great cultural and economic importance [3, 19, 39, 61]. This identification process is analogous to solving a complex mystery. It often involves corroboration of multiple research processes such as investigating visual clues in a photo, finding relevant reference resources, and comparing multiple low-resolution reference photos [31, 32, 37]. Historical photos pose many challenges, including low-resolution images, scattered reference materials, limited domain expertise, and lack of suitable verification tools. These hurdles often result in misidentifications which can have negative consequences, ranging from distorting historical narratives [60] and fueling conspiracy theories [12] to spreading disinformation [16] and unwarranted financial gains from inaccurate representations [21]. As online platforms such as Ancestry.com, Find-a-Grave, and FamilySearch democratize historical and genealogical research, the risk of misidentification is further amplified due to factors such as inadequate experience, confirmation bias, and automation bias introduced by imperfect automated tools [44, 75].\r\nTo address these challenges, we introduce Photo Steward, a deliberative workflow that leverages collective intelligence to validate historical photo identifications (IDs). Photo Steward's architecture builds upon the concept of information stewardship [18, 72], which involves community-driven validation of content, as seen in online communities like iNaturalist and Wikipedia. We designed and applied the Photo Steward architecture to Civil War Photo Sleuth (CWPS),1 an AI-infused online platform for identifying historical photos. CWPS has over 20,000 registered users and over 25,000 identified Civil War portraits, and faces the problem of historical photo misidentification [43]. Photo Steward provides a validation workflow that promotes careful deliberation during facial similarity comparison and photo ID verification, while enabling users to share opinions. It also visualizes community opinions on the reliability of photo IDs and facial similarity, fostering information stewardship at multiple levels.\r\nWe publicly released Photo Steward on CWPS and conducted an exploratory evaluation of twelve months of usage, including interviews with users of different expertise levels and log analysis of stewardship behaviors on the platform. We found that Photo Steward's stewardship visualizations helped users to find additional evidence (e.g., external sources, uniform clues matching service records, etc.) for assessing the reliability of photo IDs. Users validated hundreds of different IDs on the platform, and found the workflow to be useful for deliberating on facial similarity comparison and fine-grained photo ID decisions.\r\nWe also discuss the implications of community participation, deliberative human-AI interaction, and assessable designs for historical photo identification.\r\n2 RELATED WORK\r\n2.1 Misinformation in a Historical Context\r\nNumerous online communities, forums, and websites have emerged in recent years for archiving and documenting history [57], generating family histories [74, 75], identifying and sharing historical photos [43], trading antiques [1, 8], and facilitating discussions around history [20]. Much like popular social media platforms such as Facebook and Twitter, these history-based platforms are also prone to the problem of misinformation, albeit in a more specialized historical research context.\r\nPrior work has shown that erroneous family history trees were being disseminated across Find-a-Grave and Ancestry, two popular genealogy research communities, as a result of the platform's low bar to entry and inexperienced contributors' over-reliance on inaccurate automated features [75]. Mohanty et al. [43] showed that despite successful identifications on Civil War Photo Sleuth (CWPS), several photos were misidentified in the first month, particularly for photos without period inscriptions or duplicate views (12 misidentifications out of 37). Multiple factors — the correct candidate not present in the search pool, or the user incorrectly assessing facial similarity [53] and picking the wrong match — can lead to incorrect IDs. A follow-up benchmarking study of the underlying face recognition algorithm [44] highlighted its low precision (i.e., it retrieves over hundreds of search results), raising the possibility of errors due to automation bias, i.e., the tendency of users to over-rely on automation for making a decision [47, 50, 64].\r\nIdentifying historical photos is a complex investigative process, often involving the corroboration of multiple evidence pieces [31, 33] and can be seen analogous to \"finding a needle in a haystack\". As a result, historical photo IDs run a high risk of getting misidentified even with the best of intentions. Multiple Civil War photos have also been misidentified in the collections of professionally managed museums and archives, such as the US Library of Congress [36] and the Abraham Lincoln Presidential Library [34]. At the same time, historical photo IDs also have the potential to generate significant monetary value [1, 3, 8, 10], and such financial incentives might also lead to falsified identifications [21]. Validating these historical photo IDs, which are a result of complex, subjective original research, becomes tricky without the lack of domain expertise and access to investigative tools.\r\nWe addressed these challenges in Photo Steward by designing a stewardship architecture that allows users to share their expertise with others. To address the impact of automation, we introduced a two-step validation workflow for the users to deliberate on decisions while interacting with the AI's recommendations.\r\n2.2 Data Validation in Online Communities\r\nMultiple online platforms have leveraged the strengths of crowdsourced contributions for validating the quality of data generated on those sites. Elliott discusses how stigmergic collaboration, where indirect coordination within a community stimulates subsequent actions, plays a role in maintaining articles on Wikipedia [13]. This concept was observed by Wiggins et al. in their study of iNaturalist, an online platform for identifying species, wherein community stewardship behaviors were seen as users agreed on organism identifications to influence the platform's quality grade status [72]. Prior work has shown that stewardship visualizations on Wikipedia (i.e., article quality) [18] and iNaturalist (i.e., ID research grade status) [72] have a positive impact on users' assessment of the information.\r\nAlong these lines, we also built DoubleCheck [42], a quality assessment framework that builds upon the concepts of provenance and stewardship for verifying historical photo IDs. DoubleCheck focused on displaying quality indicator badges for historical photo IDs by capturing accurate provenance information and combining the source trustworthiness information with community opinions on the ID. In this work, we focus solely on the underlying stewardship architecture that helped facilitate the community opinions. Both DoubleCheck and Photo Steward were evaluated in the same lab study, but there is no overlapping data.\r\nVisualizations displaying (surrogate) quality metrics, such as popularity among expert users, social reputation, and content coverage, have been effective in helping users assess the credibility of websites and search results [62]. Prior work has also shown that visualizing the history of edits for a Wikipedia article can have a significant impact on users' perceived trustworthiness of the article [54, 67]. Similarly, Chevalier et al. [7] showed that visualizing the number of contributors, length of the article and discussion, and the history of edits helped users assess the quality of Wikipedia articles faster. On the other hand, Towne et al. [68] found that being exposed to editor conflicts in the discussion of a Wikipedia article lowered the perception of the article's quality, even though the users reported that the transparency raised their perceptions of the page and Wikipedia in general. Morris et al. [45] found that Twitter users relied on the author information for making assessments about the credibility of information in a tweet.\r\nDrawing from this prior work, we designed Photo Steward's stewardship visualizations to highlight the role of collective intelligence, while fostering stigmergic collaboration on CWPS to validate the quality of photo identifications.\r\n2.3 Background: Civil War Photo Sleuthing\r\nThe American Civil War (1861–65) was one of the first major conflicts to be extensively photographed. Over 3 million soldiers fought in the war, with many of them having been photographed at least once. Over 150 years, many of these photos have survived in museums, libraries, and personal collections, but only 10–20% are are identified [69, 77]. Civil War photography has garnered a lot of interest among historians, collectors, dealers, genealogists, archivists, and other experts, who often try to identify unknown photos for personal, cultural, and economic reasons. However, the identification process is complex and challenging, which often involves identifying visual clues in a photo and manually scanning through hundreds of low-resolution photos, military records, and reference books for corroborating evidence [31, 33, 38].\r\n3 ENHANCING CIVIL WAR PHOTO SLEUTH: DESIGN OPPORTUNITIES\r\n\r\nFigure 2: CWPS Haystack Model: Person Identification Pipeline [43]\r\nCivil War Photo Sleuth (CWPS) is a free, public website where users can identify unknown portraits from the American Civil War era using a person identification pipeline that combines crowdsourced human expertise and face recognition [43]. Drawing analogies to finding a needle in a haystack, Mohanty et al. propose a 'haystack model' to describe CWPS's person identification pipeline. In this pipeline, a user begins the identification process by first tagging a photo for uniform clues, which then generates search filters based on service records, and then facial recognition returns facially similar-looking results from a pool of potential candidates, ordered by similarity to the query photo, that satisfy the search filters (see Figure 2).\r\nThe CWPS haystack model is designed to prevent misidentifications by placing human decision-making at the forefront and treating AI as a supportive tool. It avoids automatically selecting the best match or displaying the algorithm's inconsistent confidence levels [41, 44]. Instead, the user carefully inspects search results for potential matches based on facial similarity and corresponding biographical details. Once a photo is identified, CWPS links the face and identity together and displays the ID on the photo page.\r\nDespite these measures, the open participation model of CWPS, which lacks verification, has raised concerns about the trustworthiness of proposed identities and the potential increase of \"false positives\" as the site grows [22]. To address these concerns, we enumerate three design goals which draw upon prior work on Civil War photo identifications and CWPS system designs, evaluations, and critiques [22, 30, 33, 35, 41, 43, 44], as well as our own observations and experiences using the publicly available version of the website. We provide further details in Appendix A.\r\nDesign Goal 1: Decouple facial similarity comparison from the overall task of person identification.. The current CWPS workflow conflates facial similarity and person identification into a single decision-making process (see Figure 2-C). Facial similarity, while important, can conflict with the identity suggested by personal details like biographical information and service records. The facial recognition algorithm's low precision [44] adds to the complexity, with the possibility of users interacting with false positives. In order to discourage over-reliance on facial similarity, we propose this design goal of separating both these tasks, allowing users to deliberate on the facial similarity and other person identification attributes separately, thereby minimizing inaccuracies.\r\nDesign Goal 2: Support fine-grained, deliberative decision-making.. With the current CWPS interface only permitting binary feedback during photo identification, there is a heightened risk of misrepresentation and misidentifications (see Figure 2-C). Historical photo identification is intricate, often demanding careful corroboration of numerous evidence pieces, with user confidence varying according to the evidence at hand [33]. To more effectively facilitate this complex process, we advocate for a design that enables users to express their level of certainty in their identification decisions. This design goal seeks to foster more accurate and nuanced user feedback, as well as reflective and deliberate decision-making [28, 29].\r\nDesign Goal 3: Encourage community contribution and transparency for validating photo IDs.. Identifying individuals in photos can be considered a subjective process and often benefits from multiple perspectives [41]. Currently, CWPS lacks mechanisms for community feedback and transparency about the process of photo identification, leaving potential misidentifications unchecked (see Figure 8 in Appendix). To support accurate original historical research, the platform should encourage community participation in validating identifications and promoting transparency around the roles of community contribution and facial recognition [18, 72]. This approach also encourages collective responsibility, facilitating stigmergic collaboration [13, 14], where user contributions guide future validation efforts.\r\n4 SYSTEM DESCRIPTION: PHOTO STEWARD\r\nWe developed Photo Steward, an information stewardship architecture that integrates a deliberative workflow for the community to validate historical photo identifications, which we then integrated into CWPS. Photo Steward's architecture has three main components (see Figure 1): 1) a deliberative decision-making interface for facial similarity comparison and photo identification, 2) new access points for validating photo identities, and 3) stewardship evidence for fostering stigmergic collaborations.\r\n4.1 DECISION-MAKING: Deliberating on facial similarity and photo identification\r\nAs part of Photo Steward, we introduce a multi-step \"Validation Interface\" (see Figures 3 and 4) to replace CWPS's single-step comparison interface. Photo Steward's validation interface allows users to deliberate while interacting with the facial recognition results. Meeting Design Goal 1, the validation workflow separates the task of facial similarity comparison from the overall goal of identifying the photo.\r\nTo inform our design, we draw on evidence-based decision-making [11], a model primarily used in healthcare, policymaking, and judicial sectors, which advocates for justifying decisions (photo IDs in this case) by gathering available evidence (facial similarity as visual evidence here). In the first step, the user compares the query photo to all other photos with the same identity for facial similarity. After deliberating on the facial similarity evidence, the user then votes on whether the query photo fits the target identity in the second step (which is the user's primary goal).\r\nThe validation interface is divided into four columns (from left to right): 1) the task description, 2) the query photo, 3) the evidence that is being weighed, and 4) the biographical information. The query photo and evidence are positioned in the two middle panels for easy side-by-side comparison. The task description panel displays the rating question for both the facial similarity comparison and the identification steps. Here, we used structured feedback to capture both the user's facial similarity comparison and their confidence on the photo ID, in an effort for encouraging users to exercise personal deliberation on all available evidence before making a decision on the ID. The interface updates the task description and evidence column depending on which task the user is performing.\r\nTo investigate the identity of a query photo, the user opens the validation interface which loads all the photos and biographical information available for the target identity.\r\n4.1.1 Validation Step 1: Facial Similarity Comparison. For the first step of the validation process, the interface displays the target photo in the evidence column next to the query photo for easy facial similarity comparison (see Figure 3).\r\n\r\nFigure 3: Step 1 of Photo Steward's Validation Interface. Users can compare two photos and answer whether they show the same person or not. They have the option of selecting whether the two photos are a facial match (i.e., same person, different views) or a replica (i.e., same person, same view). Here, the user is comparing whether Photo 41929 and Photo 1 show the same person or not. If multiple faces are available for the same ID, they appear one after the other in the order in which they were uploaded to CWPS.\r\n\r\nThe user's task is to determine whether both photos show the same person (regardless of whether the identity is known). Users can select from the following options: No (Different Person), Not Sure, Yes (Facial Match) and Yes (Replica). Mohanty et al. found that photos correctly identified on CWPS were either facial matches (i.e., same person, different view) or replicas (i.e., same person, same view) [43], which informed the design of this input scale. Since facial similarity does not have any standard scale and users may perceive the similarity or dissimilarity of two faces differently [40, 76], we chose not to capture any further granularity in their responses for facial match, replica or different person as this might lead to inconsistent data collection.\r\nCapturing these responses in a structured way allows users to deliberate on the task of facial similarity; this becomes more critical as users are also interacting with the results of a low-precision facial recognition algorithm [44]. In this step, the user compares facial similarity of the query photo with all available photos of the target identity, one photo at a time.\r\n4.1.2 Validation Step 2: Fine-Grained Photo Identification. In this step, the user analyzes the biographical information and incorporates the facial similarity evidence from the previous step to make a decision on the photo's identity.\r\nThe validation interface displays information in the same four-column layout (see Figure 4), with the evidence column now displaying a summary of the user's responses about facial similarity between the query photo and the target photo(s). The biography column shows the name and the service records for the user to analyze.\r\n\r\nFigure 4: Step 2 of Photo Steward's Validation Interface. Users vote on whether the query photo can be identified as the target identity by expressing their confidence. They can also add an optional note to justify their decision. The evidence panel displays a summary of the user's responses from the first step, where the faces are displayed next to the user's facial similarity comparison with the query photo. The faces are ordered in the way they appear for comparison, i.e., the order in which they were uploaded to CWPS.\r\n\r\nThe user now decides whether the query photo can be identified as the target identity (see Figure 4). The instruction above nudges the user to factor in the prior photo comparison evidence and the biography information. Meeting Design Goal 2, users indicate their confidence about the task question by selecting one of the five options displayed in radio buttons: No (Highly Confident), No (Slightly Confident), Not Sure, Yes (Slightly Confident) and Yes (Highly Confident). This scale, which offers more nuance than a binary decision, serves as a proxy measure for the ID's reliability given the complexities of quantifying accuracy in a historical photo ID investigation. These options reflect the varying degrees of confidence users have based on the quality and quantity of corroborating evidence, such as reputable sources, facial similarity to additional photos, and expert opinions. Users also have the option to elaborate their decision rationale in a free-text note.\r\n4.2 ACCESS: Expanding Validation Opportunities for Photo Identifications\r\nPhoto Steward provides stewardship capabilities for the CWPS user community by allowing them to access and use the validation interface at different stages of the photo identification process from multiple gateways (see Figure 1). On the \"Search Results\" page, it can be used for identifying a photo from a pool of potential similar-looking candidates, or ruling out some potential candidates. After a photo has been identified, users can also access it on the \"Photo Page\" to either validate an existing ID or dispute an incorrect one, and collaborate with other users in a stigmergic manner (Design Goal 3).\r\n4.2.1 Search Results Page: Matching and Ruling Out Candidates. While identifying a query photo, users can now inspect potential matches on the search results page with the help of the validation interface. The \"Compare\" button on a search result brings up the validation interface, loading all the target information for the corresponding search result. The target identities in the validation interface will update as the users check new search candidates for matches. The interface allows users to make two types of decisions, depending on their confidence response: 1) either of the \"Yes\" responses will match the photo with the target identity with varying degrees of confidence, and 2) either the \"No\" or \"Not Sure\" responses will rule out the search candidate as a potential match for the current user.\r\n4.2.2 Photo Page: Validating and Disputing Existing IDs. After a photo has been identified, Photo Steward allows users to review opinions from other users (described in Section 4.3) and contribute their own for a given photo ID on CWPS's photo page, fulfilling Design Goal 3. By clicking the \"Give Your Opinion\" button, users launch the validation interface featuring the query photo, the linked target identity, and target photos of the same ID, a new feature previously absent from CWPS. The two-step process mirrors that on the search results page, enabling community deliberation on the validity of an ID. Users can validate the facial match among photos linked to the same ID, express their agreement or disagreement on an ID with varying confidence levels, and optionally add a note explaining their decision. Thus, each vote contributes to a stigmergic collaboration, enhancing the reliability of photo IDs on CWPS. Consistent with CWPS's open participation model, Photo Steward allows any registered user to share their opinion on an identification.\r\n4.3 EVIDENCE: Visualizing Information Stewardship\r\nAs part of Photo Steward, we designed stewardship visualizations to help users assess the reliability of 1) facial matches (i.e., photos that were matched to each other by the user), and 2) photo IDs. The CWPS community's opinions on facial similarity comparison and photo IDs, captured through the validation interface, feeds into these reliability visualizations. These visualizations not only promote user accountability through social translucence [15], but also serve as deliberative evidence for subsequent stigmergic user collaborations (Design Goal 3).\r\n4.3.1 Reliability of Facial Similarity. For each photo pair that has been compared, the system aggregates the community's decisions for the visual match type and generates a distribution, which is displayed in the form of an interactive horizontal bar chart on the photo page. This chart appears next to the corresponding photo matched to the query photo (see Figure 5). Users can click the \"View Details\" button or an individual bar to see how each user voted. When multiple photos have been matched to the query photo, the matched photos appear one below the other, with each having its own visualization next to it. The bar charts are stacked vertically above each other to allow users to easily see and compare the reliability of every match.\r\nTo complement the community stewardship visualization, we also added an AI stewardship badge that indicates whether the particular match is supported by facial recognition (see Figure 5). On the search results page, CWPS retrieves those search results that have a facial similarity score greater than 0.50, so we use the same threshold here. However, the badge intentionally does not display the exact similarity scores (which have been found to be inconsistent [41, 44]) to avoid a false perception of precision, and cautions users to carefully analyze all the context and evidence, as there is a possibility of false positives with face recognition.\r\n\r\nFigure 5: Facial Similarity Reliability Visualization on the Photo Page. The photo matched to the query photo is displayed here, whereas the query photo is displayed on top of the page. Other photos matched to the query photo are displayed vertically one below the other. Users can bring up the query photo and the matched photo side-by-side by clicking the \"Compare\" button. Each matched photo has its own visualization next to it, and the responses are visible to everyone.\r\n\r\n4.3.2 Reliability of Photo Identifications. Similar to the facial similarity visualization, the system aggregates the community's confidence levels for an identification and generates a distribution, displayed in the form of an interactive horizontal bar chart. This visualization is displayed for every proposed identity on the photo page, below the biography subsection (see Figure 6a). If users propose multiple IDs for a given photo, they are displayed one below the other; each ID will have its own visualizations. The community's confidence votes are aggregated to order these IDs. Users can click the \"View Details\" button or the individual bars to view a modal dialog box with each user's confidence scores and optional text comments (see Figure 6b).\r\n\r\nFigure 6: Photo ID Reliability Visualization on the Photo Page for a given ID. If multiple IDs are present, they are displayed vertically one below the other, ordered in terms of aggregate votes. Each ID will have a separate visualization listed under the respective IDs.\r\n\r\nThis visualization reflects the community's expertise, and is intended for users to quickly assess the reliability of an identification. Prior work on crowdsourced person identification [41] suggests that airtight identifications are likely to show consensus from the community, whereas potential misidentifications are likely to reflect disagreement from the community. Further, the comments may reflect the voters' decision rationale and any external research they conducted before giving their decision on the photo's identification, allowing users to build on the work of others in making their own assessment [17].\r\n4.4 Summary\r\nPhoto Steward augments the CWPS platform with an information stewardship architecture to support community validation of historical photo IDs in a stigmergic manner. We summarize all the changes in the Appendix (see Table 6).\r\n5 EVALUATION\r\nWe obtained permission to publicly launch Photo Steward on CWPS in December 2020. We conducted a mixed-methods, exploratory evaluation study to understand how well users with different expertise levels could validate and assess Civil War photo IDs using CWPS with Photo Steward. Specifically, we wanted to understand 1) how users validated photo IDs using Photo Steward, 2) how the stewardship visualizations (i.e., ID and facial similarity reliability visualizations) impacted users' assessment of an ID. The study was approved by our university's IRB.\r\n5.1 Log Analysis\r\nTo understand the community's stewardship behaviors, we analyzed website logs of all user activities for a year after new features were launched, which included 5843 voting instances on 5672 photos for 5355 unique IDs. Our analysis included categorization of user deliberations as 'pre-identified' (i.e., the user knew the ID of the photo ahead of time and therefore, might be looking for a specific target on the search results page) or 'post-identified' (i.e., the user did not know the ID of the photo and therefore, may not have a specific target while analyzing the search results), coding of user comments (see Appendix C), and comparison of community's facial similarity comparisons against facial recognition scores. Details of these analysis methods are provided in Appendix B.\r\n5.2 Lab Study\r\nIn order to understand how well Photo Steward supports diverse users in validating the quality of photo identifications, we also conducted an exploratory lab study.\r\n5.2.1 Participants. We recruited 15 participants representing the three major expertise levels: 5 history students, 5 amateur experts (experienced users of CWPS), and 5 expert historians. Participant details can be found in Appendix D. We anonymize these groups with the following identifiers, respectively: S1–S5, C1—C5, and H1–H5.\r\n5.2.2 Dataset. For the study, we created a dataset of 10 different photos identified on CWPS. Three of these photos had an ID conflict, i.e., multiple identities were proposed. For two of these photos, one ID was correct and the other one was incorrect. The community had already researched both photos, voted on the correct ID, and left credible evidence in the comments. Both IDs were linked to additional photos as well. The third photo was one of the seeded photos on CWPS, but was originally misidentified. We added another false ID, making both IDs for the third photo incorrect. All photos had multiple photos matched to them; eight of them were linked via facial matches, while two of them had replicas.\r\n5.2.3 Procedure. The entire study was conducted online via recorded Zoom sessions, with at least one researcher attending each session. Each participant first completed a consent form and a pre-survey describing their demographics and Civil War photography experience.\r\nAs part of the study, participants reviewed three randomly assigned photos from the dataset one-by-one in the original CWPS system first, followed by the same photos on the Photo Steward version. Participants used a think-aloud protocol while using the two systems; after the completion of the task with each system, they were asked a few semi-structured questions about their experience. Finally, the participants completed a summative post-survey of standard usability questions (e.g., ease of use, usefulness of features, instruction clarity, preferred system, etc.) (see Appendix E)\r\nWe maintained this sequence (original CWPS first, CWPS with Photo Steward second) for all the participants, rather than using a randomized sequence, for two reasons. First, we did not want participants' assessments to be biased in favor of Photo Steward after seeing additional features in the new interface. This design allowed us to observe if the original interface misled the participants towards incorrect assessments, and if, subsequently, the Photo Steward interface helped correct them. Second, in a randomized sequence, Photo Steward would expose the participants to new information in the form of prior user votes and responses, and therefore, may confound how they assess the information on the original CWPS version.\r\n5.2.4 Data Analysis. The first author fully transcribed and analyzed the interviews and think-aloud recordings using an inductive qualitative thematic approach [4]. The transcript sections were first divided according to the interface in question (i.e., original CWPS or Photo Steward), followed by an open coding of the transcripts using MAXQDA 2020 [65]. The first author iterated and settled on a total of 28 codes (e.g., change in opinions, comparison interface, source trustworthiness, etc.) for 634 coded segments across all the transcripts. These codes were then organized into themes as described in Section 6 after discussing with the co-author.\r\n5.2.5 Limitations. We conducted a qualitative lab study to understand how users with different backgrounds and expertises validated photo IDs using Photo Steward and hit theoretical saturation. However, there are a couple of limitations with the study: 1) limited insights on the role of expertise, and 2) the task sequence could have order effects. Further, the large-scale analysis of Photo Steward logs provided us with insights of its usage amongst users. However, it lacked an expert-prepared gold standard dataset, which hindered our ability to conduct specific performance analyses as part of this study.\r\n6 FINDINGS\r\nUsing the methods above, we evaluated how well Photo Steward's stewardship architecture supported CWPS users in validating photo identifications, compared to the original version of CWPS.\r\n6.1 Validation Interface\r\nUsers found Photo Steward's validation interface to be useful for comparing different photos.. While assessing the IDs with the original interface, participants would go back and forth between different photos to compare whether they are the same person or not. Some participants opened the photos in two different browser windows and kept them side-by-side. While using the validation interface in the new system, participants appreciated being able to see the photos side-by-side at the same time.\r\nH1 said, \"As an historian using this, this is really great to see them both together. It just makes a comparison a lot easy for me to do. I mean, this is the same gentleman, he's got a little dark facial hair. It looks a little bit different there and the photo on the right, but the facial match is definitely there. \" This was also echoed by C3, who said, \"This, I really find extremely useful, especially when I'm trying to do facial recognition. I can zoom in and have them side by side here. [...] Where in the past, I would have to go back and forth between tabs or cut and paste them into a different document to look at them side by side.\"\r\nFrom our logs, we found that 223 users had compared 2319 unique photo pairs for facial similarity, with 156 pairs receiving comparisons from at least 2 different users. The facial similarity responses were distributed as follows: 763 replicas, 1232 facial matches, 283 unsure, and 280 different people.\r\nTable 1: Distribution of User Votes.\r\n\r\nVotes\r\n/ ID\r\n# of IDs\r\nNote\r\nPresent\r\nNegative\r\nVotes\r\n1\r\n5650\r\n511\r\n61\r\n2\r\n157\r\n(Agreement: 119)\r\n(Disagreement: 38)\r\n83\r\n12\r\n3+\r\n36\r\n(Agreement: 21)\r\n(Disagreement: 15)\r\n31\r\n4\r\nTable 2: Distribution of Confidence Levels.\r\n\r\nMean\r\nConfidence\r\n1\r\nVote / ID\r\n2\r\nVotes / ID\r\n3+\r\nVotes / ID\r\n-2 (No - Highly Confident)\r\nto\r\n-1 (No - Slightly Confident)\r\n45\r\n3\r\n2\r\n-1 (No - Slightly Confident)\r\nto\r\n0 (Not Sure)\r\n16\r\n3\r\n2\r\n0 (Not Sure)\r\n99\r\n8\r\n4\r\n0 (Not Sure)\r\nto\r\n1 (Yes - Slightly Confident)\r\n760\r\n15\r\n5\r\n1 (Yes - Slightly Confident)\r\nto\r\n2 (Yes - Highly Confident)\r\n4730\r\n128\r\n23\r\nUsers preferred the ability to provide granular feedback for photo IDs using Photo Steward's validation interface.. All participants expressed preference for the fine-grained confidence levels, including the ability to dispute an ID, in Photo Steward's validation interface, appreciating how it more accurately mirrored the inherent uncertainty present when assessing photo IDs. S3 said, \"I definitely like the five levels. I think it leaves more room for interpretation. Like sometimes it's kind of hard to just say yes or a hard no because so much goes into it. Especially because a lot of this stuff was so long ago, there's so many unanswered questions.\" H4 initially defended the original interface's binary vote, but changed her mind after experiencing Photo Steward's confidence levels: \"[W]hat I had said has this very black and white feel to it, you're wrong or you're right. I like these degrees of disagreement or agreement. I think that's way more helpful broadly.\"\r\nThe usage of the voting feature was reflected in our logs, which showed 5843 voting instances from 328 unique users (see Table 1). Table 2 shows that while users utilized the full range of confidence levels, including when they were unsure (mean confidence = 0) or slightly confident (mean confidence = 0 to 1) about the ID, the vast majority of the votes were highly confident ones (mean confidence = 1 to 2). A small proportion of votes (77) were cast for disputing an ID.\r\nUsers justified their voting decisions through notes covering an extensive range of topics..\r\nTable 3: Distribution of Note Topics. The table also displays how the notes are distributed for different user confidence levels.\r\n\r\nCategory\r\nSub-Category\r\nTotal\r\nNumber of\r\nNotes\r\nNo\r\n(Highly\r\nConfident)\r\nNo\r\n(Slightly\r\nConfident)\r\nNot\r\nSure\r\nYes\r\n(Slightly\r\nConfident)\r\nYes\r\n(Highly\r\nConfident)\r\nPhoto\r\nComparison\r\nHigh-Level\r\nComparison\r\n206\r\n9\r\n5\r\n9\r\n20\r\n163\r\n\r\nDescribing\r\nFacial Features\r\n37\r\n4\r\n3\r\n9\r\n8\r\n13\r\nWord-of-Mouth\r\nDescendant\r\n55\r\n1\r\n0\r\n0\r\n8\r\n46\r\n\r\nOwnership\r\n47\r\n1\r\n0\r\n0\r\n0\r\n46\r\n\r\nFamiliarity\r\n18\r\n0\r\n0\r\n0\r\n2\r\n16\r\nVisual\r\nEvidence\r\nUniform\r\n73\r\n7\r\n1\r\n8\r\n17\r\n40\r\n\r\nInscription\r\n145\r\n4\r\n3\r\n3\r\n6\r\n129\r\n\r\nOther\r\nVisual Clues\r\n17\r\n1\r\n0\r\n1\r\n2\r\n13\r\nExternal\r\nInformation\r\nExternal URL\r\n47\r\n1\r\n0\r\n1\r\n4\r\n41\r\n\r\nOther Sources\r\n172\r\n9\r\n1\r\n1\r\n21\r\n140\r\n\r\nLack of\r\nInformation\r\n21\r\n0\r\n2\r\n13\r\n6\r\n0\r\nProviding\r\nAdditional\r\nInformation\r\nBiographical\r\nInformation\r\n46\r\n11\r\n2\r\n3\r\n3\r\n27\r\n\r\nAdditional\r\nContext\r\n117\r\n5\r\n0\r\n2\r\n11\r\n99\r\nFrom our logs, we found that 155 users had left 682 notes for 600 different photos. However, as Table 1 shows, around 10% of the votes had a note. Table 3 shows the different topics covered by the notes. We observe that users' voting patterns are significantly influenced by the availability and quality of evidence, with clear facial similarity, period inscriptions, personal anecdotes, visible clues in the photograph, and added biographical context often leading to high confidence \"Yes\" votes, while lack of information typically results in \"Not Sure\" votes.\r\nUsers most frequently left a comment attributing facial similarity (after comparing it in the first step of the validation process) to be the reason for their decision (e.g., \"Identical to the other CDV\"). In some instances, they would expound on it by discussing facial features: \"The eyes, nose, cheek bones, shape of face, all look similar to George Pickett, although possibly reversed based on hair part\". In many instances, we found users inferring biographical information (service records, location, etc.) from visual evidence in the photo, be it uniform or backmarks (e.g., \"Initials MN on chinstrap (brass letters). Signature on verso is made out to Marlin's oldest sister. Style of insignia is consistent with other 1862 recruits for Co. B 1st USSS\").\r\nInterestingly, we also observed a large number of word-of-mouth evidence notes for justifying the user's decisions, such as claiming to be a descendant, or owning the original copy of the photo, or having seen the photo somewhere. For example, one user noted, \"He is my great-great grandfather and this photo has been passed down through the generations to me and was identified by his son John Albert Johnson, my father's grandfather.\" Users also left external URLs and source details in the notes as evidence. Sometimes, they provided additional context (e.g., \"This image came with a group of 7th Iowa images. The majority were of Company G., but there is only one person in the entire 7th Iowa Infantry that could be identified by the first or last name of 'Nelson.'\").\r\nThe validation workflow encouraged users to exercise careful deliberation while making photo ID decisions.. Users felt that the questions in each step of the validation process helped them to carefully weigh in all the evidence and deliberate while voting on the ID (Q4, mean = 4.60, SD = 0.49). C3 explained why the two steps were necessary: \"It's two separate things. One is asking, do you think that this face is the same face? Then the second is, do you think that this face matches this name? I think that that is a necessary question for both of those scenarios. I don't think it's redundant, I think it's necessary.\" H2 appreciated the thoughtfulness that the two-step process encouraged, saying, \"It could be the same guy, but it might be a different guy, but now that you know the other interface kind of forces me to slow down a little bit and think more carefully, because it's asking specific questions about things.\" A couple of participants, however, expressed initial confusion over the separation and order of these two tasks.\r\n\r\nFigure 7: Deliberation in case of post-identification voting. The table shows the distribution of confidence votes (second step of the validation process) against the user's facial similarity comparison (first step of the validation process). The comparison scores were computed by aggregating all the facial similarity comparisons in the first step (replica = 2, facial match = 1, not sure = 0, and different people = -1).\r\n\r\nOur logs affirm the deliberative aspect of the two-step validation process. We observed that when users compared one or more photos in the first step, they factored in the facial similarity evidence for their ID vote decision. Figure 7 shows that when the cumulative comparison scores are positive (i.e., majority of the comparisons were a facial match or a replica), the confidence scores are generally positive. Similarly, when the cumulative comparison scores are negative, the confidence votes are also largely negative. When the users are unsure about the facial similarity comparison, it reflects in their final ID vote. Notably, when there were mismatches between comparison scores and ID votes, users provided reasons in the notes, citing visual evidence and an unsure photo comparison.\r\nPhoto Steward's stewardship architecture fostered diverse forms of meaningful stigmergic collaboration amongst users.. Although most IDs received only a single vote, about 3% (193 IDs) received multiple votes (see Table 1). Among these, 140 IDs saw total consensus among voters on confidence scores, with half involving an explanatory note. When the initial vote lacked a note, subsequent voters often added information like external sources or context. Almost all IDs where voters agreed positively were supported by metadata such as period inscriptions, scholarly sources, or additional comparison photos, underscoring the credibility of these agreements.\r\nWhen users disagreed on an ID, they would almost always justify in notes why they differed from the previous voters' opinions (43 out of 53 disagreement instances). From analyzing the notes, we found that the subsequent voters provided additional information about external sources or service records to disagree (e.g., \"Please see Military Images magazine, MI, Volume XVI, Number 3, November - December 1994 for the correct identification of this image. The correct identification via the Michael McAfee collection is Richard Cramer, 4th Michigan Infantry\"). Among 18 instances of conflicting IDs for the same photo, seven saw resolution through a process of voting one ID over another, often accompanied by external evidence in notes or citing facial similarity.\r\nUser agreement was particularly strong in facial similarity comparisons. Of the 2200 unique photo pairs compared, 141 received multiple user reviews, with 114 seeing total agreement. The remaining cases typically involved user errors in distinguishing between 'facial match' and 'replica', or uncertainty about facial similarity.\r\n6.2 Stewardship Visualizations\r\nStewardship visualizations reflecting community insights enhanced the accuracy of photo ID assessments.. Participants found Photo Steward's bar chart visualization showing the five confidence levels to be simple and easy to understand (Q1, mean = 4.93, SD = 0.25). S1 said, \"I'm a visual person. Bar graphs or charts like statistical data helps me put things into a better perspective or gives me an idea of what I am working with versus just something more plain [like] the other interface.\"\r\nWhen participants were uncertain about an ID, they saw the additional evidence and justification that the community members had provided along with their vote as essential to taking the vote seriously. S5 said, \"If there's other users giving comments like 'I've used this source,' and you know you get information from a source where they found it, I'm probably gonna agree with them. If they're just voting yes without anything else, then I am probably more likely to go out and find sources for the photo myself and make my own decision.\"\r\nIn three instances, students (S1, S3, and S5) initially made incorrect assessments on the original interface but rectified these after using Photo Steward. The community's consensus and evidence in Photo Steward were pivotal in these corrections. S5, confronted with ID conflicts, stated after using Photo Steward: \"You know that helps a lot with what people are thinking and presumably these people have also gone to the Maine State Archives or something and verified and then given their opinion so that helps.\" Similarly, historian H2 and collector C3 initially made incorrect choices but amended their decisions after analyzing community-based evidence on Photo Steward. H2 remarked: \"So I believe it's the first person, of course, because it's based on the Maine State Archives. And then you've got the piece on there that said that you looked and found a different man. With that name in the actual regimental history. I say that's fairly accurate information.\" This highlights the critical role of community consensus in resolving photo ID conflicts.\r\nOn the other hand, when participants were confident about an ID, seeing the community's opinions affirmed their own assessments, for better or worse. In the words of H5: \"I really liked the very clear community consensus, and the ability to be able to see the identities of the people who were looking at these images. It was something that (gave) a boost of confidence in terms of my final decision.\" Overall, the participants found the community opinions to be useful for assessing the IDs (Q2, mean = 4.93, SD = 0.25).\r\nUsers gave additional weight to the opinions of members they were familiar with and desired more contextual information about all contributors.. Participants examined the bar chart visualization details and gave higher weight to the opinions of prominent names from the Civil War photography community while assessing the ID. C3 said, \"Here's <name redacted>, period inscription with valediction, the uniform matches his service record. <name redacted> said the same thing. These two uploaders I hold in very high regard to their opinions on this site. If they're saying that they think highly confident of this identification, that gives me a lot of confidence as well.\" H5 became further unsure about an ID after seeing a fellow historian's vote: \"And, you know, sort of knowing <name redacted>, someone who I know deals with primary source material a lot, and sort of being on the fence with it as well, leaves me in that unsure position.\"\r\nParticipants also sought additional details about community members, proposing indicators of professional status or active participation in the CWPS community. H1 emphasized his appreciation of amateur experts and opposition to gatekeeping, yet he suggested having some kind of credential indicator next to the username would be helpful: \"It will be great if, you know, somebody was an academic historian or a published Civil War author, if there was some way to just say some little tidbit next to <name redacted> 'Oh, saying, hey, I'm from <university redacted>, Professor or, you know, author of whatever.'\" Others preferred indicators of community activity levels or personal connections (i.e., descendants) to the identified individual in the photo.\r\nOur log analysis showed revealed the presence of certain active voters, with 9 out of 328 unique voters voting on more than 50 photos each, and 2 voting on over 2000 photos each (mostly ones that they had uploaded and identified). If we consider only votes on photos identified by someone else, we have 8 users who have voted on more than 15 IDs each, with 1 user voting on over 150 IDs.\r\nThe community opinions made the platform feel more engaging, but users had mixed thoughts about the ideal number of votes per photo.. Participants, in general, felt the community opinions made Photo Steward more engaging compared to the original system. H5 said, \"I thought it appeared more user-centric and user-friendly and it felt more participatory.\" C3 saw the benefits of increased engagement for making IDs more reliable: \"I think for this crowdsourcing project that we're building on this database [it] is very important to have those comments, those feedback, that we see in the voting system. It only makes this ID stronger and makes the project and the database a more trustworthy and reliable source.\"\r\nUsers wanted to see more community opinions and comments for IDs that had few votes. S1, on seeing only one vote for an ID, said, \"That doesn't make me feel as confident because that's not too many for me to give an answer.\" S3 said it would have been easier to assess some photos if \"there was more input from other people.\" Users had mixed opinions about how many votes they wanted to see for verifying an ID. In general, they wanted to see consensus among the community for an identity and at least three votes. C3 said, \"I usually shoot for like three to four [votes] as the lowest where I take some good quality out of those votes. [...] If there's only one or two, and especially if there's two that are split, that is not as reliable to me.\"\r\nParticipants' concerns about spreading voters too thin were borne out in the log data (see Table 1). We analyzed the logs to check how often CWPS users vote on the photos they are browsing. We found 1784 instances (out of 5843 voting instances) where the number of \"lurkers\" for a given photo page exceeded the number of voters on that photo.\r\nTable 4: Face Recognition Similarity Score Stats vs. User Comparisons (Facial Similarity).\r\n\r\nUser Comparison\r\nMean\r\nMedian\r\nSD\r\nCount\r\nReplica\r\n0.88\r\n0.91\r\n0.16\r\n659\r\nFacial Match\r\n0.52\r\n0.65\r\n0.31\r\n1015\r\nDifferent Person\r\n0.55\r\n0.59\r\n0.16\r\n265\r\nNot Sure\r\n0.51\r\n0.58\r\n0.23\r\n261\r\nUsers found the face recognition badge and community's opinions complementary for assessing the reliability of matched photos..\r\nTable 5: User Comparisons (Facial Similarity) vs. Face Recognition.\r\n\r\n\r\n# of Photo Pairs\r\n# of Photo Pairs\r\ncompared by 1 user\r\n# of Photo Pairs\r\ncompared by 2 users\r\n# of Photo Pairs\r\ncompared by 2+ users\r\nSupported by users\r\nand face recognition\r\n1408\r\n1281\r\n109\r\n18\r\nDisputed by users\r\nand face recognition\r\n26\r\n24\r\n0\r\n1\r\nUsers unsure,\r\nface recognition disputes\r\n42\r\n42\r\n0\r\n0\r\nUsers unsure,\r\nface recognition supports\r\n219\r\n217\r\n2\r\n0\r\nUsers dispute,\r\nface recognition supports\r\n239\r\n238\r\n1\r\n0\r\nUsers support,\r\nface recognition disputes\r\n266\r\n257\r\n8\r\n1\r\nFrom our logs, we found that 1408 photo pair comparisons were supported by both users and facial recognition, by far the most common outcome (see Table 5). This information would be visible to the larger user community in the form of stewardship visualizations (see Figure 5). Interestingly, we also see 266 cases where a comparison is supported by users, but disputed by facial recognition (similarity confidence score < 0.50). That outcome was approximately as common as when users disputed a comparison but face recognition supported it (239 pairs) or when a user was unsure but face recognition supported it (219 pairs). However, it was far less common for facial recognition to dispute a comparison when a user also disputed it (26 pairs) or was unsure (42 pairs).\r\nWhen we analyze the face recognition's confidence scores in more detail, Table 4 shows that there is a clear separation between replicas and other types of user comparison scores. Face recognition confidence scores for photo pairs that users labeled as replicas were much higher in terms of both mean and median (0.88 and 0.91, respectively, versus scores in the 0.50s and 0.60s for all others). While these fine-grained scores are not displayed to users — they intentionally see only the face recognition badge — the very close mean and median confidence scores for \"facial match\" versus \"different person\" illustrate the difficulty of automatically identifying non-replica matches and offer support for a hybrid human-AI approach (cf. Section 4.3.1).\r\nParticipants found the community's opinions to be helpful for assessing whether two photos were facial matches or not. S4 said, \"I mean, I think it's cool to see what the community is saying, because I do feel when it comes to saying 'Is this the same person in both these pictures?', that's really the best way to do it if you don't have any [other] information.\" C1 said that he found the community opinions for facial matches especially helpful because he is \"face blind\": \"I can't really identify the face-to-face, but the hair and the mustache and all the stuff that, in addition, it helps me with that for sure. It's good because other than having to find somebody close to me and be like, 'Do you think this is the same people?' [I] have that community right there.\"\r\nParticipants had mixed opinions about facial recognition technology, but most found the badge indicating whether it supported the two photos being a match to be a useful data point. S3 said, \"The facial recognition saying they are similar — I would go ahead and trust that but I don't know if I would trust it enough to make a verification on my own.\" In general, participants found the strengths of facial recognition and the community to be complementary in determining whether two photos showed the same person, and liked seeing both results together. H4 felt the community and the technology had separate roles:\r\nThis is facial recognition, and this is the historical background. I trust facial recognition, but it makes me feel better to have that historical background. I think the human eye can be tricked by different hairstyles and different beards. Just to have this outside historical verification to say like, 'Okay, maybe you or I was tricked, but the machine was not,' I think that's really helpful.\r\nS1 relied on both the community and facial recognition to make a decision on a facial match: \"I think both give me kind of an idea. Okay, there's this facial recognition technology being used, but also there's other users that are leaning towards that this is the same person.\" H5 got a similar boost of confidence: \"This is certainly reassuring seeing not only the AI match, but also in terms of the community — seeing that seven users have said that this is a facial match as well. I'd be quite convinced by this.\"\r\n7 DISCUSSION\r\n7.1 Leveraging Collective Intelligence for Validating Person Identification\r\nPrior work has raised concerns about misinformation in online history communities [43, 75]. To address these problems on CWPS, we built Photo Steward for supporting community-based validation of photo IDs. Users found Photo Steward's stewardship visualizations not only helpful for affirming their own assessment, but also for discovering new knowledge and correcting their decisions, if need be. These visualizations, combined with the validation workflow, exhibit a form of stigmergic collaboration, where users build on prior knowledge left by the community and leave their own assessment for other users [14, 17, 27, 56].\r\nPhoto Steward allowed users to express how confident they are about an ID in a fine-grained manner, in contrast to the binary agreements or disagreements observed on iNaturalist by Wiggins et al. [72]. The CWPS community preferred this nuanced form of stewardship as users are likely to have different degrees of confidence based on the evidence available for identifying a photo, thus demonstrating the effectiveness of Design Goal 2. As S3 pointed out, users often experience difficulty in making a binary decision about individuals who lived 150 years back due to the lack of surviving documentation.\r\nBeyond its basic usefulness, Photo Steward's full potential can best be realized through sustained community participation, but most IDs on CWPS only received one vote. To address this challenge, we can leverage different crowdsourcing and online community strategies. For example, we can draw the community's attention towards IDs that are \"more of a puzzle,\" as H5 suggested, similar to Twitter's Birdwatch promoting tweets for fact-checking [52]. Designing nudges to encourage lurkers to vote on the IDs they are viewing can further help in these efforts. Organizing community events can help foster interest and participation in collaboratively verifying IDs, drawing inspiration from crowdsourcing events like CrowdSolve, where experts and novices collaborate on solving missing persons cold cases [70]. Incentive mechanisms such as leaderboards and challenges [46] can drive extrinsic motivation within the community for verifying the IDs. Finding users who are more likely to vote on an ID, based on their skills and interests, can also be an effective collaboration strategy [71]. In future work, we plan to integrate these strategies and introduce explicit \"calls to action\" [51, 58] on the home feed, guiding the community's attention towards IDs that require validation and fostering more sustainable, collaborative participation in historical photo identification.\r\n7.2 Exercising Deliberation in Human-AI Teams\r\nWe found that Photo Steward's validation workflow was effective not only for voting on the IDs and comparing the photos side-by-side, but also encouraged users to deliberate on their decision, drawing parallels to other social computing systems that support reflection and deliberation (e.g., [28, 29]). This deliberative intervention was non-trivial as users on CWPS follow an identification pipeline which is powered by facial recognition, an AI algorithm that is far from perfect [44]. Users are trying to find the correct match, if present at all, from a pool of potential candidates, which are largely comprised of similar-looking false positives — akin to finding a needle in a haystack. Further, the task of comparing photos of people is by no means an easy task for humans, even in a modern context [53]. While Photo Steward can not completely curb automation bias, an issue that has been previously observed in multiple online history communities [44, 75], its multi-step, validation workflow with structured feedback interventions encouraged users to deliberate over AI suggestions before making a decision.\r\nPhoto Steward's workflow also compartmentalizes the tasks that AI is good at – such as quickly retrieving similar-looking candidates from a large search pool – from the tasks where the AI makes more errors – such as verifying whether two faces show the same person or not [5, 55]. Decoupling facial similarity comparison from the person identification task (Design Goal 1) allows the users to now focus on the face verification task. In doing so, Photo Steward's workflow supports effective human-AI teaming in the context of person identification by allowing the user to make a granular assessment for the face verification task instead of the AI, while also ensuring that an AI-retrieved, similar-looking potential candidate is being compared against.\r\nAs imperfect AI algorithms get deployed in high-stakes scenarios such as medical imaging, law enforcement, etc. [6], it becomes more critical to reduce automation bias and encourage more deliberative decision-making. Amershi et al. recommend granular user feedback while interacting with AI systems as part of their \"Guidelines for Human-AI Interaction\" [2]. Similarly, other forms of design interventions, such as counterfactual AI explanations [63], chatbots [26], and community opinions [59] can also be explored for encouraging deliberative decision-making with AI assistance.\r\nPrior work in human-face recognition teams has shown that algorithmic suggestions can have a significant biasing effect on a user's decision [23]. Our findings showed that Photo Steward was able to encourage users to exercise deliberation while interacting with results retrieved by facial recognition. At the same time, Table 4 also showed that users can differ from the algorithm's suggestions, thus necessitating a deeper dive analysis of this dissonance as part of future work.\r\n7.3 Assessing Quality in Crowdsourced Original Historical Research\r\nWe found that Photo Steward's stewardship visualizations helped users assess the reliability of photo IDs on CWPS, which were a result of Design Goal 3. However, crowdsourced identifications always run the risk of groupthink [24, 25], which can eventually mislead users into believing and amplifying misidentifications, a concern also raised by H4. Public deliberation of modern photo IDs on social media can have profound negative consequences for false targets, as exemplified by the Boston Marathon bombing [49, 66] and the recent US Capitol riot [48]. This raises the question: are Photo Steward's stewardship visualizations sufficient for assessing the quality of photo identifications made on CWPS?\r\nPrior work on crowdsourced scholarship suggests an answer. Rosenzweig [57] analyzed Wikipedia as a source of historical scholarship, noting its policy against original research, and advocating for it as a tool for teaching the limitations of information sources and critical analysis of primary and secondary sources. Motivated along similar lines, Forte et al [18] proposed the assessability framework for designing assessable participatory information systems, based on information provenance and stewardship. The concept of provenance, extensively used in history and archival studies, describes information that makes it possible to trace the ownership or origins of the content, while stewardship refers to the processes that were used for maintaining the content, including its authorship. In the case of Wikipedia, Forte et al. found that visualizing provenance (i.e., citation types) and stewardship (i.e., article quality) had a significant impact on assessments of articles and Wikipedia as an information source.\r\nWhile Photo Steward enables information stewardship on CWPS, there is an opportunity for incorporating provenance into the CWPS platform to make it a truly assessable online platform. A significant proportion of the notes left by users on Photo Steward qualified as provenance information, namely comments about period inscriptions, family trees, external sources and URLs. The challenges of assessing IDs on CWPS are, however, different from assessing information on Wikipedia, primarily because CWPS supports original research unlike Wikipedia's no original research policy [73]. This was also the reason why we designed Photo Steward to be a review system rather than a single editable output such as Wikipedia; original research such as historical photo identifications is often times an evolving investigation rather than a final decision. To assess the reliability of original photo IDs made on CWPS, users may want to factor in the provenance of the reference photos that were used in the identification process. In such cases, Photo Steward's stewardship visualizations (i.e., facial similarity reliability) can further help the user in assessing whether the reference photos can be used as reliable provenance or not.\r\n8 CONCLUSION\r\nPhoto Steward attempts to help users assess and validate photo IDs better on CWPS. We present an information stewardship architecture, and adapt it for the task of historical person identification. We demonstrate the effectiveness of Photo Steward on CWPS, an existing online platform, where users found the stewardship visualizations, which included the community opinions and the AI verdict, useful for making accurate assessments of photo IDs on the platform. Further, users found Photo Sleuth's multi-step, structured validation workflow to help them deliberate before making decisions about the photo's identity. This work opens doors for exploring new ways to leverage collective intelligence and AI in creating assessable online information systems for historical archives.\r\nACKNOWLEDGMENTS\r\nWe wish to thank Ron Coddington, Paul Quigley, Liling Yuan, and our study participants. This research was supported by NSF IIS-1651969 and a Virginia Tech ICTAS Junior Faculty Award.\r\nREFERENCES\r\n2021. Heritage Auctions: World's Largest Collectibles Auctioneer. https://www.ha.com/ Navigate tocitation 1citation 2\r\nSaleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. 2019. Guidelines for human-AI interaction. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–13. Navigate tocitation 1\r\nBrakkton Booker. 2015. $2 photo found at Junk Store has Billy the kid in it, could be worth $5M. https://www.npr.org/sections/thetwo-way/2015/10/15/448993361/-2-photo-found-at-junk-store-has-billy-the-kid-in-it-could-be-worth-5-million Navigate tocitation 1citation 2\r\nVirginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3, 2 (2006), 77–101. Navigate tocitation 1\r\nJoy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Conference on Fairness, Accountability and Transparency, FAT 2018, 23-24 February 2018, New York, NY, USA. 77–91. http://proceedings.mlr.press/v81/buolamwini18a.html Navigate tocitation 1\r\nCarrie J Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg S Corrado, Martin C Stumpe, et al. 2019. Human-centered tools for coping with imperfect algorithms during medical decision-making. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–14. Navigate tocitation 1\r\nFanny Chevalier, Stéphane Huot, and Jean-Daniel Fekete. 2010. Wikipediaviz: Conveying article quality for casual wikipedia readers. In 2010 IEEE Pacific Visualization Symposium (PacificVis). IEEE, 49–56. Navigate tocitation 1\r\nH Jason Combs. 2005. The Internet's Impact on the Antiques Trade. Material Culture (2005), 26–41. Navigate tocitation 1citation 2\r\nAnthony DeBartolo. 1975. Appraisers can tell when old photo really may be golden. https://www.chicagotribune.com/news/ct-xpm-1985-12-27-8503300067-story.html Navigate tocitation 1\r\nAnthony DeBartolo. 2021. Appraisers can tell when old photo really may be golden. https://www.chicagotribune.com/news/ct-xpm-1985-12-27-8503300067-story.html Navigate tocitation 1\r\nHarley D Dickinson. 1998. Evidence-based decision-making: an argumentative approach. International Journal of Medical Informatics 51, 2-3 (1998), 71–81. Navigate tocitation 1\r\nFor The Inquirer Edward Colimore. 2019. Did John Wilkes Booth get away with murdering President Abraham Lincoln?https://www.inquirer.com/news/john-wilkes-booth-lincoln-conspiracy-photo-recognition-20190415.html Navigate tocitation 1\r\nMark Elliott. 2006. Stigmergic Collaboration: The Evolution of Group Work: Introduction. m/c journal 9, 2 (2006). Navigate tocitation 1citation 2citation 3\r\nMark Elliott. 2016. Stigmergic collaboration: A framework for understanding and designing mass collaboration. In Mass collaboration and education. Springer, 65–84. Navigate tocitation 1citation 2citation 3\r\nThomas Erickson and Wendy A Kellogg. 2000. Social translucence: an approach to designing systems that support social processes. ACM transactions on computer-human interaction (TOCHI) 7, 1 (2000), 59–83. Navigate tocitation 1\r\nDan Evon. 2020. Did Joe Biden's Great-Grandfather Own Slaves?https://www.snopes.com/fact-check/joe-biden-slaves-great-grandfather/ Navigate tocitation 1\r\nKristie Fisher, Scott Counts, and Aniket Kittur. 2012. Distributed sensemaking: improving sensemaking by leveraging the efforts of previous users. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 247–256. Navigate tocitation 1citation 2\r\nAndrea Forte, Nazanin Andalibi, Thomas Park, and Heather Willever-Farr. 2014. Designing information savvy societies: an introduction to assessability. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2471–2480. Navigate tocitation 1citation 2citation 3citation 4\r\nJacey Fortin. 2018. She Was the Only Woman in a Photo of 38 Scientists, and Now She's Been Identified. The New York Times (Mar 2018). https://www.nytimes.com/2018/03/19/us/twitter-mystery-photo.html Navigate tocitation 1\r\nSarah A Gilbert. 2020. \" I run the world's largest historical outreach project and it's on a cesspool of a website.\" Moderating a Public Scholarship Site on Reddit: A Case Study of r/AskHistorians. Proceedings of the ACM on Human-Computer Interaction 4, CSCW1 (2020), 1–27. Navigate tocitation 1\r\nJerome S Handler and Michael L Tuite. 2007. Retouching History: The Modern Falsification of a Civil War Photograph. Navigate tocitation 1citation 2\r\nM. Keith Harris. 2019. Civil War Photo Sleuth. Journal of American History 106, 2 (2019), 544–546. https://doi.org/10.1093/jahist/jaz498 Navigate tocitation 1citation 2\r\nJohn J Howard, Laura R Rabbitt, and Yevgeniy B Sirotin. 2020. Human-algorithm teaming in face recognition: How algorithm outcomes cognitively bias human decision-making. Plos one 15, 8 (2020), e0237855. Navigate tocitation 1\r\nPan Hui and Sonja Buchegger. 2009. Groupthink and peer pressure: Social influence in online social network groups. In 2009 International Conference on Advances in Social Network Analysis and Mining. IEEE, 53–59. Navigate tocitation 1\r\nNassim JafariNaimi and Eric M Meyers. 2015. Collective intelligence or group think? Engaging participation patterns in World Without Oil. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing. 1872–1881. Navigate tocitation 1\r\nSoomin Kim, Jinsu Eun, Joseph Seering, and Joonhwan Lee. 2021. Moderator chatbot for deliberative discussion: Effects of discussion structure and discussant facilitation. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–26. Navigate tocitation 1\r\nAniket Kittur, Andrew M Peters, Abdigani Diriye, and Michael Bove. 2014. Standing on the schemas of giants: socially augmented information foraging. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. 999–1010. Navigate tocitation 1\r\nTravis Kriplean, Caitlin Bonnar, Alan Borning, Bo Kinney, and Brian Gill. 2014. Integrating On-demand Fact-checking with Public Dialogue. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing(CSCW '14). ACM, New York, NY, USA, 1188–1199. https://doi.org/10.1145/2531602.2531677 Navigate tocitation 1citation 2citation 3\r\nTravis Kriplean, Michael Toomim, Jonathan Morgan, Alan Borning, and Andrew Ko. 2012. Is This What You Meant?: Promoting Listening on the Web with Reflect. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems(CHI '12). ACM, New York, NY, USA, 1559–1568. https://doi.org/10.1145/2207676.2208621 Navigate tocitation 1citation 2citation 3\r\nKurt Luther. 2016. How Fellow Collectors, Field Photos and Muttonchops Identified an Unknown Officer. Military Images 34, 1 (2016), 29–31. Navigate tocitation 1\r\nKurt Luther. 2017. Merrill Carbine Leads to a Soldier's Identification. Military Images 35, 2 (2017), 64–65. Navigate tocitation 1citation 2citation 3\r\nKurt Luther. 2018. Non-Traditional Research Tools—and Serendipity. Military Images 36, 3 (2018), 12–13. Navigate tocitation 1\r\nKurt Luther. 2018. What are the odds? Photo sleuthing by the numbers. Military Images 36, 1 (2018), 12–15. Navigate tocitation 1citation 2citation 3citation 4citation 5\r\nKurt Luther. 2019. What to Do When Gold Standards Go Wrong?Military Images 37, 1 (2019), 8–9. https://www.jstor.org/stable/26532101 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: How to Trust the Worthiness of an Identification. Military Images 38, 3 (213) (2020), 8–11. https://www.jstor.org/stable/26914966 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: Lost and Found in the Library of Congress. Military Images 38, 2 (212) (2020), 10–13. https://www.jstor.org/stable/26890126 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: The Art of Photo Sleuthing. Military Images 38, 4 (214) (2020), 8–11. https://www.jstor.org/stable/26925454 Navigate tocitation 1\r\nRamona Martinez. 2012. Photo mystery solved, then doubted, then deciphered, thanks to readers. https://www.npr.org/sections/pictureshow/2012/04/17/150801239/photo-mystery-solved-then-doubted-then-resolved-thanks-to-readers Navigate tocitation 1\r\nRamona Martinez. 2012. Unknown No More: Identifying A Civil War Soldier. http://www.npr.org/2012/04/11/150288978/unknown-no-more-identifying-a-civil-war-soldier Navigate tocitation 1\r\nChristian A Meissner and John C Brigham. 2001. Thirty years of investigating the own-race bias in memory for faces: A meta-analytic review.Psychology, Public Policy, and Law 7, 1 (2001), 3. Navigate tocitation 1\r\nVikram Mohanty, Kareem Abdol-Hamid, Courtney Ebersohl, and Kurt Luther. 2019. Second opinion: Supporting last-mile person identification with crowdsourcing and face recognition. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 86–96. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6\r\nVikram Mohanty and Kurt Luther. 2023. DoubleCheck: Designing Community-based Assessability for Historical Person Identification. ACM Journal on Computing and Cultural Heritage (JOCCH) (to appear) (2023). Navigate tocitation 1\r\nVikram Mohanty, David Thames, Sneha Mehta, and Kurt Luther. 2019. Photo sleuth: Combining human expertise and face recognition to identify historical portraits. In Proceedings of the 24th International Conference on Intelligent User Interfaces. 547–557. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6citation 7citation 8\r\nVikram Mohanty, David Thames, Sneha Mehta, and Kurt Luther. 2020. Photo Sleuth: Identifying Historical Portraits with Face Recognition and Crowdsourced Human Expertise. ACM Transactions on Interactive Intelligent Systems (TiiS) 10, 4 (2020), 1–36. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6citation 7citation 8citation 9citation 10\r\nMeredith Ringel Morris, Scott Counts, Asta Roseway, Aaron Hoff, and Julia Schwarz. 2012. Tweeting is believing? Understanding microblog credibility perceptions. In Proceedings of the ACM 2012 conference on computer supported cooperative work. 441–450. Navigate tocitation 1\r\nBenedikt Morschheuser, Juho Hamari, and Jonna Koivisto. 2016. Gamification in crowdsourcing: a review. In 2016 49th Hawaii International Conference on System Sciences (HICSS). IEEE, 4375–4384. Navigate tocitation 1\r\nKathleen L Mosier and Linda J Skitka. 1999. Automation use and automation bias. In Proceedings of the human factors and ergonomics society annual meeting, Vol. 43. SAGE Publications Sage CA: Los Angeles, CA, 344–348. Navigate tocitation 1citation 2\r\nGreg Myre. 2021. How Online Sleuths Identified Rioters At The Capitol. https://www.npr.org/2021/01/11/955513539/how-online-sleuths-identified-rioters-at-the-capitol Navigate tocitation 1\r\nJohnny Nhan, Laura Huey, and Ryan Broll. 2017. Digilantism: An analysis of crowdsourcing and the Boston marathon bombings. The British journal of criminology 57, 2 (2017), 341–361. Navigate tocitation 1\r\nRaja Parasuraman and Dietrich H Manzey. 2010. Complacency and bias in human use of automation: An attentional integration. Human factors 52, 3 (2010), 381–410. Navigate tocitation 1\r\nJunwon Park, Ranjay Krishna, Pranav Khadpe, Li Fei-Fei, and Michael Bernstein. 2019. AI-based request augmentation to increase crowdsourcing participation. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 115–124. Navigate tocitation 1\r\nSarah Perez. 2022. Twitter to show 'Birdwatch' community fact-checks to more users, following criticism. https://techcrunch.com/2022/03/03/twitter-to-show-birdwatch-community-fact-checks-to-more-users-following-criticism/ Navigate tocitation 1\r\nP Jonathon Phillips, Amy N Yates, Ying Hu, Carina A Hahn, Eilidh Noyes, Kelsey Jackson, Jacqueline G Cavazos, Géraldine Jeckeln, Rajeev Ranjan, Swami Sankaranarayanan, et al. 2018. Face recognition accuracy of forensic examiners, superrecognizers, and face recognition algorithms. Proceedings of the National Academy of Sciences 115, 24 (2018), 6171–6176. Navigate tocitation 1citation 2\r\nPeter Pirolli, Evelin Wollny, and Bongwon Suh. 2009. So you know you're getting the best possible information: a tool that increases Wikipedia credibility. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1505–1508. Navigate tocitation 1\r\nInioluwa Deborah Raji and Joy Buolamwini. 2019. Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products. Navigate tocitation 1\r\nAmira Rezgui and Kevin Crowston. 2018. Stigmergic coordination in Wikipedia. In Proceedings of the 14th International Symposium on Open Collaboration. 1–12. Navigate tocitation 1\r\nRoy Rosenzweig. 2006. Can History Be Open Source? Wikipedia and the Future of the Past. Journal of American History 93, 1 (June 2006), 117–146. Navigate tocitation 1citation 2\r\nSaiph Savage, Andres Monroy-Hernandez, and Tobias Höllerer. 2016. Botivist: Calling volunteers to action using online bots. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing. 813–822. Navigate tocitation 1\r\nMike Schaekermann, Joslin Goh, Kate Larson, and Edith Law. 2018. Resolvable vs. irresolvable disagreement: A study on worker deliberation in crowd work. Proceedings of the ACM on Human-Computer Interaction 2, CSCW (2018), 1–19. Navigate tocitation 1\r\nMichael S. Schmidt. 2018. 'Flags of Our Fathers' Author Now Doubts His Father Was in Iwo Jima Photo. The New York Times (Jan 2018). https://www.nytimes.com/2016/05/04/us/iwo-jima-marines-bradley.html Navigate tocitation 1\r\nJennifer Schuessler. 2017. Found: Oldest Known Photo of a U.S. President (Socks and All). https://www.nytimes.com/2017/08/16/arts/design/john-quincy-adams-daguerreotype-sothebys-auction.html Navigate tocitation 1\r\nJulia Schwarz and Meredith Morris. 2011. Augmenting web pages and search results to support credibility assessment. In Proceedings of the SIGCHI conference on human factors in computing systems. 1245–1254. Navigate tocitation 1\r\nRuoxi Shang, KJ Kevin Feng, and Chirag Shah. 2022. Why Am I Not Seeing It? Understanding Users' Needs for Counterfactual Explanations in Everyday Recommendations. In 2022 ACM Conference on Fairness, Accountability, and Transparency. 1330–1340. Navigate tocitation 1\r\nLinda J Skitka, Kathleen L Mosier, and Mark Burdick. 1999. Does automation bias decision-making?International Journal of Human-Computer Studies 51, 5 (1999), 991–1006. Navigate tocitation 1\r\nVerbi Software. 2019. MAXQDA 2020 [computer software]. VERBI Software. Available from maxqda.com. Navigate tocitation 1\r\nNPR Staff. 2016. How Social Media Smeared A Missing Student As A Terrorism Suspect. https://www.npr.org/sections/codeswitch/2016/04/18/474671097/how-social-media-smeared-a-missing-student-as-a-terrorism-suspect Navigate tocitation 1\r\nBongwon Suh, Ed H Chi, Aniket Kittur, and Bryan A Pendleton. 2008. Lifting the veil: improving accountability and social transparency in Wikipedia with wikidashboard. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1037–1040. Navigate tocitation 1\r\nW Ben Towne, Aniket Kittur, Peter Kinnaird, and James Herbsleb. 2013. Your process is showing: controversy management and perceived quality in Wikipedia. In Proceedings of the 2013 conference on Computer supported cooperative work. 1059–1068. Navigate tocitation 1\r\nCivil War Trust. 2021. Military Images Magazine | Interview with Ron Coddington. https://www.battlefields.org/learn/articles/military-images-magazine Navigate tocitation 1\r\nSukrit Venkatagiri, Aakash Gautam, and Kurt Luther. 2021. CrowdSolve: Managing Tensions in an Expert-Led Crowdsourced Investigation. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–30. Navigate tocitation 1\r\nShaun Wallace, Lucy Van Kleunen, Marianne Aubin-Le Quere, Abraham Peterkin, Yirui Huang, and Jeff Huang. 2017. Drafty: Enlisting Users To Be Editors Who Maintain Structured Data. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 5. Navigate tocitation 1\r\nAndrea Wiggins and Yurong He. 2016. Community-based data validation practices in citizen science. In Proceedings of the 19th ACM Conference on computer-supported cooperative work & social computing. 1548–1559. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6\r\nFoundation Wikimedia. 2022. No original research. https://en.wikipedia.org/wiki/Wikipedia:No_original_research Navigate tocitation 1\r\nHeather Willever-Farr, Lisl Zach, and Andrea Forte. 2012. Tell me about my family: A study of cooperative research on Ancestry. com. In Proceedings of the 2012 iConference. ACM, 303–310. Navigate tocitation 1\r\nHeather L Willever-Farr and Andrea Forte. 2014. Family matters: Control and conflict in online family history production. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. ACM, 475–486. Navigate tocitation 1citation 2citation 3citation 4citation 5\r\nJeremy B Wilmer. 2017. Individual differences in face recognition: A decade of discovery. Current Directions in Psychological Science 26, 3 (2017), 225–230. Navigate tocitation 1\r\nBob Zeller. 2022. Searching for photos of Civil War Soldiers | David Wynn Vaughan. https://www.civilwarphotography.org/searching-for-photos-of-civil-war-soldiers/ Navigate tocitation 1\r\nA CIVIL WAR PHOTO SLEUTH: DESIGN CHALLENGES AND OPPORTUNITIES\r\n \r\nHere, we provide details of three key challenges posed by Civil War Photo Sleuth that might have contributed towards misidentification and subsequently, inaccurate assessments, and how they present design opportunities for Photo Steward.\r\nA.1 Conflating facial similarity with photo identification\r\nA.1.1 Challenges: Mohanty et al. conducted a benchmarking study of CWPS [44], and found the face recognition algorithm to be of low precision; i.e., it retrieved hundreds of search results which may look similar to the query photo but are actually different people (false positives). Low precision increases the chances that users will interact with a lot of false positives. In such cases, one may need to garner additional information (i.e., comparing biographical information) before making a decision. However, it is plausible that automation bias may play a role in non-expert users making a match solely based on facial similarity [47]. As a result, there is strong potential for misidentification (see Figure 8).\r\nA.1.2 Current Workflow: CWPS's compare interface allows users to closely inspect the search results for a potential match, but does not make any distinction between facial similarity comparison and photo identification. Both are conflated into a one-step process, with one \"Identify\" button for the users to make their decisions (see Figure 2-C). Yet, users may want to indicate agreement with just the facial similarity (i.e., query photo and the search result showing the same person) but not the identity (i.e., name and biographical information), or vice versa.\r\nA.1.3 Design Goal 1: To support accurate investigation of photo identifications, users should be able to deliberate on the different aspects of the decision-making process. Providing users with a decision-making workflow that decouples facial similarity comparison from the overall photo identification task would allow them to focus on these tasks separately, while discouraging them from making decisions solely on the basis of facial similarity.\r\n\r\nFigure 8: An example of a misidentified photo on CWPS. This photo was identified as John C Whiteside based on facial similarity with the top search result: an identified reference photo of John C Whiteside. However, other visual clues, such as the photographer studio, do not align with Whiteside's biographical information.\r\n\r\nA.2 Lacking support for fine-grained, deliberative decision making\r\nA.2.1 Challenges: Historical photo identification is a complex task, where experts often corroborate multiple pieces of evidence, including facial similarity comparison, before reaching a decision about the identity of the photo [33]. While confirming an identity, experts may be highly confident if the source, military records, uniform clues, and additional photos of the same person all line up, or slightly confident if they need additional evidence. Conflicting evidence pieces may also affect their confidence levels. Similarly, they may have different degrees of certainty while ruling out an identity for a photo. A lack of support for expressing and displaying granularity in these photo identification decisions can lead to varying degrees of uncertainty being captured and misinterpreted as a confirmation, and eventually propagating misidentifications. Further, while it is safe to assume the vast majority of the Civil War photography community care about the accuracy of the photo IDs, there exists a small risk of financially-driven misidentifications since certain types of identified photos are considered to be more valuable [9].\r\nA.2.2 Current Workflow: The compare interface's \"Identify\" button (see Figure 2-C) is the only mechanism available on the platform for users to provide (a unary) input on photo identifications. CWPS users currently cannot provide fine-grained feedback on a given photo's identity, either at the time of identifying or afterwards on the photo page.\r\nA.2.3 Design Goal 2 : Users should be able to express how certain or uncertain they are about a photo identification. Interventions for allowing users to provide structured feedback can not only facilitate more accurate, fine-grained responses, but also encourage deliberation on the available evidence before making a decision, borrowing inspiration from other social computing systems that support users reflecting and deliberating on available information [28, 29].\r\nA.3 Limited validation opportunities for the community\r\nA.3.1 Challenges: For humans, deciding whether two photos show the same person is a highly subjective experience. A study by Mohanty et al. [41] showed that participants in a crowdsourcing study often disagree with each other and with facial recognition results in person identification tasks. The same study showed that people often seek a second opinion from peers for validating photo identifications. Without validation, original crowdsourced historical research may result in unresolved cases of conflicting IDs proposed for the same face, incorrect IDs remaining unchecked, or airtight IDs not being confirmed.\r\nA.3.2 Current Workflow: Once a photo has been identified, CWPS does not offer other users the option to express their opinions on whether two photos show the same person or not, or if the photo has been identified correctly or not (see Figure 8). The photo page does not indicate if (or why) the photos are linked to each other, how they are similar, or which user(s) considered them to be similar. Further, it does not indicate the role of facial recognition in linking them together.\r\nA.3.3 Design Goal 3 : To support accurate original historical research, the platform should encourage information stewardship from the community by allowing members to give their opinions on existing photo identifications [72]. Further, the platform should also be transparent about the role of community stewardship and facial recognition for a given photo identification, which can then act as evidence for aiding subsequent validation efforts by other users, thus supporting a form of stigmergic collaboration [13, 14].\r\nB DETAILS FOR LOG ANALYSIS\r\n \r\nTo understand the community's stewardship behaviors, we examined website logs for all user activities for one year since we launched the new features (December 2020 – December 2021). During this period, we observed 5843 voting instances on 5672 photos for 5355 unique IDs, where a user voting on whether Photo N can be identified as a Person M or not is considered to be one voting instance. For a given voting instance, we analyzed 1) any associated facial similarity comparisons to understand how they deliberated on their final voting decision, and 2) the user's confidence vote plus any justification notes left by the user to understand their decision rationale. Of the 5672 query photos, 4297 photos (4377 voting instances) did not have any facial similarity comparisons — only the user's confidence on the ID (plus any notes) was captured.\r\nFor the remaining voting instances which had at least one associated facial similarity comparison (i.e., users deliberate through the two-step validation process), we broke them down into pre-identified (i.e., the user knew the ID of the photo ahead of time and therefore, might be looking for a specific target on the search results page) and post-identified (i.e., the user did not know the ID of the photo and therefore, may not have a specific target while analyzing the search results) cases. As mentioned earlier in Section 4.2, users had the opportunity to validate IDs either on the search results page on the photo page. While a pre-identified voting instance almost certainly originated from the search results page, a post-identified case could be from either page. Since the logs did not give us the page origin of each vote directly, we triangulated from CWPS's timestamps to determine whether a given voting instance was for a pre-identified case or a post-identified one.\r\nAfter collating all the associated facial similarity comparisons for a given voting instance, we had 1064 pre-identified and 576 post-identified user deliberations to analyze. Each deliberation instance is a user's attempt to identify a query photo as a given target (person) ID, where they first compare facial similarity with all other photos that have been identified as the target ID, followed by the user's confidence on the query photo being the target ID. We analyzed the user responses to see whether the facial similarity comparisons had any impact on the user's confidence.\r\nUsers had provided comments in 682 (out of 5843, or 11.7%) voting instances. We coded these comments using an iterative, inductive approach, which resulted in five high-level themes, which can broken down into 13 sub-categories (see Appendix C).\r\nTo understand any stigmergic collaboration processes at play, we also analyzed the IDs which had multiple votes to check for agreements and disagreements between the voters. We further analyzed how the community's facial similarity comparisons compare against the facial recognition scores.\r\nC THEMES FOR NOTES ANALYSIS\r\nPhoto Comparisons\r\nHigh-Level Comparison: The note mentions \"replica\", \"facial similarity\", \"facial match\", \"identical\", \"visual comparison\", and other similar terms that describe comparisons with a prior identified photo.\r\nDescribing facial features: The note mention facial features like \"eyes\", \"hairline\", \"ears\", etc. to make comparisons\r\nWord-of-Mouth\r\nDescendant: The note either mentions that the user is a descendant of the person being identified, or they got the information from the family of the person.\r\nOwnership: The note either mentions that the user owns a printed version of the photo, compared with a photo in their collection, or they know the owner of the photo.\r\nFamiliarity/Self-Reported Research: The note mentions that the user has seen the photo somewhere, be it in a book, museum, etc.\r\nVisual Clues\r\nUniform: The note mentions visual clues that pertain to the uniform of the person (e.g., hat insignia, shoulder straps, etc.) The user may infer the possible service information (i.e., ranks, branches, regiments, etc.) from the uniform clues.\r\nInscription: The note mentions the presence of a period inscription on the photo (a highly trustworthy primary source for a person's ID), or an album case, or modern inscriptions such as books, which is generally the name of the person being identified. In some instances, the inscribed text may point to the person's service information.\r\nOther Visual Clues: The note mentions visual clues in the photo (e.g., backmarks, borders, etc.) beyond the person's face.\r\nExternal Information\r\nExternal URL: The note mentions an external URL, which supposedly has additional information about the photo's ID.\r\nOther sources (e.g., museum, website, book, etc.): The note mentions an external source (e.g., museum, book, etc.) that supposedly has evidence for the photo's ID, but no URLs are provided. Details about the source may or may not be available.\r\nLack of Information / Seeking Additional Evidence: The note mentions the lack of evidence or seeking additional evidence, be it about the source or the service information.\r\nProviding Additional Information\r\nBiographical Information: The notes mentions additional information about the person's service records, specific regiments, biographical information (name, year, location), etc.\r\nAdditional Context: The note mentions some additional context provided by the user to justify their decision, such as information about the photo collection, or pointing to someone else's research, or some historical context, or incorrect evidence, or if the person is prominent.\r\nD PARTICIPANT DETAILS\r\nUndergraduate and master's students concentrating in history who use Civil War photos for their coursework and research projects, but are not (yet) employed in a professional capacity as historians. We recruited five students via recommendations from our university's history department. None of the students had used CWPS before, or were known to the authors prior to the study. Three students were men and two were women, and all were in the \"18 to 30\" age group. We anonymize them with identifiers S1–S5.\r\nExperienced users of Civil War Photo Sleuth who have added over 50 photos each and have substantial knowledge of Civil War history, but are not professional historians. We recruited five amateur experts from the CWPS contact list. All five users were men, and they were distributed across different age groups (two in \"18 to 30\", two in \"31 to 40\", and one in \"51 to 60\"). We anonymize them with identifiers C1–C5. C1 and C3 are among the most active daily users on CWPS. Only two of the five had used Photo Steward before.\r\nExpert historians with a graduate degree in history, specializing in American Civil War history, but with little or no previous experience with CWPS. We recruited five historians via recommendations from our university's history department. Three historians were men and two were women. They were distributed across different age groups (two in \"18 to 30\", two in \"31 to 40\", and one in \"51 to 60\"). We anonymize them with identifiers H1–H5. None of them had used Photo Steward before.\r\nE LAB STUDY QUESTIONS\r\nE.1 Semi-Structured Questions\r\nIs there a way that you would like to capture your thoughts on this ID and share them with others, if possible? If so, what would that look like? If not, can you explain why not?\r\nWhat did you think about the community opinions?\r\nWhat do you think about the ID quality visualization?\r\nWhat did you think about the 2-step process while agreeing/disagreeing on an identity?\r\nWhat is your overall opinion of both the interfaces?\r\nWhich interface would you prefer for validating the information? And why?\r\nWhat would you change or improve?\r\nE.2 Usability Survey\r\nQ1. The community's opinions about an identity were clear and easy to understand in the 2nd system. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ2. The community's opinions about an identity were useful for assessing the information. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ3. The process of voting on an identity was clear and easy to understand. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ4. Comparing other photos first and then voting on an identity helped me deliberate and make more accurate decisions. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ5. I was able to validate the information better using the 2nd system. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nF SUMMARY OF CHANGES\r\nWe summarize in Table 6 about how Photo Steward differs from CWPS.\r\nTable 6: Summary of changes: Civil War Photo Sleuth (CWPS) with and without Photo Steward.\r\n\r\nCWPS\r\nCWPS + Photo Steward\r\nDecision-Making: How do users make identification decisions for the query photo when they see the similar-looking search results retrieved by facial recognition?\r\nA single-step comparison interface that allows the user to compare the query photo with one similar-looking photo and the associated biographical information of the target ID. The single-step workflow conflates the two tasks of facial similarity and person identification.\r\nA multi-step validation interface that allows the user to compare the query photo with all previously identified photos of a similar-looking target ID (search result). The two-step workflow decouples the two tasks of facial similarity and person identification, thus allowing the user to deliberate on the facial similarity comparison between the query photo and the target photo(s) before making a decision on the target identity.\r\nUsers make a unary input on the query photo's ID by clicking an \"Identify\" button in the comparison interface, which will link the target ID to the query photo. The comparison interface does not allow users to rule out candidates.\r\nUsers can provide fine-grained decisions for both facial similarity and person identification steps in the validation interface. In Step 1, the user compares the query photo and the target photo for facial similarity by selecting from the following options: No (Different Person), Not Sure, Yes (Facial Match), and Yes (Replica). In Step 2, users indicate how confident they are about the query photo's ID (as the proposed target ID) by selecting the following options: No (Highly Confident), No (Slightly Confident), Not Sure, Yes (Slightly Confident), and Yes (Highly Confident).\r\nAccess: What kind of validation opportunities are available for users?\r\nUsers can only access the comparison interface from the search results page if they search for similar-looking candidates using facial recognition. Once the photo has been identified, users cannot access the comparison interface on the photo page. The community cannot weigh in on an ID's reliability on the photo page.\r\nUsers can access the validation interface on both the search results page (while identifying the query photo) and the photo page (after the photo has been identified). The community can weigh in on the reliability of the proposed ID(s) and facial matches using the validation interface on the photo page, thus engaging in a form of stigmergic collaboration.\r\nEvidence: What kind of stewardship evidence is presented to the users?\r\nThe photo page displays the proposed ID(s) for the photo without any additional information about the ID's reliability. Similarly, other photos that have been matched to the query photo are also displayed without any reliability indicator. Since community opinions are not captured on CWPS, they are not displayed.\r\nThe photo page displays the proposed ID(s) for the photo along with stewardship visualizations of the a) community's confidence on the ID, and b) facial similarity comparison with other photos of the same ID by both the community and AI.\r\nFOOTNOTE\r\n1 www.civilwarphotosleuth.com\r\n\r\n\r\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs International 4.0 License.\r\nCI '23, November 06–09, 2023, Delft, Netherlands\r\n© 2023 Copyright held by the owner/author(s).\r\nACM ISBN 979-8-4007-0113-9/23/11.\r\nDOI: https://doi.org/10.1145/3582269.3615600\n",
    "totalFiles": 2
  },
  {
    "id": "extraction_1754351834850_esrvfiw9h",
    "timestamp": "2025-08-04T23:57:14.850Z",
    "files": [
      {
        "name": "story.txt",
        "size": 2786,
        "type": "text/plain",
        "extractedText": "Ron: Great. I want to start with a detail about you that I’ve always appreciated, which is that you started out as a collector and someone who was deeply interested in the Civil War. Talk a little about how you became interested and got started.\r\n\r\nKurt: Sure thing. I am a bit of a Civil War collector. I grew up in a family of Civil War and history collectors and appreciators. I spent many childhood hours at antique shops and flea markets, and our home was full of interesting Civil War collectibles—books, artifacts, and all kinds of interesting stuff. That was my world as a kid. As I grew older, I realized that I could actually have my own Civil War collection. What was a little different about me is that I gravitated toward Civil War photographs—a category my family didn’t have too much of. I just felt drawn to the imagery of people from that era, their incredible stories, learning about the names behind the faces, and ultimately building my own collection of images while loving the process of researching those individuals.\r\n\r\nRon: One of my favorite stories is your family connection—how there’s a photograph connected to it all. I’d love to hear you tell that again.\r\n\r\nKurt: Absolutely. One of the reasons my family had an interest in the Civil War is that we have some Civil War ancestors. Three of them fought with the Union Army in Pennsylvania regiments, and I grew up learning about their adventures. We researched all we could about them through military records, but we didn’t have any images—we couldn’t see them or know what they looked like. I got really lucky during the 150th-anniversary commemorations of the Civil War. In Pittsburgh, where I grew up and where my Civil War ancestors lived, there was an exhibit at the Heinz History Center. I noticed a really cool photo album of Civil War soldiers. The label said that the company in the album was one that my ancestor had fought in. Out of thousands of possibilities, that single regiment appeared there. I asked the museum staff if they could have the owner, Ken Turner, look inside to see if my ancestor’s photo was in there. And sure enough, inside the album was a photograph of my Civil War ancestor, Corporal Oliver Croxton in uniform. I was looking into the eyes of my great-great-great grand uncle.\r\n\r\nRon: How did you feel when you saw that for the first time?\r\n\r\nKurt: It felt amazing. Here was someone I had thought about my whole life, trying to learn as much as I could about him. And once you start studying Civil War portraits, you learn that the odds of finding one specific photo of a soldier—especially a lower-ranked one—are really low. So having that opportunity, seeing his face, and getting a much clearer picture of who he was… it was really powerful.",
        "extractionStatus": "success"
      },
      {
        "name": "Kurt paper.txt",
        "size": 103748,
        "type": "text/plain",
        "extractedText": "Photo Steward: A Deliberative Collective Intelligence Workflow for Validating Historical Archives\r\nVikram Mohanty, Computer Science, Virginia Tech, USA, vikrammohanty@vt.edu\r\nKurt Luther, Computer Science, Virginia Tech, USA, kluther@vt.edu\r\n\r\nDOI: https://doi.org/10.1145/3582269.3615600\r\nCI '23: Collective Intelligence Conference, Delft, Netherlands, November 2023\r\nHistorical photographs of people generate significant cultural and economic value, but correctly identifying the subjects of photos can be a difficult task, requiring careful attention to detail while synthesizing large amounts of data from diverse sources. When photos are misidentified, the negative consequences can include financial losses and inaccuracies in the historical record, and even the spread of mis- and disinformation. To address this challenge, we introduce Photo Steward, an information stewardship architecture that leverages a deliberative workflow for validating historical photo IDs. We explored Photo Steward in the context of Civil War Photo Sleuth (CWPS), a popular online community dedicated to identifying photos from the American Civil War era (1861–65) using facial recognition and crowdsourcing. While the platform has been successful in identifying hundreds of unknown photographs, there have been concerns about unverified identifications and misidentifications. Our exploratory evaluation of Photo Steward on CWPS showed that its validation workflow encouraged users to deliberate while making photo ID decisions. Further, its stewardship visualizations helped users to assess photo ID information accurately, while fostering diverse forms of stigmergic collaboration.\r\nCCS Concepts: • Human-centered computing → Collaborative and social computing design and evaluation methods; • Human-centered computing → Interactive systems and tools;\r\n\r\nKeywords: crowdsourcing, human-AI interaction, online deliberation, community stewardship, information assessability, online communities, history, person identification, facial recognition, stigmergic collaboration\r\n\r\nACM Reference Format:\r\nVikram Mohanty and Kurt Luther. 2023. Photo Steward: A Deliberative Collective Intelligence Workflow for Validating Historical Archives. In Collective Intelligence Conference (CI '23), November 06--09, 2023, Delft, Netherlands. ACM, New York, NY, USA 19 Pages. https://doi.org/10.1145/3582269.3615600\r\n\r\nFigure 1: Workflow of Photo Steward. (1) Decision-Making: The user compares photos for facial similarity and make decisions on the photo ID using Photo Steward's deliberative validation interface. (2) Access: The user accesses the validation interface from Civil War Photo Sleuth's search results and photo page. (3) Evidence: The community's responses from the validation interface feed into stewardship visualizations that are visible on the Photo Page, which subsequently foster a form of stigmergic collaboration among the users.\r\n1 INTRODUCTION\r\nThe task of correctly identifying individuals in historical photos holds great cultural and economic importance [3, 19, 39, 61]. This identification process is analogous to solving a complex mystery. It often involves corroboration of multiple research processes such as investigating visual clues in a photo, finding relevant reference resources, and comparing multiple low-resolution reference photos [31, 32, 37]. Historical photos pose many challenges, including low-resolution images, scattered reference materials, limited domain expertise, and lack of suitable verification tools. These hurdles often result in misidentifications which can have negative consequences, ranging from distorting historical narratives [60] and fueling conspiracy theories [12] to spreading disinformation [16] and unwarranted financial gains from inaccurate representations [21]. As online platforms such as Ancestry.com, Find-a-Grave, and FamilySearch democratize historical and genealogical research, the risk of misidentification is further amplified due to factors such as inadequate experience, confirmation bias, and automation bias introduced by imperfect automated tools [44, 75].\r\nTo address these challenges, we introduce Photo Steward, a deliberative workflow that leverages collective intelligence to validate historical photo identifications (IDs). Photo Steward's architecture builds upon the concept of information stewardship [18, 72], which involves community-driven validation of content, as seen in online communities like iNaturalist and Wikipedia. We designed and applied the Photo Steward architecture to Civil War Photo Sleuth (CWPS),1 an AI-infused online platform for identifying historical photos. CWPS has over 20,000 registered users and over 25,000 identified Civil War portraits, and faces the problem of historical photo misidentification [43]. Photo Steward provides a validation workflow that promotes careful deliberation during facial similarity comparison and photo ID verification, while enabling users to share opinions. It also visualizes community opinions on the reliability of photo IDs and facial similarity, fostering information stewardship at multiple levels.\r\nWe publicly released Photo Steward on CWPS and conducted an exploratory evaluation of twelve months of usage, including interviews with users of different expertise levels and log analysis of stewardship behaviors on the platform. We found that Photo Steward's stewardship visualizations helped users to find additional evidence (e.g., external sources, uniform clues matching service records, etc.) for assessing the reliability of photo IDs. Users validated hundreds of different IDs on the platform, and found the workflow to be useful for deliberating on facial similarity comparison and fine-grained photo ID decisions.\r\nWe also discuss the implications of community participation, deliberative human-AI interaction, and assessable designs for historical photo identification.\r\n2 RELATED WORK\r\n2.1 Misinformation in a Historical Context\r\nNumerous online communities, forums, and websites have emerged in recent years for archiving and documenting history [57], generating family histories [74, 75], identifying and sharing historical photos [43], trading antiques [1, 8], and facilitating discussions around history [20]. Much like popular social media platforms such as Facebook and Twitter, these history-based platforms are also prone to the problem of misinformation, albeit in a more specialized historical research context.\r\nPrior work has shown that erroneous family history trees were being disseminated across Find-a-Grave and Ancestry, two popular genealogy research communities, as a result of the platform's low bar to entry and inexperienced contributors' over-reliance on inaccurate automated features [75]. Mohanty et al. [43] showed that despite successful identifications on Civil War Photo Sleuth (CWPS), several photos were misidentified in the first month, particularly for photos without period inscriptions or duplicate views (12 misidentifications out of 37). Multiple factors — the correct candidate not present in the search pool, or the user incorrectly assessing facial similarity [53] and picking the wrong match — can lead to incorrect IDs. A follow-up benchmarking study of the underlying face recognition algorithm [44] highlighted its low precision (i.e., it retrieves over hundreds of search results), raising the possibility of errors due to automation bias, i.e., the tendency of users to over-rely on automation for making a decision [47, 50, 64].\r\nIdentifying historical photos is a complex investigative process, often involving the corroboration of multiple evidence pieces [31, 33] and can be seen analogous to \"finding a needle in a haystack\". As a result, historical photo IDs run a high risk of getting misidentified even with the best of intentions. Multiple Civil War photos have also been misidentified in the collections of professionally managed museums and archives, such as the US Library of Congress [36] and the Abraham Lincoln Presidential Library [34]. At the same time, historical photo IDs also have the potential to generate significant monetary value [1, 3, 8, 10], and such financial incentives might also lead to falsified identifications [21]. Validating these historical photo IDs, which are a result of complex, subjective original research, becomes tricky without the lack of domain expertise and access to investigative tools.\r\nWe addressed these challenges in Photo Steward by designing a stewardship architecture that allows users to share their expertise with others. To address the impact of automation, we introduced a two-step validation workflow for the users to deliberate on decisions while interacting with the AI's recommendations.\r\n2.2 Data Validation in Online Communities\r\nMultiple online platforms have leveraged the strengths of crowdsourced contributions for validating the quality of data generated on those sites. Elliott discusses how stigmergic collaboration, where indirect coordination within a community stimulates subsequent actions, plays a role in maintaining articles on Wikipedia [13]. This concept was observed by Wiggins et al. in their study of iNaturalist, an online platform for identifying species, wherein community stewardship behaviors were seen as users agreed on organism identifications to influence the platform's quality grade status [72]. Prior work has shown that stewardship visualizations on Wikipedia (i.e., article quality) [18] and iNaturalist (i.e., ID research grade status) [72] have a positive impact on users' assessment of the information.\r\nAlong these lines, we also built DoubleCheck [42], a quality assessment framework that builds upon the concepts of provenance and stewardship for verifying historical photo IDs. DoubleCheck focused on displaying quality indicator badges for historical photo IDs by capturing accurate provenance information and combining the source trustworthiness information with community opinions on the ID. In this work, we focus solely on the underlying stewardship architecture that helped facilitate the community opinions. Both DoubleCheck and Photo Steward were evaluated in the same lab study, but there is no overlapping data.\r\nVisualizations displaying (surrogate) quality metrics, such as popularity among expert users, social reputation, and content coverage, have been effective in helping users assess the credibility of websites and search results [62]. Prior work has also shown that visualizing the history of edits for a Wikipedia article can have a significant impact on users' perceived trustworthiness of the article [54, 67]. Similarly, Chevalier et al. [7] showed that visualizing the number of contributors, length of the article and discussion, and the history of edits helped users assess the quality of Wikipedia articles faster. On the other hand, Towne et al. [68] found that being exposed to editor conflicts in the discussion of a Wikipedia article lowered the perception of the article's quality, even though the users reported that the transparency raised their perceptions of the page and Wikipedia in general. Morris et al. [45] found that Twitter users relied on the author information for making assessments about the credibility of information in a tweet.\r\nDrawing from this prior work, we designed Photo Steward's stewardship visualizations to highlight the role of collective intelligence, while fostering stigmergic collaboration on CWPS to validate the quality of photo identifications.\r\n2.3 Background: Civil War Photo Sleuthing\r\nThe American Civil War (1861–65) was one of the first major conflicts to be extensively photographed. Over 3 million soldiers fought in the war, with many of them having been photographed at least once. Over 150 years, many of these photos have survived in museums, libraries, and personal collections, but only 10–20% are are identified [69, 77]. Civil War photography has garnered a lot of interest among historians, collectors, dealers, genealogists, archivists, and other experts, who often try to identify unknown photos for personal, cultural, and economic reasons. However, the identification process is complex and challenging, which often involves identifying visual clues in a photo and manually scanning through hundreds of low-resolution photos, military records, and reference books for corroborating evidence [31, 33, 38].\r\n3 ENHANCING CIVIL WAR PHOTO SLEUTH: DESIGN OPPORTUNITIES\r\n\r\nFigure 2: CWPS Haystack Model: Person Identification Pipeline [43]\r\nCivil War Photo Sleuth (CWPS) is a free, public website where users can identify unknown portraits from the American Civil War era using a person identification pipeline that combines crowdsourced human expertise and face recognition [43]. Drawing analogies to finding a needle in a haystack, Mohanty et al. propose a 'haystack model' to describe CWPS's person identification pipeline. In this pipeline, a user begins the identification process by first tagging a photo for uniform clues, which then generates search filters based on service records, and then facial recognition returns facially similar-looking results from a pool of potential candidates, ordered by similarity to the query photo, that satisfy the search filters (see Figure 2).\r\nThe CWPS haystack model is designed to prevent misidentifications by placing human decision-making at the forefront and treating AI as a supportive tool. It avoids automatically selecting the best match or displaying the algorithm's inconsistent confidence levels [41, 44]. Instead, the user carefully inspects search results for potential matches based on facial similarity and corresponding biographical details. Once a photo is identified, CWPS links the face and identity together and displays the ID on the photo page.\r\nDespite these measures, the open participation model of CWPS, which lacks verification, has raised concerns about the trustworthiness of proposed identities and the potential increase of \"false positives\" as the site grows [22]. To address these concerns, we enumerate three design goals which draw upon prior work on Civil War photo identifications and CWPS system designs, evaluations, and critiques [22, 30, 33, 35, 41, 43, 44], as well as our own observations and experiences using the publicly available version of the website. We provide further details in Appendix A.\r\nDesign Goal 1: Decouple facial similarity comparison from the overall task of person identification.. The current CWPS workflow conflates facial similarity and person identification into a single decision-making process (see Figure 2-C). Facial similarity, while important, can conflict with the identity suggested by personal details like biographical information and service records. The facial recognition algorithm's low precision [44] adds to the complexity, with the possibility of users interacting with false positives. In order to discourage over-reliance on facial similarity, we propose this design goal of separating both these tasks, allowing users to deliberate on the facial similarity and other person identification attributes separately, thereby minimizing inaccuracies.\r\nDesign Goal 2: Support fine-grained, deliberative decision-making.. With the current CWPS interface only permitting binary feedback during photo identification, there is a heightened risk of misrepresentation and misidentifications (see Figure 2-C). Historical photo identification is intricate, often demanding careful corroboration of numerous evidence pieces, with user confidence varying according to the evidence at hand [33]. To more effectively facilitate this complex process, we advocate for a design that enables users to express their level of certainty in their identification decisions. This design goal seeks to foster more accurate and nuanced user feedback, as well as reflective and deliberate decision-making [28, 29].\r\nDesign Goal 3: Encourage community contribution and transparency for validating photo IDs.. Identifying individuals in photos can be considered a subjective process and often benefits from multiple perspectives [41]. Currently, CWPS lacks mechanisms for community feedback and transparency about the process of photo identification, leaving potential misidentifications unchecked (see Figure 8 in Appendix). To support accurate original historical research, the platform should encourage community participation in validating identifications and promoting transparency around the roles of community contribution and facial recognition [18, 72]. This approach also encourages collective responsibility, facilitating stigmergic collaboration [13, 14], where user contributions guide future validation efforts.\r\n4 SYSTEM DESCRIPTION: PHOTO STEWARD\r\nWe developed Photo Steward, an information stewardship architecture that integrates a deliberative workflow for the community to validate historical photo identifications, which we then integrated into CWPS. Photo Steward's architecture has three main components (see Figure 1): 1) a deliberative decision-making interface for facial similarity comparison and photo identification, 2) new access points for validating photo identities, and 3) stewardship evidence for fostering stigmergic collaborations.\r\n4.1 DECISION-MAKING: Deliberating on facial similarity and photo identification\r\nAs part of Photo Steward, we introduce a multi-step \"Validation Interface\" (see Figures 3 and 4) to replace CWPS's single-step comparison interface. Photo Steward's validation interface allows users to deliberate while interacting with the facial recognition results. Meeting Design Goal 1, the validation workflow separates the task of facial similarity comparison from the overall goal of identifying the photo.\r\nTo inform our design, we draw on evidence-based decision-making [11], a model primarily used in healthcare, policymaking, and judicial sectors, which advocates for justifying decisions (photo IDs in this case) by gathering available evidence (facial similarity as visual evidence here). In the first step, the user compares the query photo to all other photos with the same identity for facial similarity. After deliberating on the facial similarity evidence, the user then votes on whether the query photo fits the target identity in the second step (which is the user's primary goal).\r\nThe validation interface is divided into four columns (from left to right): 1) the task description, 2) the query photo, 3) the evidence that is being weighed, and 4) the biographical information. The query photo and evidence are positioned in the two middle panels for easy side-by-side comparison. The task description panel displays the rating question for both the facial similarity comparison and the identification steps. Here, we used structured feedback to capture both the user's facial similarity comparison and their confidence on the photo ID, in an effort for encouraging users to exercise personal deliberation on all available evidence before making a decision on the ID. The interface updates the task description and evidence column depending on which task the user is performing.\r\nTo investigate the identity of a query photo, the user opens the validation interface which loads all the photos and biographical information available for the target identity.\r\n4.1.1 Validation Step 1: Facial Similarity Comparison. For the first step of the validation process, the interface displays the target photo in the evidence column next to the query photo for easy facial similarity comparison (see Figure 3).\r\n\r\nFigure 3: Step 1 of Photo Steward's Validation Interface. Users can compare two photos and answer whether they show the same person or not. They have the option of selecting whether the two photos are a facial match (i.e., same person, different views) or a replica (i.e., same person, same view). Here, the user is comparing whether Photo 41929 and Photo 1 show the same person or not. If multiple faces are available for the same ID, they appear one after the other in the order in which they were uploaded to CWPS.\r\n\r\nThe user's task is to determine whether both photos show the same person (regardless of whether the identity is known). Users can select from the following options: No (Different Person), Not Sure, Yes (Facial Match) and Yes (Replica). Mohanty et al. found that photos correctly identified on CWPS were either facial matches (i.e., same person, different view) or replicas (i.e., same person, same view) [43], which informed the design of this input scale. Since facial similarity does not have any standard scale and users may perceive the similarity or dissimilarity of two faces differently [40, 76], we chose not to capture any further granularity in their responses for facial match, replica or different person as this might lead to inconsistent data collection.\r\nCapturing these responses in a structured way allows users to deliberate on the task of facial similarity; this becomes more critical as users are also interacting with the results of a low-precision facial recognition algorithm [44]. In this step, the user compares facial similarity of the query photo with all available photos of the target identity, one photo at a time.\r\n4.1.2 Validation Step 2: Fine-Grained Photo Identification. In this step, the user analyzes the biographical information and incorporates the facial similarity evidence from the previous step to make a decision on the photo's identity.\r\nThe validation interface displays information in the same four-column layout (see Figure 4), with the evidence column now displaying a summary of the user's responses about facial similarity between the query photo and the target photo(s). The biography column shows the name and the service records for the user to analyze.\r\n\r\nFigure 4: Step 2 of Photo Steward's Validation Interface. Users vote on whether the query photo can be identified as the target identity by expressing their confidence. They can also add an optional note to justify their decision. The evidence panel displays a summary of the user's responses from the first step, where the faces are displayed next to the user's facial similarity comparison with the query photo. The faces are ordered in the way they appear for comparison, i.e., the order in which they were uploaded to CWPS.\r\n\r\nThe user now decides whether the query photo can be identified as the target identity (see Figure 4). The instruction above nudges the user to factor in the prior photo comparison evidence and the biography information. Meeting Design Goal 2, users indicate their confidence about the task question by selecting one of the five options displayed in radio buttons: No (Highly Confident), No (Slightly Confident), Not Sure, Yes (Slightly Confident) and Yes (Highly Confident). This scale, which offers more nuance than a binary decision, serves as a proxy measure for the ID's reliability given the complexities of quantifying accuracy in a historical photo ID investigation. These options reflect the varying degrees of confidence users have based on the quality and quantity of corroborating evidence, such as reputable sources, facial similarity to additional photos, and expert opinions. Users also have the option to elaborate their decision rationale in a free-text note.\r\n4.2 ACCESS: Expanding Validation Opportunities for Photo Identifications\r\nPhoto Steward provides stewardship capabilities for the CWPS user community by allowing them to access and use the validation interface at different stages of the photo identification process from multiple gateways (see Figure 1). On the \"Search Results\" page, it can be used for identifying a photo from a pool of potential similar-looking candidates, or ruling out some potential candidates. After a photo has been identified, users can also access it on the \"Photo Page\" to either validate an existing ID or dispute an incorrect one, and collaborate with other users in a stigmergic manner (Design Goal 3).\r\n4.2.1 Search Results Page: Matching and Ruling Out Candidates. While identifying a query photo, users can now inspect potential matches on the search results page with the help of the validation interface. The \"Compare\" button on a search result brings up the validation interface, loading all the target information for the corresponding search result. The target identities in the validation interface will update as the users check new search candidates for matches. The interface allows users to make two types of decisions, depending on their confidence response: 1) either of the \"Yes\" responses will match the photo with the target identity with varying degrees of confidence, and 2) either the \"No\" or \"Not Sure\" responses will rule out the search candidate as a potential match for the current user.\r\n4.2.2 Photo Page: Validating and Disputing Existing IDs. After a photo has been identified, Photo Steward allows users to review opinions from other users (described in Section 4.3) and contribute their own for a given photo ID on CWPS's photo page, fulfilling Design Goal 3. By clicking the \"Give Your Opinion\" button, users launch the validation interface featuring the query photo, the linked target identity, and target photos of the same ID, a new feature previously absent from CWPS. The two-step process mirrors that on the search results page, enabling community deliberation on the validity of an ID. Users can validate the facial match among photos linked to the same ID, express their agreement or disagreement on an ID with varying confidence levels, and optionally add a note explaining their decision. Thus, each vote contributes to a stigmergic collaboration, enhancing the reliability of photo IDs on CWPS. Consistent with CWPS's open participation model, Photo Steward allows any registered user to share their opinion on an identification.\r\n4.3 EVIDENCE: Visualizing Information Stewardship\r\nAs part of Photo Steward, we designed stewardship visualizations to help users assess the reliability of 1) facial matches (i.e., photos that were matched to each other by the user), and 2) photo IDs. The CWPS community's opinions on facial similarity comparison and photo IDs, captured through the validation interface, feeds into these reliability visualizations. These visualizations not only promote user accountability through social translucence [15], but also serve as deliberative evidence for subsequent stigmergic user collaborations (Design Goal 3).\r\n4.3.1 Reliability of Facial Similarity. For each photo pair that has been compared, the system aggregates the community's decisions for the visual match type and generates a distribution, which is displayed in the form of an interactive horizontal bar chart on the photo page. This chart appears next to the corresponding photo matched to the query photo (see Figure 5). Users can click the \"View Details\" button or an individual bar to see how each user voted. When multiple photos have been matched to the query photo, the matched photos appear one below the other, with each having its own visualization next to it. The bar charts are stacked vertically above each other to allow users to easily see and compare the reliability of every match.\r\nTo complement the community stewardship visualization, we also added an AI stewardship badge that indicates whether the particular match is supported by facial recognition (see Figure 5). On the search results page, CWPS retrieves those search results that have a facial similarity score greater than 0.50, so we use the same threshold here. However, the badge intentionally does not display the exact similarity scores (which have been found to be inconsistent [41, 44]) to avoid a false perception of precision, and cautions users to carefully analyze all the context and evidence, as there is a possibility of false positives with face recognition.\r\n\r\nFigure 5: Facial Similarity Reliability Visualization on the Photo Page. The photo matched to the query photo is displayed here, whereas the query photo is displayed on top of the page. Other photos matched to the query photo are displayed vertically one below the other. Users can bring up the query photo and the matched photo side-by-side by clicking the \"Compare\" button. Each matched photo has its own visualization next to it, and the responses are visible to everyone.\r\n\r\n4.3.2 Reliability of Photo Identifications. Similar to the facial similarity visualization, the system aggregates the community's confidence levels for an identification and generates a distribution, displayed in the form of an interactive horizontal bar chart. This visualization is displayed for every proposed identity on the photo page, below the biography subsection (see Figure 6a). If users propose multiple IDs for a given photo, they are displayed one below the other; each ID will have its own visualizations. The community's confidence votes are aggregated to order these IDs. Users can click the \"View Details\" button or the individual bars to view a modal dialog box with each user's confidence scores and optional text comments (see Figure 6b).\r\n\r\nFigure 6: Photo ID Reliability Visualization on the Photo Page for a given ID. If multiple IDs are present, they are displayed vertically one below the other, ordered in terms of aggregate votes. Each ID will have a separate visualization listed under the respective IDs.\r\n\r\nThis visualization reflects the community's expertise, and is intended for users to quickly assess the reliability of an identification. Prior work on crowdsourced person identification [41] suggests that airtight identifications are likely to show consensus from the community, whereas potential misidentifications are likely to reflect disagreement from the community. Further, the comments may reflect the voters' decision rationale and any external research they conducted before giving their decision on the photo's identification, allowing users to build on the work of others in making their own assessment [17].\r\n4.4 Summary\r\nPhoto Steward augments the CWPS platform with an information stewardship architecture to support community validation of historical photo IDs in a stigmergic manner. We summarize all the changes in the Appendix (see Table 6).\r\n5 EVALUATION\r\nWe obtained permission to publicly launch Photo Steward on CWPS in December 2020. We conducted a mixed-methods, exploratory evaluation study to understand how well users with different expertise levels could validate and assess Civil War photo IDs using CWPS with Photo Steward. Specifically, we wanted to understand 1) how users validated photo IDs using Photo Steward, 2) how the stewardship visualizations (i.e., ID and facial similarity reliability visualizations) impacted users' assessment of an ID. The study was approved by our university's IRB.\r\n5.1 Log Analysis\r\nTo understand the community's stewardship behaviors, we analyzed website logs of all user activities for a year after new features were launched, which included 5843 voting instances on 5672 photos for 5355 unique IDs. Our analysis included categorization of user deliberations as 'pre-identified' (i.e., the user knew the ID of the photo ahead of time and therefore, might be looking for a specific target on the search results page) or 'post-identified' (i.e., the user did not know the ID of the photo and therefore, may not have a specific target while analyzing the search results), coding of user comments (see Appendix C), and comparison of community's facial similarity comparisons against facial recognition scores. Details of these analysis methods are provided in Appendix B.\r\n5.2 Lab Study\r\nIn order to understand how well Photo Steward supports diverse users in validating the quality of photo identifications, we also conducted an exploratory lab study.\r\n5.2.1 Participants. We recruited 15 participants representing the three major expertise levels: 5 history students, 5 amateur experts (experienced users of CWPS), and 5 expert historians. Participant details can be found in Appendix D. We anonymize these groups with the following identifiers, respectively: S1–S5, C1—C5, and H1–H5.\r\n5.2.2 Dataset. For the study, we created a dataset of 10 different photos identified on CWPS. Three of these photos had an ID conflict, i.e., multiple identities were proposed. For two of these photos, one ID was correct and the other one was incorrect. The community had already researched both photos, voted on the correct ID, and left credible evidence in the comments. Both IDs were linked to additional photos as well. The third photo was one of the seeded photos on CWPS, but was originally misidentified. We added another false ID, making both IDs for the third photo incorrect. All photos had multiple photos matched to them; eight of them were linked via facial matches, while two of them had replicas.\r\n5.2.3 Procedure. The entire study was conducted online via recorded Zoom sessions, with at least one researcher attending each session. Each participant first completed a consent form and a pre-survey describing their demographics and Civil War photography experience.\r\nAs part of the study, participants reviewed three randomly assigned photos from the dataset one-by-one in the original CWPS system first, followed by the same photos on the Photo Steward version. Participants used a think-aloud protocol while using the two systems; after the completion of the task with each system, they were asked a few semi-structured questions about their experience. Finally, the participants completed a summative post-survey of standard usability questions (e.g., ease of use, usefulness of features, instruction clarity, preferred system, etc.) (see Appendix E)\r\nWe maintained this sequence (original CWPS first, CWPS with Photo Steward second) for all the participants, rather than using a randomized sequence, for two reasons. First, we did not want participants' assessments to be biased in favor of Photo Steward after seeing additional features in the new interface. This design allowed us to observe if the original interface misled the participants towards incorrect assessments, and if, subsequently, the Photo Steward interface helped correct them. Second, in a randomized sequence, Photo Steward would expose the participants to new information in the form of prior user votes and responses, and therefore, may confound how they assess the information on the original CWPS version.\r\n5.2.4 Data Analysis. The first author fully transcribed and analyzed the interviews and think-aloud recordings using an inductive qualitative thematic approach [4]. The transcript sections were first divided according to the interface in question (i.e., original CWPS or Photo Steward), followed by an open coding of the transcripts using MAXQDA 2020 [65]. The first author iterated and settled on a total of 28 codes (e.g., change in opinions, comparison interface, source trustworthiness, etc.) for 634 coded segments across all the transcripts. These codes were then organized into themes as described in Section 6 after discussing with the co-author.\r\n5.2.5 Limitations. We conducted a qualitative lab study to understand how users with different backgrounds and expertises validated photo IDs using Photo Steward and hit theoretical saturation. However, there are a couple of limitations with the study: 1) limited insights on the role of expertise, and 2) the task sequence could have order effects. Further, the large-scale analysis of Photo Steward logs provided us with insights of its usage amongst users. However, it lacked an expert-prepared gold standard dataset, which hindered our ability to conduct specific performance analyses as part of this study.\r\n6 FINDINGS\r\nUsing the methods above, we evaluated how well Photo Steward's stewardship architecture supported CWPS users in validating photo identifications, compared to the original version of CWPS.\r\n6.1 Validation Interface\r\nUsers found Photo Steward's validation interface to be useful for comparing different photos.. While assessing the IDs with the original interface, participants would go back and forth between different photos to compare whether they are the same person or not. Some participants opened the photos in two different browser windows and kept them side-by-side. While using the validation interface in the new system, participants appreciated being able to see the photos side-by-side at the same time.\r\nH1 said, \"As an historian using this, this is really great to see them both together. It just makes a comparison a lot easy for me to do. I mean, this is the same gentleman, he's got a little dark facial hair. It looks a little bit different there and the photo on the right, but the facial match is definitely there. \" This was also echoed by C3, who said, \"This, I really find extremely useful, especially when I'm trying to do facial recognition. I can zoom in and have them side by side here. [...] Where in the past, I would have to go back and forth between tabs or cut and paste them into a different document to look at them side by side.\"\r\nFrom our logs, we found that 223 users had compared 2319 unique photo pairs for facial similarity, with 156 pairs receiving comparisons from at least 2 different users. The facial similarity responses were distributed as follows: 763 replicas, 1232 facial matches, 283 unsure, and 280 different people.\r\nTable 1: Distribution of User Votes.\r\n\r\nVotes\r\n/ ID\r\n# of IDs\r\nNote\r\nPresent\r\nNegative\r\nVotes\r\n1\r\n5650\r\n511\r\n61\r\n2\r\n157\r\n(Agreement: 119)\r\n(Disagreement: 38)\r\n83\r\n12\r\n3+\r\n36\r\n(Agreement: 21)\r\n(Disagreement: 15)\r\n31\r\n4\r\nTable 2: Distribution of Confidence Levels.\r\n\r\nMean\r\nConfidence\r\n1\r\nVote / ID\r\n2\r\nVotes / ID\r\n3+\r\nVotes / ID\r\n-2 (No - Highly Confident)\r\nto\r\n-1 (No - Slightly Confident)\r\n45\r\n3\r\n2\r\n-1 (No - Slightly Confident)\r\nto\r\n0 (Not Sure)\r\n16\r\n3\r\n2\r\n0 (Not Sure)\r\n99\r\n8\r\n4\r\n0 (Not Sure)\r\nto\r\n1 (Yes - Slightly Confident)\r\n760\r\n15\r\n5\r\n1 (Yes - Slightly Confident)\r\nto\r\n2 (Yes - Highly Confident)\r\n4730\r\n128\r\n23\r\nUsers preferred the ability to provide granular feedback for photo IDs using Photo Steward's validation interface.. All participants expressed preference for the fine-grained confidence levels, including the ability to dispute an ID, in Photo Steward's validation interface, appreciating how it more accurately mirrored the inherent uncertainty present when assessing photo IDs. S3 said, \"I definitely like the five levels. I think it leaves more room for interpretation. Like sometimes it's kind of hard to just say yes or a hard no because so much goes into it. Especially because a lot of this stuff was so long ago, there's so many unanswered questions.\" H4 initially defended the original interface's binary vote, but changed her mind after experiencing Photo Steward's confidence levels: \"[W]hat I had said has this very black and white feel to it, you're wrong or you're right. I like these degrees of disagreement or agreement. I think that's way more helpful broadly.\"\r\nThe usage of the voting feature was reflected in our logs, which showed 5843 voting instances from 328 unique users (see Table 1). Table 2 shows that while users utilized the full range of confidence levels, including when they were unsure (mean confidence = 0) or slightly confident (mean confidence = 0 to 1) about the ID, the vast majority of the votes were highly confident ones (mean confidence = 1 to 2). A small proportion of votes (77) were cast for disputing an ID.\r\nUsers justified their voting decisions through notes covering an extensive range of topics..\r\nTable 3: Distribution of Note Topics. The table also displays how the notes are distributed for different user confidence levels.\r\n\r\nCategory\r\nSub-Category\r\nTotal\r\nNumber of\r\nNotes\r\nNo\r\n(Highly\r\nConfident)\r\nNo\r\n(Slightly\r\nConfident)\r\nNot\r\nSure\r\nYes\r\n(Slightly\r\nConfident)\r\nYes\r\n(Highly\r\nConfident)\r\nPhoto\r\nComparison\r\nHigh-Level\r\nComparison\r\n206\r\n9\r\n5\r\n9\r\n20\r\n163\r\n\r\nDescribing\r\nFacial Features\r\n37\r\n4\r\n3\r\n9\r\n8\r\n13\r\nWord-of-Mouth\r\nDescendant\r\n55\r\n1\r\n0\r\n0\r\n8\r\n46\r\n\r\nOwnership\r\n47\r\n1\r\n0\r\n0\r\n0\r\n46\r\n\r\nFamiliarity\r\n18\r\n0\r\n0\r\n0\r\n2\r\n16\r\nVisual\r\nEvidence\r\nUniform\r\n73\r\n7\r\n1\r\n8\r\n17\r\n40\r\n\r\nInscription\r\n145\r\n4\r\n3\r\n3\r\n6\r\n129\r\n\r\nOther\r\nVisual Clues\r\n17\r\n1\r\n0\r\n1\r\n2\r\n13\r\nExternal\r\nInformation\r\nExternal URL\r\n47\r\n1\r\n0\r\n1\r\n4\r\n41\r\n\r\nOther Sources\r\n172\r\n9\r\n1\r\n1\r\n21\r\n140\r\n\r\nLack of\r\nInformation\r\n21\r\n0\r\n2\r\n13\r\n6\r\n0\r\nProviding\r\nAdditional\r\nInformation\r\nBiographical\r\nInformation\r\n46\r\n11\r\n2\r\n3\r\n3\r\n27\r\n\r\nAdditional\r\nContext\r\n117\r\n5\r\n0\r\n2\r\n11\r\n99\r\nFrom our logs, we found that 155 users had left 682 notes for 600 different photos. However, as Table 1 shows, around 10% of the votes had a note. Table 3 shows the different topics covered by the notes. We observe that users' voting patterns are significantly influenced by the availability and quality of evidence, with clear facial similarity, period inscriptions, personal anecdotes, visible clues in the photograph, and added biographical context often leading to high confidence \"Yes\" votes, while lack of information typically results in \"Not Sure\" votes.\r\nUsers most frequently left a comment attributing facial similarity (after comparing it in the first step of the validation process) to be the reason for their decision (e.g., \"Identical to the other CDV\"). In some instances, they would expound on it by discussing facial features: \"The eyes, nose, cheek bones, shape of face, all look similar to George Pickett, although possibly reversed based on hair part\". In many instances, we found users inferring biographical information (service records, location, etc.) from visual evidence in the photo, be it uniform or backmarks (e.g., \"Initials MN on chinstrap (brass letters). Signature on verso is made out to Marlin's oldest sister. Style of insignia is consistent with other 1862 recruits for Co. B 1st USSS\").\r\nInterestingly, we also observed a large number of word-of-mouth evidence notes for justifying the user's decisions, such as claiming to be a descendant, or owning the original copy of the photo, or having seen the photo somewhere. For example, one user noted, \"He is my great-great grandfather and this photo has been passed down through the generations to me and was identified by his son John Albert Johnson, my father's grandfather.\" Users also left external URLs and source details in the notes as evidence. Sometimes, they provided additional context (e.g., \"This image came with a group of 7th Iowa images. The majority were of Company G., but there is only one person in the entire 7th Iowa Infantry that could be identified by the first or last name of 'Nelson.'\").\r\nThe validation workflow encouraged users to exercise careful deliberation while making photo ID decisions.. Users felt that the questions in each step of the validation process helped them to carefully weigh in all the evidence and deliberate while voting on the ID (Q4, mean = 4.60, SD = 0.49). C3 explained why the two steps were necessary: \"It's two separate things. One is asking, do you think that this face is the same face? Then the second is, do you think that this face matches this name? I think that that is a necessary question for both of those scenarios. I don't think it's redundant, I think it's necessary.\" H2 appreciated the thoughtfulness that the two-step process encouraged, saying, \"It could be the same guy, but it might be a different guy, but now that you know the other interface kind of forces me to slow down a little bit and think more carefully, because it's asking specific questions about things.\" A couple of participants, however, expressed initial confusion over the separation and order of these two tasks.\r\n\r\nFigure 7: Deliberation in case of post-identification voting. The table shows the distribution of confidence votes (second step of the validation process) against the user's facial similarity comparison (first step of the validation process). The comparison scores were computed by aggregating all the facial similarity comparisons in the first step (replica = 2, facial match = 1, not sure = 0, and different people = -1).\r\n\r\nOur logs affirm the deliberative aspect of the two-step validation process. We observed that when users compared one or more photos in the first step, they factored in the facial similarity evidence for their ID vote decision. Figure 7 shows that when the cumulative comparison scores are positive (i.e., majority of the comparisons were a facial match or a replica), the confidence scores are generally positive. Similarly, when the cumulative comparison scores are negative, the confidence votes are also largely negative. When the users are unsure about the facial similarity comparison, it reflects in their final ID vote. Notably, when there were mismatches between comparison scores and ID votes, users provided reasons in the notes, citing visual evidence and an unsure photo comparison.\r\nPhoto Steward's stewardship architecture fostered diverse forms of meaningful stigmergic collaboration amongst users.. Although most IDs received only a single vote, about 3% (193 IDs) received multiple votes (see Table 1). Among these, 140 IDs saw total consensus among voters on confidence scores, with half involving an explanatory note. When the initial vote lacked a note, subsequent voters often added information like external sources or context. Almost all IDs where voters agreed positively were supported by metadata such as period inscriptions, scholarly sources, or additional comparison photos, underscoring the credibility of these agreements.\r\nWhen users disagreed on an ID, they would almost always justify in notes why they differed from the previous voters' opinions (43 out of 53 disagreement instances). From analyzing the notes, we found that the subsequent voters provided additional information about external sources or service records to disagree (e.g., \"Please see Military Images magazine, MI, Volume XVI, Number 3, November - December 1994 for the correct identification of this image. The correct identification via the Michael McAfee collection is Richard Cramer, 4th Michigan Infantry\"). Among 18 instances of conflicting IDs for the same photo, seven saw resolution through a process of voting one ID over another, often accompanied by external evidence in notes or citing facial similarity.\r\nUser agreement was particularly strong in facial similarity comparisons. Of the 2200 unique photo pairs compared, 141 received multiple user reviews, with 114 seeing total agreement. The remaining cases typically involved user errors in distinguishing between 'facial match' and 'replica', or uncertainty about facial similarity.\r\n6.2 Stewardship Visualizations\r\nStewardship visualizations reflecting community insights enhanced the accuracy of photo ID assessments.. Participants found Photo Steward's bar chart visualization showing the five confidence levels to be simple and easy to understand (Q1, mean = 4.93, SD = 0.25). S1 said, \"I'm a visual person. Bar graphs or charts like statistical data helps me put things into a better perspective or gives me an idea of what I am working with versus just something more plain [like] the other interface.\"\r\nWhen participants were uncertain about an ID, they saw the additional evidence and justification that the community members had provided along with their vote as essential to taking the vote seriously. S5 said, \"If there's other users giving comments like 'I've used this source,' and you know you get information from a source where they found it, I'm probably gonna agree with them. If they're just voting yes without anything else, then I am probably more likely to go out and find sources for the photo myself and make my own decision.\"\r\nIn three instances, students (S1, S3, and S5) initially made incorrect assessments on the original interface but rectified these after using Photo Steward. The community's consensus and evidence in Photo Steward were pivotal in these corrections. S5, confronted with ID conflicts, stated after using Photo Steward: \"You know that helps a lot with what people are thinking and presumably these people have also gone to the Maine State Archives or something and verified and then given their opinion so that helps.\" Similarly, historian H2 and collector C3 initially made incorrect choices but amended their decisions after analyzing community-based evidence on Photo Steward. H2 remarked: \"So I believe it's the first person, of course, because it's based on the Maine State Archives. And then you've got the piece on there that said that you looked and found a different man. With that name in the actual regimental history. I say that's fairly accurate information.\" This highlights the critical role of community consensus in resolving photo ID conflicts.\r\nOn the other hand, when participants were confident about an ID, seeing the community's opinions affirmed their own assessments, for better or worse. In the words of H5: \"I really liked the very clear community consensus, and the ability to be able to see the identities of the people who were looking at these images. It was something that (gave) a boost of confidence in terms of my final decision.\" Overall, the participants found the community opinions to be useful for assessing the IDs (Q2, mean = 4.93, SD = 0.25).\r\nUsers gave additional weight to the opinions of members they were familiar with and desired more contextual information about all contributors.. Participants examined the bar chart visualization details and gave higher weight to the opinions of prominent names from the Civil War photography community while assessing the ID. C3 said, \"Here's <name redacted>, period inscription with valediction, the uniform matches his service record. <name redacted> said the same thing. These two uploaders I hold in very high regard to their opinions on this site. If they're saying that they think highly confident of this identification, that gives me a lot of confidence as well.\" H5 became further unsure about an ID after seeing a fellow historian's vote: \"And, you know, sort of knowing <name redacted>, someone who I know deals with primary source material a lot, and sort of being on the fence with it as well, leaves me in that unsure position.\"\r\nParticipants also sought additional details about community members, proposing indicators of professional status or active participation in the CWPS community. H1 emphasized his appreciation of amateur experts and opposition to gatekeeping, yet he suggested having some kind of credential indicator next to the username would be helpful: \"It will be great if, you know, somebody was an academic historian or a published Civil War author, if there was some way to just say some little tidbit next to <name redacted> 'Oh, saying, hey, I'm from <university redacted>, Professor or, you know, author of whatever.'\" Others preferred indicators of community activity levels or personal connections (i.e., descendants) to the identified individual in the photo.\r\nOur log analysis showed revealed the presence of certain active voters, with 9 out of 328 unique voters voting on more than 50 photos each, and 2 voting on over 2000 photos each (mostly ones that they had uploaded and identified). If we consider only votes on photos identified by someone else, we have 8 users who have voted on more than 15 IDs each, with 1 user voting on over 150 IDs.\r\nThe community opinions made the platform feel more engaging, but users had mixed thoughts about the ideal number of votes per photo.. Participants, in general, felt the community opinions made Photo Steward more engaging compared to the original system. H5 said, \"I thought it appeared more user-centric and user-friendly and it felt more participatory.\" C3 saw the benefits of increased engagement for making IDs more reliable: \"I think for this crowdsourcing project that we're building on this database [it] is very important to have those comments, those feedback, that we see in the voting system. It only makes this ID stronger and makes the project and the database a more trustworthy and reliable source.\"\r\nUsers wanted to see more community opinions and comments for IDs that had few votes. S1, on seeing only one vote for an ID, said, \"That doesn't make me feel as confident because that's not too many for me to give an answer.\" S3 said it would have been easier to assess some photos if \"there was more input from other people.\" Users had mixed opinions about how many votes they wanted to see for verifying an ID. In general, they wanted to see consensus among the community for an identity and at least three votes. C3 said, \"I usually shoot for like three to four [votes] as the lowest where I take some good quality out of those votes. [...] If there's only one or two, and especially if there's two that are split, that is not as reliable to me.\"\r\nParticipants' concerns about spreading voters too thin were borne out in the log data (see Table 1). We analyzed the logs to check how often CWPS users vote on the photos they are browsing. We found 1784 instances (out of 5843 voting instances) where the number of \"lurkers\" for a given photo page exceeded the number of voters on that photo.\r\nTable 4: Face Recognition Similarity Score Stats vs. User Comparisons (Facial Similarity).\r\n\r\nUser Comparison\r\nMean\r\nMedian\r\nSD\r\nCount\r\nReplica\r\n0.88\r\n0.91\r\n0.16\r\n659\r\nFacial Match\r\n0.52\r\n0.65\r\n0.31\r\n1015\r\nDifferent Person\r\n0.55\r\n0.59\r\n0.16\r\n265\r\nNot Sure\r\n0.51\r\n0.58\r\n0.23\r\n261\r\nUsers found the face recognition badge and community's opinions complementary for assessing the reliability of matched photos..\r\nTable 5: User Comparisons (Facial Similarity) vs. Face Recognition.\r\n\r\n\r\n# of Photo Pairs\r\n# of Photo Pairs\r\ncompared by 1 user\r\n# of Photo Pairs\r\ncompared by 2 users\r\n# of Photo Pairs\r\ncompared by 2+ users\r\nSupported by users\r\nand face recognition\r\n1408\r\n1281\r\n109\r\n18\r\nDisputed by users\r\nand face recognition\r\n26\r\n24\r\n0\r\n1\r\nUsers unsure,\r\nface recognition disputes\r\n42\r\n42\r\n0\r\n0\r\nUsers unsure,\r\nface recognition supports\r\n219\r\n217\r\n2\r\n0\r\nUsers dispute,\r\nface recognition supports\r\n239\r\n238\r\n1\r\n0\r\nUsers support,\r\nface recognition disputes\r\n266\r\n257\r\n8\r\n1\r\nFrom our logs, we found that 1408 photo pair comparisons were supported by both users and facial recognition, by far the most common outcome (see Table 5). This information would be visible to the larger user community in the form of stewardship visualizations (see Figure 5). Interestingly, we also see 266 cases where a comparison is supported by users, but disputed by facial recognition (similarity confidence score < 0.50). That outcome was approximately as common as when users disputed a comparison but face recognition supported it (239 pairs) or when a user was unsure but face recognition supported it (219 pairs). However, it was far less common for facial recognition to dispute a comparison when a user also disputed it (26 pairs) or was unsure (42 pairs).\r\nWhen we analyze the face recognition's confidence scores in more detail, Table 4 shows that there is a clear separation between replicas and other types of user comparison scores. Face recognition confidence scores for photo pairs that users labeled as replicas were much higher in terms of both mean and median (0.88 and 0.91, respectively, versus scores in the 0.50s and 0.60s for all others). While these fine-grained scores are not displayed to users — they intentionally see only the face recognition badge — the very close mean and median confidence scores for \"facial match\" versus \"different person\" illustrate the difficulty of automatically identifying non-replica matches and offer support for a hybrid human-AI approach (cf. Section 4.3.1).\r\nParticipants found the community's opinions to be helpful for assessing whether two photos were facial matches or not. S4 said, \"I mean, I think it's cool to see what the community is saying, because I do feel when it comes to saying 'Is this the same person in both these pictures?', that's really the best way to do it if you don't have any [other] information.\" C1 said that he found the community opinions for facial matches especially helpful because he is \"face blind\": \"I can't really identify the face-to-face, but the hair and the mustache and all the stuff that, in addition, it helps me with that for sure. It's good because other than having to find somebody close to me and be like, 'Do you think this is the same people?' [I] have that community right there.\"\r\nParticipants had mixed opinions about facial recognition technology, but most found the badge indicating whether it supported the two photos being a match to be a useful data point. S3 said, \"The facial recognition saying they are similar — I would go ahead and trust that but I don't know if I would trust it enough to make a verification on my own.\" In general, participants found the strengths of facial recognition and the community to be complementary in determining whether two photos showed the same person, and liked seeing both results together. H4 felt the community and the technology had separate roles:\r\nThis is facial recognition, and this is the historical background. I trust facial recognition, but it makes me feel better to have that historical background. I think the human eye can be tricked by different hairstyles and different beards. Just to have this outside historical verification to say like, 'Okay, maybe you or I was tricked, but the machine was not,' I think that's really helpful.\r\nS1 relied on both the community and facial recognition to make a decision on a facial match: \"I think both give me kind of an idea. Okay, there's this facial recognition technology being used, but also there's other users that are leaning towards that this is the same person.\" H5 got a similar boost of confidence: \"This is certainly reassuring seeing not only the AI match, but also in terms of the community — seeing that seven users have said that this is a facial match as well. I'd be quite convinced by this.\"\r\n7 DISCUSSION\r\n7.1 Leveraging Collective Intelligence for Validating Person Identification\r\nPrior work has raised concerns about misinformation in online history communities [43, 75]. To address these problems on CWPS, we built Photo Steward for supporting community-based validation of photo IDs. Users found Photo Steward's stewardship visualizations not only helpful for affirming their own assessment, but also for discovering new knowledge and correcting their decisions, if need be. These visualizations, combined with the validation workflow, exhibit a form of stigmergic collaboration, where users build on prior knowledge left by the community and leave their own assessment for other users [14, 17, 27, 56].\r\nPhoto Steward allowed users to express how confident they are about an ID in a fine-grained manner, in contrast to the binary agreements or disagreements observed on iNaturalist by Wiggins et al. [72]. The CWPS community preferred this nuanced form of stewardship as users are likely to have different degrees of confidence based on the evidence available for identifying a photo, thus demonstrating the effectiveness of Design Goal 2. As S3 pointed out, users often experience difficulty in making a binary decision about individuals who lived 150 years back due to the lack of surviving documentation.\r\nBeyond its basic usefulness, Photo Steward's full potential can best be realized through sustained community participation, but most IDs on CWPS only received one vote. To address this challenge, we can leverage different crowdsourcing and online community strategies. For example, we can draw the community's attention towards IDs that are \"more of a puzzle,\" as H5 suggested, similar to Twitter's Birdwatch promoting tweets for fact-checking [52]. Designing nudges to encourage lurkers to vote on the IDs they are viewing can further help in these efforts. Organizing community events can help foster interest and participation in collaboratively verifying IDs, drawing inspiration from crowdsourcing events like CrowdSolve, where experts and novices collaborate on solving missing persons cold cases [70]. Incentive mechanisms such as leaderboards and challenges [46] can drive extrinsic motivation within the community for verifying the IDs. Finding users who are more likely to vote on an ID, based on their skills and interests, can also be an effective collaboration strategy [71]. In future work, we plan to integrate these strategies and introduce explicit \"calls to action\" [51, 58] on the home feed, guiding the community's attention towards IDs that require validation and fostering more sustainable, collaborative participation in historical photo identification.\r\n7.2 Exercising Deliberation in Human-AI Teams\r\nWe found that Photo Steward's validation workflow was effective not only for voting on the IDs and comparing the photos side-by-side, but also encouraged users to deliberate on their decision, drawing parallels to other social computing systems that support reflection and deliberation (e.g., [28, 29]). This deliberative intervention was non-trivial as users on CWPS follow an identification pipeline which is powered by facial recognition, an AI algorithm that is far from perfect [44]. Users are trying to find the correct match, if present at all, from a pool of potential candidates, which are largely comprised of similar-looking false positives — akin to finding a needle in a haystack. Further, the task of comparing photos of people is by no means an easy task for humans, even in a modern context [53]. While Photo Steward can not completely curb automation bias, an issue that has been previously observed in multiple online history communities [44, 75], its multi-step, validation workflow with structured feedback interventions encouraged users to deliberate over AI suggestions before making a decision.\r\nPhoto Steward's workflow also compartmentalizes the tasks that AI is good at – such as quickly retrieving similar-looking candidates from a large search pool – from the tasks where the AI makes more errors – such as verifying whether two faces show the same person or not [5, 55]. Decoupling facial similarity comparison from the person identification task (Design Goal 1) allows the users to now focus on the face verification task. In doing so, Photo Steward's workflow supports effective human-AI teaming in the context of person identification by allowing the user to make a granular assessment for the face verification task instead of the AI, while also ensuring that an AI-retrieved, similar-looking potential candidate is being compared against.\r\nAs imperfect AI algorithms get deployed in high-stakes scenarios such as medical imaging, law enforcement, etc. [6], it becomes more critical to reduce automation bias and encourage more deliberative decision-making. Amershi et al. recommend granular user feedback while interacting with AI systems as part of their \"Guidelines for Human-AI Interaction\" [2]. Similarly, other forms of design interventions, such as counterfactual AI explanations [63], chatbots [26], and community opinions [59] can also be explored for encouraging deliberative decision-making with AI assistance.\r\nPrior work in human-face recognition teams has shown that algorithmic suggestions can have a significant biasing effect on a user's decision [23]. Our findings showed that Photo Steward was able to encourage users to exercise deliberation while interacting with results retrieved by facial recognition. At the same time, Table 4 also showed that users can differ from the algorithm's suggestions, thus necessitating a deeper dive analysis of this dissonance as part of future work.\r\n7.3 Assessing Quality in Crowdsourced Original Historical Research\r\nWe found that Photo Steward's stewardship visualizations helped users assess the reliability of photo IDs on CWPS, which were a result of Design Goal 3. However, crowdsourced identifications always run the risk of groupthink [24, 25], which can eventually mislead users into believing and amplifying misidentifications, a concern also raised by H4. Public deliberation of modern photo IDs on social media can have profound negative consequences for false targets, as exemplified by the Boston Marathon bombing [49, 66] and the recent US Capitol riot [48]. This raises the question: are Photo Steward's stewardship visualizations sufficient for assessing the quality of photo identifications made on CWPS?\r\nPrior work on crowdsourced scholarship suggests an answer. Rosenzweig [57] analyzed Wikipedia as a source of historical scholarship, noting its policy against original research, and advocating for it as a tool for teaching the limitations of information sources and critical analysis of primary and secondary sources. Motivated along similar lines, Forte et al [18] proposed the assessability framework for designing assessable participatory information systems, based on information provenance and stewardship. The concept of provenance, extensively used in history and archival studies, describes information that makes it possible to trace the ownership or origins of the content, while stewardship refers to the processes that were used for maintaining the content, including its authorship. In the case of Wikipedia, Forte et al. found that visualizing provenance (i.e., citation types) and stewardship (i.e., article quality) had a significant impact on assessments of articles and Wikipedia as an information source.\r\nWhile Photo Steward enables information stewardship on CWPS, there is an opportunity for incorporating provenance into the CWPS platform to make it a truly assessable online platform. A significant proportion of the notes left by users on Photo Steward qualified as provenance information, namely comments about period inscriptions, family trees, external sources and URLs. The challenges of assessing IDs on CWPS are, however, different from assessing information on Wikipedia, primarily because CWPS supports original research unlike Wikipedia's no original research policy [73]. This was also the reason why we designed Photo Steward to be a review system rather than a single editable output such as Wikipedia; original research such as historical photo identifications is often times an evolving investigation rather than a final decision. To assess the reliability of original photo IDs made on CWPS, users may want to factor in the provenance of the reference photos that were used in the identification process. In such cases, Photo Steward's stewardship visualizations (i.e., facial similarity reliability) can further help the user in assessing whether the reference photos can be used as reliable provenance or not.\r\n8 CONCLUSION\r\nPhoto Steward attempts to help users assess and validate photo IDs better on CWPS. We present an information stewardship architecture, and adapt it for the task of historical person identification. We demonstrate the effectiveness of Photo Steward on CWPS, an existing online platform, where users found the stewardship visualizations, which included the community opinions and the AI verdict, useful for making accurate assessments of photo IDs on the platform. Further, users found Photo Sleuth's multi-step, structured validation workflow to help them deliberate before making decisions about the photo's identity. This work opens doors for exploring new ways to leverage collective intelligence and AI in creating assessable online information systems for historical archives.\r\nACKNOWLEDGMENTS\r\nWe wish to thank Ron Coddington, Paul Quigley, Liling Yuan, and our study participants. This research was supported by NSF IIS-1651969 and a Virginia Tech ICTAS Junior Faculty Award.\r\nREFERENCES\r\n2021. Heritage Auctions: World's Largest Collectibles Auctioneer. https://www.ha.com/ Navigate tocitation 1citation 2\r\nSaleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. 2019. Guidelines for human-AI interaction. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–13. Navigate tocitation 1\r\nBrakkton Booker. 2015. $2 photo found at Junk Store has Billy the kid in it, could be worth $5M. https://www.npr.org/sections/thetwo-way/2015/10/15/448993361/-2-photo-found-at-junk-store-has-billy-the-kid-in-it-could-be-worth-5-million Navigate tocitation 1citation 2\r\nVirginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3, 2 (2006), 77–101. Navigate tocitation 1\r\nJoy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Conference on Fairness, Accountability and Transparency, FAT 2018, 23-24 February 2018, New York, NY, USA. 77–91. http://proceedings.mlr.press/v81/buolamwini18a.html Navigate tocitation 1\r\nCarrie J Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg S Corrado, Martin C Stumpe, et al. 2019. Human-centered tools for coping with imperfect algorithms during medical decision-making. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–14. Navigate tocitation 1\r\nFanny Chevalier, Stéphane Huot, and Jean-Daniel Fekete. 2010. Wikipediaviz: Conveying article quality for casual wikipedia readers. In 2010 IEEE Pacific Visualization Symposium (PacificVis). IEEE, 49–56. Navigate tocitation 1\r\nH Jason Combs. 2005. The Internet's Impact on the Antiques Trade. Material Culture (2005), 26–41. Navigate tocitation 1citation 2\r\nAnthony DeBartolo. 1975. Appraisers can tell when old photo really may be golden. https://www.chicagotribune.com/news/ct-xpm-1985-12-27-8503300067-story.html Navigate tocitation 1\r\nAnthony DeBartolo. 2021. Appraisers can tell when old photo really may be golden. https://www.chicagotribune.com/news/ct-xpm-1985-12-27-8503300067-story.html Navigate tocitation 1\r\nHarley D Dickinson. 1998. Evidence-based decision-making: an argumentative approach. International Journal of Medical Informatics 51, 2-3 (1998), 71–81. Navigate tocitation 1\r\nFor The Inquirer Edward Colimore. 2019. Did John Wilkes Booth get away with murdering President Abraham Lincoln?https://www.inquirer.com/news/john-wilkes-booth-lincoln-conspiracy-photo-recognition-20190415.html Navigate tocitation 1\r\nMark Elliott. 2006. Stigmergic Collaboration: The Evolution of Group Work: Introduction. m/c journal 9, 2 (2006). Navigate tocitation 1citation 2citation 3\r\nMark Elliott. 2016. Stigmergic collaboration: A framework for understanding and designing mass collaboration. In Mass collaboration and education. Springer, 65–84. Navigate tocitation 1citation 2citation 3\r\nThomas Erickson and Wendy A Kellogg. 2000. Social translucence: an approach to designing systems that support social processes. ACM transactions on computer-human interaction (TOCHI) 7, 1 (2000), 59–83. Navigate tocitation 1\r\nDan Evon. 2020. Did Joe Biden's Great-Grandfather Own Slaves?https://www.snopes.com/fact-check/joe-biden-slaves-great-grandfather/ Navigate tocitation 1\r\nKristie Fisher, Scott Counts, and Aniket Kittur. 2012. Distributed sensemaking: improving sensemaking by leveraging the efforts of previous users. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 247–256. Navigate tocitation 1citation 2\r\nAndrea Forte, Nazanin Andalibi, Thomas Park, and Heather Willever-Farr. 2014. Designing information savvy societies: an introduction to assessability. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2471–2480. Navigate tocitation 1citation 2citation 3citation 4\r\nJacey Fortin. 2018. She Was the Only Woman in a Photo of 38 Scientists, and Now She's Been Identified. The New York Times (Mar 2018). https://www.nytimes.com/2018/03/19/us/twitter-mystery-photo.html Navigate tocitation 1\r\nSarah A Gilbert. 2020. \" I run the world's largest historical outreach project and it's on a cesspool of a website.\" Moderating a Public Scholarship Site on Reddit: A Case Study of r/AskHistorians. Proceedings of the ACM on Human-Computer Interaction 4, CSCW1 (2020), 1–27. Navigate tocitation 1\r\nJerome S Handler and Michael L Tuite. 2007. Retouching History: The Modern Falsification of a Civil War Photograph. Navigate tocitation 1citation 2\r\nM. Keith Harris. 2019. Civil War Photo Sleuth. Journal of American History 106, 2 (2019), 544–546. https://doi.org/10.1093/jahist/jaz498 Navigate tocitation 1citation 2\r\nJohn J Howard, Laura R Rabbitt, and Yevgeniy B Sirotin. 2020. Human-algorithm teaming in face recognition: How algorithm outcomes cognitively bias human decision-making. Plos one 15, 8 (2020), e0237855. Navigate tocitation 1\r\nPan Hui and Sonja Buchegger. 2009. Groupthink and peer pressure: Social influence in online social network groups. In 2009 International Conference on Advances in Social Network Analysis and Mining. IEEE, 53–59. Navigate tocitation 1\r\nNassim JafariNaimi and Eric M Meyers. 2015. Collective intelligence or group think? Engaging participation patterns in World Without Oil. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing. 1872–1881. Navigate tocitation 1\r\nSoomin Kim, Jinsu Eun, Joseph Seering, and Joonhwan Lee. 2021. Moderator chatbot for deliberative discussion: Effects of discussion structure and discussant facilitation. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–26. Navigate tocitation 1\r\nAniket Kittur, Andrew M Peters, Abdigani Diriye, and Michael Bove. 2014. Standing on the schemas of giants: socially augmented information foraging. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. 999–1010. Navigate tocitation 1\r\nTravis Kriplean, Caitlin Bonnar, Alan Borning, Bo Kinney, and Brian Gill. 2014. Integrating On-demand Fact-checking with Public Dialogue. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing(CSCW '14). ACM, New York, NY, USA, 1188–1199. https://doi.org/10.1145/2531602.2531677 Navigate tocitation 1citation 2citation 3\r\nTravis Kriplean, Michael Toomim, Jonathan Morgan, Alan Borning, and Andrew Ko. 2012. Is This What You Meant?: Promoting Listening on the Web with Reflect. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems(CHI '12). ACM, New York, NY, USA, 1559–1568. https://doi.org/10.1145/2207676.2208621 Navigate tocitation 1citation 2citation 3\r\nKurt Luther. 2016. How Fellow Collectors, Field Photos and Muttonchops Identified an Unknown Officer. Military Images 34, 1 (2016), 29–31. Navigate tocitation 1\r\nKurt Luther. 2017. Merrill Carbine Leads to a Soldier's Identification. Military Images 35, 2 (2017), 64–65. Navigate tocitation 1citation 2citation 3\r\nKurt Luther. 2018. Non-Traditional Research Tools—and Serendipity. Military Images 36, 3 (2018), 12–13. Navigate tocitation 1\r\nKurt Luther. 2018. What are the odds? Photo sleuthing by the numbers. Military Images 36, 1 (2018), 12–15. Navigate tocitation 1citation 2citation 3citation 4citation 5\r\nKurt Luther. 2019. What to Do When Gold Standards Go Wrong?Military Images 37, 1 (2019), 8–9. https://www.jstor.org/stable/26532101 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: How to Trust the Worthiness of an Identification. Military Images 38, 3 (213) (2020), 8–11. https://www.jstor.org/stable/26914966 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: Lost and Found in the Library of Congress. Military Images 38, 2 (212) (2020), 10–13. https://www.jstor.org/stable/26890126 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: The Art of Photo Sleuthing. Military Images 38, 4 (214) (2020), 8–11. https://www.jstor.org/stable/26925454 Navigate tocitation 1\r\nRamona Martinez. 2012. Photo mystery solved, then doubted, then deciphered, thanks to readers. https://www.npr.org/sections/pictureshow/2012/04/17/150801239/photo-mystery-solved-then-doubted-then-resolved-thanks-to-readers Navigate tocitation 1\r\nRamona Martinez. 2012. Unknown No More: Identifying A Civil War Soldier. http://www.npr.org/2012/04/11/150288978/unknown-no-more-identifying-a-civil-war-soldier Navigate tocitation 1\r\nChristian A Meissner and John C Brigham. 2001. Thirty years of investigating the own-race bias in memory for faces: A meta-analytic review.Psychology, Public Policy, and Law 7, 1 (2001), 3. Navigate tocitation 1\r\nVikram Mohanty, Kareem Abdol-Hamid, Courtney Ebersohl, and Kurt Luther. 2019. Second opinion: Supporting last-mile person identification with crowdsourcing and face recognition. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 86–96. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6\r\nVikram Mohanty and Kurt Luther. 2023. DoubleCheck: Designing Community-based Assessability for Historical Person Identification. ACM Journal on Computing and Cultural Heritage (JOCCH) (to appear) (2023). Navigate tocitation 1\r\nVikram Mohanty, David Thames, Sneha Mehta, and Kurt Luther. 2019. Photo sleuth: Combining human expertise and face recognition to identify historical portraits. In Proceedings of the 24th International Conference on Intelligent User Interfaces. 547–557. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6citation 7citation 8\r\nVikram Mohanty, David Thames, Sneha Mehta, and Kurt Luther. 2020. Photo Sleuth: Identifying Historical Portraits with Face Recognition and Crowdsourced Human Expertise. ACM Transactions on Interactive Intelligent Systems (TiiS) 10, 4 (2020), 1–36. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6citation 7citation 8citation 9citation 10\r\nMeredith Ringel Morris, Scott Counts, Asta Roseway, Aaron Hoff, and Julia Schwarz. 2012. Tweeting is believing? Understanding microblog credibility perceptions. In Proceedings of the ACM 2012 conference on computer supported cooperative work. 441–450. Navigate tocitation 1\r\nBenedikt Morschheuser, Juho Hamari, and Jonna Koivisto. 2016. Gamification in crowdsourcing: a review. In 2016 49th Hawaii International Conference on System Sciences (HICSS). IEEE, 4375–4384. Navigate tocitation 1\r\nKathleen L Mosier and Linda J Skitka. 1999. Automation use and automation bias. In Proceedings of the human factors and ergonomics society annual meeting, Vol. 43. SAGE Publications Sage CA: Los Angeles, CA, 344–348. Navigate tocitation 1citation 2\r\nGreg Myre. 2021. How Online Sleuths Identified Rioters At The Capitol. https://www.npr.org/2021/01/11/955513539/how-online-sleuths-identified-rioters-at-the-capitol Navigate tocitation 1\r\nJohnny Nhan, Laura Huey, and Ryan Broll. 2017. Digilantism: An analysis of crowdsourcing and the Boston marathon bombings. The British journal of criminology 57, 2 (2017), 341–361. Navigate tocitation 1\r\nRaja Parasuraman and Dietrich H Manzey. 2010. Complacency and bias in human use of automation: An attentional integration. Human factors 52, 3 (2010), 381–410. Navigate tocitation 1\r\nJunwon Park, Ranjay Krishna, Pranav Khadpe, Li Fei-Fei, and Michael Bernstein. 2019. AI-based request augmentation to increase crowdsourcing participation. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 115–124. Navigate tocitation 1\r\nSarah Perez. 2022. Twitter to show 'Birdwatch' community fact-checks to more users, following criticism. https://techcrunch.com/2022/03/03/twitter-to-show-birdwatch-community-fact-checks-to-more-users-following-criticism/ Navigate tocitation 1\r\nP Jonathon Phillips, Amy N Yates, Ying Hu, Carina A Hahn, Eilidh Noyes, Kelsey Jackson, Jacqueline G Cavazos, Géraldine Jeckeln, Rajeev Ranjan, Swami Sankaranarayanan, et al. 2018. Face recognition accuracy of forensic examiners, superrecognizers, and face recognition algorithms. Proceedings of the National Academy of Sciences 115, 24 (2018), 6171–6176. Navigate tocitation 1citation 2\r\nPeter Pirolli, Evelin Wollny, and Bongwon Suh. 2009. So you know you're getting the best possible information: a tool that increases Wikipedia credibility. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1505–1508. Navigate tocitation 1\r\nInioluwa Deborah Raji and Joy Buolamwini. 2019. Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products. Navigate tocitation 1\r\nAmira Rezgui and Kevin Crowston. 2018. Stigmergic coordination in Wikipedia. In Proceedings of the 14th International Symposium on Open Collaboration. 1–12. Navigate tocitation 1\r\nRoy Rosenzweig. 2006. Can History Be Open Source? Wikipedia and the Future of the Past. Journal of American History 93, 1 (June 2006), 117–146. Navigate tocitation 1citation 2\r\nSaiph Savage, Andres Monroy-Hernandez, and Tobias Höllerer. 2016. Botivist: Calling volunteers to action using online bots. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing. 813–822. Navigate tocitation 1\r\nMike Schaekermann, Joslin Goh, Kate Larson, and Edith Law. 2018. Resolvable vs. irresolvable disagreement: A study on worker deliberation in crowd work. Proceedings of the ACM on Human-Computer Interaction 2, CSCW (2018), 1–19. Navigate tocitation 1\r\nMichael S. Schmidt. 2018. 'Flags of Our Fathers' Author Now Doubts His Father Was in Iwo Jima Photo. The New York Times (Jan 2018). https://www.nytimes.com/2016/05/04/us/iwo-jima-marines-bradley.html Navigate tocitation 1\r\nJennifer Schuessler. 2017. Found: Oldest Known Photo of a U.S. President (Socks and All). https://www.nytimes.com/2017/08/16/arts/design/john-quincy-adams-daguerreotype-sothebys-auction.html Navigate tocitation 1\r\nJulia Schwarz and Meredith Morris. 2011. Augmenting web pages and search results to support credibility assessment. In Proceedings of the SIGCHI conference on human factors in computing systems. 1245–1254. Navigate tocitation 1\r\nRuoxi Shang, KJ Kevin Feng, and Chirag Shah. 2022. Why Am I Not Seeing It? Understanding Users' Needs for Counterfactual Explanations in Everyday Recommendations. In 2022 ACM Conference on Fairness, Accountability, and Transparency. 1330–1340. Navigate tocitation 1\r\nLinda J Skitka, Kathleen L Mosier, and Mark Burdick. 1999. Does automation bias decision-making?International Journal of Human-Computer Studies 51, 5 (1999), 991–1006. Navigate tocitation 1\r\nVerbi Software. 2019. MAXQDA 2020 [computer software]. VERBI Software. Available from maxqda.com. Navigate tocitation 1\r\nNPR Staff. 2016. How Social Media Smeared A Missing Student As A Terrorism Suspect. https://www.npr.org/sections/codeswitch/2016/04/18/474671097/how-social-media-smeared-a-missing-student-as-a-terrorism-suspect Navigate tocitation 1\r\nBongwon Suh, Ed H Chi, Aniket Kittur, and Bryan A Pendleton. 2008. Lifting the veil: improving accountability and social transparency in Wikipedia with wikidashboard. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1037–1040. Navigate tocitation 1\r\nW Ben Towne, Aniket Kittur, Peter Kinnaird, and James Herbsleb. 2013. Your process is showing: controversy management and perceived quality in Wikipedia. In Proceedings of the 2013 conference on Computer supported cooperative work. 1059–1068. Navigate tocitation 1\r\nCivil War Trust. 2021. Military Images Magazine | Interview with Ron Coddington. https://www.battlefields.org/learn/articles/military-images-magazine Navigate tocitation 1\r\nSukrit Venkatagiri, Aakash Gautam, and Kurt Luther. 2021. CrowdSolve: Managing Tensions in an Expert-Led Crowdsourced Investigation. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–30. Navigate tocitation 1\r\nShaun Wallace, Lucy Van Kleunen, Marianne Aubin-Le Quere, Abraham Peterkin, Yirui Huang, and Jeff Huang. 2017. Drafty: Enlisting Users To Be Editors Who Maintain Structured Data. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 5. Navigate tocitation 1\r\nAndrea Wiggins and Yurong He. 2016. Community-based data validation practices in citizen science. In Proceedings of the 19th ACM Conference on computer-supported cooperative work & social computing. 1548–1559. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6\r\nFoundation Wikimedia. 2022. No original research. https://en.wikipedia.org/wiki/Wikipedia:No_original_research Navigate tocitation 1\r\nHeather Willever-Farr, Lisl Zach, and Andrea Forte. 2012. Tell me about my family: A study of cooperative research on Ancestry. com. In Proceedings of the 2012 iConference. ACM, 303–310. Navigate tocitation 1\r\nHeather L Willever-Farr and Andrea Forte. 2014. Family matters: Control and conflict in online family history production. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. ACM, 475–486. Navigate tocitation 1citation 2citation 3citation 4citation 5\r\nJeremy B Wilmer. 2017. Individual differences in face recognition: A decade of discovery. Current Directions in Psychological Science 26, 3 (2017), 225–230. Navigate tocitation 1\r\nBob Zeller. 2022. Searching for photos of Civil War Soldiers | David Wynn Vaughan. https://www.civilwarphotography.org/searching-for-photos-of-civil-war-soldiers/ Navigate tocitation 1\r\nA CIVIL WAR PHOTO SLEUTH: DESIGN CHALLENGES AND OPPORTUNITIES\r\n \r\nHere, we provide details of three key challenges posed by Civil War Photo Sleuth that might have contributed towards misidentification and subsequently, inaccurate assessments, and how they present design opportunities for Photo Steward.\r\nA.1 Conflating facial similarity with photo identification\r\nA.1.1 Challenges: Mohanty et al. conducted a benchmarking study of CWPS [44], and found the face recognition algorithm to be of low precision; i.e., it retrieved hundreds of search results which may look similar to the query photo but are actually different people (false positives). Low precision increases the chances that users will interact with a lot of false positives. In such cases, one may need to garner additional information (i.e., comparing biographical information) before making a decision. However, it is plausible that automation bias may play a role in non-expert users making a match solely based on facial similarity [47]. As a result, there is strong potential for misidentification (see Figure 8).\r\nA.1.2 Current Workflow: CWPS's compare interface allows users to closely inspect the search results for a potential match, but does not make any distinction between facial similarity comparison and photo identification. Both are conflated into a one-step process, with one \"Identify\" button for the users to make their decisions (see Figure 2-C). Yet, users may want to indicate agreement with just the facial similarity (i.e., query photo and the search result showing the same person) but not the identity (i.e., name and biographical information), or vice versa.\r\nA.1.3 Design Goal 1: To support accurate investigation of photo identifications, users should be able to deliberate on the different aspects of the decision-making process. Providing users with a decision-making workflow that decouples facial similarity comparison from the overall photo identification task would allow them to focus on these tasks separately, while discouraging them from making decisions solely on the basis of facial similarity.\r\n\r\nFigure 8: An example of a misidentified photo on CWPS. This photo was identified as John C Whiteside based on facial similarity with the top search result: an identified reference photo of John C Whiteside. However, other visual clues, such as the photographer studio, do not align with Whiteside's biographical information.\r\n\r\nA.2 Lacking support for fine-grained, deliberative decision making\r\nA.2.1 Challenges: Historical photo identification is a complex task, where experts often corroborate multiple pieces of evidence, including facial similarity comparison, before reaching a decision about the identity of the photo [33]. While confirming an identity, experts may be highly confident if the source, military records, uniform clues, and additional photos of the same person all line up, or slightly confident if they need additional evidence. Conflicting evidence pieces may also affect their confidence levels. Similarly, they may have different degrees of certainty while ruling out an identity for a photo. A lack of support for expressing and displaying granularity in these photo identification decisions can lead to varying degrees of uncertainty being captured and misinterpreted as a confirmation, and eventually propagating misidentifications. Further, while it is safe to assume the vast majority of the Civil War photography community care about the accuracy of the photo IDs, there exists a small risk of financially-driven misidentifications since certain types of identified photos are considered to be more valuable [9].\r\nA.2.2 Current Workflow: The compare interface's \"Identify\" button (see Figure 2-C) is the only mechanism available on the platform for users to provide (a unary) input on photo identifications. CWPS users currently cannot provide fine-grained feedback on a given photo's identity, either at the time of identifying or afterwards on the photo page.\r\nA.2.3 Design Goal 2 : Users should be able to express how certain or uncertain they are about a photo identification. Interventions for allowing users to provide structured feedback can not only facilitate more accurate, fine-grained responses, but also encourage deliberation on the available evidence before making a decision, borrowing inspiration from other social computing systems that support users reflecting and deliberating on available information [28, 29].\r\nA.3 Limited validation opportunities for the community\r\nA.3.1 Challenges: For humans, deciding whether two photos show the same person is a highly subjective experience. A study by Mohanty et al. [41] showed that participants in a crowdsourcing study often disagree with each other and with facial recognition results in person identification tasks. The same study showed that people often seek a second opinion from peers for validating photo identifications. Without validation, original crowdsourced historical research may result in unresolved cases of conflicting IDs proposed for the same face, incorrect IDs remaining unchecked, or airtight IDs not being confirmed.\r\nA.3.2 Current Workflow: Once a photo has been identified, CWPS does not offer other users the option to express their opinions on whether two photos show the same person or not, or if the photo has been identified correctly or not (see Figure 8). The photo page does not indicate if (or why) the photos are linked to each other, how they are similar, or which user(s) considered them to be similar. Further, it does not indicate the role of facial recognition in linking them together.\r\nA.3.3 Design Goal 3 : To support accurate original historical research, the platform should encourage information stewardship from the community by allowing members to give their opinions on existing photo identifications [72]. Further, the platform should also be transparent about the role of community stewardship and facial recognition for a given photo identification, which can then act as evidence for aiding subsequent validation efforts by other users, thus supporting a form of stigmergic collaboration [13, 14].\r\nB DETAILS FOR LOG ANALYSIS\r\n \r\nTo understand the community's stewardship behaviors, we examined website logs for all user activities for one year since we launched the new features (December 2020 – December 2021). During this period, we observed 5843 voting instances on 5672 photos for 5355 unique IDs, where a user voting on whether Photo N can be identified as a Person M or not is considered to be one voting instance. For a given voting instance, we analyzed 1) any associated facial similarity comparisons to understand how they deliberated on their final voting decision, and 2) the user's confidence vote plus any justification notes left by the user to understand their decision rationale. Of the 5672 query photos, 4297 photos (4377 voting instances) did not have any facial similarity comparisons — only the user's confidence on the ID (plus any notes) was captured.\r\nFor the remaining voting instances which had at least one associated facial similarity comparison (i.e., users deliberate through the two-step validation process), we broke them down into pre-identified (i.e., the user knew the ID of the photo ahead of time and therefore, might be looking for a specific target on the search results page) and post-identified (i.e., the user did not know the ID of the photo and therefore, may not have a specific target while analyzing the search results) cases. As mentioned earlier in Section 4.2, users had the opportunity to validate IDs either on the search results page on the photo page. While a pre-identified voting instance almost certainly originated from the search results page, a post-identified case could be from either page. Since the logs did not give us the page origin of each vote directly, we triangulated from CWPS's timestamps to determine whether a given voting instance was for a pre-identified case or a post-identified one.\r\nAfter collating all the associated facial similarity comparisons for a given voting instance, we had 1064 pre-identified and 576 post-identified user deliberations to analyze. Each deliberation instance is a user's attempt to identify a query photo as a given target (person) ID, where they first compare facial similarity with all other photos that have been identified as the target ID, followed by the user's confidence on the query photo being the target ID. We analyzed the user responses to see whether the facial similarity comparisons had any impact on the user's confidence.\r\nUsers had provided comments in 682 (out of 5843, or 11.7%) voting instances. We coded these comments using an iterative, inductive approach, which resulted in five high-level themes, which can broken down into 13 sub-categories (see Appendix C).\r\nTo understand any stigmergic collaboration processes at play, we also analyzed the IDs which had multiple votes to check for agreements and disagreements between the voters. We further analyzed how the community's facial similarity comparisons compare against the facial recognition scores.\r\nC THEMES FOR NOTES ANALYSIS\r\nPhoto Comparisons\r\nHigh-Level Comparison: The note mentions \"replica\", \"facial similarity\", \"facial match\", \"identical\", \"visual comparison\", and other similar terms that describe comparisons with a prior identified photo.\r\nDescribing facial features: The note mention facial features like \"eyes\", \"hairline\", \"ears\", etc. to make comparisons\r\nWord-of-Mouth\r\nDescendant: The note either mentions that the user is a descendant of the person being identified, or they got the information from the family of the person.\r\nOwnership: The note either mentions that the user owns a printed version of the photo, compared with a photo in their collection, or they know the owner of the photo.\r\nFamiliarity/Self-Reported Research: The note mentions that the user has seen the photo somewhere, be it in a book, museum, etc.\r\nVisual Clues\r\nUniform: The note mentions visual clues that pertain to the uniform of the person (e.g., hat insignia, shoulder straps, etc.) The user may infer the possible service information (i.e., ranks, branches, regiments, etc.) from the uniform clues.\r\nInscription: The note mentions the presence of a period inscription on the photo (a highly trustworthy primary source for a person's ID), or an album case, or modern inscriptions such as books, which is generally the name of the person being identified. In some instances, the inscribed text may point to the person's service information.\r\nOther Visual Clues: The note mentions visual clues in the photo (e.g., backmarks, borders, etc.) beyond the person's face.\r\nExternal Information\r\nExternal URL: The note mentions an external URL, which supposedly has additional information about the photo's ID.\r\nOther sources (e.g., museum, website, book, etc.): The note mentions an external source (e.g., museum, book, etc.) that supposedly has evidence for the photo's ID, but no URLs are provided. Details about the source may or may not be available.\r\nLack of Information / Seeking Additional Evidence: The note mentions the lack of evidence or seeking additional evidence, be it about the source or the service information.\r\nProviding Additional Information\r\nBiographical Information: The notes mentions additional information about the person's service records, specific regiments, biographical information (name, year, location), etc.\r\nAdditional Context: The note mentions some additional context provided by the user to justify their decision, such as information about the photo collection, or pointing to someone else's research, or some historical context, or incorrect evidence, or if the person is prominent.\r\nD PARTICIPANT DETAILS\r\nUndergraduate and master's students concentrating in history who use Civil War photos for their coursework and research projects, but are not (yet) employed in a professional capacity as historians. We recruited five students via recommendations from our university's history department. None of the students had used CWPS before, or were known to the authors prior to the study. Three students were men and two were women, and all were in the \"18 to 30\" age group. We anonymize them with identifiers S1–S5.\r\nExperienced users of Civil War Photo Sleuth who have added over 50 photos each and have substantial knowledge of Civil War history, but are not professional historians. We recruited five amateur experts from the CWPS contact list. All five users were men, and they were distributed across different age groups (two in \"18 to 30\", two in \"31 to 40\", and one in \"51 to 60\"). We anonymize them with identifiers C1–C5. C1 and C3 are among the most active daily users on CWPS. Only two of the five had used Photo Steward before.\r\nExpert historians with a graduate degree in history, specializing in American Civil War history, but with little or no previous experience with CWPS. We recruited five historians via recommendations from our university's history department. Three historians were men and two were women. They were distributed across different age groups (two in \"18 to 30\", two in \"31 to 40\", and one in \"51 to 60\"). We anonymize them with identifiers H1–H5. None of them had used Photo Steward before.\r\nE LAB STUDY QUESTIONS\r\nE.1 Semi-Structured Questions\r\nIs there a way that you would like to capture your thoughts on this ID and share them with others, if possible? If so, what would that look like? If not, can you explain why not?\r\nWhat did you think about the community opinions?\r\nWhat do you think about the ID quality visualization?\r\nWhat did you think about the 2-step process while agreeing/disagreeing on an identity?\r\nWhat is your overall opinion of both the interfaces?\r\nWhich interface would you prefer for validating the information? And why?\r\nWhat would you change or improve?\r\nE.2 Usability Survey\r\nQ1. The community's opinions about an identity were clear and easy to understand in the 2nd system. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ2. The community's opinions about an identity were useful for assessing the information. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ3. The process of voting on an identity was clear and easy to understand. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ4. Comparing other photos first and then voting on an identity helped me deliberate and make more accurate decisions. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ5. I was able to validate the information better using the 2nd system. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nF SUMMARY OF CHANGES\r\nWe summarize in Table 6 about how Photo Steward differs from CWPS.\r\nTable 6: Summary of changes: Civil War Photo Sleuth (CWPS) with and without Photo Steward.\r\n\r\nCWPS\r\nCWPS + Photo Steward\r\nDecision-Making: How do users make identification decisions for the query photo when they see the similar-looking search results retrieved by facial recognition?\r\nA single-step comparison interface that allows the user to compare the query photo with one similar-looking photo and the associated biographical information of the target ID. The single-step workflow conflates the two tasks of facial similarity and person identification.\r\nA multi-step validation interface that allows the user to compare the query photo with all previously identified photos of a similar-looking target ID (search result). The two-step workflow decouples the two tasks of facial similarity and person identification, thus allowing the user to deliberate on the facial similarity comparison between the query photo and the target photo(s) before making a decision on the target identity.\r\nUsers make a unary input on the query photo's ID by clicking an \"Identify\" button in the comparison interface, which will link the target ID to the query photo. The comparison interface does not allow users to rule out candidates.\r\nUsers can provide fine-grained decisions for both facial similarity and person identification steps in the validation interface. In Step 1, the user compares the query photo and the target photo for facial similarity by selecting from the following options: No (Different Person), Not Sure, Yes (Facial Match), and Yes (Replica). In Step 2, users indicate how confident they are about the query photo's ID (as the proposed target ID) by selecting the following options: No (Highly Confident), No (Slightly Confident), Not Sure, Yes (Slightly Confident), and Yes (Highly Confident).\r\nAccess: What kind of validation opportunities are available for users?\r\nUsers can only access the comparison interface from the search results page if they search for similar-looking candidates using facial recognition. Once the photo has been identified, users cannot access the comparison interface on the photo page. The community cannot weigh in on an ID's reliability on the photo page.\r\nUsers can access the validation interface on both the search results page (while identifying the query photo) and the photo page (after the photo has been identified). The community can weigh in on the reliability of the proposed ID(s) and facial matches using the validation interface on the photo page, thus engaging in a form of stigmergic collaboration.\r\nEvidence: What kind of stewardship evidence is presented to the users?\r\nThe photo page displays the proposed ID(s) for the photo without any additional information about the ID's reliability. Similarly, other photos that have been matched to the query photo are also displayed without any reliability indicator. Since community opinions are not captured on CWPS, they are not displayed.\r\nThe photo page displays the proposed ID(s) for the photo along with stewardship visualizations of the a) community's confidence on the ID, and b) facial similarity comparison with other photos of the same ID by both the community and AI.\r\nFOOTNOTE\r\n1 www.civilwarphotosleuth.com\r\n\r\n\r\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs International 4.0 License.\r\nCI '23, November 06–09, 2023, Delft, Netherlands\r\n© 2023 Copyright held by the owner/author(s).\r\nACM ISBN 979-8-4007-0113-9/23/11.\r\nDOI: https://doi.org/10.1145/3582269.3615600",
        "extractionStatus": "success"
      }
    ],
    "combinedText": "=== story.txt ===\nRon: Great. I want to start with a detail about you that I’ve always appreciated, which is that you started out as a collector and someone who was deeply interested in the Civil War. Talk a little about how you became interested and got started.\r\n\r\nKurt: Sure thing. I am a bit of a Civil War collector. I grew up in a family of Civil War and history collectors and appreciators. I spent many childhood hours at antique shops and flea markets, and our home was full of interesting Civil War collectibles—books, artifacts, and all kinds of interesting stuff. That was my world as a kid. As I grew older, I realized that I could actually have my own Civil War collection. What was a little different about me is that I gravitated toward Civil War photographs—a category my family didn’t have too much of. I just felt drawn to the imagery of people from that era, their incredible stories, learning about the names behind the faces, and ultimately building my own collection of images while loving the process of researching those individuals.\r\n\r\nRon: One of my favorite stories is your family connection—how there’s a photograph connected to it all. I’d love to hear you tell that again.\r\n\r\nKurt: Absolutely. One of the reasons my family had an interest in the Civil War is that we have some Civil War ancestors. Three of them fought with the Union Army in Pennsylvania regiments, and I grew up learning about their adventures. We researched all we could about them through military records, but we didn’t have any images—we couldn’t see them or know what they looked like. I got really lucky during the 150th-anniversary commemorations of the Civil War. In Pittsburgh, where I grew up and where my Civil War ancestors lived, there was an exhibit at the Heinz History Center. I noticed a really cool photo album of Civil War soldiers. The label said that the company in the album was one that my ancestor had fought in. Out of thousands of possibilities, that single regiment appeared there. I asked the museum staff if they could have the owner, Ken Turner, look inside to see if my ancestor’s photo was in there. And sure enough, inside the album was a photograph of my Civil War ancestor, Corporal Oliver Croxton in uniform. I was looking into the eyes of my great-great-great grand uncle.\r\n\r\nRon: How did you feel when you saw that for the first time?\r\n\r\nKurt: It felt amazing. Here was someone I had thought about my whole life, trying to learn as much as I could about him. And once you start studying Civil War portraits, you learn that the odds of finding one specific photo of a soldier—especially a lower-ranked one—are really low. So having that opportunity, seeing his face, and getting a much clearer picture of who he was… it was really powerful.\n\n==================================================\n\n=== Kurt paper.txt ===\nPhoto Steward: A Deliberative Collective Intelligence Workflow for Validating Historical Archives\r\nVikram Mohanty, Computer Science, Virginia Tech, USA, vikrammohanty@vt.edu\r\nKurt Luther, Computer Science, Virginia Tech, USA, kluther@vt.edu\r\n\r\nDOI: https://doi.org/10.1145/3582269.3615600\r\nCI '23: Collective Intelligence Conference, Delft, Netherlands, November 2023\r\nHistorical photographs of people generate significant cultural and economic value, but correctly identifying the subjects of photos can be a difficult task, requiring careful attention to detail while synthesizing large amounts of data from diverse sources. When photos are misidentified, the negative consequences can include financial losses and inaccuracies in the historical record, and even the spread of mis- and disinformation. To address this challenge, we introduce Photo Steward, an information stewardship architecture that leverages a deliberative workflow for validating historical photo IDs. We explored Photo Steward in the context of Civil War Photo Sleuth (CWPS), a popular online community dedicated to identifying photos from the American Civil War era (1861–65) using facial recognition and crowdsourcing. While the platform has been successful in identifying hundreds of unknown photographs, there have been concerns about unverified identifications and misidentifications. Our exploratory evaluation of Photo Steward on CWPS showed that its validation workflow encouraged users to deliberate while making photo ID decisions. Further, its stewardship visualizations helped users to assess photo ID information accurately, while fostering diverse forms of stigmergic collaboration.\r\nCCS Concepts: • Human-centered computing → Collaborative and social computing design and evaluation methods; • Human-centered computing → Interactive systems and tools;\r\n\r\nKeywords: crowdsourcing, human-AI interaction, online deliberation, community stewardship, information assessability, online communities, history, person identification, facial recognition, stigmergic collaboration\r\n\r\nACM Reference Format:\r\nVikram Mohanty and Kurt Luther. 2023. Photo Steward: A Deliberative Collective Intelligence Workflow for Validating Historical Archives. In Collective Intelligence Conference (CI '23), November 06--09, 2023, Delft, Netherlands. ACM, New York, NY, USA 19 Pages. https://doi.org/10.1145/3582269.3615600\r\n\r\nFigure 1: Workflow of Photo Steward. (1) Decision-Making: The user compares photos for facial similarity and make decisions on the photo ID using Photo Steward's deliberative validation interface. (2) Access: The user accesses the validation interface from Civil War Photo Sleuth's search results and photo page. (3) Evidence: The community's responses from the validation interface feed into stewardship visualizations that are visible on the Photo Page, which subsequently foster a form of stigmergic collaboration among the users.\r\n1 INTRODUCTION\r\nThe task of correctly identifying individuals in historical photos holds great cultural and economic importance [3, 19, 39, 61]. This identification process is analogous to solving a complex mystery. It often involves corroboration of multiple research processes such as investigating visual clues in a photo, finding relevant reference resources, and comparing multiple low-resolution reference photos [31, 32, 37]. Historical photos pose many challenges, including low-resolution images, scattered reference materials, limited domain expertise, and lack of suitable verification tools. These hurdles often result in misidentifications which can have negative consequences, ranging from distorting historical narratives [60] and fueling conspiracy theories [12] to spreading disinformation [16] and unwarranted financial gains from inaccurate representations [21]. As online platforms such as Ancestry.com, Find-a-Grave, and FamilySearch democratize historical and genealogical research, the risk of misidentification is further amplified due to factors such as inadequate experience, confirmation bias, and automation bias introduced by imperfect automated tools [44, 75].\r\nTo address these challenges, we introduce Photo Steward, a deliberative workflow that leverages collective intelligence to validate historical photo identifications (IDs). Photo Steward's architecture builds upon the concept of information stewardship [18, 72], which involves community-driven validation of content, as seen in online communities like iNaturalist and Wikipedia. We designed and applied the Photo Steward architecture to Civil War Photo Sleuth (CWPS),1 an AI-infused online platform for identifying historical photos. CWPS has over 20,000 registered users and over 25,000 identified Civil War portraits, and faces the problem of historical photo misidentification [43]. Photo Steward provides a validation workflow that promotes careful deliberation during facial similarity comparison and photo ID verification, while enabling users to share opinions. It also visualizes community opinions on the reliability of photo IDs and facial similarity, fostering information stewardship at multiple levels.\r\nWe publicly released Photo Steward on CWPS and conducted an exploratory evaluation of twelve months of usage, including interviews with users of different expertise levels and log analysis of stewardship behaviors on the platform. We found that Photo Steward's stewardship visualizations helped users to find additional evidence (e.g., external sources, uniform clues matching service records, etc.) for assessing the reliability of photo IDs. Users validated hundreds of different IDs on the platform, and found the workflow to be useful for deliberating on facial similarity comparison and fine-grained photo ID decisions.\r\nWe also discuss the implications of community participation, deliberative human-AI interaction, and assessable designs for historical photo identification.\r\n2 RELATED WORK\r\n2.1 Misinformation in a Historical Context\r\nNumerous online communities, forums, and websites have emerged in recent years for archiving and documenting history [57], generating family histories [74, 75], identifying and sharing historical photos [43], trading antiques [1, 8], and facilitating discussions around history [20]. Much like popular social media platforms such as Facebook and Twitter, these history-based platforms are also prone to the problem of misinformation, albeit in a more specialized historical research context.\r\nPrior work has shown that erroneous family history trees were being disseminated across Find-a-Grave and Ancestry, two popular genealogy research communities, as a result of the platform's low bar to entry and inexperienced contributors' over-reliance on inaccurate automated features [75]. Mohanty et al. [43] showed that despite successful identifications on Civil War Photo Sleuth (CWPS), several photos were misidentified in the first month, particularly for photos without period inscriptions or duplicate views (12 misidentifications out of 37). Multiple factors — the correct candidate not present in the search pool, or the user incorrectly assessing facial similarity [53] and picking the wrong match — can lead to incorrect IDs. A follow-up benchmarking study of the underlying face recognition algorithm [44] highlighted its low precision (i.e., it retrieves over hundreds of search results), raising the possibility of errors due to automation bias, i.e., the tendency of users to over-rely on automation for making a decision [47, 50, 64].\r\nIdentifying historical photos is a complex investigative process, often involving the corroboration of multiple evidence pieces [31, 33] and can be seen analogous to \"finding a needle in a haystack\". As a result, historical photo IDs run a high risk of getting misidentified even with the best of intentions. Multiple Civil War photos have also been misidentified in the collections of professionally managed museums and archives, such as the US Library of Congress [36] and the Abraham Lincoln Presidential Library [34]. At the same time, historical photo IDs also have the potential to generate significant monetary value [1, 3, 8, 10], and such financial incentives might also lead to falsified identifications [21]. Validating these historical photo IDs, which are a result of complex, subjective original research, becomes tricky without the lack of domain expertise and access to investigative tools.\r\nWe addressed these challenges in Photo Steward by designing a stewardship architecture that allows users to share their expertise with others. To address the impact of automation, we introduced a two-step validation workflow for the users to deliberate on decisions while interacting with the AI's recommendations.\r\n2.2 Data Validation in Online Communities\r\nMultiple online platforms have leveraged the strengths of crowdsourced contributions for validating the quality of data generated on those sites. Elliott discusses how stigmergic collaboration, where indirect coordination within a community stimulates subsequent actions, plays a role in maintaining articles on Wikipedia [13]. This concept was observed by Wiggins et al. in their study of iNaturalist, an online platform for identifying species, wherein community stewardship behaviors were seen as users agreed on organism identifications to influence the platform's quality grade status [72]. Prior work has shown that stewardship visualizations on Wikipedia (i.e., article quality) [18] and iNaturalist (i.e., ID research grade status) [72] have a positive impact on users' assessment of the information.\r\nAlong these lines, we also built DoubleCheck [42], a quality assessment framework that builds upon the concepts of provenance and stewardship for verifying historical photo IDs. DoubleCheck focused on displaying quality indicator badges for historical photo IDs by capturing accurate provenance information and combining the source trustworthiness information with community opinions on the ID. In this work, we focus solely on the underlying stewardship architecture that helped facilitate the community opinions. Both DoubleCheck and Photo Steward were evaluated in the same lab study, but there is no overlapping data.\r\nVisualizations displaying (surrogate) quality metrics, such as popularity among expert users, social reputation, and content coverage, have been effective in helping users assess the credibility of websites and search results [62]. Prior work has also shown that visualizing the history of edits for a Wikipedia article can have a significant impact on users' perceived trustworthiness of the article [54, 67]. Similarly, Chevalier et al. [7] showed that visualizing the number of contributors, length of the article and discussion, and the history of edits helped users assess the quality of Wikipedia articles faster. On the other hand, Towne et al. [68] found that being exposed to editor conflicts in the discussion of a Wikipedia article lowered the perception of the article's quality, even though the users reported that the transparency raised their perceptions of the page and Wikipedia in general. Morris et al. [45] found that Twitter users relied on the author information for making assessments about the credibility of information in a tweet.\r\nDrawing from this prior work, we designed Photo Steward's stewardship visualizations to highlight the role of collective intelligence, while fostering stigmergic collaboration on CWPS to validate the quality of photo identifications.\r\n2.3 Background: Civil War Photo Sleuthing\r\nThe American Civil War (1861–65) was one of the first major conflicts to be extensively photographed. Over 3 million soldiers fought in the war, with many of them having been photographed at least once. Over 150 years, many of these photos have survived in museums, libraries, and personal collections, but only 10–20% are are identified [69, 77]. Civil War photography has garnered a lot of interest among historians, collectors, dealers, genealogists, archivists, and other experts, who often try to identify unknown photos for personal, cultural, and economic reasons. However, the identification process is complex and challenging, which often involves identifying visual clues in a photo and manually scanning through hundreds of low-resolution photos, military records, and reference books for corroborating evidence [31, 33, 38].\r\n3 ENHANCING CIVIL WAR PHOTO SLEUTH: DESIGN OPPORTUNITIES\r\n\r\nFigure 2: CWPS Haystack Model: Person Identification Pipeline [43]\r\nCivil War Photo Sleuth (CWPS) is a free, public website where users can identify unknown portraits from the American Civil War era using a person identification pipeline that combines crowdsourced human expertise and face recognition [43]. Drawing analogies to finding a needle in a haystack, Mohanty et al. propose a 'haystack model' to describe CWPS's person identification pipeline. In this pipeline, a user begins the identification process by first tagging a photo for uniform clues, which then generates search filters based on service records, and then facial recognition returns facially similar-looking results from a pool of potential candidates, ordered by similarity to the query photo, that satisfy the search filters (see Figure 2).\r\nThe CWPS haystack model is designed to prevent misidentifications by placing human decision-making at the forefront and treating AI as a supportive tool. It avoids automatically selecting the best match or displaying the algorithm's inconsistent confidence levels [41, 44]. Instead, the user carefully inspects search results for potential matches based on facial similarity and corresponding biographical details. Once a photo is identified, CWPS links the face and identity together and displays the ID on the photo page.\r\nDespite these measures, the open participation model of CWPS, which lacks verification, has raised concerns about the trustworthiness of proposed identities and the potential increase of \"false positives\" as the site grows [22]. To address these concerns, we enumerate three design goals which draw upon prior work on Civil War photo identifications and CWPS system designs, evaluations, and critiques [22, 30, 33, 35, 41, 43, 44], as well as our own observations and experiences using the publicly available version of the website. We provide further details in Appendix A.\r\nDesign Goal 1: Decouple facial similarity comparison from the overall task of person identification.. The current CWPS workflow conflates facial similarity and person identification into a single decision-making process (see Figure 2-C). Facial similarity, while important, can conflict with the identity suggested by personal details like biographical information and service records. The facial recognition algorithm's low precision [44] adds to the complexity, with the possibility of users interacting with false positives. In order to discourage over-reliance on facial similarity, we propose this design goal of separating both these tasks, allowing users to deliberate on the facial similarity and other person identification attributes separately, thereby minimizing inaccuracies.\r\nDesign Goal 2: Support fine-grained, deliberative decision-making.. With the current CWPS interface only permitting binary feedback during photo identification, there is a heightened risk of misrepresentation and misidentifications (see Figure 2-C). Historical photo identification is intricate, often demanding careful corroboration of numerous evidence pieces, with user confidence varying according to the evidence at hand [33]. To more effectively facilitate this complex process, we advocate for a design that enables users to express their level of certainty in their identification decisions. This design goal seeks to foster more accurate and nuanced user feedback, as well as reflective and deliberate decision-making [28, 29].\r\nDesign Goal 3: Encourage community contribution and transparency for validating photo IDs.. Identifying individuals in photos can be considered a subjective process and often benefits from multiple perspectives [41]. Currently, CWPS lacks mechanisms for community feedback and transparency about the process of photo identification, leaving potential misidentifications unchecked (see Figure 8 in Appendix). To support accurate original historical research, the platform should encourage community participation in validating identifications and promoting transparency around the roles of community contribution and facial recognition [18, 72]. This approach also encourages collective responsibility, facilitating stigmergic collaboration [13, 14], where user contributions guide future validation efforts.\r\n4 SYSTEM DESCRIPTION: PHOTO STEWARD\r\nWe developed Photo Steward, an information stewardship architecture that integrates a deliberative workflow for the community to validate historical photo identifications, which we then integrated into CWPS. Photo Steward's architecture has three main components (see Figure 1): 1) a deliberative decision-making interface for facial similarity comparison and photo identification, 2) new access points for validating photo identities, and 3) stewardship evidence for fostering stigmergic collaborations.\r\n4.1 DECISION-MAKING: Deliberating on facial similarity and photo identification\r\nAs part of Photo Steward, we introduce a multi-step \"Validation Interface\" (see Figures 3 and 4) to replace CWPS's single-step comparison interface. Photo Steward's validation interface allows users to deliberate while interacting with the facial recognition results. Meeting Design Goal 1, the validation workflow separates the task of facial similarity comparison from the overall goal of identifying the photo.\r\nTo inform our design, we draw on evidence-based decision-making [11], a model primarily used in healthcare, policymaking, and judicial sectors, which advocates for justifying decisions (photo IDs in this case) by gathering available evidence (facial similarity as visual evidence here). In the first step, the user compares the query photo to all other photos with the same identity for facial similarity. After deliberating on the facial similarity evidence, the user then votes on whether the query photo fits the target identity in the second step (which is the user's primary goal).\r\nThe validation interface is divided into four columns (from left to right): 1) the task description, 2) the query photo, 3) the evidence that is being weighed, and 4) the biographical information. The query photo and evidence are positioned in the two middle panels for easy side-by-side comparison. The task description panel displays the rating question for both the facial similarity comparison and the identification steps. Here, we used structured feedback to capture both the user's facial similarity comparison and their confidence on the photo ID, in an effort for encouraging users to exercise personal deliberation on all available evidence before making a decision on the ID. The interface updates the task description and evidence column depending on which task the user is performing.\r\nTo investigate the identity of a query photo, the user opens the validation interface which loads all the photos and biographical information available for the target identity.\r\n4.1.1 Validation Step 1: Facial Similarity Comparison. For the first step of the validation process, the interface displays the target photo in the evidence column next to the query photo for easy facial similarity comparison (see Figure 3).\r\n\r\nFigure 3: Step 1 of Photo Steward's Validation Interface. Users can compare two photos and answer whether they show the same person or not. They have the option of selecting whether the two photos are a facial match (i.e., same person, different views) or a replica (i.e., same person, same view). Here, the user is comparing whether Photo 41929 and Photo 1 show the same person or not. If multiple faces are available for the same ID, they appear one after the other in the order in which they were uploaded to CWPS.\r\n\r\nThe user's task is to determine whether both photos show the same person (regardless of whether the identity is known). Users can select from the following options: No (Different Person), Not Sure, Yes (Facial Match) and Yes (Replica). Mohanty et al. found that photos correctly identified on CWPS were either facial matches (i.e., same person, different view) or replicas (i.e., same person, same view) [43], which informed the design of this input scale. Since facial similarity does not have any standard scale and users may perceive the similarity or dissimilarity of two faces differently [40, 76], we chose not to capture any further granularity in their responses for facial match, replica or different person as this might lead to inconsistent data collection.\r\nCapturing these responses in a structured way allows users to deliberate on the task of facial similarity; this becomes more critical as users are also interacting with the results of a low-precision facial recognition algorithm [44]. In this step, the user compares facial similarity of the query photo with all available photos of the target identity, one photo at a time.\r\n4.1.2 Validation Step 2: Fine-Grained Photo Identification. In this step, the user analyzes the biographical information and incorporates the facial similarity evidence from the previous step to make a decision on the photo's identity.\r\nThe validation interface displays information in the same four-column layout (see Figure 4), with the evidence column now displaying a summary of the user's responses about facial similarity between the query photo and the target photo(s). The biography column shows the name and the service records for the user to analyze.\r\n\r\nFigure 4: Step 2 of Photo Steward's Validation Interface. Users vote on whether the query photo can be identified as the target identity by expressing their confidence. They can also add an optional note to justify their decision. The evidence panel displays a summary of the user's responses from the first step, where the faces are displayed next to the user's facial similarity comparison with the query photo. The faces are ordered in the way they appear for comparison, i.e., the order in which they were uploaded to CWPS.\r\n\r\nThe user now decides whether the query photo can be identified as the target identity (see Figure 4). The instruction above nudges the user to factor in the prior photo comparison evidence and the biography information. Meeting Design Goal 2, users indicate their confidence about the task question by selecting one of the five options displayed in radio buttons: No (Highly Confident), No (Slightly Confident), Not Sure, Yes (Slightly Confident) and Yes (Highly Confident). This scale, which offers more nuance than a binary decision, serves as a proxy measure for the ID's reliability given the complexities of quantifying accuracy in a historical photo ID investigation. These options reflect the varying degrees of confidence users have based on the quality and quantity of corroborating evidence, such as reputable sources, facial similarity to additional photos, and expert opinions. Users also have the option to elaborate their decision rationale in a free-text note.\r\n4.2 ACCESS: Expanding Validation Opportunities for Photo Identifications\r\nPhoto Steward provides stewardship capabilities for the CWPS user community by allowing them to access and use the validation interface at different stages of the photo identification process from multiple gateways (see Figure 1). On the \"Search Results\" page, it can be used for identifying a photo from a pool of potential similar-looking candidates, or ruling out some potential candidates. After a photo has been identified, users can also access it on the \"Photo Page\" to either validate an existing ID or dispute an incorrect one, and collaborate with other users in a stigmergic manner (Design Goal 3).\r\n4.2.1 Search Results Page: Matching and Ruling Out Candidates. While identifying a query photo, users can now inspect potential matches on the search results page with the help of the validation interface. The \"Compare\" button on a search result brings up the validation interface, loading all the target information for the corresponding search result. The target identities in the validation interface will update as the users check new search candidates for matches. The interface allows users to make two types of decisions, depending on their confidence response: 1) either of the \"Yes\" responses will match the photo with the target identity with varying degrees of confidence, and 2) either the \"No\" or \"Not Sure\" responses will rule out the search candidate as a potential match for the current user.\r\n4.2.2 Photo Page: Validating and Disputing Existing IDs. After a photo has been identified, Photo Steward allows users to review opinions from other users (described in Section 4.3) and contribute their own for a given photo ID on CWPS's photo page, fulfilling Design Goal 3. By clicking the \"Give Your Opinion\" button, users launch the validation interface featuring the query photo, the linked target identity, and target photos of the same ID, a new feature previously absent from CWPS. The two-step process mirrors that on the search results page, enabling community deliberation on the validity of an ID. Users can validate the facial match among photos linked to the same ID, express their agreement or disagreement on an ID with varying confidence levels, and optionally add a note explaining their decision. Thus, each vote contributes to a stigmergic collaboration, enhancing the reliability of photo IDs on CWPS. Consistent with CWPS's open participation model, Photo Steward allows any registered user to share their opinion on an identification.\r\n4.3 EVIDENCE: Visualizing Information Stewardship\r\nAs part of Photo Steward, we designed stewardship visualizations to help users assess the reliability of 1) facial matches (i.e., photos that were matched to each other by the user), and 2) photo IDs. The CWPS community's opinions on facial similarity comparison and photo IDs, captured through the validation interface, feeds into these reliability visualizations. These visualizations not only promote user accountability through social translucence [15], but also serve as deliberative evidence for subsequent stigmergic user collaborations (Design Goal 3).\r\n4.3.1 Reliability of Facial Similarity. For each photo pair that has been compared, the system aggregates the community's decisions for the visual match type and generates a distribution, which is displayed in the form of an interactive horizontal bar chart on the photo page. This chart appears next to the corresponding photo matched to the query photo (see Figure 5). Users can click the \"View Details\" button or an individual bar to see how each user voted. When multiple photos have been matched to the query photo, the matched photos appear one below the other, with each having its own visualization next to it. The bar charts are stacked vertically above each other to allow users to easily see and compare the reliability of every match.\r\nTo complement the community stewardship visualization, we also added an AI stewardship badge that indicates whether the particular match is supported by facial recognition (see Figure 5). On the search results page, CWPS retrieves those search results that have a facial similarity score greater than 0.50, so we use the same threshold here. However, the badge intentionally does not display the exact similarity scores (which have been found to be inconsistent [41, 44]) to avoid a false perception of precision, and cautions users to carefully analyze all the context and evidence, as there is a possibility of false positives with face recognition.\r\n\r\nFigure 5: Facial Similarity Reliability Visualization on the Photo Page. The photo matched to the query photo is displayed here, whereas the query photo is displayed on top of the page. Other photos matched to the query photo are displayed vertically one below the other. Users can bring up the query photo and the matched photo side-by-side by clicking the \"Compare\" button. Each matched photo has its own visualization next to it, and the responses are visible to everyone.\r\n\r\n4.3.2 Reliability of Photo Identifications. Similar to the facial similarity visualization, the system aggregates the community's confidence levels for an identification and generates a distribution, displayed in the form of an interactive horizontal bar chart. This visualization is displayed for every proposed identity on the photo page, below the biography subsection (see Figure 6a). If users propose multiple IDs for a given photo, they are displayed one below the other; each ID will have its own visualizations. The community's confidence votes are aggregated to order these IDs. Users can click the \"View Details\" button or the individual bars to view a modal dialog box with each user's confidence scores and optional text comments (see Figure 6b).\r\n\r\nFigure 6: Photo ID Reliability Visualization on the Photo Page for a given ID. If multiple IDs are present, they are displayed vertically one below the other, ordered in terms of aggregate votes. Each ID will have a separate visualization listed under the respective IDs.\r\n\r\nThis visualization reflects the community's expertise, and is intended for users to quickly assess the reliability of an identification. Prior work on crowdsourced person identification [41] suggests that airtight identifications are likely to show consensus from the community, whereas potential misidentifications are likely to reflect disagreement from the community. Further, the comments may reflect the voters' decision rationale and any external research they conducted before giving their decision on the photo's identification, allowing users to build on the work of others in making their own assessment [17].\r\n4.4 Summary\r\nPhoto Steward augments the CWPS platform with an information stewardship architecture to support community validation of historical photo IDs in a stigmergic manner. We summarize all the changes in the Appendix (see Table 6).\r\n5 EVALUATION\r\nWe obtained permission to publicly launch Photo Steward on CWPS in December 2020. We conducted a mixed-methods, exploratory evaluation study to understand how well users with different expertise levels could validate and assess Civil War photo IDs using CWPS with Photo Steward. Specifically, we wanted to understand 1) how users validated photo IDs using Photo Steward, 2) how the stewardship visualizations (i.e., ID and facial similarity reliability visualizations) impacted users' assessment of an ID. The study was approved by our university's IRB.\r\n5.1 Log Analysis\r\nTo understand the community's stewardship behaviors, we analyzed website logs of all user activities for a year after new features were launched, which included 5843 voting instances on 5672 photos for 5355 unique IDs. Our analysis included categorization of user deliberations as 'pre-identified' (i.e., the user knew the ID of the photo ahead of time and therefore, might be looking for a specific target on the search results page) or 'post-identified' (i.e., the user did not know the ID of the photo and therefore, may not have a specific target while analyzing the search results), coding of user comments (see Appendix C), and comparison of community's facial similarity comparisons against facial recognition scores. Details of these analysis methods are provided in Appendix B.\r\n5.2 Lab Study\r\nIn order to understand how well Photo Steward supports diverse users in validating the quality of photo identifications, we also conducted an exploratory lab study.\r\n5.2.1 Participants. We recruited 15 participants representing the three major expertise levels: 5 history students, 5 amateur experts (experienced users of CWPS), and 5 expert historians. Participant details can be found in Appendix D. We anonymize these groups with the following identifiers, respectively: S1–S5, C1—C5, and H1–H5.\r\n5.2.2 Dataset. For the study, we created a dataset of 10 different photos identified on CWPS. Three of these photos had an ID conflict, i.e., multiple identities were proposed. For two of these photos, one ID was correct and the other one was incorrect. The community had already researched both photos, voted on the correct ID, and left credible evidence in the comments. Both IDs were linked to additional photos as well. The third photo was one of the seeded photos on CWPS, but was originally misidentified. We added another false ID, making both IDs for the third photo incorrect. All photos had multiple photos matched to them; eight of them were linked via facial matches, while two of them had replicas.\r\n5.2.3 Procedure. The entire study was conducted online via recorded Zoom sessions, with at least one researcher attending each session. Each participant first completed a consent form and a pre-survey describing their demographics and Civil War photography experience.\r\nAs part of the study, participants reviewed three randomly assigned photos from the dataset one-by-one in the original CWPS system first, followed by the same photos on the Photo Steward version. Participants used a think-aloud protocol while using the two systems; after the completion of the task with each system, they were asked a few semi-structured questions about their experience. Finally, the participants completed a summative post-survey of standard usability questions (e.g., ease of use, usefulness of features, instruction clarity, preferred system, etc.) (see Appendix E)\r\nWe maintained this sequence (original CWPS first, CWPS with Photo Steward second) for all the participants, rather than using a randomized sequence, for two reasons. First, we did not want participants' assessments to be biased in favor of Photo Steward after seeing additional features in the new interface. This design allowed us to observe if the original interface misled the participants towards incorrect assessments, and if, subsequently, the Photo Steward interface helped correct them. Second, in a randomized sequence, Photo Steward would expose the participants to new information in the form of prior user votes and responses, and therefore, may confound how they assess the information on the original CWPS version.\r\n5.2.4 Data Analysis. The first author fully transcribed and analyzed the interviews and think-aloud recordings using an inductive qualitative thematic approach [4]. The transcript sections were first divided according to the interface in question (i.e., original CWPS or Photo Steward), followed by an open coding of the transcripts using MAXQDA 2020 [65]. The first author iterated and settled on a total of 28 codes (e.g., change in opinions, comparison interface, source trustworthiness, etc.) for 634 coded segments across all the transcripts. These codes were then organized into themes as described in Section 6 after discussing with the co-author.\r\n5.2.5 Limitations. We conducted a qualitative lab study to understand how users with different backgrounds and expertises validated photo IDs using Photo Steward and hit theoretical saturation. However, there are a couple of limitations with the study: 1) limited insights on the role of expertise, and 2) the task sequence could have order effects. Further, the large-scale analysis of Photo Steward logs provided us with insights of its usage amongst users. However, it lacked an expert-prepared gold standard dataset, which hindered our ability to conduct specific performance analyses as part of this study.\r\n6 FINDINGS\r\nUsing the methods above, we evaluated how well Photo Steward's stewardship architecture supported CWPS users in validating photo identifications, compared to the original version of CWPS.\r\n6.1 Validation Interface\r\nUsers found Photo Steward's validation interface to be useful for comparing different photos.. While assessing the IDs with the original interface, participants would go back and forth between different photos to compare whether they are the same person or not. Some participants opened the photos in two different browser windows and kept them side-by-side. While using the validation interface in the new system, participants appreciated being able to see the photos side-by-side at the same time.\r\nH1 said, \"As an historian using this, this is really great to see them both together. It just makes a comparison a lot easy for me to do. I mean, this is the same gentleman, he's got a little dark facial hair. It looks a little bit different there and the photo on the right, but the facial match is definitely there. \" This was also echoed by C3, who said, \"This, I really find extremely useful, especially when I'm trying to do facial recognition. I can zoom in and have them side by side here. [...] Where in the past, I would have to go back and forth between tabs or cut and paste them into a different document to look at them side by side.\"\r\nFrom our logs, we found that 223 users had compared 2319 unique photo pairs for facial similarity, with 156 pairs receiving comparisons from at least 2 different users. The facial similarity responses were distributed as follows: 763 replicas, 1232 facial matches, 283 unsure, and 280 different people.\r\nTable 1: Distribution of User Votes.\r\n\r\nVotes\r\n/ ID\r\n# of IDs\r\nNote\r\nPresent\r\nNegative\r\nVotes\r\n1\r\n5650\r\n511\r\n61\r\n2\r\n157\r\n(Agreement: 119)\r\n(Disagreement: 38)\r\n83\r\n12\r\n3+\r\n36\r\n(Agreement: 21)\r\n(Disagreement: 15)\r\n31\r\n4\r\nTable 2: Distribution of Confidence Levels.\r\n\r\nMean\r\nConfidence\r\n1\r\nVote / ID\r\n2\r\nVotes / ID\r\n3+\r\nVotes / ID\r\n-2 (No - Highly Confident)\r\nto\r\n-1 (No - Slightly Confident)\r\n45\r\n3\r\n2\r\n-1 (No - Slightly Confident)\r\nto\r\n0 (Not Sure)\r\n16\r\n3\r\n2\r\n0 (Not Sure)\r\n99\r\n8\r\n4\r\n0 (Not Sure)\r\nto\r\n1 (Yes - Slightly Confident)\r\n760\r\n15\r\n5\r\n1 (Yes - Slightly Confident)\r\nto\r\n2 (Yes - Highly Confident)\r\n4730\r\n128\r\n23\r\nUsers preferred the ability to provide granular feedback for photo IDs using Photo Steward's validation interface.. All participants expressed preference for the fine-grained confidence levels, including the ability to dispute an ID, in Photo Steward's validation interface, appreciating how it more accurately mirrored the inherent uncertainty present when assessing photo IDs. S3 said, \"I definitely like the five levels. I think it leaves more room for interpretation. Like sometimes it's kind of hard to just say yes or a hard no because so much goes into it. Especially because a lot of this stuff was so long ago, there's so many unanswered questions.\" H4 initially defended the original interface's binary vote, but changed her mind after experiencing Photo Steward's confidence levels: \"[W]hat I had said has this very black and white feel to it, you're wrong or you're right. I like these degrees of disagreement or agreement. I think that's way more helpful broadly.\"\r\nThe usage of the voting feature was reflected in our logs, which showed 5843 voting instances from 328 unique users (see Table 1). Table 2 shows that while users utilized the full range of confidence levels, including when they were unsure (mean confidence = 0) or slightly confident (mean confidence = 0 to 1) about the ID, the vast majority of the votes were highly confident ones (mean confidence = 1 to 2). A small proportion of votes (77) were cast for disputing an ID.\r\nUsers justified their voting decisions through notes covering an extensive range of topics..\r\nTable 3: Distribution of Note Topics. The table also displays how the notes are distributed for different user confidence levels.\r\n\r\nCategory\r\nSub-Category\r\nTotal\r\nNumber of\r\nNotes\r\nNo\r\n(Highly\r\nConfident)\r\nNo\r\n(Slightly\r\nConfident)\r\nNot\r\nSure\r\nYes\r\n(Slightly\r\nConfident)\r\nYes\r\n(Highly\r\nConfident)\r\nPhoto\r\nComparison\r\nHigh-Level\r\nComparison\r\n206\r\n9\r\n5\r\n9\r\n20\r\n163\r\n\r\nDescribing\r\nFacial Features\r\n37\r\n4\r\n3\r\n9\r\n8\r\n13\r\nWord-of-Mouth\r\nDescendant\r\n55\r\n1\r\n0\r\n0\r\n8\r\n46\r\n\r\nOwnership\r\n47\r\n1\r\n0\r\n0\r\n0\r\n46\r\n\r\nFamiliarity\r\n18\r\n0\r\n0\r\n0\r\n2\r\n16\r\nVisual\r\nEvidence\r\nUniform\r\n73\r\n7\r\n1\r\n8\r\n17\r\n40\r\n\r\nInscription\r\n145\r\n4\r\n3\r\n3\r\n6\r\n129\r\n\r\nOther\r\nVisual Clues\r\n17\r\n1\r\n0\r\n1\r\n2\r\n13\r\nExternal\r\nInformation\r\nExternal URL\r\n47\r\n1\r\n0\r\n1\r\n4\r\n41\r\n\r\nOther Sources\r\n172\r\n9\r\n1\r\n1\r\n21\r\n140\r\n\r\nLack of\r\nInformation\r\n21\r\n0\r\n2\r\n13\r\n6\r\n0\r\nProviding\r\nAdditional\r\nInformation\r\nBiographical\r\nInformation\r\n46\r\n11\r\n2\r\n3\r\n3\r\n27\r\n\r\nAdditional\r\nContext\r\n117\r\n5\r\n0\r\n2\r\n11\r\n99\r\nFrom our logs, we found that 155 users had left 682 notes for 600 different photos. However, as Table 1 shows, around 10% of the votes had a note. Table 3 shows the different topics covered by the notes. We observe that users' voting patterns are significantly influenced by the availability and quality of evidence, with clear facial similarity, period inscriptions, personal anecdotes, visible clues in the photograph, and added biographical context often leading to high confidence \"Yes\" votes, while lack of information typically results in \"Not Sure\" votes.\r\nUsers most frequently left a comment attributing facial similarity (after comparing it in the first step of the validation process) to be the reason for their decision (e.g., \"Identical to the other CDV\"). In some instances, they would expound on it by discussing facial features: \"The eyes, nose, cheek bones, shape of face, all look similar to George Pickett, although possibly reversed based on hair part\". In many instances, we found users inferring biographical information (service records, location, etc.) from visual evidence in the photo, be it uniform or backmarks (e.g., \"Initials MN on chinstrap (brass letters). Signature on verso is made out to Marlin's oldest sister. Style of insignia is consistent with other 1862 recruits for Co. B 1st USSS\").\r\nInterestingly, we also observed a large number of word-of-mouth evidence notes for justifying the user's decisions, such as claiming to be a descendant, or owning the original copy of the photo, or having seen the photo somewhere. For example, one user noted, \"He is my great-great grandfather and this photo has been passed down through the generations to me and was identified by his son John Albert Johnson, my father's grandfather.\" Users also left external URLs and source details in the notes as evidence. Sometimes, they provided additional context (e.g., \"This image came with a group of 7th Iowa images. The majority were of Company G., but there is only one person in the entire 7th Iowa Infantry that could be identified by the first or last name of 'Nelson.'\").\r\nThe validation workflow encouraged users to exercise careful deliberation while making photo ID decisions.. Users felt that the questions in each step of the validation process helped them to carefully weigh in all the evidence and deliberate while voting on the ID (Q4, mean = 4.60, SD = 0.49). C3 explained why the two steps were necessary: \"It's two separate things. One is asking, do you think that this face is the same face? Then the second is, do you think that this face matches this name? I think that that is a necessary question for both of those scenarios. I don't think it's redundant, I think it's necessary.\" H2 appreciated the thoughtfulness that the two-step process encouraged, saying, \"It could be the same guy, but it might be a different guy, but now that you know the other interface kind of forces me to slow down a little bit and think more carefully, because it's asking specific questions about things.\" A couple of participants, however, expressed initial confusion over the separation and order of these two tasks.\r\n\r\nFigure 7: Deliberation in case of post-identification voting. The table shows the distribution of confidence votes (second step of the validation process) against the user's facial similarity comparison (first step of the validation process). The comparison scores were computed by aggregating all the facial similarity comparisons in the first step (replica = 2, facial match = 1, not sure = 0, and different people = -1).\r\n\r\nOur logs affirm the deliberative aspect of the two-step validation process. We observed that when users compared one or more photos in the first step, they factored in the facial similarity evidence for their ID vote decision. Figure 7 shows that when the cumulative comparison scores are positive (i.e., majority of the comparisons were a facial match or a replica), the confidence scores are generally positive. Similarly, when the cumulative comparison scores are negative, the confidence votes are also largely negative. When the users are unsure about the facial similarity comparison, it reflects in their final ID vote. Notably, when there were mismatches between comparison scores and ID votes, users provided reasons in the notes, citing visual evidence and an unsure photo comparison.\r\nPhoto Steward's stewardship architecture fostered diverse forms of meaningful stigmergic collaboration amongst users.. Although most IDs received only a single vote, about 3% (193 IDs) received multiple votes (see Table 1). Among these, 140 IDs saw total consensus among voters on confidence scores, with half involving an explanatory note. When the initial vote lacked a note, subsequent voters often added information like external sources or context. Almost all IDs where voters agreed positively were supported by metadata such as period inscriptions, scholarly sources, or additional comparison photos, underscoring the credibility of these agreements.\r\nWhen users disagreed on an ID, they would almost always justify in notes why they differed from the previous voters' opinions (43 out of 53 disagreement instances). From analyzing the notes, we found that the subsequent voters provided additional information about external sources or service records to disagree (e.g., \"Please see Military Images magazine, MI, Volume XVI, Number 3, November - December 1994 for the correct identification of this image. The correct identification via the Michael McAfee collection is Richard Cramer, 4th Michigan Infantry\"). Among 18 instances of conflicting IDs for the same photo, seven saw resolution through a process of voting one ID over another, often accompanied by external evidence in notes or citing facial similarity.\r\nUser agreement was particularly strong in facial similarity comparisons. Of the 2200 unique photo pairs compared, 141 received multiple user reviews, with 114 seeing total agreement. The remaining cases typically involved user errors in distinguishing between 'facial match' and 'replica', or uncertainty about facial similarity.\r\n6.2 Stewardship Visualizations\r\nStewardship visualizations reflecting community insights enhanced the accuracy of photo ID assessments.. Participants found Photo Steward's bar chart visualization showing the five confidence levels to be simple and easy to understand (Q1, mean = 4.93, SD = 0.25). S1 said, \"I'm a visual person. Bar graphs or charts like statistical data helps me put things into a better perspective or gives me an idea of what I am working with versus just something more plain [like] the other interface.\"\r\nWhen participants were uncertain about an ID, they saw the additional evidence and justification that the community members had provided along with their vote as essential to taking the vote seriously. S5 said, \"If there's other users giving comments like 'I've used this source,' and you know you get information from a source where they found it, I'm probably gonna agree with them. If they're just voting yes without anything else, then I am probably more likely to go out and find sources for the photo myself and make my own decision.\"\r\nIn three instances, students (S1, S3, and S5) initially made incorrect assessments on the original interface but rectified these after using Photo Steward. The community's consensus and evidence in Photo Steward were pivotal in these corrections. S5, confronted with ID conflicts, stated after using Photo Steward: \"You know that helps a lot with what people are thinking and presumably these people have also gone to the Maine State Archives or something and verified and then given their opinion so that helps.\" Similarly, historian H2 and collector C3 initially made incorrect choices but amended their decisions after analyzing community-based evidence on Photo Steward. H2 remarked: \"So I believe it's the first person, of course, because it's based on the Maine State Archives. And then you've got the piece on there that said that you looked and found a different man. With that name in the actual regimental history. I say that's fairly accurate information.\" This highlights the critical role of community consensus in resolving photo ID conflicts.\r\nOn the other hand, when participants were confident about an ID, seeing the community's opinions affirmed their own assessments, for better or worse. In the words of H5: \"I really liked the very clear community consensus, and the ability to be able to see the identities of the people who were looking at these images. It was something that (gave) a boost of confidence in terms of my final decision.\" Overall, the participants found the community opinions to be useful for assessing the IDs (Q2, mean = 4.93, SD = 0.25).\r\nUsers gave additional weight to the opinions of members they were familiar with and desired more contextual information about all contributors.. Participants examined the bar chart visualization details and gave higher weight to the opinions of prominent names from the Civil War photography community while assessing the ID. C3 said, \"Here's <name redacted>, period inscription with valediction, the uniform matches his service record. <name redacted> said the same thing. These two uploaders I hold in very high regard to their opinions on this site. If they're saying that they think highly confident of this identification, that gives me a lot of confidence as well.\" H5 became further unsure about an ID after seeing a fellow historian's vote: \"And, you know, sort of knowing <name redacted>, someone who I know deals with primary source material a lot, and sort of being on the fence with it as well, leaves me in that unsure position.\"\r\nParticipants also sought additional details about community members, proposing indicators of professional status or active participation in the CWPS community. H1 emphasized his appreciation of amateur experts and opposition to gatekeeping, yet he suggested having some kind of credential indicator next to the username would be helpful: \"It will be great if, you know, somebody was an academic historian or a published Civil War author, if there was some way to just say some little tidbit next to <name redacted> 'Oh, saying, hey, I'm from <university redacted>, Professor or, you know, author of whatever.'\" Others preferred indicators of community activity levels or personal connections (i.e., descendants) to the identified individual in the photo.\r\nOur log analysis showed revealed the presence of certain active voters, with 9 out of 328 unique voters voting on more than 50 photos each, and 2 voting on over 2000 photos each (mostly ones that they had uploaded and identified). If we consider only votes on photos identified by someone else, we have 8 users who have voted on more than 15 IDs each, with 1 user voting on over 150 IDs.\r\nThe community opinions made the platform feel more engaging, but users had mixed thoughts about the ideal number of votes per photo.. Participants, in general, felt the community opinions made Photo Steward more engaging compared to the original system. H5 said, \"I thought it appeared more user-centric and user-friendly and it felt more participatory.\" C3 saw the benefits of increased engagement for making IDs more reliable: \"I think for this crowdsourcing project that we're building on this database [it] is very important to have those comments, those feedback, that we see in the voting system. It only makes this ID stronger and makes the project and the database a more trustworthy and reliable source.\"\r\nUsers wanted to see more community opinions and comments for IDs that had few votes. S1, on seeing only one vote for an ID, said, \"That doesn't make me feel as confident because that's not too many for me to give an answer.\" S3 said it would have been easier to assess some photos if \"there was more input from other people.\" Users had mixed opinions about how many votes they wanted to see for verifying an ID. In general, they wanted to see consensus among the community for an identity and at least three votes. C3 said, \"I usually shoot for like three to four [votes] as the lowest where I take some good quality out of those votes. [...] If there's only one or two, and especially if there's two that are split, that is not as reliable to me.\"\r\nParticipants' concerns about spreading voters too thin were borne out in the log data (see Table 1). We analyzed the logs to check how often CWPS users vote on the photos they are browsing. We found 1784 instances (out of 5843 voting instances) where the number of \"lurkers\" for a given photo page exceeded the number of voters on that photo.\r\nTable 4: Face Recognition Similarity Score Stats vs. User Comparisons (Facial Similarity).\r\n\r\nUser Comparison\r\nMean\r\nMedian\r\nSD\r\nCount\r\nReplica\r\n0.88\r\n0.91\r\n0.16\r\n659\r\nFacial Match\r\n0.52\r\n0.65\r\n0.31\r\n1015\r\nDifferent Person\r\n0.55\r\n0.59\r\n0.16\r\n265\r\nNot Sure\r\n0.51\r\n0.58\r\n0.23\r\n261\r\nUsers found the face recognition badge and community's opinions complementary for assessing the reliability of matched photos..\r\nTable 5: User Comparisons (Facial Similarity) vs. Face Recognition.\r\n\r\n\r\n# of Photo Pairs\r\n# of Photo Pairs\r\ncompared by 1 user\r\n# of Photo Pairs\r\ncompared by 2 users\r\n# of Photo Pairs\r\ncompared by 2+ users\r\nSupported by users\r\nand face recognition\r\n1408\r\n1281\r\n109\r\n18\r\nDisputed by users\r\nand face recognition\r\n26\r\n24\r\n0\r\n1\r\nUsers unsure,\r\nface recognition disputes\r\n42\r\n42\r\n0\r\n0\r\nUsers unsure,\r\nface recognition supports\r\n219\r\n217\r\n2\r\n0\r\nUsers dispute,\r\nface recognition supports\r\n239\r\n238\r\n1\r\n0\r\nUsers support,\r\nface recognition disputes\r\n266\r\n257\r\n8\r\n1\r\nFrom our logs, we found that 1408 photo pair comparisons were supported by both users and facial recognition, by far the most common outcome (see Table 5). This information would be visible to the larger user community in the form of stewardship visualizations (see Figure 5). Interestingly, we also see 266 cases where a comparison is supported by users, but disputed by facial recognition (similarity confidence score < 0.50). That outcome was approximately as common as when users disputed a comparison but face recognition supported it (239 pairs) or when a user was unsure but face recognition supported it (219 pairs). However, it was far less common for facial recognition to dispute a comparison when a user also disputed it (26 pairs) or was unsure (42 pairs).\r\nWhen we analyze the face recognition's confidence scores in more detail, Table 4 shows that there is a clear separation between replicas and other types of user comparison scores. Face recognition confidence scores for photo pairs that users labeled as replicas were much higher in terms of both mean and median (0.88 and 0.91, respectively, versus scores in the 0.50s and 0.60s for all others). While these fine-grained scores are not displayed to users — they intentionally see only the face recognition badge — the very close mean and median confidence scores for \"facial match\" versus \"different person\" illustrate the difficulty of automatically identifying non-replica matches and offer support for a hybrid human-AI approach (cf. Section 4.3.1).\r\nParticipants found the community's opinions to be helpful for assessing whether two photos were facial matches or not. S4 said, \"I mean, I think it's cool to see what the community is saying, because I do feel when it comes to saying 'Is this the same person in both these pictures?', that's really the best way to do it if you don't have any [other] information.\" C1 said that he found the community opinions for facial matches especially helpful because he is \"face blind\": \"I can't really identify the face-to-face, but the hair and the mustache and all the stuff that, in addition, it helps me with that for sure. It's good because other than having to find somebody close to me and be like, 'Do you think this is the same people?' [I] have that community right there.\"\r\nParticipants had mixed opinions about facial recognition technology, but most found the badge indicating whether it supported the two photos being a match to be a useful data point. S3 said, \"The facial recognition saying they are similar — I would go ahead and trust that but I don't know if I would trust it enough to make a verification on my own.\" In general, participants found the strengths of facial recognition and the community to be complementary in determining whether two photos showed the same person, and liked seeing both results together. H4 felt the community and the technology had separate roles:\r\nThis is facial recognition, and this is the historical background. I trust facial recognition, but it makes me feel better to have that historical background. I think the human eye can be tricked by different hairstyles and different beards. Just to have this outside historical verification to say like, 'Okay, maybe you or I was tricked, but the machine was not,' I think that's really helpful.\r\nS1 relied on both the community and facial recognition to make a decision on a facial match: \"I think both give me kind of an idea. Okay, there's this facial recognition technology being used, but also there's other users that are leaning towards that this is the same person.\" H5 got a similar boost of confidence: \"This is certainly reassuring seeing not only the AI match, but also in terms of the community — seeing that seven users have said that this is a facial match as well. I'd be quite convinced by this.\"\r\n7 DISCUSSION\r\n7.1 Leveraging Collective Intelligence for Validating Person Identification\r\nPrior work has raised concerns about misinformation in online history communities [43, 75]. To address these problems on CWPS, we built Photo Steward for supporting community-based validation of photo IDs. Users found Photo Steward's stewardship visualizations not only helpful for affirming their own assessment, but also for discovering new knowledge and correcting their decisions, if need be. These visualizations, combined with the validation workflow, exhibit a form of stigmergic collaboration, where users build on prior knowledge left by the community and leave their own assessment for other users [14, 17, 27, 56].\r\nPhoto Steward allowed users to express how confident they are about an ID in a fine-grained manner, in contrast to the binary agreements or disagreements observed on iNaturalist by Wiggins et al. [72]. The CWPS community preferred this nuanced form of stewardship as users are likely to have different degrees of confidence based on the evidence available for identifying a photo, thus demonstrating the effectiveness of Design Goal 2. As S3 pointed out, users often experience difficulty in making a binary decision about individuals who lived 150 years back due to the lack of surviving documentation.\r\nBeyond its basic usefulness, Photo Steward's full potential can best be realized through sustained community participation, but most IDs on CWPS only received one vote. To address this challenge, we can leverage different crowdsourcing and online community strategies. For example, we can draw the community's attention towards IDs that are \"more of a puzzle,\" as H5 suggested, similar to Twitter's Birdwatch promoting tweets for fact-checking [52]. Designing nudges to encourage lurkers to vote on the IDs they are viewing can further help in these efforts. Organizing community events can help foster interest and participation in collaboratively verifying IDs, drawing inspiration from crowdsourcing events like CrowdSolve, where experts and novices collaborate on solving missing persons cold cases [70]. Incentive mechanisms such as leaderboards and challenges [46] can drive extrinsic motivation within the community for verifying the IDs. Finding users who are more likely to vote on an ID, based on their skills and interests, can also be an effective collaboration strategy [71]. In future work, we plan to integrate these strategies and introduce explicit \"calls to action\" [51, 58] on the home feed, guiding the community's attention towards IDs that require validation and fostering more sustainable, collaborative participation in historical photo identification.\r\n7.2 Exercising Deliberation in Human-AI Teams\r\nWe found that Photo Steward's validation workflow was effective not only for voting on the IDs and comparing the photos side-by-side, but also encouraged users to deliberate on their decision, drawing parallels to other social computing systems that support reflection and deliberation (e.g., [28, 29]). This deliberative intervention was non-trivial as users on CWPS follow an identification pipeline which is powered by facial recognition, an AI algorithm that is far from perfect [44]. Users are trying to find the correct match, if present at all, from a pool of potential candidates, which are largely comprised of similar-looking false positives — akin to finding a needle in a haystack. Further, the task of comparing photos of people is by no means an easy task for humans, even in a modern context [53]. While Photo Steward can not completely curb automation bias, an issue that has been previously observed in multiple online history communities [44, 75], its multi-step, validation workflow with structured feedback interventions encouraged users to deliberate over AI suggestions before making a decision.\r\nPhoto Steward's workflow also compartmentalizes the tasks that AI is good at – such as quickly retrieving similar-looking candidates from a large search pool – from the tasks where the AI makes more errors – such as verifying whether two faces show the same person or not [5, 55]. Decoupling facial similarity comparison from the person identification task (Design Goal 1) allows the users to now focus on the face verification task. In doing so, Photo Steward's workflow supports effective human-AI teaming in the context of person identification by allowing the user to make a granular assessment for the face verification task instead of the AI, while also ensuring that an AI-retrieved, similar-looking potential candidate is being compared against.\r\nAs imperfect AI algorithms get deployed in high-stakes scenarios such as medical imaging, law enforcement, etc. [6], it becomes more critical to reduce automation bias and encourage more deliberative decision-making. Amershi et al. recommend granular user feedback while interacting with AI systems as part of their \"Guidelines for Human-AI Interaction\" [2]. Similarly, other forms of design interventions, such as counterfactual AI explanations [63], chatbots [26], and community opinions [59] can also be explored for encouraging deliberative decision-making with AI assistance.\r\nPrior work in human-face recognition teams has shown that algorithmic suggestions can have a significant biasing effect on a user's decision [23]. Our findings showed that Photo Steward was able to encourage users to exercise deliberation while interacting with results retrieved by facial recognition. At the same time, Table 4 also showed that users can differ from the algorithm's suggestions, thus necessitating a deeper dive analysis of this dissonance as part of future work.\r\n7.3 Assessing Quality in Crowdsourced Original Historical Research\r\nWe found that Photo Steward's stewardship visualizations helped users assess the reliability of photo IDs on CWPS, which were a result of Design Goal 3. However, crowdsourced identifications always run the risk of groupthink [24, 25], which can eventually mislead users into believing and amplifying misidentifications, a concern also raised by H4. Public deliberation of modern photo IDs on social media can have profound negative consequences for false targets, as exemplified by the Boston Marathon bombing [49, 66] and the recent US Capitol riot [48]. This raises the question: are Photo Steward's stewardship visualizations sufficient for assessing the quality of photo identifications made on CWPS?\r\nPrior work on crowdsourced scholarship suggests an answer. Rosenzweig [57] analyzed Wikipedia as a source of historical scholarship, noting its policy against original research, and advocating for it as a tool for teaching the limitations of information sources and critical analysis of primary and secondary sources. Motivated along similar lines, Forte et al [18] proposed the assessability framework for designing assessable participatory information systems, based on information provenance and stewardship. The concept of provenance, extensively used in history and archival studies, describes information that makes it possible to trace the ownership or origins of the content, while stewardship refers to the processes that were used for maintaining the content, including its authorship. In the case of Wikipedia, Forte et al. found that visualizing provenance (i.e., citation types) and stewardship (i.e., article quality) had a significant impact on assessments of articles and Wikipedia as an information source.\r\nWhile Photo Steward enables information stewardship on CWPS, there is an opportunity for incorporating provenance into the CWPS platform to make it a truly assessable online platform. A significant proportion of the notes left by users on Photo Steward qualified as provenance information, namely comments about period inscriptions, family trees, external sources and URLs. The challenges of assessing IDs on CWPS are, however, different from assessing information on Wikipedia, primarily because CWPS supports original research unlike Wikipedia's no original research policy [73]. This was also the reason why we designed Photo Steward to be a review system rather than a single editable output such as Wikipedia; original research such as historical photo identifications is often times an evolving investigation rather than a final decision. To assess the reliability of original photo IDs made on CWPS, users may want to factor in the provenance of the reference photos that were used in the identification process. In such cases, Photo Steward's stewardship visualizations (i.e., facial similarity reliability) can further help the user in assessing whether the reference photos can be used as reliable provenance or not.\r\n8 CONCLUSION\r\nPhoto Steward attempts to help users assess and validate photo IDs better on CWPS. We present an information stewardship architecture, and adapt it for the task of historical person identification. We demonstrate the effectiveness of Photo Steward on CWPS, an existing online platform, where users found the stewardship visualizations, which included the community opinions and the AI verdict, useful for making accurate assessments of photo IDs on the platform. Further, users found Photo Sleuth's multi-step, structured validation workflow to help them deliberate before making decisions about the photo's identity. This work opens doors for exploring new ways to leverage collective intelligence and AI in creating assessable online information systems for historical archives.\r\nACKNOWLEDGMENTS\r\nWe wish to thank Ron Coddington, Paul Quigley, Liling Yuan, and our study participants. This research was supported by NSF IIS-1651969 and a Virginia Tech ICTAS Junior Faculty Award.\r\nREFERENCES\r\n2021. Heritage Auctions: World's Largest Collectibles Auctioneer. https://www.ha.com/ Navigate tocitation 1citation 2\r\nSaleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. 2019. Guidelines for human-AI interaction. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–13. Navigate tocitation 1\r\nBrakkton Booker. 2015. $2 photo found at Junk Store has Billy the kid in it, could be worth $5M. https://www.npr.org/sections/thetwo-way/2015/10/15/448993361/-2-photo-found-at-junk-store-has-billy-the-kid-in-it-could-be-worth-5-million Navigate tocitation 1citation 2\r\nVirginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3, 2 (2006), 77–101. Navigate tocitation 1\r\nJoy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Conference on Fairness, Accountability and Transparency, FAT 2018, 23-24 February 2018, New York, NY, USA. 77–91. http://proceedings.mlr.press/v81/buolamwini18a.html Navigate tocitation 1\r\nCarrie J Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg S Corrado, Martin C Stumpe, et al. 2019. Human-centered tools for coping with imperfect algorithms during medical decision-making. In Proceedings of the 2019 chi conference on human factors in computing systems. 1–14. Navigate tocitation 1\r\nFanny Chevalier, Stéphane Huot, and Jean-Daniel Fekete. 2010. Wikipediaviz: Conveying article quality for casual wikipedia readers. In 2010 IEEE Pacific Visualization Symposium (PacificVis). IEEE, 49–56. Navigate tocitation 1\r\nH Jason Combs. 2005. The Internet's Impact on the Antiques Trade. Material Culture (2005), 26–41. Navigate tocitation 1citation 2\r\nAnthony DeBartolo. 1975. Appraisers can tell when old photo really may be golden. https://www.chicagotribune.com/news/ct-xpm-1985-12-27-8503300067-story.html Navigate tocitation 1\r\nAnthony DeBartolo. 2021. Appraisers can tell when old photo really may be golden. https://www.chicagotribune.com/news/ct-xpm-1985-12-27-8503300067-story.html Navigate tocitation 1\r\nHarley D Dickinson. 1998. Evidence-based decision-making: an argumentative approach. International Journal of Medical Informatics 51, 2-3 (1998), 71–81. Navigate tocitation 1\r\nFor The Inquirer Edward Colimore. 2019. Did John Wilkes Booth get away with murdering President Abraham Lincoln?https://www.inquirer.com/news/john-wilkes-booth-lincoln-conspiracy-photo-recognition-20190415.html Navigate tocitation 1\r\nMark Elliott. 2006. Stigmergic Collaboration: The Evolution of Group Work: Introduction. m/c journal 9, 2 (2006). Navigate tocitation 1citation 2citation 3\r\nMark Elliott. 2016. Stigmergic collaboration: A framework for understanding and designing mass collaboration. In Mass collaboration and education. Springer, 65–84. Navigate tocitation 1citation 2citation 3\r\nThomas Erickson and Wendy A Kellogg. 2000. Social translucence: an approach to designing systems that support social processes. ACM transactions on computer-human interaction (TOCHI) 7, 1 (2000), 59–83. Navigate tocitation 1\r\nDan Evon. 2020. Did Joe Biden's Great-Grandfather Own Slaves?https://www.snopes.com/fact-check/joe-biden-slaves-great-grandfather/ Navigate tocitation 1\r\nKristie Fisher, Scott Counts, and Aniket Kittur. 2012. Distributed sensemaking: improving sensemaking by leveraging the efforts of previous users. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 247–256. Navigate tocitation 1citation 2\r\nAndrea Forte, Nazanin Andalibi, Thomas Park, and Heather Willever-Farr. 2014. Designing information savvy societies: an introduction to assessability. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2471–2480. Navigate tocitation 1citation 2citation 3citation 4\r\nJacey Fortin. 2018. She Was the Only Woman in a Photo of 38 Scientists, and Now She's Been Identified. The New York Times (Mar 2018). https://www.nytimes.com/2018/03/19/us/twitter-mystery-photo.html Navigate tocitation 1\r\nSarah A Gilbert. 2020. \" I run the world's largest historical outreach project and it's on a cesspool of a website.\" Moderating a Public Scholarship Site on Reddit: A Case Study of r/AskHistorians. Proceedings of the ACM on Human-Computer Interaction 4, CSCW1 (2020), 1–27. Navigate tocitation 1\r\nJerome S Handler and Michael L Tuite. 2007. Retouching History: The Modern Falsification of a Civil War Photograph. Navigate tocitation 1citation 2\r\nM. Keith Harris. 2019. Civil War Photo Sleuth. Journal of American History 106, 2 (2019), 544–546. https://doi.org/10.1093/jahist/jaz498 Navigate tocitation 1citation 2\r\nJohn J Howard, Laura R Rabbitt, and Yevgeniy B Sirotin. 2020. Human-algorithm teaming in face recognition: How algorithm outcomes cognitively bias human decision-making. Plos one 15, 8 (2020), e0237855. Navigate tocitation 1\r\nPan Hui and Sonja Buchegger. 2009. Groupthink and peer pressure: Social influence in online social network groups. In 2009 International Conference on Advances in Social Network Analysis and Mining. IEEE, 53–59. Navigate tocitation 1\r\nNassim JafariNaimi and Eric M Meyers. 2015. Collective intelligence or group think? Engaging participation patterns in World Without Oil. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing. 1872–1881. Navigate tocitation 1\r\nSoomin Kim, Jinsu Eun, Joseph Seering, and Joonhwan Lee. 2021. Moderator chatbot for deliberative discussion: Effects of discussion structure and discussant facilitation. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–26. Navigate tocitation 1\r\nAniket Kittur, Andrew M Peters, Abdigani Diriye, and Michael Bove. 2014. Standing on the schemas of giants: socially augmented information foraging. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. 999–1010. Navigate tocitation 1\r\nTravis Kriplean, Caitlin Bonnar, Alan Borning, Bo Kinney, and Brian Gill. 2014. Integrating On-demand Fact-checking with Public Dialogue. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing(CSCW '14). ACM, New York, NY, USA, 1188–1199. https://doi.org/10.1145/2531602.2531677 Navigate tocitation 1citation 2citation 3\r\nTravis Kriplean, Michael Toomim, Jonathan Morgan, Alan Borning, and Andrew Ko. 2012. Is This What You Meant?: Promoting Listening on the Web with Reflect. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems(CHI '12). ACM, New York, NY, USA, 1559–1568. https://doi.org/10.1145/2207676.2208621 Navigate tocitation 1citation 2citation 3\r\nKurt Luther. 2016. How Fellow Collectors, Field Photos and Muttonchops Identified an Unknown Officer. Military Images 34, 1 (2016), 29–31. Navigate tocitation 1\r\nKurt Luther. 2017. Merrill Carbine Leads to a Soldier's Identification. Military Images 35, 2 (2017), 64–65. Navigate tocitation 1citation 2citation 3\r\nKurt Luther. 2018. Non-Traditional Research Tools—and Serendipity. Military Images 36, 3 (2018), 12–13. Navigate tocitation 1\r\nKurt Luther. 2018. What are the odds? Photo sleuthing by the numbers. Military Images 36, 1 (2018), 12–15. Navigate tocitation 1citation 2citation 3citation 4citation 5\r\nKurt Luther. 2019. What to Do When Gold Standards Go Wrong?Military Images 37, 1 (2019), 8–9. https://www.jstor.org/stable/26532101 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: How to Trust the Worthiness of an Identification. Military Images 38, 3 (213) (2020), 8–11. https://www.jstor.org/stable/26914966 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: Lost and Found in the Library of Congress. Military Images 38, 2 (212) (2020), 10–13. https://www.jstor.org/stable/26890126 Navigate tocitation 1\r\nKurt Luther. 2020. Real-life accounts on the research trail: The Art of Photo Sleuthing. Military Images 38, 4 (214) (2020), 8–11. https://www.jstor.org/stable/26925454 Navigate tocitation 1\r\nRamona Martinez. 2012. Photo mystery solved, then doubted, then deciphered, thanks to readers. https://www.npr.org/sections/pictureshow/2012/04/17/150801239/photo-mystery-solved-then-doubted-then-resolved-thanks-to-readers Navigate tocitation 1\r\nRamona Martinez. 2012. Unknown No More: Identifying A Civil War Soldier. http://www.npr.org/2012/04/11/150288978/unknown-no-more-identifying-a-civil-war-soldier Navigate tocitation 1\r\nChristian A Meissner and John C Brigham. 2001. Thirty years of investigating the own-race bias in memory for faces: A meta-analytic review.Psychology, Public Policy, and Law 7, 1 (2001), 3. Navigate tocitation 1\r\nVikram Mohanty, Kareem Abdol-Hamid, Courtney Ebersohl, and Kurt Luther. 2019. Second opinion: Supporting last-mile person identification with crowdsourcing and face recognition. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 86–96. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6\r\nVikram Mohanty and Kurt Luther. 2023. DoubleCheck: Designing Community-based Assessability for Historical Person Identification. ACM Journal on Computing and Cultural Heritage (JOCCH) (to appear) (2023). Navigate tocitation 1\r\nVikram Mohanty, David Thames, Sneha Mehta, and Kurt Luther. 2019. Photo sleuth: Combining human expertise and face recognition to identify historical portraits. In Proceedings of the 24th International Conference on Intelligent User Interfaces. 547–557. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6citation 7citation 8\r\nVikram Mohanty, David Thames, Sneha Mehta, and Kurt Luther. 2020. Photo Sleuth: Identifying Historical Portraits with Face Recognition and Crowdsourced Human Expertise. ACM Transactions on Interactive Intelligent Systems (TiiS) 10, 4 (2020), 1–36. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6citation 7citation 8citation 9citation 10\r\nMeredith Ringel Morris, Scott Counts, Asta Roseway, Aaron Hoff, and Julia Schwarz. 2012. Tweeting is believing? Understanding microblog credibility perceptions. In Proceedings of the ACM 2012 conference on computer supported cooperative work. 441–450. Navigate tocitation 1\r\nBenedikt Morschheuser, Juho Hamari, and Jonna Koivisto. 2016. Gamification in crowdsourcing: a review. In 2016 49th Hawaii International Conference on System Sciences (HICSS). IEEE, 4375–4384. Navigate tocitation 1\r\nKathleen L Mosier and Linda J Skitka. 1999. Automation use and automation bias. In Proceedings of the human factors and ergonomics society annual meeting, Vol. 43. SAGE Publications Sage CA: Los Angeles, CA, 344–348. Navigate tocitation 1citation 2\r\nGreg Myre. 2021. How Online Sleuths Identified Rioters At The Capitol. https://www.npr.org/2021/01/11/955513539/how-online-sleuths-identified-rioters-at-the-capitol Navigate tocitation 1\r\nJohnny Nhan, Laura Huey, and Ryan Broll. 2017. Digilantism: An analysis of crowdsourcing and the Boston marathon bombings. The British journal of criminology 57, 2 (2017), 341–361. Navigate tocitation 1\r\nRaja Parasuraman and Dietrich H Manzey. 2010. Complacency and bias in human use of automation: An attentional integration. Human factors 52, 3 (2010), 381–410. Navigate tocitation 1\r\nJunwon Park, Ranjay Krishna, Pranav Khadpe, Li Fei-Fei, and Michael Bernstein. 2019. AI-based request augmentation to increase crowdsourcing participation. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 7. 115–124. Navigate tocitation 1\r\nSarah Perez. 2022. Twitter to show 'Birdwatch' community fact-checks to more users, following criticism. https://techcrunch.com/2022/03/03/twitter-to-show-birdwatch-community-fact-checks-to-more-users-following-criticism/ Navigate tocitation 1\r\nP Jonathon Phillips, Amy N Yates, Ying Hu, Carina A Hahn, Eilidh Noyes, Kelsey Jackson, Jacqueline G Cavazos, Géraldine Jeckeln, Rajeev Ranjan, Swami Sankaranarayanan, et al. 2018. Face recognition accuracy of forensic examiners, superrecognizers, and face recognition algorithms. Proceedings of the National Academy of Sciences 115, 24 (2018), 6171–6176. Navigate tocitation 1citation 2\r\nPeter Pirolli, Evelin Wollny, and Bongwon Suh. 2009. So you know you're getting the best possible information: a tool that increases Wikipedia credibility. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1505–1508. Navigate tocitation 1\r\nInioluwa Deborah Raji and Joy Buolamwini. 2019. Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products. Navigate tocitation 1\r\nAmira Rezgui and Kevin Crowston. 2018. Stigmergic coordination in Wikipedia. In Proceedings of the 14th International Symposium on Open Collaboration. 1–12. Navigate tocitation 1\r\nRoy Rosenzweig. 2006. Can History Be Open Source? Wikipedia and the Future of the Past. Journal of American History 93, 1 (June 2006), 117–146. Navigate tocitation 1citation 2\r\nSaiph Savage, Andres Monroy-Hernandez, and Tobias Höllerer. 2016. Botivist: Calling volunteers to action using online bots. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing. 813–822. Navigate tocitation 1\r\nMike Schaekermann, Joslin Goh, Kate Larson, and Edith Law. 2018. Resolvable vs. irresolvable disagreement: A study on worker deliberation in crowd work. Proceedings of the ACM on Human-Computer Interaction 2, CSCW (2018), 1–19. Navigate tocitation 1\r\nMichael S. Schmidt. 2018. 'Flags of Our Fathers' Author Now Doubts His Father Was in Iwo Jima Photo. The New York Times (Jan 2018). https://www.nytimes.com/2016/05/04/us/iwo-jima-marines-bradley.html Navigate tocitation 1\r\nJennifer Schuessler. 2017. Found: Oldest Known Photo of a U.S. President (Socks and All). https://www.nytimes.com/2017/08/16/arts/design/john-quincy-adams-daguerreotype-sothebys-auction.html Navigate tocitation 1\r\nJulia Schwarz and Meredith Morris. 2011. Augmenting web pages and search results to support credibility assessment. In Proceedings of the SIGCHI conference on human factors in computing systems. 1245–1254. Navigate tocitation 1\r\nRuoxi Shang, KJ Kevin Feng, and Chirag Shah. 2022. Why Am I Not Seeing It? Understanding Users' Needs for Counterfactual Explanations in Everyday Recommendations. In 2022 ACM Conference on Fairness, Accountability, and Transparency. 1330–1340. Navigate tocitation 1\r\nLinda J Skitka, Kathleen L Mosier, and Mark Burdick. 1999. Does automation bias decision-making?International Journal of Human-Computer Studies 51, 5 (1999), 991–1006. Navigate tocitation 1\r\nVerbi Software. 2019. MAXQDA 2020 [computer software]. VERBI Software. Available from maxqda.com. Navigate tocitation 1\r\nNPR Staff. 2016. How Social Media Smeared A Missing Student As A Terrorism Suspect. https://www.npr.org/sections/codeswitch/2016/04/18/474671097/how-social-media-smeared-a-missing-student-as-a-terrorism-suspect Navigate tocitation 1\r\nBongwon Suh, Ed H Chi, Aniket Kittur, and Bryan A Pendleton. 2008. Lifting the veil: improving accountability and social transparency in Wikipedia with wikidashboard. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1037–1040. Navigate tocitation 1\r\nW Ben Towne, Aniket Kittur, Peter Kinnaird, and James Herbsleb. 2013. Your process is showing: controversy management and perceived quality in Wikipedia. In Proceedings of the 2013 conference on Computer supported cooperative work. 1059–1068. Navigate tocitation 1\r\nCivil War Trust. 2021. Military Images Magazine | Interview with Ron Coddington. https://www.battlefields.org/learn/articles/military-images-magazine Navigate tocitation 1\r\nSukrit Venkatagiri, Aakash Gautam, and Kurt Luther. 2021. CrowdSolve: Managing Tensions in an Expert-Led Crowdsourced Investigation. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–30. Navigate tocitation 1\r\nShaun Wallace, Lucy Van Kleunen, Marianne Aubin-Le Quere, Abraham Peterkin, Yirui Huang, and Jeff Huang. 2017. Drafty: Enlisting Users To Be Editors Who Maintain Structured Data. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 5. Navigate tocitation 1\r\nAndrea Wiggins and Yurong He. 2016. Community-based data validation practices in citizen science. In Proceedings of the 19th ACM Conference on computer-supported cooperative work & social computing. 1548–1559. Navigate tocitation 1citation 2citation 3citation 4citation 5citation 6\r\nFoundation Wikimedia. 2022. No original research. https://en.wikipedia.org/wiki/Wikipedia:No_original_research Navigate tocitation 1\r\nHeather Willever-Farr, Lisl Zach, and Andrea Forte. 2012. Tell me about my family: A study of cooperative research on Ancestry. com. In Proceedings of the 2012 iConference. ACM, 303–310. Navigate tocitation 1\r\nHeather L Willever-Farr and Andrea Forte. 2014. Family matters: Control and conflict in online family history production. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. ACM, 475–486. Navigate tocitation 1citation 2citation 3citation 4citation 5\r\nJeremy B Wilmer. 2017. Individual differences in face recognition: A decade of discovery. Current Directions in Psychological Science 26, 3 (2017), 225–230. Navigate tocitation 1\r\nBob Zeller. 2022. Searching for photos of Civil War Soldiers | David Wynn Vaughan. https://www.civilwarphotography.org/searching-for-photos-of-civil-war-soldiers/ Navigate tocitation 1\r\nA CIVIL WAR PHOTO SLEUTH: DESIGN CHALLENGES AND OPPORTUNITIES\r\n \r\nHere, we provide details of three key challenges posed by Civil War Photo Sleuth that might have contributed towards misidentification and subsequently, inaccurate assessments, and how they present design opportunities for Photo Steward.\r\nA.1 Conflating facial similarity with photo identification\r\nA.1.1 Challenges: Mohanty et al. conducted a benchmarking study of CWPS [44], and found the face recognition algorithm to be of low precision; i.e., it retrieved hundreds of search results which may look similar to the query photo but are actually different people (false positives). Low precision increases the chances that users will interact with a lot of false positives. In such cases, one may need to garner additional information (i.e., comparing biographical information) before making a decision. However, it is plausible that automation bias may play a role in non-expert users making a match solely based on facial similarity [47]. As a result, there is strong potential for misidentification (see Figure 8).\r\nA.1.2 Current Workflow: CWPS's compare interface allows users to closely inspect the search results for a potential match, but does not make any distinction between facial similarity comparison and photo identification. Both are conflated into a one-step process, with one \"Identify\" button for the users to make their decisions (see Figure 2-C). Yet, users may want to indicate agreement with just the facial similarity (i.e., query photo and the search result showing the same person) but not the identity (i.e., name and biographical information), or vice versa.\r\nA.1.3 Design Goal 1: To support accurate investigation of photo identifications, users should be able to deliberate on the different aspects of the decision-making process. Providing users with a decision-making workflow that decouples facial similarity comparison from the overall photo identification task would allow them to focus on these tasks separately, while discouraging them from making decisions solely on the basis of facial similarity.\r\n\r\nFigure 8: An example of a misidentified photo on CWPS. This photo was identified as John C Whiteside based on facial similarity with the top search result: an identified reference photo of John C Whiteside. However, other visual clues, such as the photographer studio, do not align with Whiteside's biographical information.\r\n\r\nA.2 Lacking support for fine-grained, deliberative decision making\r\nA.2.1 Challenges: Historical photo identification is a complex task, where experts often corroborate multiple pieces of evidence, including facial similarity comparison, before reaching a decision about the identity of the photo [33]. While confirming an identity, experts may be highly confident if the source, military records, uniform clues, and additional photos of the same person all line up, or slightly confident if they need additional evidence. Conflicting evidence pieces may also affect their confidence levels. Similarly, they may have different degrees of certainty while ruling out an identity for a photo. A lack of support for expressing and displaying granularity in these photo identification decisions can lead to varying degrees of uncertainty being captured and misinterpreted as a confirmation, and eventually propagating misidentifications. Further, while it is safe to assume the vast majority of the Civil War photography community care about the accuracy of the photo IDs, there exists a small risk of financially-driven misidentifications since certain types of identified photos are considered to be more valuable [9].\r\nA.2.2 Current Workflow: The compare interface's \"Identify\" button (see Figure 2-C) is the only mechanism available on the platform for users to provide (a unary) input on photo identifications. CWPS users currently cannot provide fine-grained feedback on a given photo's identity, either at the time of identifying or afterwards on the photo page.\r\nA.2.3 Design Goal 2 : Users should be able to express how certain or uncertain they are about a photo identification. Interventions for allowing users to provide structured feedback can not only facilitate more accurate, fine-grained responses, but also encourage deliberation on the available evidence before making a decision, borrowing inspiration from other social computing systems that support users reflecting and deliberating on available information [28, 29].\r\nA.3 Limited validation opportunities for the community\r\nA.3.1 Challenges: For humans, deciding whether two photos show the same person is a highly subjective experience. A study by Mohanty et al. [41] showed that participants in a crowdsourcing study often disagree with each other and with facial recognition results in person identification tasks. The same study showed that people often seek a second opinion from peers for validating photo identifications. Without validation, original crowdsourced historical research may result in unresolved cases of conflicting IDs proposed for the same face, incorrect IDs remaining unchecked, or airtight IDs not being confirmed.\r\nA.3.2 Current Workflow: Once a photo has been identified, CWPS does not offer other users the option to express their opinions on whether two photos show the same person or not, or if the photo has been identified correctly or not (see Figure 8). The photo page does not indicate if (or why) the photos are linked to each other, how they are similar, or which user(s) considered them to be similar. Further, it does not indicate the role of facial recognition in linking them together.\r\nA.3.3 Design Goal 3 : To support accurate original historical research, the platform should encourage information stewardship from the community by allowing members to give their opinions on existing photo identifications [72]. Further, the platform should also be transparent about the role of community stewardship and facial recognition for a given photo identification, which can then act as evidence for aiding subsequent validation efforts by other users, thus supporting a form of stigmergic collaboration [13, 14].\r\nB DETAILS FOR LOG ANALYSIS\r\n \r\nTo understand the community's stewardship behaviors, we examined website logs for all user activities for one year since we launched the new features (December 2020 – December 2021). During this period, we observed 5843 voting instances on 5672 photos for 5355 unique IDs, where a user voting on whether Photo N can be identified as a Person M or not is considered to be one voting instance. For a given voting instance, we analyzed 1) any associated facial similarity comparisons to understand how they deliberated on their final voting decision, and 2) the user's confidence vote plus any justification notes left by the user to understand their decision rationale. Of the 5672 query photos, 4297 photos (4377 voting instances) did not have any facial similarity comparisons — only the user's confidence on the ID (plus any notes) was captured.\r\nFor the remaining voting instances which had at least one associated facial similarity comparison (i.e., users deliberate through the two-step validation process), we broke them down into pre-identified (i.e., the user knew the ID of the photo ahead of time and therefore, might be looking for a specific target on the search results page) and post-identified (i.e., the user did not know the ID of the photo and therefore, may not have a specific target while analyzing the search results) cases. As mentioned earlier in Section 4.2, users had the opportunity to validate IDs either on the search results page on the photo page. While a pre-identified voting instance almost certainly originated from the search results page, a post-identified case could be from either page. Since the logs did not give us the page origin of each vote directly, we triangulated from CWPS's timestamps to determine whether a given voting instance was for a pre-identified case or a post-identified one.\r\nAfter collating all the associated facial similarity comparisons for a given voting instance, we had 1064 pre-identified and 576 post-identified user deliberations to analyze. Each deliberation instance is a user's attempt to identify a query photo as a given target (person) ID, where they first compare facial similarity with all other photos that have been identified as the target ID, followed by the user's confidence on the query photo being the target ID. We analyzed the user responses to see whether the facial similarity comparisons had any impact on the user's confidence.\r\nUsers had provided comments in 682 (out of 5843, or 11.7%) voting instances. We coded these comments using an iterative, inductive approach, which resulted in five high-level themes, which can broken down into 13 sub-categories (see Appendix C).\r\nTo understand any stigmergic collaboration processes at play, we also analyzed the IDs which had multiple votes to check for agreements and disagreements between the voters. We further analyzed how the community's facial similarity comparisons compare against the facial recognition scores.\r\nC THEMES FOR NOTES ANALYSIS\r\nPhoto Comparisons\r\nHigh-Level Comparison: The note mentions \"replica\", \"facial similarity\", \"facial match\", \"identical\", \"visual comparison\", and other similar terms that describe comparisons with a prior identified photo.\r\nDescribing facial features: The note mention facial features like \"eyes\", \"hairline\", \"ears\", etc. to make comparisons\r\nWord-of-Mouth\r\nDescendant: The note either mentions that the user is a descendant of the person being identified, or they got the information from the family of the person.\r\nOwnership: The note either mentions that the user owns a printed version of the photo, compared with a photo in their collection, or they know the owner of the photo.\r\nFamiliarity/Self-Reported Research: The note mentions that the user has seen the photo somewhere, be it in a book, museum, etc.\r\nVisual Clues\r\nUniform: The note mentions visual clues that pertain to the uniform of the person (e.g., hat insignia, shoulder straps, etc.) The user may infer the possible service information (i.e., ranks, branches, regiments, etc.) from the uniform clues.\r\nInscription: The note mentions the presence of a period inscription on the photo (a highly trustworthy primary source for a person's ID), or an album case, or modern inscriptions such as books, which is generally the name of the person being identified. In some instances, the inscribed text may point to the person's service information.\r\nOther Visual Clues: The note mentions visual clues in the photo (e.g., backmarks, borders, etc.) beyond the person's face.\r\nExternal Information\r\nExternal URL: The note mentions an external URL, which supposedly has additional information about the photo's ID.\r\nOther sources (e.g., museum, website, book, etc.): The note mentions an external source (e.g., museum, book, etc.) that supposedly has evidence for the photo's ID, but no URLs are provided. Details about the source may or may not be available.\r\nLack of Information / Seeking Additional Evidence: The note mentions the lack of evidence or seeking additional evidence, be it about the source or the service information.\r\nProviding Additional Information\r\nBiographical Information: The notes mentions additional information about the person's service records, specific regiments, biographical information (name, year, location), etc.\r\nAdditional Context: The note mentions some additional context provided by the user to justify their decision, such as information about the photo collection, or pointing to someone else's research, or some historical context, or incorrect evidence, or if the person is prominent.\r\nD PARTICIPANT DETAILS\r\nUndergraduate and master's students concentrating in history who use Civil War photos for their coursework and research projects, but are not (yet) employed in a professional capacity as historians. We recruited five students via recommendations from our university's history department. None of the students had used CWPS before, or were known to the authors prior to the study. Three students were men and two were women, and all were in the \"18 to 30\" age group. We anonymize them with identifiers S1–S5.\r\nExperienced users of Civil War Photo Sleuth who have added over 50 photos each and have substantial knowledge of Civil War history, but are not professional historians. We recruited five amateur experts from the CWPS contact list. All five users were men, and they were distributed across different age groups (two in \"18 to 30\", two in \"31 to 40\", and one in \"51 to 60\"). We anonymize them with identifiers C1–C5. C1 and C3 are among the most active daily users on CWPS. Only two of the five had used Photo Steward before.\r\nExpert historians with a graduate degree in history, specializing in American Civil War history, but with little or no previous experience with CWPS. We recruited five historians via recommendations from our university's history department. Three historians were men and two were women. They were distributed across different age groups (two in \"18 to 30\", two in \"31 to 40\", and one in \"51 to 60\"). We anonymize them with identifiers H1–H5. None of them had used Photo Steward before.\r\nE LAB STUDY QUESTIONS\r\nE.1 Semi-Structured Questions\r\nIs there a way that you would like to capture your thoughts on this ID and share them with others, if possible? If so, what would that look like? If not, can you explain why not?\r\nWhat did you think about the community opinions?\r\nWhat do you think about the ID quality visualization?\r\nWhat did you think about the 2-step process while agreeing/disagreeing on an identity?\r\nWhat is your overall opinion of both the interfaces?\r\nWhich interface would you prefer for validating the information? And why?\r\nWhat would you change or improve?\r\nE.2 Usability Survey\r\nQ1. The community's opinions about an identity were clear and easy to understand in the 2nd system. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ2. The community's opinions about an identity were useful for assessing the information. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ3. The process of voting on an identity was clear and easy to understand. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ4. Comparing other photos first and then voting on an identity helped me deliberate and make more accurate decisions. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nQ5. I was able to validate the information better using the 2nd system. (1 = Strongly Disagree to 5 = Strongly Agree)\r\nF SUMMARY OF CHANGES\r\nWe summarize in Table 6 about how Photo Steward differs from CWPS.\r\nTable 6: Summary of changes: Civil War Photo Sleuth (CWPS) with and without Photo Steward.\r\n\r\nCWPS\r\nCWPS + Photo Steward\r\nDecision-Making: How do users make identification decisions for the query photo when they see the similar-looking search results retrieved by facial recognition?\r\nA single-step comparison interface that allows the user to compare the query photo with one similar-looking photo and the associated biographical information of the target ID. The single-step workflow conflates the two tasks of facial similarity and person identification.\r\nA multi-step validation interface that allows the user to compare the query photo with all previously identified photos of a similar-looking target ID (search result). The two-step workflow decouples the two tasks of facial similarity and person identification, thus allowing the user to deliberate on the facial similarity comparison between the query photo and the target photo(s) before making a decision on the target identity.\r\nUsers make a unary input on the query photo's ID by clicking an \"Identify\" button in the comparison interface, which will link the target ID to the query photo. The comparison interface does not allow users to rule out candidates.\r\nUsers can provide fine-grained decisions for both facial similarity and person identification steps in the validation interface. In Step 1, the user compares the query photo and the target photo for facial similarity by selecting from the following options: No (Different Person), Not Sure, Yes (Facial Match), and Yes (Replica). In Step 2, users indicate how confident they are about the query photo's ID (as the proposed target ID) by selecting the following options: No (Highly Confident), No (Slightly Confident), Not Sure, Yes (Slightly Confident), and Yes (Highly Confident).\r\nAccess: What kind of validation opportunities are available for users?\r\nUsers can only access the comparison interface from the search results page if they search for similar-looking candidates using facial recognition. Once the photo has been identified, users cannot access the comparison interface on the photo page. The community cannot weigh in on an ID's reliability on the photo page.\r\nUsers can access the validation interface on both the search results page (while identifying the query photo) and the photo page (after the photo has been identified). The community can weigh in on the reliability of the proposed ID(s) and facial matches using the validation interface on the photo page, thus engaging in a form of stigmergic collaboration.\r\nEvidence: What kind of stewardship evidence is presented to the users?\r\nThe photo page displays the proposed ID(s) for the photo without any additional information about the ID's reliability. Similarly, other photos that have been matched to the query photo are also displayed without any reliability indicator. Since community opinions are not captured on CWPS, they are not displayed.\r\nThe photo page displays the proposed ID(s) for the photo along with stewardship visualizations of the a) community's confidence on the ID, and b) facial similarity comparison with other photos of the same ID by both the community and AI.\r\nFOOTNOTE\r\n1 www.civilwarphotosleuth.com\r\n\r\n\r\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs International 4.0 License.\r\nCI '23, November 06–09, 2023, Delft, Netherlands\r\n© 2023 Copyright held by the owner/author(s).\r\nACM ISBN 979-8-4007-0113-9/23/11.\r\nDOI: https://doi.org/10.1145/3582269.3615600\n",
    "totalFiles": 2
  }
]